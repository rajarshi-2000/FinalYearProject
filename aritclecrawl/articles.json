[
{"title": "Wrist posture affects hand and forearm muscle stress during tapping", "highlights": ["We quantified the effect of four wrist postures during tapping on resulting finger and wrist muscle stress (including both active and passive component).", "Neutral wrist posture was the optimal option among four tested wrist postures when all muscles were considered.", "Extensor muscles exhibited higher muscle stresses than flexors.", "Wrist extensors stress remained higher than 4.5\u00a0N/cm\u00b2 and wrist flexor stress remained below 0.5\u00a0N/cm\u00b2 during tapping."], "abstract": ["Non-neutral wrist posture is a risk factor of the musculoskeletal disorders among computer users. This study aimed to assess internal loads on hand and forearm musculature while tapping in different wrist postures. Ten healthy subjects tapped on a key switch using their index finger in four wrist postures: straight, ulnar deviated, flexed and extended. Torque at the finger and wrist joints were calculated from measured joint postures and fingertip force. Muscle stresses of the six finger muscles and four wrist muscles that balanced the calculated joint torques were estimated using a musculoskeletal model and optimization algorithm minimizing the squared sum of muscle stress. Non-neutral wrist postures resulted in greater muscle stresses than the neutral (straight) wrist posture, and the stress in the extensor muscles were greater than the flexors in all conditions. Wrist extensors stress remained higher than 4.5\u00a0N/cm\u00b2 and wrist flexor stress remained below 0.5\u00a0N/cm\u00b2 during tapping. The sustained high motor unit recruitment of extensors suggests a greater risk than other muscles especially in flexed wrist posture. This study demonstrated from the perspective of internal tissue loading the importance of maintaining neutral wrist posture during keying activities."]},
{"title": "Human interaction with policy flight simulators", "highlights": ["One of few human factors descriptions of modeling policy setting.", "Contribution to understanding of truly complex systems.", "Explanation of policy (flight) simulators."], "abstract": ["Policy flight simulators are designed for the purpose of exploring alternative management policies at levels ranging from individual organizations to national strategy. This article focuses on how such simulators are developed and on the nature of how people interact with these simulators. These interactions almost always involve groups of people rather than individuals, often with different stakeholders in conflict about priorities and courses of action. The ways in which these interactions are framed and conducted are discussed, as well as the nature of typical results."]},
{"title": "Designing sustainable work systems: The need for a systems approach", "highlights": ["Describing the understanding of sustainability on different levels (society, corporation, work system).", "Discussing the similarities and differences between sustainability and human factors.", "Delivering a first frame work for designing sustainable work systems.", "Discussing the understanding of systems ergonomics in relation to sustainability."], "abstract": ["There is a growing discussion concerning sustainability. While this discussion was at first mainly focused on a society level \u2013 and sometimes regarding especially environmental problems, one can now see that this topic is of increasing relevance for companies worldwide and even the social dimension of this three pillar approach is gaining more and more importance. This leads to some questions: Is sustainability already a part of human factors thinking or do we have to further develop our discipline? How can we define sustainable work systems? What are the topics we have to consider? Do we need a new systems ergonomics perspective regarding whole value creation chains and a life-cycle perspective concerning products (and work systems)? How can we deal with potential contradictions about social, ecological, and economic goals?"]},
{"title": "Fundamentals of systems ergonomics/human factors", "highlights": ["New definition of systems ergonomics/human factors.", "Identification of six fundamental components of systems ergonomics/human factors.", "Illustrations of ideas through real contemporary examples from rail systems."], "abstract": ["Ergonomics/human factors is, above anything else, a systems discipline and profession, applying a systems philosophy and systems approaches. Many things are labelled as system in today's world, and this paper specifies just what attributes and notions define ergonomics/human factors in systems terms. These are obviously a systems focus, but also concern for context, acknowledgement of interactions and complexity, a holistic approach, recognition of emergence and embedding of the professional effort involved within organization system. These six notions are illustrated with examples from a large body of work on rail human factors."]},
{"title": "The development of guidelines for the design and evaluation of\u00a0warning signs for young children", "highlights": ["We carried out an evaluation of a set of pilot guidelines for the design/evaluation of warning signs for young children.", "The evaluation involved interviews and focus groups with parents, teachers, human factors experts and other groups.", "The findings offered broad support for the guidelines and some revisions.", "The paper concludes with a set of recommendations for future work (e.g., the need for behavioral testing)."], "abstract": ["We report a study which aimed to provide further development and refinement of a set of guidelines (", ") for the design and evaluation of warning signs and other visual material for young children (i.e., aged 5\u201311 years). The study involved a set of semi-structured interviews and focus groups with the parents of young children, teachers, human factors experts and other groups (", "\u00a0=\u00a038). The findings from the study provided broad support for the guidelines, as well as highlighting a number of issues which need to be addressed. These included the need to consider the target audience in more detail and provide additional guidance covering possible tie-ins with safety campaigns, sign location, age differences, gender and children's special needs. Similar findings were obtained with regard to the evaluation guidelines and their coverage of methods and activities for testing signs (e.g., simulation, role playing). We discuss our findings within the context of a revised set of guidelines and a set of suggestions aimed at working towards a more comprehensive approach to the design/evaluation of signs for young children. The paper concludes with a set of future topics for research including a discussion of ways forward in terms of improving support for design and evaluation including behavioural testing with children, their parents and other care givers."]},
{"title": "Human factors/ergonomics as a systems discipline? \u201cThe human use of human beings\u201d revisited", "highlights": ["HFE is today facing work environments that did not exist at its inception.", "It is therefore necessary critically to assess the built-in assumptions and traditions.", "In today's work environments, effective solutions depend on coping rather than design.", "HFE should contribute to the ability to manage expected as well as unexpected work situations.", "HFE competence (methods and theories) should correspond to the world of today and tomorrow, rather than the world of yesterday."], "abstract": ["Discussions of the possible future of Human factors/ergonomics (HFE) usually take the past for granted in the sense that the future of HFE is assumed to be more of the same. This paper argues that the nature of work in the early 2010s is so different from the nature of work when HFE was formulated 60\u201370 years ago that a critical reassessment of the basis for HFE is needed. If HFE should be a systems discipline, it should be a soft systems rather than a hard systems discipline. It is not enough for HFE to seek to improve performance and well-being through systems design, since any change to the work environment in principle alters the very basis for the change. Instead HFE should try to anticipate how the nature of work will change so that it can both foresee what work ", " be and propose what work ", " be."]},
{"title": "Improving meat cutters' work: Changes and effects following an intervention", "highlights": ["We describe ergonomic changes implemented within meat cutting.", "Main changes were reducing knife work and job rotation.", "Results showed a significant reduction in perceived physical work load.", "In general, the participative changes were perceived positively.", "The company did not report important setbacks due to the changes."], "abstract": ["Meat cutters face higher risks of injury and musculoskeletal problems than most other occupational groups. The aims of this paper were to describe ergonomics changes implemented in three meat cutting plants and to evaluate effects related to ergonomics on the individual meat cutters and their work. Data was collected by interviews, observations, document studies and a questionnaire (", "\u00a0=\u00a0247), as a post intervention study. The changes implemented consisted of reducing knife work to a maximum of 6\u00a0h per day and introducing a job rotation scheme with work periods of equal length. Tasks other than traditional meat cutting were added. A competence development plan for each meat cutter and easy adjustment of workplace height were introduced. The questionnaire showed a reduction in perceived physical work load. In general, the changes were perceived positively. Figures\u00a0from the company showed a positive trend for injuries and sick leave."]},
{"title": "Management of personal safety risk for lever operation in mechanical railway signal boxes", "highlights": ["First published results on acceptable weights/forces for pulling long levers in rail mechanical signal boxes.", "Development of an injury risk management plan for health and safety in lever operation.", "Identification of lever pulling strategies and postures.", "Comparison of operator estimates of heavy (or difficult), medium and light lever weights/forces with actual weights.", "Comparison of measured lever weights and subjective estimates of risk with biomechanical modelling estimates."], "abstract": ["Despite increased implementation of computer control systems in managing and regulating rail networks, mechanical signal boxes using lever operation will be in place for years to come. A rolling risk assessment programme identified a number of levers in mechanical signal boxes within the UK rail network which potentially presented unacceptable personal safety risk to signallers. These levers operate both points and signals and the risk is primarily the weights which have to be moved when pulling and pushing the levers. Operating difficulties are often compounded by the design and condition of lever frames, the linkages to the points/signals, maintenance regimes, the workspace and the postures and strategies adopted by signallers. Lever weights were measured as from 15\u00a0kg to 180\u00a0kg at over 160 boxes, using a specially designed and constructed device. Taken together with examination of injury and sickness absence data, interviews and field observations, and biomechanical computer modelling, the measurement programme confirmed the potential risks. A risk management programme has been implemented, comprising lever weight measurement, training of operations staff, a structured maintenance regime and renewal or redesign for boxes/levers where, after maintenance, a criterion weight level is still exceeded. For a feasible management programme, the first alert (or 1st action) value for further assessment is 55\u00a0kg, a second action level requiring specified maintenance is 80\u201399\u00a0kg, and a third action level requiring the lever to be signed out of use is 100\u00a0kg."]},
{"title": "Effects of restrictive clothing on lumbar range of motion and trunk muscle activity in young adult worker manual material handling", "highlights": ["Wearing hip restrictive clothing may increase the potential for low-back discomfort.", "Comparisons were made of tight vs. fit pants during manual material handling tasks.", "Tight pants led to an increase in lumbar motion and reduced trunk muscle activity.", "Results provide a basis for guidance on clothing fit for manual material handling."], "abstract": ["The objective of this study was to examine the effect of wearing restrictive trousers on lumbar spine movement, trunk muscle activity and low back discomfort (LBD) in simulations of manual material handling (MMH) tasks. Twenty-eight young adults participated in the study performing box lifting, liquid container handling while squatting, and forward reaching while sitting on a task chair when wearing tight pants (sizes too small for the wearer) vs. fit pants (correct size according to anthropometry). Each task was repeated three times and video recordings were used as a basis for measuring lumbar range of motion (LRoM). The response was normalized in terms on baseline hip mobility. Trunk muscle activity of rectus abdominis (RA) and erector spinae (ES) muscles were also measured in each trial and normalized. At the close of each trial, participants rated LBD using a visual analog scale. Results revealed significant effects of both pants and task types on the normalized LRoM, trunk muscle activity and subjective ratings of LBD. The LRoM was higher and trunk muscle (ES) activity was lower for participants when wearing tight pants, as compared to fit pants. Discomfort ratings were significantly higher for tight pants than fit. These results provide guidance for recommendations on work clothing fit in specific types of MMH activities in order to reduce the potential of low-back pain among younger workers in industrial companies."]},
{"title": "Variation in work tasks in relation to pinch grip strength among middle-aged female dentists", "highlights": ["The relationship of task variation during dental work history with pinch grip strength among dentists was investigated.", "The dentists with the most hand-loading tasks were at an increased risk of low pinch grip strength.", "It is advisable among dentists to perform as diverse work tasks as possible to reduce the risk of decreased pinch grip strength."], "abstract": ["We aimed to investigate the relationship of task variation during dental work history with pinch grip strength among dentists.", "We measured pinch grip strength among 295 female Finnish dentists aged 45\u201363 years. Variation in dental work tasks during work history was empirically defined by cluster analysis. Three clusters of task variation emerged: low (most work time in restoration treatment/endodontics), moderate (about 50% in the former and 50% in prosthodontics/periodontics/surgery), and high (variable tasks including administrative duties). Hand radiographs were examined for the presence of OA in the wrist and each joint of the 1\u20133rd fingers. Information on hand-loading leisure-time activities, and joint pain was obtained by questionnaire. Glove size was used as a proxy for hand size. BMI (kg/m2) was based on measured weight and self-reported height.", "Dentists with low variation of work task history had an increased risk of low pinch grip strength in the right hand (OR 2.3, 95% CI 1.2\u20134.3), but not in the left (1.13, 0.62\u20132.08), compared to dentists with high task variation, independent of age, hand size, hand-loading leisure-time activities, BMI and symptomatic hand OA.", "The dentists with the most hand-loading tasks were at an increased risk of low pinch grip strength, independent of e.g. symptomatic hand OA. It is advisable among dentists to perform as diverse work tasks as possible to reduce the risk of decreased pinch grip strength."]},
{"title": "Acceptability of robotic manipulators in shared working environments through human-like redundancy resolution", "highlights": ["Resolution of the kinematic redundancy in robotic manipulators inspired by human movements.", "Experimental campaign on volunteers to gather physiological signals.", "Assessment of physiological comfort/discomfort as a function of the adopted redundancy resolution criterion.", "Human-like movements of the robot alleviate the stress during human-robot coexistence.", "Non human-like movements are responsible for a more stressful affective state of humans."], "abstract": ["Next generation robotic manipulators are expected to resemble a human-like behavior at kinematic level, in order to reach the same level of dexterity of humans in operations like assembly of small pieces. These manipulators are also expected to share the same working environments with humans without artificial barriers.", "In this work we conjecture that making robots not only kinematically similar but also able to move and act in the same way as humans do, might facilitate their social acceptance. For this the kinematic redundancy of such new generation manipulators can be exploited. An experimental campaign has been organized to assess the physiological comfort/discomfort perceived by humans working side-by-side with robots. For comparison, a human-like and two alternative redundancy resolution strategies have been implemented. The analysis confirmed the hypothesis that a human-like motion of the robot helps in facilitating social acceptance, by reducing the perceived stress by humans in human-robot coexistence."]},
{"title": "Extending systems ergonomics thinking to accommodate the socio-technical issues of Systems of Systems", "highlights": ["SoS differ from systems with subsystems, because of multiple ownership.", "This requires a different socio-technical approach to sustain them.", "\u2018Surprises\u2019 are endemic to the SoS and resilience is a key attribute.", "It is sensible to treat the SoS as a \u2018wicked problem\u2019, and use appropriate methods.", "Governance is therefore of primary importance as a feedback mechanism."], "abstract": ["Socio-technical issues for Systems of Systems (SoS) differ in several ways from those for systems, mainly because the individual systems that are components of the SoS are usually owned by different organisations, each responsible for the optimisation and operation of its own system. Consequently, management of the SoS is about negotiation and management of the interfaces. Because of issues of Intellectual Property Rights (IPRs), commercial confidence, and the like, there is seldom sufficient, timely information in circulation about the SoS. Surprises are endemic to SoS, and resilience is a fundamental requirement. This paper outlines the different characteristics of SoS compared to ordinary systems, discusses many of the socio-technical issues involved, and then outlines a generic approach to these issues, treating the SoS as a \u2018wicked problem\u2019. Endemic to this is the need for governance, which is discussed briefly. This is followed by a description of the evident gaps in knowledge about the functioning of SoS, and a listing of tool classes, the development of which would enable progress to be made more effectively. Finally, the paper discusses how the SoS approach might be the best way to entrain ICT to address global drivers, thus pointing to the importance of the SoS approach."]},
{"title": "Impact of police body armour and equipment on mobility", "highlights": ["Many police and service occupations are required to wear body armour.", "Little known about the effects of wearing body armour in a law enforcement context.", "Body armour and equipment increase mobility restriction and physiological cost.", "Implications for employee safety/effectiveness in specific and critical work scenarios.", "Facilitates policy changes in overall and specific training emphases or procedures."], "abstract": ["Body armour is used widely by law enforcement and other agencies but has received mixed reviews. This study examined the influence of stab resistant body armour (SRBA) and mandated accessories on physiological responses to, and the performance of, simulated mobility tasks. Fifty-two males (37\u00a0\u00b1\u00a09.2\u00a0yr, 180.7\u00a0\u00b1\u00a06.1\u00a0cm, 90.2\u00a0\u00b1\u00a011.6\u00a0kg, VO", " 50\u00a0\u00b1\u00a08.5\u00a0ml\u00a0kg", "\u00a0min", ", BMI 27.6\u00a0\u00b1\u00a03.1, mean\u00a0\u00b1\u00a0SD) completed a running VO", " test and task familiarisation. Two experimental sessions were completed (\u22654 days in between) in a randomised counterbalanced order, one while wearing SRBA and appointments (loaded) and one without additional load (unloaded). During each session participants performed five mobility tasks: a balance task, an acceleration task that simulated exiting a vehicle, chin-ups, a grappling task, and a manoeuvrability task. A 5-min treadmill run (zero-incline at 13\u00a0km\u00b7h", ", running start) was then completed. One min after the run the five mobility tasks were repeated.", "There was a significant decrease in performance during all tasks with loading (", "\u00a0<\u00a00.001). Participants were off-balance longer; slower to complete the acceleration, grapple and mobility tasks; completed fewer chin-ups; and had greater physiological cost (\u2191\u00a0%HR", ", \u2191\u00a0%VO", ", \u2191\u00a0RER) and perceptual effort (\u2191\u00a0RPE) during the 5-min run. Mean performance decreases ranged from 13 to 42% while loaded, with further decreases of 6\u201316% noted after the 5-min run. Unloaded task performance was no different between phases.", "Wearing SRBA and appointments significantly reduced mobility during key task elements and resulted in greater physiological effort. These findings could have consequences for optimal function in the working environment and therefore officer and public safety."]},
{"title": "The effects of sign design features on bicycle pictorial symbols for bicycling facility signs", "highlights": ["We examined two different designs (inanimate symbol vs. animate symbol) on bicycle pictorial symbols.", "The alternative designs (animate symbol) performed better in both the preference test and the glance legibility test.", "The alternative designs (animate symbol) showed better conceptual compatibility for bicycling facility signs.", "The alternative designs (animate symbol) were more legible than the current sign (inanimate symbol)."], "abstract": ["The inanimate bicycle symbol has long been used to indicate the animate activity of bicycling facility signs. In contrast, either the inanimate bicycle symbol or the animate bicycle symbol has been used interchangeably for the standard pavement symbols in bike lanes. This has led to confusion among pedestrians and cyclists alike. The purpose of this study was to examine two different designs (inanimate symbol vs. animate symbol) involved in the evaluation of perceived preference and glance legibility, and investigate sign design features on bicycle pictorial symbols. Thirty-five participants compared current bicycle signs (inanimate symbols) to alternative designs (animate symbols) in a controlled laboratory setting. The results indicated that the alternative designs (animate symbols) showed better performance in both preference and glance legibility tests. Conceptual compatibility, familiarity, and perceptual affordances were found to be important factors as well."]},
{"title": "Influence of snow shovel shaft configuration on lumbosacral biomechanics during a load-lifting task", "highlights": ["3D L5/S1 angular impulses were measured during a simulated snow shovelling task with two shovel designs.", "The two designs compared were a bent-shaft shovel and a straight-shaft shovel.", "L5/S1 extension angular impulses were lower in the bent-shaft condition.", "This occurred due to reduced extension moment duration, reduced extension moment magnitude and reduced upper body flexion."], "abstract": ["Lower-back injury from snow shovelling may be related to excessive joint loading. Bent-shaft snow shovels are commonly available for purchase; however, their influence on lower back-joint loading is currently not known. Therefore, the purpose of this study was to compare L5/S1 extension angular impulses between a bent-shaft and a standard straight-shaft snow shovel. Eight healthy subjects participated in this study. Each completed a simulated snow-lifting task in a biomechanics laboratory with each shovel design. A standard motion analysis procedure was used to determine L5/S1 angular impulses during each trial, as well as peak L5/S1 extension moments and peak upper body flexion angle. Paired-samples ", "-tests (", "\u00a0=\u00a00.05) were used to compare variables between shovel designs. Correlation was used to determine the relationship between peak flexion and peak moments. Results of this study show that the bent-shaft snow shovel reduced L5/S1 extension angular impulses by 16.5% (", "\u00a0=\u00a00.022), decreased peak moments by 11.8% (", "\u00a0=\u00a00.044), and peak flexion by 13.0% (", "\u00a0=\u00a00.002) compared to the straight-shaft shovel. Peak L5/S1 extension moment magnitude was correlated with peak upper body flexion angle (", "\u00a0=\u00a00.70). Based on these results, it is concluded that the bent-shaft snow shovel can likely reduce lower-back joint loading during snow shovelling, and thus may have a role in snow shovelling injury prevention."]},
{"title": "Survey of Korean pedestrians' natural preference for walking directions", "highlights": ["The majority of pedestrians preferred walking on the right side, regardless of the location.", "No significant differences were found in the preferred walking direction by age or gender.", "Significant discrepancies in the preferred walking direction depending on pedestrians'dominant hand and foot exist.", "Left-handed and footed pedestrians are more likely to the Left-hand Traffic rules, while right-handed and footed are on the right side.", "The dominant hand appears to have a greater impact on the pedestrian's preferred walking direction than the dominant foot."], "abstract": ["The primary objective of this study was to investigate the stereotypes of Koreans regarding preferred walking directions when encountering various public walking facilities, and to provide useful information to pedestrians and traffic policy legislators. To this end, this study was conducted in two phases. In the first phase, we conducted observational research on pedestrians' walking directions in ten different situations. In the second phase, six hundred Korean male and female subjects were selected to investigate the various statistics about their preferred walking directions and their employment characteristics in diverse walking facilities. The results showed that 59.3% abided by the Left-side Traffic rule while 40.7% abided by the Right-side rule. On the contrary, 73.7% of respondents showed preferences to the Right-side Traffic rule. Moreover, right-handed people showed strong tendencies to walk on the right side of the road and vice versa, hence suggesting that the direction people naturally prefer in walking should be a crucial determinant when regulating traffic policies."]},
{"title": "The effects of a convex rear-view mirror on ocular accommodative responses", "highlights": ["We examined the ocular accommodative responses while viewing an object in a convex rear-view mirror.", "Viewing an object in a convex mirror caused a false recognition of its position.", "It would cause large accommodative fluctuations and blurred vision for the user.", "The ocular accommodative responses should be considered as a new indicator of automotive safety."], "abstract": ["Convex mirrors are universally used as rear-view mirrors in automobiles. However, the ocular accommodative responses during the use of these mirrors have not yet been examined. This study investigated the effects of a convex mirror on the ocular accommodative systems. Seven young adults with normal visual functions were ordered to binocularly watch an object in a convex or plane mirror. The accommodative responses were measured with an infrared optometer. The average of the accommodation of all subjects while viewing the object in the convex mirror were significantly nearer than in the plane mirror, although all subjects perceived the position of the object in the convex mirror as being farther away. Moreover, the fluctuations of accommodation were significantly larger for the convex mirror. The convex mirror caused the 'false recognition of distance', which induced the large accommodative fluctuations and blurred vision. Manufactures should consider the ocular accommodative responses as a new indicator for increasing automotive safety."]},
{"title": "Postural stability when walking: Effect of the frequency and magnitude of lateral oscillatory motion", "highlights": ["When walking, lateral oscillations between 0.5 and 2\u00a0Hz cause postural instability.", "Postural instability cannot be predicted from the unweighted lateral acceleration.", "Postural instability can be predicted from the unweighted lateral velocity.", "Magnitudes of lateral vibration causing instability when walking are provided."], "abstract": ["While walking on an instrumented treadmill, 20 subjects were perturbed by lateral sinusoidal oscillations representative of those encountered in transport: frequencies in the range 0.5\u20132\u00a0Hz and accelerations in the range 0.1\u20132.0\u00a0ms", " r.m.s., corresponding to velocities in the range 0.032\u20130.16\u00a0ms", " r.m.s. Postural stability was assessed from the self-reported probability of losing balance (i.e., perceived risk of falling) and the movements of the centre of pressure beneath the feet. With the same acceleration at all frequencies, the velocities and displacements of the oscillatory perturbations were greater with the lower frequency oscillations, and these caused greater postural instability. With the same velocity at all frequencies, postural instability was almost independent of the frequency of oscillation. Movements of the centre of pressure show that subjects attempted to compensate for the perturbations by increasing their step width and increasing their step rate."]},
{"title": "Effects of child restraint system features on installation errors", "highlights": ["32 Volunteers installed 16 different child restraints using seatbelt or LATCH.", "LATCH connector type and adjustor type and the presence of belt lockoffs affected CRS installation tightness.", "Harness adjustor type affected rate of achieving a snug harness.", "Results indicate that feedback on correct installation would be helpful."], "abstract": ["This study examined how child restraint system (CRS) features contribute to CRS installation errors. Sixteen convertible CRS, selected to include a wide range of features, were used in volunteer testing with 32 subjects. Subjects were recruited based on their education level (high or low) and experience with installing CRS (none or experienced). Each subject was asked to perform four child restraint installations in the right-rear passenger seat of a 2006 Pontiac G6 sedan using a crash dummy as a child surrogate. Each subject installed two CRS forward-facing (FF), one with LATCH and one with the vehicle seatbelt, and two CRS rear-facing (RF), one with LATCH and one with the seatbelt. After each installation, the experimenter evaluated 42 factors for each installation, such as choice of belt routing path, tightness of installation, and harness snugness.", "Analyses used linear mixed models to identify CRS installation outcomes associated with CRS features. LATCH connector type, LATCH strap adjustor type, and the presence of belt lockoffs were associated with the tightness of the CRS installation. The type of harness shoulder height adjuster was associated with the rate of achieving a snug harness. Correct tether use was associated with the tether storage method. In general, subject assessments of the ease-of-use of CRS features were not highly correlated with the quality of their installation, suggesting a need for feedback with incorrect installations.", "The data from this study provide quantitative assessments of some CRS features that were associated with reductions in CRS installation errors. These results provide child restraint designers with design guidelines for developing easier-to-use products. Research on providing effective feedback during the child restraint installation process is recommended."]},
{"title": "Evaluation of multiple muscle loads through multi-objective optimization with prediction of subjective satisfaction level: Illustration by an application to handrail position for standing", "highlights": ["We proposed a simultaneous evaluation method for multiple muscle loads by applying the optimization methodology.", "A satisfaction level function was derived to reveal the relationship between the muscle load and the subjective evaluation.", "The proposed method was applied to the design problem of handrail position for standing.", "The proposed method can provide both objective and subjective information, and the method may be a great help to designers."], "abstract": ["Proposed here is an evaluation of multiple muscle loads and a procedure for determining optimum solutions to ergonomic design problems. The simultaneous muscle load evaluation is formulated as a multi-objective optimization problem, and optimum solutions are obtained for each participant. In addition, one optimum solution for all participants, which is defined as the compromise solution, is also obtained. Moreover, the proposed method provides both objective and subjective information to support the decision making of designers. The proposed method was applied to the problem of designing the handrail position for the sit-to-stand movement. The height and distance of the handrails were the design variables, and surface electromyograms of four muscles were measured. The optimization results suggest that the proposed evaluation represents the impressions of participants more completely than an independent use of muscle loads. In addition, the compromise solution is determined, and the benefits of the proposed method are examined."]},
{"title": "Potential of adjustable height carts in reducing the risk of low back injury in grocery stockers", "highlights": ["Adjustable height cart reduced the probability of LBD risk during stocking of larger items.", "Adjustable height cart reduced sagittal flexion but increased twist motion.", "Adjustable cart reduced the NIOSH lifting equation indicating a safe task.", "Targeted intervention was marginally effective due to impact at the cart and not the shelving."], "abstract": ["While the workers of the Wholesale and Retail Trade industrial sector suffer from musculoskeletal disorders at an alarming rate, there have been few investigative studies into potential effective interventions to reduce the ergonomic stress. The objective of the study was to determine whether a cart with an adjustable shelf could reduce awkward postures and motions while stocking products in a grocery store. Fifteen workers at a small grocery store in Puerto Rico completed stocking tasks with two types of carts: traditional and adjustable height cart or Ergo Cart. Trunk kinematics, LBD risk index, NIOSH lifting index, subjective ratings, and productivity indicators were collected during four typical stocking tasks. The Adjustable Ergo Cart reduced the sagittal trunk flexion by 7\u00b0 and velocity by about 5\u00b0/s but increased twisting by about 2\u00b0 and twist velocity by 4\u00b0/s as compared to the traditional cart. The LBD risk index was reduced by a small 2.4% in probability although greater reductions were found for larger items (e.g. bags of dog food and 2-L of Soda). The consensus among workers was that the adjustable cart would be easier to use. Overall, the study provides objective evidence that an ergonomically designed cart (e.g. adjustable height) has some potential to reduce sagittal trunk flexion, LBD risk index, and the NIOSH lift index. Overall, the results indicate that any intervention such as an adjustable cart can only have marginal effectiveness unless the entire systems perspective is considered."]},
{"title": "Evaluation of thermal and evaporative resistances in cricket helmets using a sweating manikin", "highlights": ["Cricket helmets impede the heat exchange between head surface and environment.", "The type and design of padding used may influence evaporative heat dissipation.", "Textile material and padding thickness influence the evaporative heat dissipations."], "abstract": ["The main objective of this study is to establish an approach for measuring the dry and evaporative heat dissipation cricket helmets. A range of cricket helmets has been tested using a sweating manikin within a controlled climatic chamber. The thermal manikin experiments were conducted in two stages, namely the (i) dry test and (ii) wet test. The ambient air temperature for the dry tests was controlled to \u223c23\u00a0\u00b0C, and the mean skin temperatures averaged \u223c35\u00a0\u00b0C. The thermal insulation value measured for the manikin with helmet ensemble ranged from 1.0 to 1.2 clo. The results showed that among the five cricket\u00a0helmets, the Masuri helmet offered slightly more thermal insulation while the Elite helmet offered the least. However, under the dry laboratory conditions and with minimal air movement (air velocity\u00a0=\u00a00.08\u00a0\u00b1\u00a00.01\u00a0ms", "), small differences exist between the thermal resistance values for the tested helmets. The wet tests were conducted in an isothermal condition, with an ambient and skin mean temperatures averaged \u223c35\u00a0\u00b0C, the evaporative resistance, ", ", varied between 36 and 60\u00a0m", "\u00a0Pa\u00a0W", ". These large variations in evaporative heat dissipation values are due to the presence of a thick layer of comfort lining in certain helmet designs. This finding suggests that the type and design of padding may influence the rate of evaporative heat dissipation from the head and face; hence the type of material and thickness of the padding is critical for the effectiveness of evaporative heat loss and comfort of the wearer. Issues for further investigations in field trials are discussed."]},
{"title": "A literature review on optimum and preferred joint angles in\u00a0automotive sitting posture", "highlights": ["Overview over the state of the art of optimum automotive sitting posture.", "We did a literature review in the field of automotive optimum and preferred joint angles.", "We compared the methods and the results of the actual available literature.", "We give recommendations for further research in this field."], "abstract": ["In this study, a survey of the scientific literature in the field of optimum and preferred human joint angles in automotive sitting posture was conducted by referring to thirty different sources published between 1940 and today. The strategy was to use only sources with numerical angle data in combination with keywords. The aim of the research was to detect commonly used joint angles in interior car design. The main analysis was on data measurement, usability and comparability of the different studies. In addition, the focus was on the reasons for the differently described results.", "It was found that there is still a lack of information in methodology and description of background. Due to these reasons published data is not always usable to design a modern ergonomic car environment. As a main result of our literature analysis we suggest undertaking further research in the field of biomechanics and ergonomics to work out scientific based and objectively determined \u201coptimum\u201d joint angles in automotive sitting position."]},
{"title": "Muscle fatigue based evaluation of bicycle design", "highlights": ["Evaluation of on-road bicycle design was performed using surface EMG on 12 male volunteers.", "Three types of bicycle design, i.e., rigid frame, suspension and sports were studied.", "Bicycles with suspension were found to have lesser rider muscle fatigue."], "abstract": ["Bicycling posture leads to considerable discomfort and a variety of chronic injuries. This necessitates a proper bicycle design to avoid injuries and thereby enhance rider comfort. The objective of this study was to investigate the muscle activity during cycling on three different bicycle designs, i.e., rigid frame (RF), suspension (SU) and sports (SP) using surface electromyography (sEMG). Twelve male volunteers participated in this study. sEMG signals were acquired bilaterally from extensor carpi radialis (ECR), trapezius medial (TM), latissimus dorsi medial (LDM) and erector spinae (ES), during 30\u00a0min of cycling on each bicycle and after cycling. Time domain (RMS) and frequency domain (MPF) parameters were extracted from acquired sEMG signals. From the sEMG study, it was found that the fatigue in right LDM and ES were significantly (", "\u00a0<\u00a00.05) higher in SP bicycle. This was corroborated by a psychophysical assessment based on RBG pain scale. The study also showed that there was a significantly lesser fatigue with the SU bicycle than the RF and SP bicycles."]},
{"title": "Effect of coating over the handle of a drill machine on vibration transmissibility", "highlights": ["Vibration levels at the surface of handle of drill machine were achieved using coating instead of gloves.", "Commonly available coatings of low cost.", "Measuring the rms vibrations at surface of handle and wrist.", "Vibration transmissibility was calculated for low cost coating materials."], "abstract": ["This study was to see the effect of different coatings on the handle of hand-held drilling machines. Out of five different handles chosen for this study, including one handle uncoated. Root mean square (rms) values of the vibration levels (acceleration) were recorded at the surface of handle and wrist of the operators. Results showed that maximum vibrations were reduced by coating of handle coated with rubber sheet and Rexene (H4) followed by handle coated with cotton sandwiched between jeans cloth (H5). Equivalent vibrations transmitted through coating of handles coated with sponge and velvet (H2) and jute and cotton (H3) were of almost same magnitude and these two coated handles were able to reduce least vibration transmitted. Transmissibility of vibrations along dominant (", ") direction was analyzed using ANOVA. Results showed that coating on handles significantly affected vibration transmitted in ", " direction. Vibration transmissibility ratios were found to be 0.354, 0.571, 0.408, 0.4326, and 0.3555 for handles H1, H2, H3, H4 and H5 respectively."]},
{"title": "Perceived discomfort functions based on joint moment for various joint motion directions of the upper limb", "highlights": ["We formulate the relationship between perceived discomfort and joint moment ratio for the upper limb.", "The relationships for twelve joint motion directions are formulated by considering between-subject variability.", "The L-R fuzzy number was applied to deal with the variability of perceived discomfort among subjects.", "We compare three approximation models (linear, exponential, and logistic function models) in terms of their accuracy.", "The logistic function model provides the best fit to the history of perceived discomfort among the three models."], "abstract": ["The aim of the present study was to formulate the relationship between the perceived discomfort and the joint moment ratio for twelve joint motion directions of the upper limb by considering the between-subject variability, and to investigate the effect of joint motion direction. Three approximation models (i.e., linear, exponential, and logistic function models) were compared in terms of the accuracy of predicting the perceived discomfort, and the logistic function was selected because its average error was lowest. The concept of L-R fuzzy number was used to consider the individual variability of perceived discomfort, and a simplified distribution of perceived discomfort was represented. Cluster analysis showed that the twelve discomfort functions formed two clusters: one for elbow flexion and a second for the remaining joint motions. The data show that elbow flexion is more sensitive than other joint motions to increases in the joint moment ratio."]},
{"title": "The effects of prototype medium on usability testing", "highlights": ["Interface medium has no effect on users' perception of usability.", "The impact of medium on usability issues detected varies by application tested.", "Usability testing can begin with low fidelity models."], "abstract": ["Inconsistencies among testing methods and results in previous research prompted this study that builds upon a systematic usability testing research framework to better understand how interface medium influences users' abilities to detect usability flaws in applications. Interface medium was tested to identify its effects on users' perceptions of usability and abilities to detect usability problems and severe usability problems. Results indicated that medium has no effect on users' abilities to detect usability problems or perceptions of usability. However, results did indicate an interaction between the medium and the tested application in which users were able to identify significantly more usability problems on a higher fidelity medium using a particular application. Results also indicated that as users' perceptions of an application's usability increases, the users are less able to detect usability problems in that application. Usability testing should begin early in the design process, even if low fidelity mediums will be used."]},
{"title": "Evaluating ecommerce websites cognitive efficiency: An integrative framework based on data envelopment analysis", "highlights": ["The website cognitive efficiency is used as a measure of website performance.", "A framework based on cognition theories is proposed to model website performance.", "DEA is used to compare ecommerce websites.", "Navigation ambiguity and website usefulness affect website cognitive efficiency."], "abstract": ["This paper presents an integrative framework to evaluate ecommerce website efficiency from the user viewpoint using Data Envelopment Analysis (DEA). This framework is inspired by concepts driven from theories of information processing and cognition and considers the website efficiency as a measure of its quality and performance. When the users interact with the website interfaces to perform a task, they are involved in a cognitive effort, sustaining a cognitive cost to search, interpret and process information, and experiencing either a sense of satisfaction or dissatisfaction for that. The amount of ambiguity and uncertainty, and the search (over-)time during navigation that they perceive determine the effort size \u2013 and, as a consequence, the cognitive cost amount \u2013 they have to bear to perform their task. On the contrary, task performing and result achievement provide the users with cognitive benefits, making interaction with the website potentially attractive, satisfying, and useful. In total, 9 variables are measured, classified in a set of 3 website macro-dimensions (user experience, site navigability and structure). The framework is implemented to compare 52 ecommerce websites that sell products in the information technology and media market. A stepwise regression is performed to assess the influence of cognitive costs and benefits that mostly affect website efficiency."]},
{"title": "Detection of vigilance performance using eye blinks", "highlights": ["We correlate performance with eye metrics and cerebral blood flow velocities.", "Blink frequency and duration increase and performance decreases.", "Blink metrics increase as right cerebral blood flow velocities decrease.", "Blink metrics may be a more robust indicator of performance than blood flow."], "abstract": ["Research has shown that sustained attention or vigilance declines over time on task. Sustained attention is necessary in many environments such as air traffic controllers, cyber operators, and imagery analysts. A lapse of attention in any one of these environments can have harmful consequences. The purpose of this study was to determine if eye blink metrics from an eye-tracker are related to changes in vigilance performance and cerebral blood flow velocities. Nineteen participants performed a vigilance task while wearing an eye-tracker on four separate days. Blink frequency and duration changed significantly over time during the task. Both blink frequency and duration increased as performance declined and right cerebral blood flow velocity declined. These results suggest that eye blink information may be an indicator of arousal levels. Using an eye-tracker to detect changes in eye blinks in an operational environment would allow preventative measures to be implemented, perhaps by providing perceptual warning signals or augmenting human cognition through non-invasive brain stimulation techniques."]},
{"title": "Customization of user interfaces to reduce errors and enhance user acceptance", "highlights": ["Reconfiguration allows customizing a user interface according to own preferences.", "Reconfiguration reduced error rates and enhanced user acceptance.", "Promising approach that warrants further exploration."], "abstract": ["Customization is assumed to reduce error and increase user acceptance in the human\u2013machine relation. Reconfiguration gives the operator the option to customize a user interface according to his or her own preferences. An experimental study with 72 computer science students using a simulated process control task was conducted. The reconfiguration group (RG) interactively reconfigured their user interfaces and used the reconfigured user interface in the subsequent test whereas the control group (CG) used a default user interface. Results showed significantly lower error rates and higher acceptance of the RG compared to the CG while there were no significant differences between the groups regarding situation awareness and mental workload. Reconfiguration seems to be promising and therefore warrants further exploration."]},
{"title": "Human factors systems approach to healthcare quality and patient safety", "highlights": ["The SEIPS model of work system and patient safety is a useful systems approach to healthcare quality and patient safety.", "The SEIPS model can be used for research and improvement activities for improving healthcare quality and patient safety.", "Balancing the work system is a key principle to improve healthcare quality and patient safety."], "abstract": ["Human factors systems approaches are critical for improving healthcare quality and patient safety. The SEIPS (Systems Engineering Initiative for Patient Safety) model of work system and patient safety is a human factors systems approach that has been successfully applied in healthcare research and practice. Several research and practical applications of the SEIPS model are described. Important implications of the SEIPS model for healthcare system and process redesign are highlighted. Principles for redesigning healthcare systems using the SEIPS model are described. Balancing the work system and encouraging the active and adaptive role of workers are key principles for improving healthcare quality and patient safety."]},
{"title": "Ergonomic evaluation and comparison of wood harvesting systems in Northwest Russia", "highlights": ["A comparison of 14 applicable harvesting systems in Russia was assessed.", "Fully mechanized cut-to-length harvesting provides the best working conditions.", "The use of tree-length systems performed with cable skidders should be limited.", "The approach can be used to evaluate various wood-to-energy harvesting systems."], "abstract": ["A comparison of 14 currently applicable wood harvesting systems was assessed with respect to ergonomic point of view. For this purpose, the research method, based on the Hodges\u2013Lehmann rule and the integrated work-severity rate of single machinery, was developed for ergonomic evaluation of cut-to-length, tree-length and full-tree harvesting systems. Altogether, about 130 different parameters of 36 units of equipment that impact on the ergonomics and work conditions were measured and estimated in interviews undertaken directly at forestry harvesting workplaces in 15 logging companies in the Republic of Karelia, Northwest Russia. Then the results were compared to the effective norms, and the degree of compliance with the stipulated values was determined. The estimates obtained for the degree of compliance were combined. This permits a direct comparison of the workload on forestry harvesting workers such as operators, lumberjacks and choker setters. In many respects, the current ergonomic standard is standard, except for the operators of cable skidders, chainsaws and choker settings. Visibility and work postures were considered to be the most critical features influencing the operator's performance. Problems still exist, despite the extensive development of cabs. The best working conditions in terms of harvesting systems were provided by \u201charvester\u00a0+\u00a0forwarder\u201d in cut-to-length harvesting, and \u201cfeller\u2013buncher\u00a0+\u00a0grapple skidder\u201d in full-tree harvesting. The motor-manual tree-length harvesting performed with cable skidders showed the worst results in terms of ergonomics."]},
{"title": "Case studies of mental models in home heat control: Searching for feedback, valve, timer and switch theories", "highlights": ["An intergroup case study identified distinct mental models of home heating that explained users reported behaviour.", "Research into alternate control devices to the thermostat, may have greater present day relevance for reducing CO", ".", "Home heating needs to be considered at the system, rather than device, level incorporating goals and the device benefit."], "abstract": ["An intergroup case study was undertaken to determine if: 1) There exist distinct mental models of home heating function, that differ significantly from the actual functioning of UK heating systems; and 2) Mental models of thermostat function can be categorized according to ", " valve and feedback shared theories, and others from the literature. Distinct, inaccurate mental models of the heating system, as well as thermostat devices in isolation, were described. It was possible to categorise thermostat models by ", " feedback shared theory, but other theories proved ambiguous. Alternate control devices could be categorized by Timer (", ") and Switch (", ") theories. The need to consider the mental models of the heating system in terms of an integrated set of control devices, and to consider user's goals and expectations of the system benefit, was highlighted. The value of discovering shared theories, and understanding user mental models, of home heating, are discussed with reference to their present day relevance for reducing energy consumption."]},
{"title": "Evaluating the physical demands on firefighters using hand-carried stair descent devices to evacuate mobility-limited occupants from high-rise buildings", "highlights": ["Evacuating people with motor disabilities from high-rise buildings produces significant physical loading of the evacuators.", "Electromyography, heart rate, and ratings of perceived exertion differentiated 3 hand-carried devices and a manual carry.", "Devices with handles that extend in the front and allow the lead evacuator to face forward reduce physical demands."], "abstract": ["The physical demands on firefighting personnel were investigated when using different types of hand-carried stair descent devices designed for the emergency evacuation of high rise buildings as a function of staircase width and evacuation urgency. Twelve firefighters used three hand-carried stair descent devices during simulated urgent and non-urgent evacuations. The devices were evaluated under three staircase width conditions (0.91, 1.12, and 1.32\u00a0m). For comparison, an urgent manual carry was also performed on the 1.12\u00a0m wide stairs. Dependent measures included electromyographic (EMG) data, heart rates, Borg Scale ratings, task durations and descent velocities. Results indicated that the ", ", which allows the front person to descend the stairs facing forward, reduced the time integrated back muscle EMG by half and showed a descent velocity that was 1.8 times faster than the other stair descent devices in the study. There were no differences across staircase widths."]},
{"title": "Impact of experience when using the Rapid Upper Limb Assessment to assess postural risk in children using information and communication technologies", "highlights": ["RULA was used to assess a child's postural risk during mobile ICT use.", "Participants were either occupational therapy students or experienced practitioners.", "No significant differences in RULA outcome scores between groups were found.", "No conclusive differences in visual search strategies between groups were found.", "RULA can be used to assess children's postural risk, regardless of experience."], "abstract": ["The Rapid Upper Limb Assessment (RULA) is an observation-based screening tool that has been used to assess postural risks of children in school settings. Studies using eye-tracking technology suggest that visual search strategies are influenced by experience in the task performed. This study investigated if experience in postural risk assessments contributed to differences in outcome scores on the RULA and the visual search strategies utilized. While wearing an eye-tracker, 16 student occupational therapists and 16 experienced occupational therapists used the RULA to assess 11 video scenarios of a child using different mobile information and communication technologies (ICT) in the home environment. No significant differences in RULA outcome scores, and no conclusive differences in visual search strategies between groups were found. RULA can be used as a screening tool for postural risks following a short training session regardless of the assessor's experience in postural risk assessments."]},
{"title": "The effects of simulated fog and motion on simulator sickness in a driving simulator and the duration of after-effects", "highlights": ["We checked how the simulator test conditions affect the simulator sickness.", "The sickness symptoms persisted at the highest level for the mobile platform.", "The simulator sickness symptoms varied depending on the time."], "abstract": ["In the study, we checked: 1) how the simulator test conditions affect the severity of simulator sickness symptoms; 2) how the severity of simulator sickness symptoms changes over time; and 3) whether the conditions of the simulator test affect the severity of these symptoms in different ways, depending on the time that has elapsed since the performance of the task in the simulator.", "We studied 12 men aged 24\u201333 years (", "\u00a0=\u00a028.8, SD\u00a0=\u00a03.26) using a truck simulator. The SSQ questionnaire was used to assess the severity of the symptoms of simulator sickness. Each of the subjects performed three 30-minute tasks running along the same route in a driving simulator. Each of these tasks was carried out in a different simulator configuration: A) fixed base platform with poor visibility; B) fixed base platform with good visibility; and C) motion base platform with good visibility. The measurement of the severity of the simulator sickness symptoms took place in five consecutive intervals.", "The results of the analysis showed that the simulator test conditions affect in different ways the severity of the simulator sickness symptoms, depending on the time which has elapsed since performing the task on the simulator. The simulator sickness symptoms persisted at the highest level for the test conditions involving the motion base platform. Also, when performing the tasks on the motion base platform, the severity of the simulator sickness symptoms varied depending on the time that had elapsed since performing the task. Specifically, the addition of motion to the simulation increased the oculomotor and disorientation symptoms reported as well as the duration of the after-effects."]},
{"title": "The effect of touch-key size on the usability of In-Vehicle Information Systems and driving safety during simulated driving", "highlights": ["The effect of touch-key sizes on the usability of IVIS's and driving safety was experimentally investigated.", "Both the measurements for usability of IVIS\u2019s and driving safety increased with increased touch-key sizes.", "However, the measurements for usability and driving safety reached an asymptote beyond certain touch-key sizes.", "The appropriated touch-key size considering both the usability and the safety was derived.", "Fitts\u2019 law analysis was performed under dual-task (pointing task while simulated driving)."], "abstract": ["Investigating the effect of touch-key size on usability of In-Vehicle Information Systems (IVISs) is one of the most important research issues since it is closely related to safety issues besides its usability. This study investigated the effects of the touch-key size of IVISs with respect to safety issues (the standard deviation of lane position, the speed variation, the total glance time, the mean glance time, the mean time between glances, and the mean number of glances) and the usability of IVISs (the task completion time, error rate, subjective preference, and NASA-TLX) through a driving simulation. A total of 30 drivers participated in the task of entering 5-digit numbers with various touch-key sizes while performing simulated driving. The size of the touch-key was 7.5\u00a0mm, 12.5\u00a0mm, 17.5\u00a0mm, 22.5\u00a0mm and 27.5\u00a0mm, and the speed of driving was set to 0\u00a0km/h (stationary state), 50\u00a0km/h and 100\u00a0km/h. As a result, both the driving safety and the usability of the IVISs increased as the touch-key size increased up to a certain size (17.5\u00a0mm in this study), at which they reached asymptotes. We performed Fitts' law analysis of our data, and this revealed that the data from the dual task experiment did not follow Fitts' law."]},
{"title": "Determining optimum flash patterns for emergency service vehicles: An experimental investigation using high definition film", "highlights": ["The 4\u00a0Hz flash rate was perceived to convey greater urgency than 1\u00a0Hz rate.", "The 1\u00a0Hz single pulse pattern was ranked least urgent of all the combinations.", "Estimates of minimum space acceptable for \u2018pulling out\u2019 reflected urgency measures.", "Less risky gap acceptance judgements were evident for the 4\u00a0Hz single pulse combination.", "HD film was found to be a low-risk, valid method for assessing EVL efficacy."], "abstract": ["An investigation of how emergency vehicle lighting (EVL) can be improved is reported with reference to an analysis of police vehicle road traffic accidents (Study 1). In Study 2, 37 regular drivers were shown film clips of a marked police vehicle, in which flash rate (1\u00a0Hz, 4\u00a0Hz) and pattern (single, triple pulse) were varied on the blue Light Emitting Diode (LED) roofbar. Results indicate a 4\u00a0Hz flash rate conveys greater urgency than a 1\u00a0Hz rate, while a 1\u00a0Hz, single flash combination was ranked the least urgent of all combinations. Participants claimed they would leave significantly more space before pulling out in front of an approaching police car (gap acceptance) in the 4\u00a0Hz single pulse condition in comparison to other EVL combinations. The preliminary implications for which flash characteristics could prove most optimal for emergency service use are discussed with regard to effects on driver perception and expected driving behaviour."]},
{"title": "Integration of human factors and ergonomics during medical device design and development: It's all about communication", "highlights": ["The aim was to understand the constraints under which medical device design and development take place.", "A thematic analysis was conducted based on 19 semi-structured interviews.", "Barriers to designing for safety and usability were identified. They included deficits in communication.", "The barriers to communication were both internal and external to the development process.", "We recommend the use of mediating representations such as personas and scenarios to address barriers to communication."], "abstract": ["Manufacturers of interactive medical devices, such as infusion pumps, need to ensure that devices minimise the risk of unintended harm during use. However, development teams face challenges in incorporating Human Factors. The aim of the research reported here was to better understand the constraints under which medical device design and development take place. We report the results of a qualitative study based on 19 semi-structured interviews with professionals involved in the design, development and deployment of interactive medical devices. A thematic analysis was conducted. Multiple barriers to designing for safety and usability were identified. In particular, we identified barriers to communication both between the development organisation and the intended users and between different teams within the development organisation. We propose the use of mediating representations. Artefacts such as personas and scenarios, known to provide integration across multiple perspectives, are an essential component of designing for safety and usability."]},
{"title": "The impact of IT over five decades \u2013 Towards the Ambient Organization", "highlights": ["Socio-technical research of IT impact over five decades shows major evolution over time.", "Dramatic lower transaction costs due to IT will make organizations source from outside.", "Five different models of sourcing are described.", "This leads to what is called Ambient Organizations."], "abstract": ["This contribution to the Ken D. Eason special issue is an illustration of the value of socio-technical analysis applied at an organizational level. We provide a brief historical overview of socio-technical IS research and review studies investigating the impact of IT on organizational structures in the last five decades, identifying a dominating (new) research theme in each decade. A key overall impact of IT in all decades has been a dramatic decrease in transaction costs making it increasingly easier for organizations to source from external providers. A five level taxonomy of sourcing arrangement is developed together with a framework of organizational activities, and a number of significant cases are offered of how organizations are sourcing practically all types of business processes, including innovation. We argue that future IT will further accelerate the movement towards more sourcing, eventually leading to a new type of organization that we call the Ambient organization."]},
{"title": "Facilitating pictorial comprehension with color highlighting", "highlights": ["Two experiments tested whether highlighting benefits warning comprehension.", "Conditions of warning pictorials were relevant, less relevant, and no highlighting.", "Relevant highlighting increased comprehension.", "Less relevant highlighting decreased comprehension.", "Appropriate highlighting could prove beneficial to the design of complex pictorials."], "abstract": ["Pictorials can aid in communicating warning information, but viewers may not always correctly comprehend them. Two experiments focused on whether the use of relevant highlighting could benefit pictorial comprehension. A set of warning-related pictorials were manipulated according to three-color highlighting conditions: highlighting areas more relevant to correct comprehension, highlighting areas less relevant to comprehension, and no highlighting. Participants were asked to describe the purpose and meaning of each pictorial presented to them. The findings from both experiments indicate that comprehension of warning pictorials is higher for the relevant highlighting condition than the other two conditions. The highlighting of less relevant areas reduced comprehension compared to no highlighting. Use of appropriately placed highlighting could benefit the design of a complex symbol by pointing out pertinent areas to aid in determining its intended conceptual meaning."]},
{"title": "Effect of base layer materials on physiological and perceptual responses to exercise in personal protective equipment", "highlights": ["We investigated the effect of different base layers under personal protective clothing.", "We used physiological (alternating work/recovery cycles) and materials testing.", "Base layer material had no effect on measures of physiological and perceptual strain.", "Significant differences among clothing ensembles were found in materials testing.", "Differences in total heat loss (material test) did not impact physiological responses."], "abstract": ["Ten men (non-firefighters) completed a 110\u00a0min walking/recovery protocol (three 20-min exercise bouts, with recovery periods of 10, 20, and 20\u00a0min following successive bouts) in a thermoneutral laboratory while wearing firefighting personal protective equipment over one of four base layers: cotton, modacrylic, wool, and phase change material. There were no significant differences in changes in heart rate, core temperature, rating of perceived exertion, thermal discomfort, and thermal strain among base layers. Sticking to skin, coolness/hotness, and clothing humidity sensation were more favorable (", "\u00a0<\u00a00.05) for wool compared with cotton; no significant differences were identified for the other 7 clothing sensations assessed. Separate materials performance testing of the individual base layers and firefighting ensembles (base layer\u00a0+\u00a0turnout gear) indicated differences in thermal protective performance and total heat loss among the base layers and among ensembles; however, differences in heat dissipation did not correspond with physiological responses during exercise or recovery."]},
{"title": "Effects of slanted ergonomic mice on task performance and subjective responses", "highlights": ["This paper compares the performance of slanted mice with a conventional mouse.", "Task completion times of the slanted mice are longer than the conventional mouse.", "Subjective discomforts of the slanted mice are severe than the conventional mouse.", "The slanted mice have disadvantages in task performance and subjective responses."], "abstract": ["The biomechanical benefits (e.g., muscular activity) of slanted ergonomic mice have been comprehensively identified; however, their effects on task performance and subjective responses have not been fully investigated. The present study examined the effects of two slanted mice (slant angle\u00a0=\u00a030\u00b0 and 50\u00b0) in comparison with a conventional mouse (slant angle\u00a0=\u00a00\u00b0) in terms of task performance (task completion time and error rate) and subjective responses (perceived discomfort score and overall satisfaction score). Experimental results showed that all of the task and subjective measures worsened as the slant angle of the target mice increases. For example, the task completion time (unit: ms) and overall satisfaction score (unit: point) of the 30\u00b0 slanted mouse (time\u00a0=\u00a00.71, satisfaction\u00a0=\u00a0\u22120.09) and 50\u00b0 slanted mouse (time\u00a0=\u00a00.73, satisfaction\u00a0=\u00a0\u22120.79) significantly deteriorated than the conventional mouse (time\u00a0=\u00a00.65, satisfaction\u00a0=\u00a01.21). The slanted mice seem to compromise biomechanical benefits with task performance and subjective responses."]},
{"title": "Effects of the resting time associated with the number of trials on the total and individual finger forces in a maximum grasping task", "highlights": ["When resting time was decreased, % reductions of the total/finger grip strength tended to increase.", "Grip strength reductions exceeded 5% in the 4th trial at all resting time (30", "\u00a0", "s\u20133", "\u00a0", "min).", "Grip strength reductions exceeded 15% in the 20th trial at 30", "\u00a0", "s and 1", "\u00a0", "min resting time.", "A guideline for resting time based on the trial (up to 20th trials) and acceptable grip strength reductions was introduced."], "abstract": ["The repetitive and excessive workload accompanying grip strength- or hand-intensive tasks are often considered to be common causes of work-related upper limb musculoskeletal disorders. For this reason, numerous experimental studies have been performed on maximum grip strength. However, due to an absence of standard guidelines, researchers have adopted different resting times and number\u00a0of trials suited for their particular research purposes. The effects of resting time and the number of trials on the maximum total grip strength and individual finger forces of 24 participants over 20 trials were investigated. Results showed that the total grip strength and individual finger strengths differed significantly according to the resting time and the number of trials (", "\u00a0", "<", "\u00a0", "0.05). Overall, grip strength tended to increase with a reduction in resting time (% reduction: 7.8%, 9.1%, 11.1%, and 13.0% for 3", "\u00a0", "min, 2", "\u00a0", "min, 1", "\u00a0", "min, and 30", "\u00a0", "s resting time, respectively) as well as with an increase in the number of trials (% reduction: 8%, 10%, 13%, and 16% for 5th, 10th, 15th, and 20th trials). The effects of resting time and the number of trials also showed statistically significant effects on individual finger forces. Regression equations of total grip strength and finger forces with resting time and number of trials were established. These equations were then applied to formulate guidelines for appropriate resting times in experiments based on the number of trials and acceptable reductions in grip strength. Data from this and future studies regarding decreasing grip strength and the contribution of each finger are expected to form the groundwork for ergonomic hand tool design and development."]},
{"title": "Neck, shoulder and low back pain in secondary schoolchildren in relation to schoolbag carriage: Should the recommended weight limits be gender-specific?", "highlights": ["Musculoskeletal complaints were common among schoolchildren, with girls being more likely to report symptoms than boys.", "The relative schoolbag weight (% body weight) was associated with neck and shoulder complaints.", "The type of schoolbag was associated with low back complaints.", "Adolescents with neck pain carried their schoolbags for a longer period of time each day.", "Gender differences may need to be considered when setting weight limits for schoolchildren."], "abstract": ["The occurrence of neck, shoulder and low back complaints in relation to schoolbag carriage and other\u00a0potential risk factors were investigated in a cross-sectional study of 586 Iranian schoolchildren aged 12\u201314 years. The average load carried by schoolchildren was 2.8\u00a0kg. Neck, shoulder and low back complaints during the preceding month were reported by 35.3%, 26.1% and 33% of the students, respectively. Gender was an independent factor predicting musculoskeletal symptoms in schoolchildren. Girls were more likely than boys to suffer from neck, shoulder and low back complaints, although there was no significant difference between genders in terms of schoolbag carriage variables. The findings suggest that the recommended weight limit for schoolbag carriage may need to differ between boys and girls. The associations between schoolbag variables and reported symptoms are also discussed. The results provide evidence that the current weight limit should consider a broader combination of factors that influence the use of schoolbags."]},
{"title": "Comparison of concepts in easy-to-use methods for MSD risk assessment", "highlights": ["A comparative analysis of easy-to-use methods assessing risk for developing MSDs.", "Actions that easy-to-use methods share to make an assessment.", "Differences at each stage of the assessment in easy-to-use methods.", "Guidelines for developing a method for assessing all work tasks and all body parts."], "abstract": ["This article presents a comparative analysis of easy-to-use methods for assessing musculoskeletal load and the risk for developing musculoskeletal disorders. In all such methods, assessment of load consists in defining input data, the procedure and the system of assessment. This article shows what assessment steps the methods have in common; it also shows how those methods differ in each step. In addition, the methods are grouped according to their characteristic features. The conclusion is that the concepts of assessing risk in different methods can be used to develop solutions leading to a comprehensive method appropriate for all work tasks and all parts of the body. However, studies are necessary to verify the accepted premises and to introduce some standardization that would make consolidation possible."]},
{"title": "Investigations into the skills of modern and traditional train driving", "highlights": ["Modern train driving has redefined the dynamic in the driver-train co-agency.", "Target tracking activities differ in modern and traditional train driving.", "Modern train driving skill equates to how well enhanced displays are pursued."], "abstract": ["Rail operations are housed inside a complex and extremely dynamic system where work is distributed in time and space. The train driver has traditionally relied on their own decisions, plans, and actions to navigate the rail environment, but the use of modern driver systems that force how these activities are regulated has altered this dynamic. This paper reports the findings of a study that set out to investigate the skills of modern (enhanced display-based) and traditional (real world) train driving. Data were collected from a variety of UK domain experts (", "\u00a0=\u00a045) using an innovative methodology that converged multiple techniques for knowledge elicitation and analysis. The findings are represented in a model of dynamic train control and discussed according to the specific features and nature of tracking skill in the rail domain. The utility of the model is demonstrated through work of its application to the design of a train simulator and research tool for systematic study of rail human factor issues."]},
{"title": "Physical fitness profile of professional Italian firefighters: Differences among age groups", "highlights": ["The percentage of overweight and obesity among Italian firefighters is lower than that was documented for the US firefighters.", "Physical fitness tests results were systematically better for the younger categories than the older age group.", "Performances values showed statistical differences for 1RM bench-press, CMJ, 20\u00a0m and endurance test with S.C.B.A.", "Wearing the protective clothing and S.C.B.A., 14% of all firefighters recruits failed to complete the endurance test."], "abstract": ["Firefighters perform many tasks which require a\u00a0high level of fitness and their personal safety may be compromised by the physiological aging process. The aim of the study was to evaluate strength (bench-press), power (countermovement jump), sprint (20\u00a0m) and endurance (with and without Self Contained Breathing Apparatus \u2013 S.C.B.A.) of 161 Italian firefighters recruits in relation to age groups (<25\u00a0yr; 26\u201330\u00a0yr; 31\u201335\u00a0yr; 36\u201340\u00a0yr; 41\u201342\u00a0yr). Descriptive statistics and an ANOVA were calculated to provide the physical fitness profile for each parameter and to assess differences (", "\u00a0<\u00a00.05) among age groups. Anthropometric values showed an age-effect for height and BMI, while performances values showed statistical differences for strength, power, sprint tests and endurance test with S.C.B.A. Wearing the S.C.B.A., 14% of all recruits failed to complete the endurance test. We propose that the firefighters should participate in an assessment of work capacity and specific fitness programs aimed to maintain an optimal fitness level for all ages."]},
{"title": "Lifting strategies of expert and novice workers during a repetitive palletizing task", "highlights": ["We examine the difference between expert and novice workers.", "The workers had to transfer 24 boxes of 15\u00a0kg from one pallet to another.", "The experts bent less their lumbar spine and are closer to the box.", "Expertise had very small effects on the back loading variables.", "An ergonomic intervention should not only focus on workers' training."], "abstract": ["Thirty manual material handlers (15 experts and 15 novices) were invited to perform series of box transfers under conditions similar to those of large distribution centers. The objective of the present study was to verify whether multiple box transfers leading to fatigue would also lead to differences between expert and novice workers in joint motions and in back loading variables (L5/S1 moments). The task consisted in transferring 24 15-kg boxes from one pallet to another (4 layers of boxes; 6 boxes/layer: 3 in the front row, 3 in the back) at a self-determined pace and then at an imposed pace of 9 lifts/min for a total of 240 lifts. The underlying idea was to set a challenging task that would force the experts to use their skills. Full-body 3D kinematic data were collected as well as external foot forces. A dynamic 3D linked segment model was used to estimate the net moments at L5/S1. The results clearly show that the experts bent their lumbar spine less (10\u00b0 less) and were closer (4\u00a0cm) to the box than novice workers. Knee flexions were similar in both groups except when the box was lifted from ground level (expert\u00a0\u2248\u00a071\u00b0, novice\u00a0\u2248\u00a048\u00b0). The peak resultant moment was not statistically different (expert\u00a0=\u00a0168 Nm, novice\u00a0=\u00a0184 Nm) although experts had lower values on average than novices when lifting heights (and deposit heights) of the boxes increased. Therefore, experts differed from novice workers mostly in the posture-related variables. These differences are especially important to consider when the box is located on the ground, as the back posture and back loading are then at their greatest magnitude and could have a major impact on the distribution of internal forces on the spine."]},
{"title": "An investigation of training strategies to improve alarm reactions", "highlights": ["Training alarm responders to analyze data can supplement alarm design improvements.", "Alarm responders benefited most from single sensor or spatial pattern training.", "Alarm responders did not benefit from temporal interval training.", "Alarm responders decided how to react before experiencing individual signals."], "abstract": ["Researchers have suggested that operator training may improve operator reactions; however, researchers have not documented this for alarm reactions. The goal of this research was to train participants to react to alarms using sensor activity patterns. In Experiment 1, 80 undergraduates monitored a simulated security screen while completing a primary word search task. They received spatial, temporal, single sensor, or no training to respond to alarms of differing reliability levels. Analyses revealed more appropriate and quicker reactions when participants were trained and when the alarms were reliable. In Experiment 2, 56 participants practiced time estimation by simple repetition, performance feedback, or performance feedback and temporal subdivision. They then reacted to alarms based on elapsed time between sensor activity and alarm onset. Surprisingly, results indicated that participants did not benefit differentially from temporal interval training, focusing instead on advertised system reliability. Researchers should replicate these findings with realistic tasks and real-world complex task operators."]},
{"title": "FMS\u2122 scores and low-back loading during lifting \u2013 Whole-body movement screening as an ergonomic tool?", "highlights": ["Firefighters performed simulated occupational lifting tasks in a laboratory setting.", "Low-back loading was compared between firefighters who scored above and below 14 on FMS.", "No differences detected in peak low-back loading magnitudes between low- and high-scorers.", "Lumbar spines of low-scorers tended to be more flexed when peak loads were applied.", "The \u201cmargin of safety\u201d may be lower in firefighters who score below 14 on the FMS."], "abstract": ["Previous research suggests that a general whole-body movement screen could be used to identify personal movement attributes that promote potentially injurious low-back loading patterns at work. The purpose of this study was to examine the relationship between Functional Movement Screen\u2122 (FMS) composite scores and the low-back loading response to lifting.", "Fifteen men who scored greater than 14 on the FMS (high-scorers) and 15 height- and weight-matched low-scorers (FMS\u00a0<\u00a014) performed sagittally symmetric and asymmetric laboratory-based lifting tasks. A three-dimensional dynamic biomechanical model was used to calculate peak low-back loading levels, and the angle of the lumbar spine was captured at the instant when the peak compressive force was applied.", "Regardless of the lifting task performed, there were no differences in peak low-back compression (", "\u00a0\u2265\u00a00.4157), anterior/posterior reaction shear (", "\u00a0\u2265\u00a00.5645), or medial/lateral reaction shear (", "\u00a0\u2265\u00a00.2581) forces between the high- and low-scorers. At the instant when peak compressive forces were applied, differences in the lumbar spine angle between high- and low-scores were not statistically significant about the lateral bend (", "\u00a0\u2265\u00a00.4215), axial twist (", "\u00a0\u2265\u00a00.2734), or flexion/extension (", "\u00a0\u2265\u00a00.1354) axes, but there was a tendency for the lumbar spine to be more deviated in the low-scorers.", "Using the previously established injury prediction threshold value of 14, the composite FMS score was not related to the peak low-back loading magnitudes in lifting. Though not statistically significant, the tendency for the lumbar spines of low-scorers to be more deviated when the peak low-back compression force was imposed could be biomechanically meaningful because spinal load tolerance varies with posture. Future attempts to modify or reinterpret FMS scoring are warranted given that several previous studies have revealed links between composite FMS scores and musculoskeletal complaints."]},
{"title": "Thermal effects on human performance in office environment measured by integrating task speed and accuracy", "highlights": ["The speed and accuracy can be integrated into one metric of human performance by presenting the task with feedback.", "A larger decrease in task performance was observed due to thermal discomfort when feedback was given.", "Task speed was affected by thermal discomfort to a much greater extent than accuracy."], "abstract": ["We have proposed a method in which the speed and accuracy can be integrated into one metric of human performance. This was achieved by designing a performance task in which the subjects receive feedback on their performance by informing them whether they have committed errors, and if did, they can only proceed when the errors are corrected. Traditionally, the tasks are presented without giving this feedback and thus the speed and accuracy are treated separately. The method was examined in a subjective experiment with thermal environment as the prototypical example. During exposure in an office, 12 subjects performed tasks under two thermal conditions (neutral & warm) repeatedly. The tasks were presented with and without feedback on errors committed, as outlined above. The results indicate that there was a greater decrease in task performance due to thermal discomfort when feedback was given, compared to the performance of tasks presented without feedback."]},
{"title": "Is workplace satisfaction associated with self-reported quad bike loss of control events among farm workers in New Zealand?", "highlights": ["Two independent samples of farmers surveyed.", "Count data regression techniques used.", "Aspects of workplace satisfaction related to quad bike loss-of-control events.", "Psychological demand, employment status and sex linked with quad bike LCEs."], "abstract": ["This study investigated whether rural workers who have higher workplace satisfaction are less likely to report quad bike loss of control events (LCEs). Two independent samples of farmers completed a survey regarding LCEs and workplace satisfaction. In the first sample (", "\u00a0=\u00a0130) analysis revealed no relationship (", "\u00a0=\u00a00.74) between workplace satisfaction and LCEs but lower rates of LCEs were reported by employees (IRR 0.52, 95%CI 0.31\u20130.86) compared to self-employed participants. In the second sample (", "\u00a0=\u00a0112), workplace satisfaction was weakly related to LCEs (IRR 1.04, 95%CI 1.00, to 1.09) with participants who found their job more psychologically demanding more likely to have had an LCE (IRR 1.14, 95%CI 1.05\u20131.23). Exploring the role of psychological demands on safety behaviour with respect to quad bike use, may help to address this important safety issue."]},
{"title": "Effects of caffeine and menthol on cognition and mood during simulated firefighting in the heat", "highlights": ["We examined changes in cognition and mood during stimulated firefighting in heat.", "We examined the separate effects of caffeine and menthol on mental efforts.", "Cognition is well maintained during short duration firefighting.", "Mood is severely deteriorated even with regular rests.", "No effects of caffeine and menthol on cognition and mood."], "abstract": ["This study examined the separate effects of caffeine and menthol on cognition and mood during simulated firefighting in the heat. Participants (", "\u00a0=\u00a010) performed three trials in a counterbalanced order, either with 400\u00a0mg caffeine, menthol lozenges, or placebo. The simulated firefighting consisted of 2 bouts of 20-min treadmill exercise and one bout of 20-min stepping exercise in the heat with two brief 15-min rest periods between each exercise phase. Exercise induced significant dehydration (>3%) and elevated rectal temperature (>38.9\u00a0\u00b0C), for all three conditions. Neither caffeine nor menthol reduced perceived exertion compared to placebo (", "\u00a0>\u00a00.05). Mood ratings (i.e., alertness, hedonic tone, tension) significantly deteriorated over time (", "\u00a0<\u00a00.05), but there was no difference among the three conditions. Simple reaction time, short-term memory, and retrieval memory did not alter with treatments or repeated evaluations. Reaction accuracy from a math test remained unchanged throughout the experimental period; reaction time from the math test was significantly faster after exposure to the heat (", "\u00a0<\u00a00.05). It is concluded that, exhaustive exercise in the heat severely impacted mood, but minimally impacted cognition. These treatments failed to show ergogenic benefits in a simulated firefighting paradigm in a hot environment."]},
{"title": "Context matters: The structure of task goals affects accuracy in multiple-target visual search", "highlights": ["Task structures typical of radiology and airport security searches are compared.", "Participants searching in a radiology structure are to achieve a fixed objective.", "Participants in an airport security structure search for a fixed duration.", "Higher errors rates for multiple targets are seen in the fixed objective condition."], "abstract": ["Career visual searchers such as radiologists and airport security screeners strive to conduct accurate visual searches, but despite extensive training, errors still occur. A key difference between searches in radiology and airport security is the structure of the search task: Radiologists typically scan a certain number of medical images (fixed objective), and airport security screeners typically search X-rays for a specified time period (fixed duration). Might these structural differences affect accuracy? We compared performance on a search task administered either under constraints that approximated radiology or airport security. Some displays contained more than one target because the presence of multiple targets is an established source of errors for career searchers, and accuracy for additional targets tends to be especially sensitive to contextual conditions. Results indicate that participants searching within the fixed objective framework produced more multiple-target search errors; thus, adopting a fixed duration framework could improve accuracy for career searchers."]},
{"title": "Microclimate in ski boots \u2013 Temperature, relative humidity, and water absorption", "highlights": ["Low temperature in the ski boot caused pain sensation.", "High moisture in the ski boot likely affected thermal insulation.", "Liner material with high water absorption capacity and hydrophobic socks were suggested to prevent wet feet."], "abstract": ["Ski boot quality is determined by mechanical properties and comfort. Comfort is strongly affected by cold feet. The purpose of this study was to determine the microclimate in ski boots. Climate chamber tests with five male subjects and field tests with two male subjects were conducted. Temperature and relative humidity were measured using four sensors placed on the foot and one on the liner. Absorbed water in liners and socks was measured with a precision balance. The subjects gave subjective ratings for comfort. The toe sensor temperature dropped below 20\u00a0\u00b0C at an ambient temperature of 0\u00a0\u00b0C,\u00a0\u221210\u00a0\u00b0C, and\u00a0\u221220\u00a0\u00b0C. Relative humidity values at the foot were as high as 78% in the climate chamber and 93% in the field. Water absorption in socks and liners ranged from 4 to 10\u00a0g in the climate chamber and 19 to 45.5\u00a0g in the field. The results reveal the importance of keeping the feet and in particular the toes warm during skiing. One possible improvement may be to construct the liner so that sweat and melted snow are kept as far away as possible from the foot. Liner material with high water absorption capacity and hydrophobic socks were suggested to prevent wet feet."]},
{"title": "Response to thermal and physical strain during flashover training in Croatian firefighters", "highlights": ["Flashover training (FOT) induced a mild physiological cardiovascular response.", "More than 25% FOT attendees were obese.", "Elevated basal blood pressure was found in >50% FOT attendees.", "Croatian firefighters need to improve their physical fitness."], "abstract": ["Flashover training (FOT) for firefighters is a simulation of the flashover phenomenon under controlled conditions. This study assessed arterial blood pressure (BP) and its response to thermal and physical strain during FOT in 48 professional and 18 volunteer firefighters.", "A high prevalence of obesity (27%), basal hypertensive (53%) and prehypertensive (33%) BP values was found. FOT induced mild hyperthermia and physical strain (average increase of 1.1\u00a0\u00b0C in tympanic temperature and 61% of the maximal heart beat predicted for age). Compared to professional firefighters, FOT in the volunteers induced a higher increase in pulse (", "\u00a0=\u00a00.050) and tympanic temperature (", "\u00a0=\u00a00.025). Systolic BP did not vary significantly, and diastolic BP slightly decreased in both groups.", "Results confirm that FOT induced only physiological cardiovascular responses to thermal and physical strain in firefighters. High prevalence of obesity and elevated BP values indicate the need for better physical fitness and BP control among firefighters."]},
{"title": "Interpretation of way-finding healthcare symbols by a multicultural population: Navigation signage design for global health", "highlights": ["Comprehension of healthcare symbols increases with literacy and decreases with age.", "Symbols for abstract referents (such as oncology and diabetes education) are more difficult to interpret correctly.", "Healthcare symbol design should consider cultural factors as may they may influence comprehension."], "abstract": ["The interpretation of way-finding symbols for healthcare facilities in a multicultural community was assessed in a cross-sectional study. One hundred participants recruited from Al Ain city in the United Arab Emirates were asked to interpret 28 healthcare symbols developed at ", " (such as vaccinations and laboratory) as well as 18 general-purpose symbols (such as elevators and restrooms). The mean age was 27.6 years (16\u201355 years) of whom 84 (84%) were females. Healthcare symbols were more difficult to comprehend than general-purpose signs. Symbols referring to abstract concepts were the most misinterpreted including oncology, diabetes education, outpatient clinic, interpretive services, pharmacy, internal medicine, registration, social services, obstetrics and gynecology, pediatrics and infectious diseases. Interpretation rates varied across cultural backgrounds and increased with higher education and younger age. Signage within healthcare facilities should be tested among older persons, those with limited literacy and across a wide range of cultures."]},
{"title": "Evaluation on an ergonomic design of functional clothing for\u00a0wheelchair users", "highlights": ["An ergonomic design work of wheelchair users' functional clothing was carried out.", "An evaluating system deriving from rehabilitation medicine and sports tournament was used.", "The newly designed functional clothing could facilitate daily living activities related with clothing for the wheelchair users."], "abstract": ["Researchers have pointed out that people with physical disabilities find it difficult to obtain suitable clothing. In this study a set of wheelchair user oriented functional clothing was designed. Attention was paid to the wheelchair users' daily living activities related with clothing. An evaluating system combined with sports tournament and rehabilitation medicine was introduced to assess the new designed clothing. Six wheelchair users (3 males and 3 females) were invited to wear the clothing. A set of normal functional clothing was employed as a comparison (Control). The time required to complete three different daily living activities, i.e. dressing and undressing, going to toilet and bathing were recorded. Results showed that with the new clothing wheelchair users' competence of managing toilet was increased by 52.9%. The time needed for toilet was reduced by 45.7%. Their capability of managing dressing and undressing was improved by 24.6%. The study indicated that the newly designed clothing could facilitate wheelchair users' daily living activities related with clothing."]},
{"title": "Recommendations for tool-handle material choice based on finite element analysis", "highlights": ["We modelled a human fingertip grasping a tool-handle, using different materials.", "We simulated steel, rubber, and two composites of rubber and hyper-elastic foam.", "The foam composites produced lower contact pressure.", "Sufficient stability of the hand-tool was maintained due to small deformations.", "Using foam composites the risk of acute and cumulative trauma disorder is lowered."], "abstract": ["Huge areas of work are still done manually and require the usages of different powered and non-powered hand tools. In order to increase the user performance, satisfaction, and lower the risk of acute and cumulative trauma disorders, several researchers have investigated the sizes and shapes of tool-handles. However, only a few authors have investigated tool-handles' materials for further optimising them. Therefore, as presented in this paper, we have utilised a finite-element method for simulating human fingertip whilst grasping tool-handles. We modelled and simulated steel and ethylene propylene diene monomer (EPDM) rubber as homogeneous tool-handle materials and two composites consisting of EPDM rubber and EPDM foam, and also EPDM rubber and PU foam. The simulated finger force was set to obtain characteristic contact pressures of 20\u00a0kPa, 40\u00a0kPa, 80\u00a0kPa, and 100\u00a0kPa. Numerical tests have shown that EPDM rubber lowers the contact pressure just slightly. On the other hand, both composites showed significant reduction in contact pressure that could lower the risks of acute and cumulative trauma disorders which are pressure-dependent. Based on the results, it is also evident that a composite containing PU foam with a more evident and flat plateau deformed less at lower strain rates and deformed more when the plateau was reached, in comparison to the composite with EPDM foam. It was shown that hyper-elastic foam materials, which take into account the non-linear behaviour of fingertip soft tissue, can lower the contact pressure whilst maintaining low deformation rate of the tool-handle material for maintaining sufficient rate of stability of the hand tool in the hands. Lower contact pressure also lowers the risk of acute and cumulative trauma disorders, and increases comfort whilst maintaining performance."]},
{"title": "Preference for newspaper size", "highlights": ["Five different standard sizes of newspaper were tested for reader preference.", "Preferences were determined in comfortable and cramped environments.", "A paired comparison method yielded scales of reading preference.", "In both reading environments, the Tabloid format was preferred over other sizes.", "Differences in preferences were greater in cramped reading environment."], "abstract": ["The past few years has seen a change in the size of newspapers, with publishers moving to a smaller size format. Five \u2018standard\u2019 newspaper sizes are used in different countries: Broadsheet, Rhensch, Tabloid, Tall Tabloid and Berliner. These papers vary in both width and height of pages and hence there are implications for human reading comfort, which may be dependent on reading location such as on a lounge chair or on a train. Experiments were carried out to determine preferences for the different sizes and to relate these preferences to the geometric characteristics of the newspapers. For both comfortable and cramped/uncomfortable reading conditions, the rank order of preference for paper types was, from least to most-preferred, Broadsheet, Rhensch, Berliner, Tall Tabloid and Tabloid. Preferences were much stronger when determined in cramped/uncomfortable reading conditions, where most comparisons were significantly different. There was good correlation between participant ratings on several scales and preference, where most factors were related to comfort of holding and controlling the paper."]},
{"title": "Pleasant music as a countermeasure against visually induced motion sickness", "highlights": ["Music as a countermeasure against visually induced motion sickness (VIMS) was tested.", "Different music types (relaxing, neutral, stressful, or no music) were chosen.", "Relaxing music showed a non-significant trend to reduce VIMS.", "Music rated as pleasant significantly reduced the severity of VIMS.", "Pleasant music could act as an effective and low-cost countermeasure against VIMS."], "abstract": ["Visually induced motion sickness (VIMS) is a well-known side-effect in virtual environments or simulators. However, effective behavioral countermeasures against VIMS are still sparse. In this study, we tested whether music can reduce the severity of VIMS. Ninety-three volunteers were immersed in an approximately 14-minute-long video taken during a bicycle ride. Participants were randomly assigned to one of four experimental groups, either including relaxing music, neutral music, stressful music, or no music. Sickness scores were collected using the Fast Motion Sickness Scale and the Simulator Sickness Questionnaire. Results showed an overall trend for relaxing music to reduce the severity of VIMS. When factoring in the subjective pleasantness of the music, a significant reduction of VIMS occurred only when the presented music was perceived as pleasant, regardless of the music type. In addition, we found a gender effect with women reporting more sickness than men. We assume that the presentation of pleasant music can be an effective, low-cost, and easy-to-administer method to reduce VIMS."]},
{"title": "Subjective analysis of cold thermal environments", "highlights": ["Food distribution industrial sector.", "Subjective assessment based on 1575 valid questionnaires.", "Extensive statistical analysis.", "Detailed characterization of the working conditions."], "abstract": ["The present work is dedicated to the study of cold thermal environments in food distribution industrial units through a subjective assessment based on an individual questionnaire which aims to describe the working conditions of employees often exposed to cold. The survey was carried out in Portugal and the sample consists of 1575 valid responses obtained in 61 industrial units. The results show that the food distribution activity sector is characterized by a female population (78.1%) and by a young work force (63.4% of the workers are less than 35 years old). Despite the availability of cold protective clothing (52.8% of the workers indicate one garment) its characteristics require improvements. In addition almost 1/3 of the respondents consider the thermal environment cold and 79.6% of the workers report that working in the cold is harder in wintertime. The results also highlight that 37.3% of the workers report having health problems."]},
{"title": "Neck and shoulder muscle activity during standardized work-related postural tasks", "highlights": ["A broad range of work-related postural tasks was standardized using a computer guided task paradigm.", "The sternocleidomastoid and upper trapezius muscles were mildly active during the majority of postures investigated.", "Certain head and neck tasks activated the sternocleidomastoid, whilst shoulder and arm tasks activated the upper trapezius.", "The muscle activity did not differ significantly between male and female participants.", "This study provides reference values for further investigation of these muscles in the natural work environment."], "abstract": ["The aim of the present study was to assess the activity levels of the sternocleidomastoid muscle and upper trapezius muscle during static postures under controlled and standardized conditions, and to determine whether the muscle activity differed between sexes. Electromyographic (EMG) activity was recorded unilaterally from the sternocleidomastoid and upper trapezius muscle in 17 participants whilst they were performing various postural tasks. EMG amplitude was measured by the root mean square values of the raw signals and normalized to peak maximum contractile values for each muscle (%MVC). The intensity of muscle activity was ranked as light (<3%MVC), moderate (3%MVC\u00a0\u2264\u00a0EMG\u00a0\u2264\u00a08%MVC), and substantial (>8%MVC). During most tasks the two muscles contracted light to moderately. Head leaning and shoulder shrugging postures yielded substantial muscle activity in both muscles. Muscle activity did not differ significantly between male and female participants (", "\u00a0=\u00a03.1; ", "\u00a0=\u00a00.078). Our findings provided normative values, which will enhance future studies of muscle activity during work in a natural, unrestrained environment."]},
{"title": "Strengths and limitations of a musculoskeletal model for an analysis of simulated meat cutting tasks", "highlights": ["The cutting movements involving arm flexion are preferable in terms of biomechanical loads.", "The optimal bench height for the meat cutting task should be 20\u201330\u00a0cm below elbow height.", "Musculoskeletal models can help to minimize the risk of developing WMSD."], "abstract": ["This study assessed the capacity of a musculoskeletal model to predict the relative muscle activation changes as a function of the workbench height and the movement direction during a simulated meat cutting task. Seven subjects performed a cutting task alternating two cutting directions for 20\u00a0s at four different workbench heights. Kinematics, electromyography (EMG), and cutting force data were collected and used to drive a musculoskeletal model of the shoulder girdle. The model predicted the muscle forces exerted during the task. Both the recorded and computed activation of the muscles was then compared by means of cross-correlation and by comparison of muscle activation trends with respect to the workstation parameters, i.e. cutting direction and workbench height. The results indicated that cutting movements involving arm flexion are preferable to movement requiring internal arm rotation and abduction. The optimal bench height for meat cutting tasks should be between 20 and 30\u00a0cm below the worker's elbow height. The present study underlines a beneficial use of musculoskeletal models for adjusting workstation parameters."]},
{"title": "Facilitating the comparison of multiple visual items on screen: The example of electronic architectural plan correction", "highlights": ["The automatic recognition of technical documents involves a correction by an operator.", "We compared three displays formats for retroconversion errors detection with architectural plans.", "Superimposing the plans improves errors correction efficiency compared to separate plans.", "In addition, the sequential display of the plans improves the completeness of the correction."], "abstract": ["This paper describes two experiments designed to (1) ascertain whether the way in which architectural plans are displayed on a computer screen influences the quality of their correction by humans, and (2) identify the visual exploration strategies adopted in this type of task. Results of the first \u201cspot the difference\u201d experiment showed that superimposing the plans yielded better error correction performances than displaying them side by side. Furthermore, a sequential display mode, where the second plan only gradually appeared on the screen, improved error search effectiveness. In the second experiment, eye movement recordings revealed that superimposition increased plan comparison efficiency by making it easier to establish coreference between the two sources of information. The improvement in effectiveness in the sequential condition was shown to be linked to the attentional guidance afforded by this display mode, which helped users to make a more thorough exploration of the plans."]},
{"title": "Influence of affective states on comprehension and hazard perception of warning pictorials", "highlights": ["Affective state influenced hazard perception of warning signs but not comprehension.", "People in the positive affect condition perceived greater hazard from warning signs.", "International Affective Picture System (IAPS) was used for affect manipulation.", "Affective state declines over time and have tendency to neutral state."], "abstract": ["The purpose of the present study was to examine the effect of positive and negative affective states on comprehension and hazard perception of warning pictorials. The International Affective Picture System (IAPS) was used to manipulate the affective states of sixty male undergraduate and graduate student participants. We used sixteen standard industrial warning pictorials, which were representative of a variety of industries, to assess changes in comprehension and hazard perception. Participants in the positive affect condition perceived greater hazards from the warning signs than those in the neutral affect condition or the negative affect condition. Post-hoc analyses confirmed this finding. We discuss implications for warning pictorials and future research."]},
{"title": "Design and evaluation of small, linear QWERTY keyboards", "highlights": ["A miniature keyboard is vital for small devices to shrink the size of the devices and expand the display area.", "The number keys of small linear keyboards should be separated from the letter keys.", "Small linear keyboards with square-shaped keycaps had better accuracy than keyboards with rectangular-shaped keycaps."], "abstract": ["Miniature keyboard design is motivated by the need for smaller mobile devices with maximum user display area. Thus, this study developed four miniature keyboard designs which varied from conventional keyboard design in terms of their configuration and layout. The purpose of this study was to evaluate the input speed, accuracy, comfort, likability and learnability of four miniature keyboards. Sixteen fast typists and 16 slow typists were recruited to use these four miniature keyboards. The results showed that the rectangular-shaped keycaps of 3 letters with separated keycaps of numerals obtained the best proficiency speed, highest comfort and greatest user acceptance among the four keyboards. Moreover, the keyboards with square-shaped keycaps had better input accuracy compared to rectangular-shaped keycaps. Finally, the proposed keyboards were smaller than current keyboards, and the performance for all of the small keyboards was worse than that of the conventional keyboard."]},
{"title": "Upper extremity hemodynamics and sensation with backpack loads", "highlights": ["Macro- and micro-vascular flows decrease with backpack donning.", "Finger sensations also decrease.", "Backpack donning may adversely affect fine motor control."], "abstract": ["Heavy backpacks are often used in extreme environments, for example by military during combat, therefore completion of tasks quickly and efficiently is of operational relevance. The purpose of this study was to quantify hemodynamic parameters (brachial artery Doppler and microvascular flow by photoplethysmography; tissue oxygenation by near-infrared spectroscopy; arterial oxygen saturation by pulse oximeter) and sensation in upper extremities and hands (Semmes-Weinstein monofilament test and 2-point discrimination test) while wearing a loaded backpack (12\u00a0kg) in healthy adults for 10\u00a0min. All values were compared to baseline before wearing a backpack. Moderate weight loaded backpack loads significantly decreased upper extremity sensation as well as all macrovascular and microvascular hemodynamic values. Decreased macrovascular and microvascular hemodynamics may produce neurological dysfunction and consequently, probably affect fine motor control of the hands."]},
{"title": "Integrating ergonomics into engineering design: The role of objects", "highlights": ["Objects can play an important but often unrecognized role in integrating ergonomic knowledge into a design process.", "Boundary objects can help to facilitate a dialog between design actors and help to bring ergonomic knowledge into action.", "In a design process ergonomic guideline texts are likely to be transformed, not just transferred.", "Ergonomic guideline texts should be supported by an ergonomist having an active role throughout design processes."], "abstract": ["The objective of this study was to explore the role of objects in integrating ergonomic knowledge in engineering design processes. An engineering design case was analyzed using the theoretical concepts of ", " and ", ": Boundary objects facilitate collaboration between different knowledge domains, while the aim of an intermediary object is to circulate knowledge and thus produce a distant effect. Adjustable layout drawings served as boundary objects and had a positive impact on the dialog between an ergonomist and designers. An ergonomic guideline document was identified as an intermediary object. However, when the ergonomic guidelines were circulated in the design process, only some of the guidelines were transferred to the design of the sterile processing plant. Based on these findings, recommendations for working with objects in design processes are included."]},
{"title": "Learning effects in the lane change task (LCT) \u2013 Realistic secondary tasks and transfer of learning", "highlights": ["Lane change task's (LCT) sensitivity towards learning effects was examined.", "Repeated use of same LCT/secondary task combination resulted in increased performance.", "Limited transfer of learning was confirmed for sufficiently similar secondary tasks.", "Results point to limitations of LCT as standardized assessment procedure.", "The LCT's sensitivity towards learning effects is interesting for research purposes."], "abstract": ["Driver distraction is a factor that is heavily involved in traffic crashes. With in-vehicle devices like navigation systems or mobile phones on the rise, the assessment of their potential to distract the driver has become a pressing issue. Several easy-to-use methods have been developed in recent years to allow for such an assessment in the early stages of product development. One of these methods is the lane change task (LCT), a simple driving simulation in which the driver has to change lanes as indicated by different signs along the road. Although the LCT is an ISO sanctioned procedure, there are still open questions. One issue are learning effects which have been found in previous studies and which have the potential to compromise the comparability of test results. In this paper, we present results on two experiments that further explored the effect of previous experience on LCT and secondary task performance. The results confirm that learning effects occur when combining the LCT with a realistic secondary task. Also, we found evidence for the transfer of learning from one secondary task to another to some degree, provided that the two tasks are sufficiently similar."]},
{"title": "Taking ergonomics to the bedside \u2013 A multi-disciplinary approach to designing safer healthcare", "highlights": ["Work analysis of bedside healthcare processes carried out based on over 70h of observation and shadowing on 3 surgical wards.", "Over 14 processes were identified and an HFMEA identified nearly 200 potential failure modes in the 5 highest risk processes.", "A causal analysis identified number of common systems influences on safety \u2013 education, lack of reminders, feedback & design.", "Designers and clinicians worked together to develop a suite of solutions, tackling failures across the range of processes."], "abstract": ["A multi-disciplinary approach to designing safer healthcare was utilised to investigate risks in the bed-space in elective surgical wards. The Designing Out Medical Error (DOME) project brought together clinicians, designers, psychologists, human factors and business expertise to develop solutions for the highest risk healthcare processes. System mapping and risk assessment techniques identified nearly 200 potential failure modes in hand hygiene, isolation of infection, vital signs monitoring, medication delivery and handover of information. Solutions addressed issues such as the design of equipment, reminders, monitoring, feedback and standardisation. Some of the solutions, such as the CareCentre\u2122, which brings many of the processes and equipment together into one easy to access workstation at the foot of the bed, have been taken forward to clinical trials and manufacture. The project showed the value of the multi-disciplinary and formal human factors approaches to healthcare design for patient safety. In particular, it demonstrates the application of human factors to a complete design cycle and provides a case study for the activities required to reach a safe, marketable product."]},
{"title": "The effect of a helmet on cognitive performance is, at worst, marginal: A controlled laboratory study", "highlights": ["Helmet-wearing.", "Only 1/9 cognitive parameters showed an effect.", "Effect on cognitive performance is at worst marginal."], "abstract": ["The present study looked at the effect of a helmet on cognitive performance under demanding conditions, so that small effects would become more detectible. Nineteen participants underwent 30\u00a0min of continuous visual vigilance, tracking, and auditory vigilance (VTT\u00a0+\u00a0AVT), while seated in a warm environment (27.2 (\u00b10.6)\u00a0\u00b0C, humidity 41 (\u00b11)%, and 0.5 (\u00b10.1)\u00a0m\u00a0s", " wind speed). The participants wore a helmet in one session and no helmet in the other, in random order. Comfort and temperature perception were measured at the end of each session. Helmet-wearing was associated with reduced comfort (", "\u00a0=\u00a00.001) and increased temperature perception (", "\u00a0<\u00a00.001), compared to not wearing a helmet. Just one out of nine cognitive parameters showed a significant effect of helmet-wearing (", "\u00a0=\u00a0.032), disappearing in a post-hoc comparison. These results resolve previous disparate studies to suggest that, although helmets can be uncomfortable, any effect of wearing a helmet on cognitive performance is at worst marginal."]},
{"title": "Usability and perceived usefulness of personal health records for preventive health care: A case study focusing on patients' and primary care providers' perspectives", "highlights": ["We examined the usefulness and usability of a Personal Health Records (PHR) application.", "Multiple methods with observations, think-aloud, surveys, interviews, focus groups.", "Patients found tailored health recommendations useful.", "Patients asked more informed questions to their providers due to the system.", "Care providers interested in PHR due to useful content and patient activation."], "abstract": ["Personal Health Records (PHR) are electronic applications for individuals to access, manage and share their health information in a secure environment. The goal of this study was to evaluate the usefulness and usability of a Web-based PHR technology aimed at improving preventive care, from both the patients' and primary care providers' perspectives. We conducted a multi-method descriptive study that included direct observations, concurrent think-aloud, surveys, interviews and focus groups in a suburban primary care clinic. Patients found the tailored health recommendations useful and the PHR easy to understand and use. They also reported asking useful health-related questions to their physicians because of using the system. Generally, care providers were interested in using the system due to its useful content and impact on patient activation. Future successful systems should be better integrated with hospital records; put more emphasis on system security; and offer more tailored health information based on comprehensive health databases."]},
{"title": "Using Signal Detection Theory and Time Window-based Human-In-The-Loop simulation as a tool for assessing the effectiveness of different qualitative shapes in continuous monitoring tasks", "highlights": ["Our findings suggest design principles for continuous monitoring multi-task environments in oil and gas refineries.", "Combinations of flow and level gauge shapes and a small set size promote effective performance.", "Combinations of pressure and temperature gauge shapes and a large set size elicit ineffective performance.", "In general, a smaller set size will elicit faster and more sensitive performance.", "However, smaller set sizes elicit more false alarms so the cost of false alarms must be considered in the design."], "abstract": ["This paper provides a case study of Signal Detection Theory (SDT) as applied to a continuous monitoring dual-task environment. Specifically, SDT was used to evaluate the independent contributions of sensitivity and bias to different qualitative gauges used in process control. To assess detection performance in monitoring the gauges, we developed a Time Window-based Human-In-The-Loop (TWHITL) simulation bed. Through this test bed, we were able to generate a display similar to those monitored by console operators in oil and gas refinery plants. By using SDT and TWHITL, we evaluated the sensitivity, operator bias, and response time of flow, level, pressure, and temperature gauge shapes developed by Abnormal Situation Management", " (ASM", ") Consortium (", "). Our findings suggest that display density influences the effectiveness of participants in detecting abnormal shapes. Furthermore, results suggest that some shapes elicit better detection performance than others."]},
{"title": "Whole-body vibration transmissibility in supine humans: Effects of board litter and neck collar", "highlights": ["The spinal-board-litter magnified the supine-human motion, especially in the vertical direction around 5\u00a0Hz.", "The neck-collar magnified the neck flexion\u2013extension motion during input fore-aft vibration.", "Inertial sensors facilitated the acceleration measurements to a certain degree, but were affected by surrounding metal.", "Comparable transmissibility magnitudes resulted from sinusoidal and random input vibrations.", "Resonance frequency shifted by 1\u20132\u00a0Hz with increased vibration magnitude (softening) from 0.5\u00a0m/s", " to 1\u00a0m/s", "."], "abstract": ["Whole-body vibration has been identified as a stressor to supine patients during medical transportation. The transmissibility between the input platform acceleration and the output acceleration of the head, sternum, pelvis, head-sternum, and pelvis-sternum of eight supine subjects were investigated. Vibration files were utilized in the fore-aft, lateral, and vertical directions. The power spectral density across the bandwidth of 0.5\u201320\u00a0Hz was approximately flat for each file. A comparison between a baseline rigid-support and a support with a long spinal board strapped to a litter has shown that the latter has considerable effects on the transmitted motion in all directions with a double magnification in the vertical direction around 5\u00a0Hz. The results also showed that the neck-collar has increased the relative head-sternum flexion\u2013extension because of the input fore-aft vibration, but reduced the head-sternum extension\u2013compression due to the input vertical vibration."]},
{"title": "Thermoregulatory modeling use and application in the military workforce", "highlights": ["We modeled physiology of soldiers inside the military vehicle driving in dry heat.", "We developed a military working dog model from human models with dog functions.", "We modeled soldiers' heat tolerance that could differ >25% by body size and shape.", "Soldiers inside the military vehicle >35\u00a0\u00b0C need cooling to avoid heat illness.", "A resting military working dog can survive in a shaded dry environment to <40\u00a0\u00b0C."], "abstract": ["Thermoregulatory models have been used in the military to quantify probabilities of individuals' thermal-related illness/injury. The uses of the models have diversified over the past decade. This paper revisits an overall view of selected thermoregulatory models used in the U.S. military and provides examples of actual practical military applications: 1) the latest military vehicle designed with armor and blast/bulletproof windows was assessed to predict crews' thermal strains levels inside vehicles under hot environment (air temperature [", "]: 29\u201343\u00a0\u00b0C, dew point: 13\u00a0\u00b0C); 2) a military working dog (MWD) model was developed by modifying existing human thermoregulatory models with canine physical appearance and physiological mechanisms; 3) thermal tolerance range of individuals from a large military group (", "\u00a0=\u00a0100) exposed to 35\u00a0\u00b0C/40% relative humidity were examined using thermoregulatory modeling and multivariate statistical analyses. Model simulation results assist in the decisions for the strategic planning and preventions of heat stress."]},
{"title": "Towards a comprehensive Functional Capacity Evaluation for hand function", "highlights": ["This study validates a short protocol for Hand Function testing.", "Hand capacity can be measured more efficiently compared to existing protocols.", "Testing work capacity benefits from shorter protocol, because of lower costs."], "abstract": ["The aim of this study was to develop a more efficient (i.e. shortened) protocol for hand function capacity evaluation and to test the agreement of the protocol compared to the original protocol. 643 Healthy subjects performed tests for hand function. Agreement between two shortened protocols was compared with an existing protocol. The original protocol was performed once and the proposed shortened protocol differed in the number of trials which were reduced by statistical elimination. Agreement was determined with Intraclass Correlation Coefficients (ICC) and Limits of Agreement (LoA). Excellent ICCs (\u22650.91) were observed in all proposed protocols except for the one trial purdue pegboard test protocol. For all tests of hand function, shorter protocols are valid to determine hand function. For Tip Pinch Strength testing, Palmar Pinch Strength testing and the Purdue Pegboard test, a two-trial protocol is recommended, because the LoA were considerable, which could affect decision-making with regards to hand capacity. For the Hand Grip strength test, the Key Pinch Strength test and the Complete Minnesota Dexterity Test, a one-trial protocol is recommended, because the LoA were acceptable. It was concluded that for healthy subjects, this shorter protocol is a reliable measure. Further testing of the short form hand FCE protocols should be completed on patients with disabling conditions prior to widespread use of these protocols among clinical samples."]},
{"title": "Human performance under two different command and control paradigms", "highlights": ["A simulated command and control task was performed thirty times by two teams who were structured differently.", "Due to the inherent instability of the human/system interaction a strong differential learning effect took place.", "The hierarchical command and control team were faster but less accurate than the network enabled team.", "The common \u2018one shot\u2019 cross-sectional psychology experiment would not have detected these differences."], "abstract": ["The paradoxical behaviour of a new command and control concept called Network Enabled Capability (NEC) provides the motivation for this paper. In it, a traditional hierarchical command and control organisation was pitted against a network centric alternative on a common task, played thirty times, by two teams. Multiple regression was used to undertake a simple form of time series analysis. It revealed that whilst the NEC condition ended up being slightly slower than its hierarchical counterpart, it was able to balance and optimise all three of the performance variables measured (task time, enemies neutralised and attrition). From this it is argued that a useful conceptual response is not to consider NEC as an end product comprised of networked computers and standard operating procedures, nor to regard the human system interaction as inherently stable, but rather to view it as a set of initial conditions from which the most adaptable component of all can be harnessed: the human."]},
{"title": "Evaluating camouflage design using eye movement data", "highlights": ["We compared existing camouflage designs using two common war field backgrounds.", "Camouflage effectiveness was evaluated using hit rate, detection time, and eye movement data.", "Detectability and discriminability were better measured with eye movement data.", "Eye movement data were shown to significantly enhance the analysis of camouflage design."], "abstract": ["This study investigates the characteristics of eye movements during a camouflaged target search task. Camouflaged targets were randomly presented on two natural landscapes. The performance of each camouflage design was assessed by target detection hit rate, detection time, number of fixations on display, first saccade amplitude to target, number of fixations on target, fixation duration on target, and subjective ratings of search task difficulty. The results showed that the camouflage patterns could significantly affect the eye-movement behavior, especially first saccade amplitude and fixation duration, and the findings could be used to increase the sensitivity of the camouflage assessment. We hypothesized that the assessment could be made with regard to the differences in detectability and discriminability of the camouflage patterns. These could explain less efficient search behavior in eye movements. Overall, data obtained from eye movements can be used to significantly enhance the interpretation of the effects of different camouflage design."]},
{"title": "A socio-technical systems approach to studying interruptions: Understanding the interrupter's perspective", "highlights": ["This study took a socio-technical systems approach to studying interruptions.", "First study to examine interruptions from the viewpoint of the interrupting nurse.", "Nurses conduct a quick cost-benefit assessment to determine interruptibility.", "Definition of value-added and non-value added interruptions is context dependent.", "Interventions must take the complexity of interruptions into consideration."], "abstract": ["The purpose of this study was to understand the cognitive processes underlying nurses' decision to interrupt other nurses. The Institute of Medicine (2000) reported that interruptions are likely contributors to medical errors. Unfortunately, the research to date has been quite homogenous, focusing only on the healthcare provider being interrupted, ignoring the true complexities of interruptions. This study took a socio-technical approach being the first to examine interruptions from the viewpoint of the interrupting nurse. Over 15\u00a0h of observations and 10 open-ended interviews with expert nurses in a Neuroscience Surgical Intensive Care Unit were conducted. It was found that nurses conduct a quick cost-benefit assessment to determine the interruptibility of other nurses and whether an interruption is value-added vs. non-value added. To complete the assessment, nurses consider several conditional factors related to the interruptee, the interrupter, and the nature of the interruption content, and different potential consequences of the interruption."]},
{"title": "Towards identifying the roll motion parameters of a motorcycle simulator", "highlights": ["An experiment aimed to identify the roll motion parameters of a motorbike simulator.", "Motorcyclists tuned the angular rolls of the mock-up and of the visual scene.", "They exceeded the theoretical lean angles, but avoided leaning the mock-up beyond 10\u00b0.", "Heterogeneity was found in the direction of roll applied to the visual scene.", "Implications of the results for future motorcycle simulations are discussed."], "abstract": ["This study aimed at identifying the roll motion parameters of a motorcycle simulator prototype. Experienced motorcyclists tuned the angular physical movement of the mock-up and that of the visual scene to achieve an optimal riding experience during curves. The participants exceeded the rolling angles that would be required in real-world riding, while avoiding leaning the mock-up beyond 10\u00b0. In addition, they were more influenced by the speed of the virtual motorcycle than by road curvature, especially in a wide field of view. Heterogeneity was found in the roll applied to the visual scene. The overall patterns suggest that at least when washout is not applied to remove the side forces that in real-world riding are compensated by a centrifugal force, greater roll of the visual at the expense of the mock-up is mandatory to avoid performance biases that might be enhanced due to fear of falling off the simulator. Future roll motion models must take into consideration factors such as riding postures, which might not only influence the forces operating on the rider-motorcycle system, but also how motorcyclists perceive the visual world."]},
{"title": "Do the coach and athlete have the same \u00abpicture\u00bb of the situation? Distributed Situation Awareness in an elite sport context", "highlights": ["We presented a method to assess the consistency of coach-athlete situation awareness.", "Results revealed similarities and important differences between the athlete and coach.", "Results suggest that SA information elements showed some common ground.", "Coaches and athletes framed information elements differently to make sense of situation."], "abstract": ["Athletes and their coach interpret the training situations differently and this can have important implications for the development of an elite athlete's performance. It is argued that, from a schema-theoretic perspective, the difference in these interpretations needs to be better understood. A post-performance, self-confrontation, interview was conducted with a number of athletes and their coaches. The interviews revealed differences between the athlete and their coach in the information they are aware of. In comparison with athletes, coaches more frequently compared the phenotype with genotype schemata rather than just describing the phenotype schemata. Results suggest SA information elements showed some common ground but also revealed some important differences between the athlete and coach. The awareness was directed externally towards the environment and internally, towards the individual, depending on his/her role. The investigation showed that the schemata used to \u2018frame\u2019 the information elements were different, but compatible, between athlete and coach."]},
{"title": "A four-year follow-up study of physical working conditions and perceived mental and physical strain among food industry workers", "highlights": ["Deteriorating physical working conditions increases physical and mental strain among food industry workers.", "The change in physical strain was associated with the change in mental strain.", "Younger employees were mostly affected due to poorer ergonomic factors at work."], "abstract": ["This study hypothesized that in a longitudinal setting deteriorating physical working conditions increases the perceived physical and mental strain among food processing employees. The study was conducted in 2003 and 2007. It examined 248 blue-collar workers, all of whom were in the same occupation throughout the entire follow-up period. The data were obtained through a structural questionnaire distributed to the employees at the workplace. Mental strain had increased (7%) significantly among younger employees during the follow-up. The changes in mental strain for the younger employees were positively associated with the changes in physical strain. The changes in physical strain were also significantly associated with the changes in physical working conditions among both younger and the older workers. The results of this study partly support the study hypothesis, namely that deteriorating physical working condition increases physical strain and also increases mental strain, especially among younger employees."]},
{"title": "Physical workload, trapezius muscle activity, and neck pain in nurses' night and day shifts: A physiological evaluation", "highlights": ["Trapezius muscle activity and workload were assessed over full day and night shifts in nurses.", "During day and night shifts, heart rate was very high compared to resting values.", "Physical workload and load on the trapezius muscle were smaller during night than during day shifts.", "Perception of neck pain and mental well-being at work were similar during day and night shifts.", "Night shifts were perceived to be as burdensome as day shifts despite the smaller physical workload."], "abstract": ["The purpose of this study was to compare physical workload, electromyography (EMG) of the trapezius muscle, neck pain and mental well-being at work between night and day shifts in twenty Swiss nurses. Work pulse (average increase of heart rate over resting heart rate) was lower during night (27\u00a0bpm) compared to day shifts (34\u00a0bpm; ", "\u00a0<\u00a00.01). Relative arm acceleration also indicated less physical activity during night (82% of average) compared to day shifts (110%; ", "\u00a0<\u00a00.01). Rest periods were significantly longer during night shifts. Trapezius muscle rest time was longer during night (13% of shift duration) than day shifts (7%; ", "\u00a0<\u00a00.01) and the 50th percentile of EMG activity was smaller (", "\u00a0=\u00a00.02), indicating more opportunities for muscle relaxation during night shifts. Neck pain and mental well-being at work were similar between shifts. Subjective perception of burden was similar between shifts despite less physical burden at night, suggesting there are other contributing factors."]},
{"title": "New chording text entry methods combining physical and virtual buttons on a mobile phone", "highlights": ["We designed two new chording text entry methods, namely MagArea and MemoryTap.", "The two presented methods used with both physical and virtual keys simultaneously.", "MemoryTap got higher text entry speed than virtual keyboard after three days of training.", "Both methods revealed lower error rates than virtual keyboard.", "Both methods had higher learning efficiency than previous chording methods."], "abstract": ["Traditional mobile phones depend on MultiTap, virtual or physical QWERTY keyboard for text entry, and they had some respective drawbacks include low input performance, occupying too large an area, high error rates, lack of feedbacks, etc. Therefore, some researches utilized the characteristics of the chording keyboard to improve input performance. Yet, as the learning efficiency of the chording keyboard is too low, users are not highly willing to learn. In view of that, this study combines the physical and virtual keys, and develops two chording input methods, MagArea and MemoryTap. After three days of learning, the fourteen experiment participants show effectively reduce error rates on MagArea, and they enhance their input speed on MemoryTap. In addition, excellent learning efficiency is found in the two methods, will be more motivated and willing to employ."]},
{"title": "Analysis of the resilience of team performance during a nuclear emergency response exercise", "highlights": ["A cognitive task analysis (CTA) performed during a nuclear disaster simulation.", "The simulation was part of a national emergency response training.", "We describe sources of resilience and brittleness in the response activities.", "Understand cognitive activities' dimensions to identify crisis management patterns.", "The results indicated strengths and weakness in nuclear emergency response exercises."], "abstract": ["The current work presents results from a cognitive task analysis (CTA) of a nuclear disaster simulation. Audio-visual records were collected from an emergency room team composed of individuals from 26 different agencies as they responded to multiple scenarios in a simulated nuclear disaster. This simulation was part of a national emergency response training activity for a nuclear power plant located in a developing country. The objectives of this paper are to describe sources of resilience and brittleness in these activities, identify cues of potential improvements for future emergency simulations, and leveraging the resilience of the emergency response system in case of a real disaster. Multiple CTA techniques were used to gain a better understanding of the cognitive dimensions of the activity and to identify team coordination and crisis management patterns that emerged from the simulation exercises."]},
{"title": "Safety climate, hardiness, and musculoskeletal complaints: A mediated moderation model", "highlights": ["Psychological safety climate was not directly related to WRMSD complaints.", "Psychological safety climate was negatively related to frustration.", "Frustration was positively related to WRMSD complaints.", "Hardiness accentuated the psychological safety climate\u2013frustration relationship.", "Safety climate had an indirect effect on WRMSD complaints via frustration."], "abstract": ["This study explores the mechanisms linking the psychosocial characteristics of the workplace with employees' work-related musculoskeletal complaints. Poor safety climate perceptions represent a stressor that may elicit frustration, and subsequently, increase employees' reports of musculoskeletal discomforts. Results from an employee sample supported that when employees' perceived safety was considered a priority, they experienced less frustration and reported fewer work-related upper body musculoskeletal symptoms. Psychological hardiness, a personality trait that is indicative of individuals' resilience and success in managing stressful circumstances, moderated these relationships. Interestingly, employees with high hardiness were more affected by poor safety climate."]},
{"title": "The stochastic distribution of available coefficient of friction for human locomotion of five different floor surfaces", "highlights": ["Available coefficient of friction (ACOF) is critical in slip incidents.", "Stochastic distribution of ACOF is critical in predicting slip probability.", "Three of 15 cases had a statistically good match with commonly-used distributions.", "Seven of 15 cases could, for practical purposes, be represented by constants.", "Two cases could, for practical purposes, be represented by normal distributions."], "abstract": ["The maximum coefficient of friction that can be supported at the shoe and floor interface without a slip is usually called the available coefficient of friction (ACOF) for human locomotion. The probability of a slip could be estimated using a statistical model by comparing the ACOF with the required coefficient of friction (RCOF), assuming that both coefficients have stochastic distributions. An investigation of the stochastic distributions of the ACOF of five different floor surfaces under dry, water and glycerol conditions is presented in this paper. One hundred friction measurements were performed on each floor surface under each surface condition. The Kolmogorov\u2013Smirnov goodness-of-fit test was used to determine if the distribution of the ACOF was a good fit with the normal, log-normal and Weibull distributions. The results indicated that the ACOF distributions had a slightly better match with the normal and log-normal distributions than with the Weibull in only three out of 15 cases with a statistical significance. The results are far more complex than what had heretofore been published and different scenarios could emerge. Since the ACOF is compared with the RCOF for the estimate of slip probability, the distribution of the ACOF in seven cases could be considered a constant for this purpose when the ACOF is much lower or higher than the RCOF. A few cases could be represented by a normal distribution for practical reasons based on their skewness and kurtosis values without a statistical significance. No representation could be found in three cases out of 15."]},
{"title": "Assessment of a simple obstacle detection device for the visually impaired", "highlights": ["Obstacle detection is a major problem to be solved to ensure safe navigation for the visually impaired.", "Static obstacles that are not located on the ground are hardly detected by white cane.", "The obstacle detector developed might increase mobility performance and was well accepted by visually impaired participants."], "abstract": ["A simple obstacle detection device, based upon an automobile parking sensor, was assessed as a mobility aid for the visually impaired. A questionnaire survey for mobility needs was performed at the start of this study. After the detector was developed, five blindfolded sighted and 15 visually impaired participants were invited to conduct travel experiments under three test conditions: (1) using a white cane only, (2) using the obstacle detector only and (3) using both devices. A post-experiment interview regarding the usefulness of the obstacle detector for the visually impaired participants was performed. The results showed that the obstacle detector could augment mobility performance with the white cane. The obstacle detection device should be used in conjunction with the white cane to achieve the best mobility speed and body protection."]},
{"title": "Understanding workplace violence: The value of a systems perspective", "highlights": ["Examines the workplace violence experience of 86 New Zealand organisations.", "Over 50% of respondents reported violence cases in their organisation.", "Perpetrators evenly split between co-workers and external sources such as patients.", "Highest risk factor ratings reported for interpersonal and organisational factors.", "Most organisations relied on single violence control measures."], "abstract": ["Workplace violence is a leading form of occupational injury and fatality, but has received little attention from the ergonomics research community. The paper reports findings from the 2012 New Zealand Workplace Violence Survey, and examines the workplace violence experience of 86 New Zealand organisations and the perceptions of occupational health and safety professionals from a systems perspective. Over 50% of respondents reported violence cases in their organisation, with perpetrators evenly split between co-workers and external sources such as patients. Highest reported levels of violence were observed for agriculture, forestry and construction sectors. Highest risk factor ratings were reported for interpersonal and organisational factors, notably interpersonal communication, time pressure and workloads, with lowest ratings for environmental factors. A range of violence prevention measures were reported, although most organisations relied on single control measures, suggesting unmanaged violence risks were common among the sample."]},
{"title": "Evaluation of the safety and usability of touch gestures in operating in-vehicle information systems with visual occlusion", "highlights": ["We investigate the applicability of touch gestures in IVISs from the viewpoints of both driving safety and usability.", "Panning gesture is the only touch gesture that can be applicable to operate IVISs while driving.", "Flicking gesture is likely to be used if the screen moving speed is slower or if the car is in heavy traffic.", "Pinching gesture might not be appropriate to operate IVISs while driving in the safety and usability aspects."], "abstract": ["Nowadays, many automobile manufacturers are interested in applying the touch gestures that are used in smart phones to operate their in-vehicle information systems (IVISs). In this study, an experiment was performed to verify the applicability of touch gestures in the operation of IVISs from the viewpoints of both driving safety and usability. In the experiment, two devices were used: one was the Apple iPad, with which various touch gestures such as flicking, panning, and pinching were enabled; the other was the SK EnNavi, which only allowed tapping touch gestures. The participants performed the touch operations using the two devices under visually occluded situations, which is a well-known technique for estimating load of visual attention while driving.", "In scrolling through a list, the flicking gestures required more time than the tapping gestures. Interestingly, both the flicking and simple tapping gestures required slightly higher visual attention. In moving a map, the average time taken per operation and the visual attention load required for the panning gestures did not differ from those of the simple tapping gestures that are used in existing car navigation systems. In zooming in/out of a map, the average time taken per pinching gesture was similar to that of the tapping gesture but required higher visual attention. Moreover, pinching gestures at a display angle of 75\u00b0 required that the participants severely bend their wrists. Because the display angles of many car navigation systems tends to be more than 75\u00b0, pinching gestures can cause severe fatigue on users' wrists. Furthermore, contrary to participants' evaluation of other gestures, several participants answered that the pinching gesture was not necessary when operating IVISs.", "It was found that the panning gesture is the only touch gesture that can be used without negative consequences when operating IVISs while driving. The flicking gesture is likely to be used if the screen moving speed is slower or if the car is in heavy traffic. However, the pinching gesture is not an appropriate method of operating IVISs while driving in the various scenarios examined in this study."]},
{"title": "Exploring physical exposures and identifying high-risk work tasks within the floor layer trade", "highlights": ["Most (91%) floor layers met caution risk levels of physical exposures daily.", "Prolonged awkward postures and high force exposures occurred in multiple body parts.", "Besides kneeling, poor neck and low back postures are common in floor layers.", "Upper extremity exposures occur from 10 to 30% of the work day.", "Hand, wrist, and shoulder exposure levels differ by the type of material installed."], "abstract": ["Floor layers have high rates of musculoskeletal disorders yet few studies have examined their work exposures. This study used observational methods to describe physical exposures within floor laying tasks.", "We analyzed 45 videos from 32 floor layers using Multimedia-Video Task Analysis software to determine the time in task, forces, postures, and repetitive hand movements for installation of four common flooring materials. We used the WISHA checklists to define exposure thresholds.", "Most workers (91%) met the caution threshold for one or more exposures. Workers showed high exposures in multiple body parts with variability in exposures across tasks and for different materials. Prolonged exposures were seen for kneeling, poor neck and low back postures, and intermittent but frequent hand grip forces.", "Floor layers experience prolonged awkward postures and high force physical exposures in multiple body parts, which probably contribute to their high rates of musculoskeletal disorders."]},
{"title": "Comprehensibility of universal healthcare symbols for wayfinding in\u00a0healthcare facilities", "highlights": ["Tested 14 universal healthcare symbols in the United States, Korea, and Turkey.", "Some signs comprehended in all three countries with high percentages.", "Significant difference of comprehension with Pharmacy sign across countries.", "Age had significant relationships with some sign comprehensions."], "abstract": ["Healthcare facilities are often complex and overwhelming for visitors, and wayfinding in healthcare facilities can be challenging. As there is an increasing number of global citizens who travel to seek medical care in another country, it is critical to make wayfinding easy for visitors who are not familiar with the language in a foreign country. Among many wayfinding aids, symbols are helpful for those visitors who have limited ability to understand written language. This study tested universal healthcare symbols in the United States, South Korea, and Turkey to compare the comprehension of symbols cross-country and identify predictors of the correct comprehension. To explore statistically significant relationships between symbol comprehension and countries, Pearson's Chi-square tests, logistic regression, and ANOVA were conducted. The test results showed that ten symbols among 14 tested have significant relationship with countries. Results of this study demonstrate that symbol comprehension can be varied significantly in different countries."]},
{"title": "What roles do team climate, roster control, and work life conflict play in shiftworkers' fatigue longitudinally?", "highlights": ["This study highlights the value and difficulties of longitudinal nursing research.", "Control over shift scheduling can have significant effects on fatigue for both two-shift and three-shift workers.", "Fatigue increased over time when there was a lack of control over rosters.", "A positive team climate reduces fatigue for 2-shift but not 3-shift workers.", "Work life conflict was the strongest predictor of concurrent fatigue, but not over time."], "abstract": ["The study aimed to examine shiftworkers fatigue and the longitudinal relationships that impact on fatigue such as team climate, work life conflict, control of shifts and shift type in shift working nurses. We used a quantitative survey methodology and analysed data with a moderated hierarchical multiple regression. After matching across two time periods 18 months apart, the sample consisted of 166 nurses from one Australian hospital. Of these nurses, 61 worked two rotating day shifts (morning & afternoon/evening) and 105 were rotating shiftworkers who worked three shifts (morning afternoon/evening and nights). The findings suggest that control over shift scheduling can have significant effects on fatigue for both two-shift and three-shift workers. A significant negative relationship between positive team climate and fatigue was moderated by shift type. At both Time 1 and Time 2, work life conflict was the strongest predictor of concurrent fatigue, but over time it was not."]},
{"title": "The impact of display angles on the legibility of Sans-Serif 5\u00a0\u00d7\u00a05 capitalized letters", "highlights": ["A laboratory study on quantifying the impact of display angles on legibility of text.", "Tested Sans-Serif 5\u00a0\u00d7\u00a05 Capitalized Letters C, D, E, F, H, K, N, P, R, U, V, and Z.", "Examined a complete range of possible display angles 0.0\u00b0\u201390.0\u00b0.", "Derived two equations and modified an existing legibility index.", "Corrected the interference of people's normal reading habit on legibility evaluation."], "abstract": ["This paper introduced a laboratory study on quantifying the impact of display angles 0.0\u00b0\u201390.0\u00b0 on the legibility of Sans-Serif 5\u00a0\u00d7\u00a05 Capitalized Letters C, D, E, F, H, K, N, P, R, U, V, and Z, commonly used for acuity tests. This study addressed three issues not tackled in the previous studies, including (a) extremely large incident angles 82.8\u00b0\u201390\u00b0, (b) multiple letters other than a single letter E previously used, and (c) the interference of people's normal reading habit on legibility evaluation. A total of 20 young college students with Snellen acuity 20/20 or better and normal color vision participated in this experiment. They were asked to read viewing materials presented at 15 display angles. This study derived two equations and modified an existing legibility index to correct the interference of people's normal reading habit on legibility evaluation at extreme oblique display angles."]},
{"title": "Worker evaluation of a macroergonomic intervention in a Brazilian footwear company", "highlights": ["Problems and solutions were effectively identified by participative ergonomics.", "Participative ergonomics facilitated the implementation of changes.", "Better work organization improves worker satisfaction.", "Worker empowerment influenced the positive outcomes of the intervention.", "A flexible work organization is more suitable for shoe manufacturing."], "abstract": ["This article presents a macroergonomic intervention in a Brazilian footwear company and its evaluation by the workers. Using participatory ergonomics, the traditional Taylor/Ford production system was transformed into a socio-technical one and tested by 100 volunteers working during 3.5 years in a pilot production line. Multiskilling and teamwork were the major changes promoted to enlarge and enrich work and make it more flexible. The workers' evaluation pre- and post-intervention showed an increase in overall satisfaction with the work and more commitment to the results and company targets.", "This study showed that problems and solutions can be identified through participatory ergonomics, that it is easier to involve workers than the managerial staff, and that a macroergonomic intervention, mainly focusing on work organization, led to positive personnel, health and production outcomes, despite management\u2019s resistance to changes."]},
{"title": "Paramedics on the job: Dynamic trunk motion assessment at the workplace", "highlights": ["Postural load of paramedics on the job was examined using direct field measurements.", "The highest trunk angles in the three planes were observed during lifting activities.", "Peak velocities of the trunk were observed during ambulance transport.", "Increased trunk motion exposure was observed in urgent and high-demand situations.", "Paramedics adopted trunk motions that may increase the risk of low back disorders."], "abstract": ["Many paramedics' work accidents are related to physical aspects of the job, and the most affected body part is the low back. This study documents the trunk motion exposure of paramedics on the job. Nine paramedics were observed over 12 shifts (120", "\u00a0", "h). Trunk postures were recorded with the computer-assisted CUELA measurement system worn on the back like a knapsack. Average duration of an emergency call was 23.5", "\u00a0", "min. Sagittal trunk flexion of >40\u00b0 and twisting rotation of >24\u00b0 were observed in 21% and 17% of time-sampled postures. Medical care on the scene (44% of total time) involved prolonged flexed and twisted postures (\u223c10", "\u00a0", "s). The highest extreme sagittal trunk flexion (63\u00b0) and twisting rotation (40\u00b0) were observed during lifting activities, which lasted 2% of the total time. Paramedics adopted trunk motions that may significantly increase the risk of low back disorders during medical care and patient-handling activities."]},
{"title": "Task-specific performance effects with different numeric keypad layouts", "highlights": ["Numeric data entry using calculator and telephone keypads was examined.", "Memory for calculator layouts was worse than telephones or ATMs.", "Participants were more accurate when entering stimuli using the calculator keypad.", "The slowest responses occurred for numeric stimuli with a telephone keypad layout."], "abstract": ["Two commonly used keypad arrangements are the telephone and calculator layouts. The purpose of this study was to determine if entering different types of numeric information was quicker and more accurate with the telephone or the calculator layout on a computer keyboard numeric keypad. Fifty-seven participants saw a 10-digit numeric stimulus to type with a computer number keypad as quickly and as accurately as possible. Stimuli were presented in either a numerical [1,234,567,890] or phone [(123) 456-7890] format. The results indicated that participants' memory of the layout for the arrangement of keys on a telephone was significantly better than the layout of a calculator. In addition, the results showed that participants were more accurate when entering stimuli using the calculator keypad layout. Critically, participants' response times showed an interaction of stimulus format and keypad layout: participants were specifically slowed when entering numeric stimuli using a telephone keypad layout. Responses made using the middle row of keys were faster and more accurate than responses using the top and bottom row of keys. Implications for keypad design and cell phone usage are discussed."]},
{"title": "Comparison of subjective comfort ratings between anatomically shaped and cylindrical handles", "highlights": ["Anatomically shaped handle based on the hand shape in an optimal power grasp posture.", "Comparison between anatomically shaped tool handle and optimal cylindrical handle.", "Anatomically shaped handle provides higher comfort ratings than cylindrical handle.", "Handle shape has significant impact on user subjective comfort rating."], "abstract": ["Most authors have provided diameter recommendations for cylindrical handle design in order to increase performance, avoid discomfort, and reduce the risk of cumulative trauma disorders. None of the studies has investigated the importance of determining the correct handle shape on the subjective comfort ratings, which could further improve the handles' ergonomics. Therefore, new methods based on a virtual hand model in its optimal power grasp posture have been developed in order to obtain customised handles with best fits for targeted subjects. Cylindrical and anatomically shaped handles were evaluated covering ten subjects by means of an extensive subjective comfort questionnaire. The results suggest large impact of the handle shape on the perceived subjective comfort ratings. Anatomically shaped handles were rated as being considerably more comfortable than cylindrical handles for almost all the subjective comfort predictors. They showed that handle shapes based on optimal power grasp postures can improve subjective comfort ratings, thus maximising performance. Future research should consider real conditions, since the comfort ratings can vary based on the specific task and by the tool selected for the task."]},
{"title": "Using Kinect\u2122 sensor in observational methods for assessing postures at work", "highlights": ["This paper examines the potential use of Kinect\u2122 range sensors in observational methods for assessing postural loads.", "The results obtained by human observers are compared with those obtained by the sensor.", "The influence of the position of the sensor with respect to the tracked user is analyzed.", "High agreement exists between human observers and the sensor when the tracked subject stands facing the sensor.", "The orientation of the sensor with respect to the worker affects the sensor's ability to identify the body positions."], "abstract": ["This paper examines the potential use of Kinect\u2122 range sensor in observational methods for assessing postural loads. Range sensors can detect the position of the joints at high sampling rates without attaching sensors or markers directly to the subject under study. First, a computerized OWAS ergonomic assessment system was implemented to permit the data acquisition from Kinect\u2122 and data processing in order to identify the risk level of each recorded postures. Output data were compared with the results provided by human observers, and were used to determine the influence of the sensor view angle relative to the worker. The tests show high inter-method agreement in the classification of risk categories (Proportion agreement index\u00a0=\u00a00.89 ", "\u00a0=\u00a00.83) when the tracked subject is facing the sensor. The camera's point of view relative to the position of the tracked subject significantly affects the correct classification of the postures. Although the results are promising, some aspects involved in the use of low-cost range sensors should be further studied for their use in real environments."]},
{"title": "Applying hierarchical task analysis to improving the patient positioning for direct lateral interbody fusion in spinal surgery", "highlights": ["The paper integrates HTA/EHI with business process management.", "Our approach identifies potential errors and selects alternative practices.", "We use patient positioning for spinal surgery as illustrative case.", "Observation was conduct to gain knowledge on current practices.", "Our approach is useful for novice and professional surgeons."], "abstract": ["The present\u00a0study brings together for the first time the techniques of hierarchical task analysis (HTA), human error identification (HEI), and business process management (BPM) to select practices that can eliminate or reduce potential errors in a surgical setting. We applied the above approaches to the improvement of the patient positioning process for lumbar spine surgery referred to as \u2018direct lateral interbody fusion\u2019 (DLIF). Observations were conducted to gain knowledge on current DLIF positioning practices, and an HTA was constructed. Potential errors associated with the practices specific to DLIF patient positioning were identified. Based on literature review and expert views alternative practices are proposed aimed at improving the DLIF patient positioning process. To our knowledge, this is the first attempt to use BPM in association with HEI/HTA for the purpose of improving the performance and safety of a surgical process \u2013 with promising results."]},
{"title": "The effects of transfer distance on spine kinematics when placing boxes at different heights", "highlights": ["Twisting and lateral bending observed during lateral transfer tasks have been associated with increased risk of back injuries.", "Physical separation of the lift origin and destination encourages stepping and turning the body rather than twisting the spine.", "The current research explored the relationship between transfer distance and spine motion during a simulated palletizing task.", "Separating the lift origin and lift destination by 1\u20131.25 m yielded the best balance between spine kinematics and productivity."], "abstract": ["Twisting and lateral bending motions in repetitive lifting tasks are associated with occupational low back injuries and can be challenging to reduce with engineering controls. This study tested the hypothesis that twisting and lateral bending can be reduced by changing the transfer distance. Eighteen males, with no material handling experience lifted 10.9\u00a0kg boxes from 0.9\u00a0m above the floor and placed the boxes at a destination located 0.50, 0.75, 1.00, 1.25, 1.50, or 1.75\u00a0m away and at heights of 0.5\u00a0m, 0.9\u00a0m, and 1.3\u00a0m above the floor. Overall, twisting and forward bending decreased with increased transfer distance when placing the box. Conversely, the lateral bending when lifting and placing the box increased with increasing transfer distance. In short, having a transfer distance between 1 and 1.25\u00a0m when performing palletizing tasks to different heights may optimally balance spine kinematics, back injury risk, and productivity measures."]},
{"title": "Recovery and well-being among Helicopter Emergency Medical Service (HEMS) pilots", "highlights": ["Helicopter Emergency Medical Service pilots reported high levels of well-being.", "Well-being decreased during night shifts.", "The association between disruptive events and decreased well-being was intensified during night shifts.", "It took a longer time to recover from night shifts than from day shifts."], "abstract": ["This study investigated the effects of a compressed working week with high cognitive and emotional work demands within the population of Dutch Helicopter Emergency Medical Service (HEMS) pilots. Work stressors were measured and levels of well-being were examined before, during and after a series of day and night shifts. Results revealed that (i) the start of a series of day shifts was more taxing for well-being than the start of a series of night shifts, (ii) there were no differences in the decrease in well-being during day and night shifts, (iii) distress during shifts was more strongly related to a decrease in well-being during night than during day shifts and (iv) it took HEMS pilots more time to recover from a series of night shifts than from a series of day shifts. It is concluded that HEMS pilots should not start earlier during day shifts, nor have longer series of night shifts."]},
{"title": "Matching performance of vehicle icons in graphical and textual formats", "highlights": ["The experienced drivers had significantly better recognition accuracy.", "The image-related format is recommended for use in the design of vehicle icons.", "Textual icon can be used for functions describable using simple English and for users with English reading ability."], "abstract": ["The current research classified 82 vehicle icons into seven categories (image-related, concept-related, semi-abstract, arbitrary, abbreviation, word, and combined) for their matching accuracy, matching sequence, and matching time. These data can be compared and used as a framework for future icon development. Forty participants, all with a university degree, took part in this experiment. Half of the participants had intensive driving experience, while the other half never driven a car. The results indicated that on average, word icons had a significantly greater matching accuracy than the other icon formats; ranging from 4.7 to 20.8% difference. Regarding the matching sequence, participants matched image-related icons before other icon formats. Arbitrary and combined icons took significantly longer to match than other icon formats by 1.4\u20136.2\u00a0s. Based on the high matching accuracy (86.3%) and high ratings on subjective design features, word format can be used for functions describable using simple English for users with English reading ability. Confusion matrices showed that 63.2% of the misunderstandings were caused by similarity in format or function."]},
{"title": "Safety intelligence: An exploration of senior managers' characteristics", "highlights": ["Senior leaders have a particular role for organisational safety.", "The relevance of senior managers' attributes for safety intelligent management is explored.", "Responses suggest social competence and safety knowledge as especially relevant."], "abstract": ["Senior managers can have a strong influence on organisational safety. But little is known about which of their personal attributes support their impact on safety. In this paper, we introduce the concept of \u2018safety intelligence\u2019 as related to senior managers' ability to develop and enact safety policies and explore possible characteristics related to it in two studies. Study 1 (", "\u00a0=\u00a076) involved direct reports to chief executive officers (CEOs) of European air traffic management (ATM) organisations, who completed a short questionnaire asking about characteristics and behaviours that are ideal for a CEO's influence on safety. Study 2 involved senior ATM managers (", "\u00a0=\u00a09) in various positions in interviews concerning their day-to-day work on safety. Both studies indicated six attributes of senior managers as relevant for their safety intelligence, particularly, social competence and safety knowledge, followed by motivation, problem-solving, personality and interpersonal leadership skills. These results have recently been applied in guidance for safety management practices in a White Paper published by EUROCONTROL."]},
{"title": "Context adaptable driver information \u2013 Or, what do whom need and want when?", "highlights": ["As guideline for design of context adaptive driver information systems or for optimization of display space.", "As a weight when evaluating future adaptive information systems.", "When deciding whether a function should be activated automatically or manually."], "abstract": ["This study deals with a first step towards context adaptive functionality of a Driver Information System.", "Driving a car is a complex task for which the driver needs appropriate information to fulfil his or her goals. New technologies enable adaptability to driver state, task, personality etcetera and also to the context.", "The aim of this study was therefore to investigate what information people perceive that they need and want from the car in different contexts and to what extent there is consensus about the function. A new methodology was developed, and 33 private car drivers were interviewed and asked to rate a number of possible abstract functions in a car in different contexts.", "It was shown that people need and want different types of information in different contexts. It was furthermore indicated that there is sometimes a difference in drivers' opinions about what should be presented by the car and that there is varying consensus over different functions in different contexts. The rating result was illustrated by an easily perceived Context Function Matrix. The results may be used in the design of a context adaptive driver information system."]},
{"title": "Comparison of firefighters and non-firefighters and the test methods used regarding the effects of personal protective equipment on individual mobility", "highlights": ["We compared the mobility with PPE using pilot test methods consisting of a balance test and a physical performance test.", "18 participants (nine professional firefighters and nine untrained males) performed the pilot test method.", "Significant differences between PPEs and CON for the test results from firefighters were found.", "The participant group with more significance for evaluation of PPE and that was physically appropriate to this test method was firefighters."], "abstract": ["The aims of this study were 1) to evaluate the current pilot test method and ascertain reliable measurements for a standard test method of mobility with personal protective equipment (PPE), such as physical performance and balance ability tests; 2) to compare two participant groups (firefighters versus non-firefighters) and to investigate whether non-firefighters are appropriate as a standard participant group in the field of PPE or not. Totally, 18 participants (nine professional firefighters and nine untrained males) performed the current pilot test method consisting of a balance test, completed prior to and after a performance test. Significant differences were found between PPE conditions and CON (the control clothing ensemble: T-shirt, shorts, and running shoes) for the functional balance test, physical performance test, heart rate, and subjective evaluations in firefighters group. Therefore, the present pilot test method is valid as a standard test method for assessing mobility while wearing PPE. Moreover, the present result shows that firefighters are more reliable than non-firefighters in testing of PPE with current test methods."]},
{"title": "A comparison of clutching movements of freely adjusted and imposed pedal configurations for identifying discomfort assessment criteria", "highlights": ["Clutching motions of freely adjusted and imposed pedal configurations were compared.", "Pedal position was adjusted to ensure a good starting pedal position.", "Pedal position adjustment seemed not motivated by reducing joint torque.", "Discomfort ratings were found significantly correlated with knee and ankle torques at the end of depression.", "Relevant biomechanical parameters were identified for defining discomfort indicators."], "abstract": ["This paper focuses on the effects of the free pedal position adjustment on clutching movements of the left lower limb as well as on the perceived discomfort. Six automotive clutch pedal configurations were tested by 20 subjects (5 young females, 5 young males, 5 older females, 5 older males) using a multi-adjustable experimental mock-up. Results showed that the pedal position was adjusted to ensure a good starting pedal position allowing a less flexed ankle and avoiding unnecessary leg displacement from the foot rest to the position at start depression. Pedal position adjustment seemed not motivated by reducing joint torque though discomfort ratings were found significantly correlated with knee and ankle torques at the end of depression. The present work also illustrates that the less-constrained motion concept is helpful for a better understanding of people preference and useful for identifying motion-related biomechanical parameters to be considered for defining assessment criteria."]},
{"title": "Exploring implicit preventive strategies in prehospital emergency workers: A novel approach for preventing back problems", "highlights": ["A Majority of ambulance professionals had experienced back pain.", "Ambulance professionals use strategies and tricks of the trade to reduce the chances of back strain.", "Use of preventive strategies was associated with less back symptoms."], "abstract": ["Back problems are a major occupational health issue for prehospital emergency care professionals. The goals of this article are to: 1) provide descriptive data about the prevalence and the severity of lower back and upper back disorders in EMTs and paramedics; 2) identify some individual and collective strategies used by EMTs and paramedics to protect their health as they perform prehospital emergency missions; 3) assess the possible effectiveness of strategies in preventing back problems by exploring associations between the use of strategies and the presence and severity of symptoms.", "The method includes a questionnaire survey (sample ", "\u00a0=\u00a0334; paramedics and emergency medical technicians) and ergonomics work practice analysis involving shadowing ambulance crews in 12 medical emergency services (over 400\u00a0h).", "A majority of ambulance professionals had experienced back pain in the twelve-month period before the survey. Work practice analysis revealed strategies and tricks of the trade used by ambulance professionals to reduce the chances of back strain while working. Multiple regression analyses showed that self-reported use of such strategies was associated with fewer back symptoms.", "Preventive strategies should be integrated into specialised training programs for prehospital medical emergency professionals. This approach could also be used in other work settings."]},
{"title": "Cardio-respiratory and subjective strains sustained by paraplegic subjects, when travelling on a cross slope in a manual wheelchair (MWC)", "highlights": ["Strains of MWC displacements increase significantly and linearly with cross slope.", "Subjective evaluation of paraplegics underestimates strains and can induce over exertion.", "Injury level is a strong determinant of displacement strains and strategy.", "2% Cross slope is an effective limit for the less fit MWC users."], "abstract": ["The aim of this study was to quantify cardiac, energetic and subjective strains during manual wheelchair (MWC) travel on cross slopes (Cs). 25 paraplegics achieved eight 300\u00a0m propulsion tests combining 4 Cs (0, 2, 8 and 12%) and 2 velocities (Vi\u00a0=\u00a00.97\u00a0m\u00a0s", ", Vc \u201ccomfortable\u201d). Heart rate and oxygen uptake were recorded continuously. Subjective rating (RPE) was made on completion of each test. Vc exceeds Vi for all Cs. Cardiac and energetic strains at Vc also exceed those at Vi (", "\u00a0<\u00a00.01). Mean cardiac cost (in bpm) at Vc is 34 (SD\u00a0=\u00a013) bpm for a 0/2% Cs and 55 (18) bpm for a 12% Cs. Mean energetic cost (in J\u00a0m", "\u00a0kg", ") is 1.20 (0.38) and 2.76 (0.97) for respectively 0/2% and 12% Cs at Vi and, at Vc 1.50 (0.43) and 3.37 (1.43) for 0/2% and 12% Cs respectively. Subjective rating was considered as moderate for a 12% Cs. MWC users with high level injuries travel faster as those with low level injuries. Strain increase is linear for Cs from 0% to 12%. The results suggest that 2% Cs is generally acceptable, while 8% would be a critical threshold."]},
{"title": "Determinants of maximal oxygen uptake (VO", "highlights": ["We evaluate the current daily practice of aerobic capacity testing in Belgian fire fighters.", "The impact of personal and test-related parameters on the outcome has been evaluated.", "Test-related parameters have to be taken into account when interpreting VO", " results.", "Standardization of aerobic capacity testing is needed in the medical evaluation of fire fighters."], "abstract": ["The aim of this study was to evaluate current daily practice of aerobic capacity testing in Belgian fire fighters. The impact of personal and test-related parameters on the outcome has been evaluated.", "Maximal oxygen uptake (VO", ") results of 605 male fire fighters gathered between 1999 and 2010 were analysed. The maximal cardio respiratory exercise tests were performed at 22 different centres using different types of tests (tread mill or bicycle), different exercise protocols and measuring equipment.", "Mean VO", " was 43.3 (SD\u00a0=\u00a09.8)\u00a0ml/kg.min. Besides waist circumference and age, the type of test, the degree of performance of the test and the test centre were statistically significant determinants of maximal oxygen uptake.", "Test-related parameters have to be taken into account when interpreting and comparing maximal oxygen uptake tests of fire fighters. It highlights the need for standardization of aerobic capacity testing in the medical evaluation of fire fighters."]},
{"title": "Measured and perceived environmental comfort: Field monitoring in an Italian school", "highlights": ["Monitoring of four classrooms and survey administration to children.", "PMV does not always match subjective thermal responses.", "Pupils' classroom position influences thermal sensation and PMV.", "Global ranking considering subjective thermal and visual comfort and air quality.", "Correspondence of pupil sensation and environmental parameters in mid season."], "abstract": ["Microclimatic conditions were recorded in an Italian school and Fanger's indexes PMV and PPD were calculated under different conditions. Students' sensations were investigated four times by means of two surveys, one related to actual microclimatic conditions and one on overall satisfaction, interaction occupant-building and reactions to discomfort. Pupils' classroom position was considered to look for possible influence on thermal comfort: a difference emerged from PMV and the survey, but the results obtained from the two approaches differ for both the entity of discomfort and its distribution within each classroom. Innovative multivariate nonparametric statistical techniques were applied to compare and rank the classrooms in accordance with students' subjective perceptions; a global ranking has been also calculated, considering thermal and visual comfort and air quality. Comparing pupil-sensation-based ranking with environmental parameters no clear correspondence was found, except for mid-season, where PMV, CO", " concentration and desk illuminance were similar in all the classrooms."]},
{"title": "The challenges of delivering validated personas for medical equipment design", "highlights": ["Personas are representations of users, applied to avoid design in isolation.", "The aim was to demonstrate a rigorous, grounded process of constructing personas.", "Checking the content resulted in disparate, possibly incommensurable, views.", "The personas were useful in supporting the elicitation of professional knowledge."], "abstract": ["Representations of archetypal users (personas) have been advocated as a way to avoid designing in isolation. They allow communication of user needs and orient teams towards user experience. One of the challenges for developers of interactive medical devices is that most devices are used by a wide variety of people, and that developers have limited access to those people; personas have the potential to make developers more aware of who they are designing for. But this raises the question of whether it is possible to deliver useful, valid personas of interactive medical device users. The aim of this research was to develop and test a rigorous, empirically grounded process of constructing personas, with the objective of reflecting on that process. Many challenges were encountered: we found that directly linking the personas to a user population was not possible and although personas were a useful tool for supporting communication and elicitation across disciplines, it was hard to make them representative when picking details that were relevant and checking accuracy. Checking the content resulted in disparate, possibly incommensurable, views. Despite this, the personas proved useful in supporting the transfer of knowledge across professional perspectives."]},
{"title": "Influence of pressure-relief insoles developed for loaded gait (backpackers and obese people) on plantar pressure distribution and ground reaction forces", "highlights": ["Backpackers and obese subjects were benefited by using pressure relief insoles.", "One of the insoles better distributed the plantar pressures during the loaded gait.", "The vertical ground reaction force was decreased when one of the insoles was used.", "Different materials in the insoles influenced the plantar pressures and forces.", "Gait biomechanics is different among normal-weight, backpackers and obese subjects."], "abstract": ["The aims of this study were to test the effects of two pressure relief insoles developed for backpackers and obese people on the ground reaction forces (GRF) and plantar pressure peaks during gait; and to compare the GRF and plantar pressures among normal-weight, backpackers, and obese participants. Based on GRF, plantar pressures, and finite element analysis two insoles were manufactured: flat cork-based insole with (i) corkgel in the rearfoot and forefoot (SLS1) and with (ii) poron foam in the great toe and lateral forefoot (SLS2). Gait data were recorded from 21 normal-weight/backpackers and 10 obese participants. The SLS1 did not influence the GRF, but it relieved the pressure peaks for both backpackers and obese participants. In SLS2 the load acceptance GRF peak was lower; however, it did not reduce the plantar pressure peaks. The GRF and plantar pressure gait pattern were different among the normal-weight, backpackers and obese participants."]},
{"title": "L4\u2013L5 compression and anterior/posterior joint shear forces in cabin attendants during the initial push/pull actions of airplane meal carts", "highlights": ["Low back load (L4\u2013L5) during cart acceleration was estimated.", "Working situations creating the highest low back load was identified.", "Effect of handling type, floor type, inclination and cart weight was investigated.", "L4\u2013L5 load was not greater than accepted values for single exertions.", "Effect of decreasing external forces on L4\u2013L5 load was smaller than expected."], "abstract": ["The aim of the present study was to assess the acute low back load of cabin attendants during cart handling and to identify working situations which present the highest strain on the worker. In a setup, 17 cabin attendants (ten females and seven males) pushed, pulled and turned a 20", "\u00a0", "kg standard meal cart (", ":\u00a00.5", "\u00a0", "m", "\u00a0", "\u00d7", "\u00a0", ": 0.3", "\u00a0", "m", "\u00a0", "\u00d7", "\u00a0", ": 0.92", "\u00a0", "m) loaded with extra 20", "\u00a0", "kg and 40", "\u00a0", "kg, respectively on two different surfaces (carpet and linoleum) and at three floor inclinations (\u22122\u00b0, 0\u00b0 and\u00a0+2\u00b0). Two force transducers were mounted as handles. Two-dimensional movement analysis was performed and a 4D WATBAK modelling tool was used to calculate the acute L4\u2013L5 load.", "No working situations created loads greater than the accepted values for single exertions, however compression and anterior/posterior shear forces during pulling and turning were much higher when compared with pushing. There were significant effects of handling the cart on different floor types, at the varying inclinations and with different cart weights. Additionally, when external forces were reduced, the cabin attendants did not decrease push/pull force proportionally and thus the L4\u2013L5 load did not decrease as much as expected."]},
{"title": "Influence of indoor and outdoor temperatures on the fingertip blood flow rate", "highlights": ["A building's air conditioning system could mitigate extremely high blood flow rates.", "In a comfortable environment, blood flow fluctuates between 15", "\u00a0", "TPU and 40", "\u00a0", "TPU.", "When outdoor weather is below 3", "\u00a0", "\u00b0C, warm indoor air can increase microcirculation.", "Remaining calm for a long time indoors could reduce the level of angiogenesis."], "abstract": ["A total of 58 healthy subjects participated to elucidate the influence of indoor and outdoor temperatures on blood flow. After walking outdoors for 20", "\u00a0", "min, the blood flow rate of a subject was measured. The subject then entered a classroom and studied for 120", "\u00a0", "min, and afterwards, the blood flow rate was measured again. The subjects were exposed to outdoor temperature ranging from\u00a0\u22122.5 to 33.7", "\u00a0", "\u00b0C. During the summer, the average blood flow rate after walking outdoors was 45.95", "\u00a0", "\u00b1", "\u00a0", "25.790", "\u00a0", "TPU (tissue perfusion units); after the class, this decreased to 36.14", "\u00a0", "\u00b1", "\u00a0", "21.837", "\u00a0", "TPU (", "\u00a0", "<", "\u00a0", "0.05). During the autumn, the blood flow rate decreased from 27.69", "\u00a0", "\u00b1", "\u00a0", "12.334", "\u00a0", "TPU to 12.47", "\u00a0", "\u00b1", "\u00a0", "12.255", "\u00a0", "TPU (", "\u00a0", "<", "\u00a0", "0.001). When the outside air temperature was below 3", "\u00a0", "\u00b0C, the blood flow rate indoors increased significantly from 6.74", "\u00a0", "\u00b1", "\u00a0", "3.540", "\u00a0", "TPU to 13.95", "\u00a0", "\u00b1", "\u00a0", "11.522", "\u00a0", "TPU (", "\u00a0", "<", "\u00a0", "0.05). In a comfortable and healthy environment, the blood flow rate was not constant but fluctuated between 15", "\u00a0", "TPU and 40", "\u00a0", "TPU."]},
{"title": "Using Failure Mode and Effects Analysis to design a comfortable automotive driver seat", "highlights": ["Automotive seating comfort is developed iteratively, which is expensive.", "DFMEA is proposed as a more efficient alternative.", "Paper includes a step-by-step guide for applying DFMEA to seat comfort development."], "abstract": ["Given enough time and use, all designs will fail. There are no fail-free designs. This is especially true when it comes to automotive seating comfort where the characteristics and preferences of individual customers are many and varied. To address this problem, individuals charged with automotive seating comfort development have, traditionally, relied on iterative and, as a result, expensive build-test cycles. Cost pressures being placed on today's vehicle manufacturers have necessitated the search for more efficient alternatives. This contribution aims to fill this need by proposing the application of an analytical technique common to engineering circles (but new to seating comfort development), namely Design Failure Mode and Effects Analysis (DFMEA). An example is offered to describe how development teams can use this systematic and disciplined approach to highlight potential seating comfort failure modes, reduce their risk, and bring capable designs to life."]},
{"title": "Matching physical work demands with functional capacity in healthy workers: Can it be more efficient?", "highlights": ["Functional capacity and work demands can be matched in most instances for a varying group of workers.", "Normative values for functional capacity are validated and compared to direct workload measurements.", "Normative values can be considered as an efficient tool in clinical decision making, although restrictions apply.", "Comparing functional capacity to normative values enables clinicians to screen for imbalances between work load and capacityt."], "abstract": ["To determine if functional capacity (FC) and physical work demands can be matched and to determine the validity of normative values for FC related to physical work demands as a screening instrument for work ability.", "Forty healthy working subjects were included in this study. Subjects were categorized into four physical work demand categories (sedentary, light, moderate and heavy). FC was tested with a standardized Functional Capacity Evaluation (FCE) following the WorkWell Protocol and physical work demands were determined with an onsite Work Load Assessment (WLA) according to the Task Recording and Analyses on Computer (TRAC) method. Physical work demands were compared to FC and normative values derived from previous research.", "88% of the subjects scored higher on FCE than observed during WLA. The tenth percentile of normative values appeared valid in 98% for sedentary/light work for the subjects tested in this study. For moderate or heavy work, the thirtieth percentile of normative values appeared valid in 78% of all cases.", "Functional capacity and physical work demands can be matched in most instances, but exceptions should be kept in mind with regards to professions classified as moderate or heavy physical work, especially concerning lifting high. Normative values may be considered as an additional screening tool for balancing workload and capacity. It is recommended to further validate normative values in a broader and more extensive working population."]},
{"title": "Display and device size effects on the usability of mini-notebooks (netbooks)/ultraportables as small form-factor Mobile PCs", "highlights": ["The closer the device sizes to the regular sizes, the shorter the operation times.", "Both the effects of screen and input-device sizes affected movement times.", "The 11.6\u2033 and 7\u2033 diagonal screens were most and least preferred, respectively.", "All performances of the 10.1\u2033 model rationalized its domination in the market.", "The 8.9\u2033 model was good in portability and task performances (except for typing)."], "abstract": ["A balance between portability and usability made the 10.1\u2033 diagonal screens popular in the Mobile PC market (e.g., 10.1\u2033 mini-notebooks/netbooks, convertible/hybrid ultraportables); yet no academic research rationalizes this phenomenon. This study investigated the size effects of display and input devices of 4 mini-notebooks (netbooks) ranged in size on their performances in 2 simple and 3 complex applied tasks. It seemed that the closer the display and/or input devices (touchpad/touchscreen/keyboard) sizes to those sizes of a generic notebook, the shorter the operation times (there was no certain phenomenon for the error rates). With non-significant differences, the 10.1\u2033 and 8.9\u2033 mini-notebooks (netbooks) were as fast as the 11.6\u2033 one in almost all the tasks, except for the 8.9\u2033 one in the typing tasks. The 11.6\u2033 mini-notebook (netbook) was most preferred; while the difference in the satisfactions was not significant between the 10.1\u2033 and 11.6\u2033 ones but between the 7\u2033 and 11.6\u2033 ones."]},
{"title": "Applying different equations to evaluate the level of mismatch between students and school furniture", "highlights": ["Fifteen studies and 21 equations to test 6 furniture dimensions were identified.", "There are differences in the results when applying the different equations.", "The interrelations between some equations are based on contradictory criteria.", "The proposed methodology should allow for a more reliable analysis of school furniture."], "abstract": ["The mismatch between students and school furniture is likely to result in a number of negative effects, such as uncomfortable body posture, pain, and ultimately, it may also affect the learning process. This study's main aim is to review the literature describing the criteria equations for defining the mismatch between students and school furniture, to apply these equations to a specific sample and, based on the results, to propose a methodology to evaluate school furniture suitability. The literature review comprises one publications database, which was used to identify the studies carried out in the field of the abovementioned mismatch. The sample used for testing the different equations was composed of 2261 volunteer subjects from 14 schools. Fifteen studies were found to meet the criteria of this review and 21 equations to test 6 furniture dimensions were identified. Regarding seat height, there are considerable differences between the two most frequently used equations. Although seat to desk clearance was evaluated by knee height, this condition seems to be based on the false assumption that students are sitting on a chair with a proper seat height. Finally, the proposed methodology for suitability evaluation of school furniture should allow for a more reliable analysis of school furniture."]},
{"title": "Assessment of early onset of driver fatigue using multimodal fatigue measures in a static simulator", "highlights": ["Quantification and objective measurement of fatigue that driver experiences during prolonged driving.", "Simultaneous recording of multimodal fatigue outcome measures.", "Analyzed both physical and mental aspects of driver fatigue.", "Addressed human factors evaluation of drivers upon driving.", "Methodology detects early on-set of fatigue during simulated driving."], "abstract": ["Driver fatigue is an important contributor to road accidents. This paper reports a study that evaluated driver fatigue using multimodal fatigue measures, i.e., surface electromyography (sEMG), electroencephalography (EEG), seat interface pressure, blood pressure, heart rate and oxygen saturation level. Twenty male participants volunteered in this study by performing 60\u00a0min of driving on a static simulator. Results from sEMG showed significant physical fatigue (", "\u00a0<\u00a00.05) in back and shoulder muscle groups. EEG showed significant (", "\u00a0<\u00a00.05) increase of alpha and theta activities and a significant decrease of beta activity during monotonous driving. Results also showed significant change in bilateral pressure distribution on thigh and buttocks region during the study. These findings demonstrate the use of multimodal measures to assess early onset of fatigue. This will help us understand the influence of physical and mental fatigue on driver during monotonous driving."]},
{"title": "Relationship between meanings, emotions, product preferences and\u00a0personal values. Application to ceramic tile floorings", "highlights": ["Relationships between elements of affective design on floor tiles are analyzed.", "The meanings given to the product can cause the generation of emotions.", "Both meanings and emotions give rise to product preferences.", "Functional meanings are less influential on emotions and preferences.", "Personal values can modify the relationship between meanings and preferences."], "abstract": ["This work aims to validate a conceptual framework which establishes the main relationships between subjective elements in human\u2013product interaction, such as meanings, emotions, product preferences, and personal values. The study analyzes the relationships between meanings and emotions, and between these and preferences, as well as the influence of personal values on such relationships. The study was applied to ceramic tile floorings.", "A questionnaire with images of a neutral room with different ceramic tile floorings was designed and distributed via the web. Results from the study suggest that both meanings and emotions must be taken into account in the generation of product preferences. The meanings given to the product can cause the generation of emotions, and both types of subjective impressions give rise to product preferences. Personal reference values influence these relationships between subjective impressions and product preferences. As a consequence, not only target customers' demographic data but specifically their values and criteria must be taken into account from the beginning of the development process. The specific results of this paper can be used directly by ceramic tile designers, who can better adjust product design (and the subjective impressions elicited) to the target market. Consequently, the chance of product success is reinforced."]},
{"title": "Reliability and validity of the Home Care STAT (Safety Task Assessment Tool)", "highlights": ["Home care workers are a high priority population for safety and health research.", "Home care workers are reliable at self-assessing task exposures.", "Several task exposures were related to daily pain and fatigue outcomes.", "Task exposures may indirectly affect worker health behaviors."], "abstract": ["Home care workers are a priority population for ergonomic assessment and intervention, but research on caregivers' exposures to hazards is limited. The current project evaluated the reliability and validity of an ergonomic self-assessment tool called Home Care STAT (Safety Task Assessment Tool). Participants (", "\u00a0=\u00a023) completed a background survey followed by 10\u201314 days of self-monitoring with the STAT. Results showed that the most frequent task was house cleaning, and that participants regularly performed dangerous manual client moving and transferring tasks. Researcher in-home observations of 14 workers (duration \u22642\u00a0h) demonstrated that workers' self-assessments were moderately reliable. Correlational and multi-level analyses of daily self-assessment data revealed that several task exposures were significantly related to daily fatigue and/or pain. Other associations have implications for Total Worker Health\u2122; for example, daily stress was positively associated with both pain and consumption of high calorie snacks. Findings support the STAT as a reliable and potentially valid tool for measuring home care workers' exposures to physically demanding tasks."]},
{"title": "Investigating shoulder muscle loading and exerted forces during wall painting tasks: Influence of gender, work height and paint tool design", "highlights": ["A novel paint tool had lower muscular demand for females than two existing designs.", "A high work height increased muscular demand compared to low and middle heights.", "Males applied greater force than females with a paint tool.", "Lower average applied paint force was produced at a high work height."], "abstract": ["The task of wall painting produces considerable risk to the workers, both male and female, primarily in the development of upper extremity musculoskeletal disorders. Insufficient information is currently available regarding the potential benefits of using different paint roller designs or the possible adverse effects of painting at different work heights. The aim of this study was to investigate the influence of gender, work height, and paint tool design on shoulder muscle activity and exerted forces during wall painting. Ten young adults, five male and five female, were recruited to perform simulated wall painting at three different work heights with three different paint roller designs while upper extremity muscle activity and horizontal push force were recorded. Results demonstrated that for female participants, significantly greater total average (", "\u00a0=\u00a00.007) and integrated (", "\u00a0=\u00a00.047) muscle activity was present while using the conventional and curly flex paint roller designs compared to the proposed design in which the load was distributed between both hands. Additionally, for both genders, the high working height imposed greater muscular demands compared to middle and low heights. These findings suggest that, if possible, avoid painting at extreme heights (low or high) and that for female painters, consider a roller that requires the use of two hands; this will reduce fatigue onset and subsequently mitigate potential musculoskeletal shoulder injury risks."]},
{"title": "Demand-specific work ability, poor health and working conditions in middle-aged full-time employees", "highlights": ["The prevalence of reduced demand-specific work ability ranged from 9% to 21%.", "Depression was associated with six measures of reduced demand-specific work ability.", "Musculoskeletal pain was associated with four measures of reduced demand-specific work ability.", "Poor health and working conditions did not interact regarding reduced demand-specific work ability."], "abstract": ["We investigated the prevalence of reduced demand-specific work ability, its association with age, gender, education, poor health, and working conditions, and the interaction between poor health and working conditions regarding reduced demand-specific work ability. We used cross-sectional questionnaire data from 3381 full-time employees responding to questions about vocational education, job demands and social support (working conditions), musculoskeletal pain (MSP) and major depression (MD) (poor health) and seven questions about difficulty managing different job demands (", "). Reduced demand-specific work ability varied from 9% to 19% among the 46-year old and from 11% to 21% among the 56-year old. Age was associated with two, gender with four, and education with all measures of reduced demand-specific work ability. MSP was associated with four and MD was associated with six measures of reduced demand-specific work ability. We found no interaction between working conditions and poor health regarding reduced demand-specific work ability."]},
{"title": "Evaluation of four steering wheels to determine driver hand placement in a static environment", "highlights": ["Baby Boomer and Generation-Y adults sat in four vehicles and chose their preferred hand placement.", "Height was significantly correlated with grip diameter preference and 3.2\u00a0cm was the most popular.", "Hand placement was correlated with height, with shorter adults using an asymmetrical grip.", "Hand placement location varies by vehicle and age group.", "Future research should consider more controlled conditions and simulator or on road studies."], "abstract": ["While much research exists on occupant packaging both proprietary and in the literature, more detailed research regarding user preferences for subjective ratings of steering wheel designs is sparse in published literature. This study aimed to explore the driver interactions with production steering wheels in four vehicles by using anthropometric data, driver hand placement, and driver grip design preferences for Generation-Y and Baby Boomers. In this study, participants selected their preferred grip diameter, responded to a series of questions about the steering wheel grip as they sat in four vehicles, and rank ordered their preferred grip design. Thirty-two male participants (16 Baby Boomers between ages 47 and 65 and 16 Generation-Y between ages 18 and 29) participated in the study. Drivers demonstrated different gripping behavior between vehicles and between groups. Recommendations for future work in steering wheel grip design and naturalistic driver hand positioning are discussed."]},
{"title": "Supervisory-level interruption recovery in time-critical control tasks", "highlights": ["We examined an interruption recovery tool to assist supervisors in C2 settings.", "The tool is an interactive timeline of historical events with discrete replay.", "Decisions were faster and more accurate with the interactive event timeline tool.", "The tool was particularly effective when participants faced complex decisions."], "abstract": ["This paper investigates the effectiveness of providing interruption recovery assistance in the form of an interactive visual timeline of historical events on a peripheral display in support of team supervision in time-critical settings. As interruptions can have detrimental effects on task performance, particularly in time-critical work environments, there is growing interest in the design of tools to assist people in resuming their pre-interruption activity. A user study was conducted to evaluate the use of an interactive event timeline that provides assistance to human supervisors in time-critical settings. The study was conducted in an experimental platform that emulated a team of operators and a mission commander performing a time-critical unmanned aerial vehicle (UAV) mission. The study results showed that providing interruption assistance enabled people to recover from interruptions faster and more accurately. These results have implications for interface design that could be adopted in similar time-critical environments such as air-traffic control, process control, and first responders."]},
{"title": "Design options for improving protective gloves for industrial assembly work", "highlights": ["Glove design 1 and barehanded produced higher handgrip strength than double gloves.", "Glove design 1 and the bare hand condition had better dexterity than double gloves.", "Selective thickness over some hand areas can be applied to improve the glove design."], "abstract": ["The study investigated the effects of wearing two new designs of cotton glove on several hand performance capabilities and compared them against the effects of barehanded, single-layered and double cotton glove conditions when working with hand tools (screwdriver and pliers). The new glove designs were based on the findings of subjective hand discomfort assessments for this type of work and aimed to match the glove thickness to the localised pressure and sensitivity in different areas of the hand as well as to provide adequate dexterity for fine manipulative tasks. The results showed that the first prototype glove and the barehanded condition were comparable and provided better dexterity and higher handgrip strength than double thickness gloves. The results support the hypothesis that selective thickness in different areas of the hand could be applied by glove manufacturers to improve the glove design, so that it can protect the hands from the environment and at the same time allow optimal hand performance capabilities."]},
{"title": "Optimal protruding node length of bicycle seats determined using cycling postures and subjective ratings", "highlights": ["No previous study has systematically examined the effects of protruding nose lengths (PNL).", "We collected body postures and subjective ratings under 5 seat PNLs\u00a0\u00d72 handle heights conditions.", "Different PNLs affected only the trunk angle, and did not affect the other body angles.", "The handle and the PNL variables had varying effects on the degree of pelvic tilt.", "When PNL\u00a0=\u00a06\u00a0cm, discomfort was more acceptable and stability was also sufficient for the riders."], "abstract": ["This study examined body posture, subjective discomfort, and stability, requiring the participants to ride a stationary bicycle for 20\u00a0min (cadence: 60\u00a0rpm; workrate: 120\u00a0W), using various combinations of two handle heights and five seat-protruding node lengths (PNLs). The results indicated that bicycle handle height significantly influenced body posture, and that seat PNL caused differences in the riders' subjective discomfort and stability scores. The various PNLs affected only the trunk angle (approximately 6\u00b0), but had significantly positive (", "\u00a0=\u00a00.994, ", "\u00a0<\u00a0.005) and negative (", "\u00a0=\u00a0\u22120.914, ", "\u00a0<\u00a0.05) correlations with the subjective discomfort rating for perineum and ischial tuberosity, respectively. When the participants were seated at PNL\u00a0=\u00a00 or 3\u00a0cm, cycling using dropped handles was less stable compared with using straight handles; however, the handle height did not affect the cycling stability when the PNL was \u22656\u00a0cm. The results suggest that a 6-cm PNL is the optimal reference for bicycle seat designs."]},
{"title": "Mapping the relationships between work and sustainability and the opportunities for ergonomic action", "highlights": ["Ergonomics can be actively influential within the organization on issues relating to work improvements, caused by sustainability policies.", "Into sustainability policies, ergonomics may boost integrated increases in the organization's performance and in workers' well-being.", "Ergonomics can provide support for changes and new sustainability-related work requirements to be considered.", "Ergonomics can contribute to the definition of the concept of work in a context of sustainable development."], "abstract": ["A map was drawn up of the relationships between work (in its multiple interpretations) and sustainability (sustainable development and corporate sustainability) based on a bibliographic analysis of articles that discuss these themes jointly in the current academic literature. The position of the discipline of ergonomics focused on work was identified from this map and, based on its specific academic literature, it was possible to identify where this discipline could contribute so that work and workers can be included in the discourse of sustainable development and considered in corporate sustainability policies. Ergonomics can be actively influential within the organization on issues relating to work improvements; it may boost integrated increases in the organization's performance and in workers' well-being; it can provide support for changes and new (environmental) sustainability-related work requirements to be considered; and it can contribute to the definition of the concept of work in a context of sustainable development."]},
{"title": "Towards successful user interaction with systems: Focusing on user-derived gestures for smart home systems", "highlights": ["The three experiments were conducted to identify the participants' decision for gestures of each command.", "As a result, 66% of the top gestures between the two experiments were different.", "Also, the selected gestures of the third experiment were similar to those from the second experiment."], "abstract": ["Various studies that derived gesture commands from users have used the frequency ratio to select popular gestures among the users. However, the users select only one gesture from a limited number of gestures that they could imagine during an experiment, and thus, the selected gesture may not always be the best gesture. Therefore, two experiments including the same participants were conducted to identify whether the participants maintain their own gestures after observing other gestures. As a result, 66% of the top gestures were different between the two experiments. Thus, to verify the changed gestures between the two experiments, a third experiment including another set of participants was conducted, which showed that the selected gestures were similar to those from the second experiment. This finding implies that the method of using the frequency in the first step does not necessarily guarantee the popularity of the gestures."]},
{"title": "Police officer in-vehicle discomfort: Appointments carriage method and vehicle seat features", "highlights": ["Carrying appointments on a load bearing vest reduces police officer subjective in-vehicle discomfort.", "Vehicle seat backrest bolsters, lumbar region and seat cushion affect discomfort of police officers.", "Police vehicle purchasers are encouraged to seek seats encompassing high degrees of adjustability."], "abstract": ["Musculoskeletal pain is commonly reported by police officers. A potential cause of officer discomfort is a mismatch between vehicle seats and the method used for carrying appointments. Twenty-five police officers rated their discomfort while seated in: (1) a standard police vehicle seat, and (2) a vehicle seat custom-designed for police use. Discomfort was recorded in both seats while wearing police appointments on: (1) a traditional appointments belt, and (2) a load-bearing vest/belt combination (LBV). Sitting in the standard vehicle seat and carrying appointments on a traditional appointments belt were both associated with significantly elevated discomfort. Four vehicle seat features were most implicated as contributing to discomfort: back rest bolster prominence; lumbar region support; seat cushion width; and seat cushion bolster depth. Authorising the carriage of appointments using a LBV is a lower cost solution with potential to reduce officer discomfort. Furthermore, the introduction of custom-designed vehicle seats should be considered."]},
{"title": "How does a lower predictability of lane changes affect performance in the Lane Change Task?", "highlights": ["Effect of lane change predictability on lane change task (LCT) performance was tested.", "Lane change position and the required response where manipulated.", "Lower predictability resulted in substantial performance degradation.", "However, the overall pattern of results was unaffected.", "Results support the validity of the LCT as a method to assess driver distraction."], "abstract": ["The Lane Change Task (LCT) is an established method to assess driver distraction caused by secondary tasks. In the LCT ISO standard, \u201ccourse following and maneuvering\u201d and \u201cevent detection\u201d are mentioned as central task properties. Especially event detection seems to be a reasonable feature, as research suggests that distraction has profound effects on drivers' reactions to sudden, unexpected events. However, closer inspection of the LCT reveals that the events to be detected (lane change signs) and the required response are highly predictable. To investigate how the LCT's distraction assessment of secondary tasks might change if lane change events and responses were less predictable, we implemented three different versions of the LCT \u2013 an \u201coriginal\u201d one, a second one with lowered predictability of event position, and a third one with lowered predictability of event position and response. We tested each of these implementations with the same set of visual and cognitive secondary tasks of varying demand. The results showed that a decrease in predictability resulted in overall degraded performance in the LCT when using the basic lane change model for analysis. However, all secondary task conditions suffered equally. No differential effects were found. We conclude that although an ISO conforming implementation of the LCT might not be excessively valid regarding its depiction of safety relevant events, the results obtained are nevertheless comparable to what would be found in settings of higher validity."]},
{"title": "A cycling workstation to facilitate physical activity in office settings", "highlights": ["We evaluated the efficacy of a cycling workstation to increase energy expenditure.", "Pedaling while typing substantially increased metabolic cost.", "Pedaling while typing did not alter typing time or number of typing errors.", "Cycling workstation can facilitate physical activity without compromising typing."], "abstract": ["Facilitating physical activity during the workday may help desk-bound workers reduce risks associated with sedentary behavior. We 1) evaluated the efficacy of a cycling workstation to increase energy expenditure while performing a typing task and 2) fabricated a power measurement system to determine the accuracy and reliability of an exercise cycle. Ten individuals performed 10\u00a0min trials of sitting while typing (SIT", ") and pedaling while typing (PED", "). Expired gases were recorded and typing performance was assessed. Metabolic cost during PED", " was \u223c2.5\u00d7 greater compared to SIT", " (255\u00a0\u00b1\u00a014 vs. 100\u00a0\u00b1\u00a011\u00a0kcal\u00a0h", ", ", "\u00a0<\u00a00.01). Typing time and number of typing errors did not differ between PED", " and SIT", " (7.7\u00a0\u00b1\u00a01.5 vs. 7.6\u00a0\u00b1\u00a01.6\u00a0min, ", "\u00a0=\u00a00.51, 3.3\u00a0\u00b1\u00a04.6 vs. 3.8\u00a0\u00b1\u00a02.7\u00a0errors, ", "\u00a0=\u00a00.80). The exercise cycle overestimated power by 14\u2013138% compared to actual power but actual power was reliable (", "\u00a0=\u00a00.998, ", "\u00a0<\u00a00.01). A cycling workstation can facilitate physical activity without compromising typing performance. The exercise cycle's inaccuracy could be misleading to users."]},
{"title": "Validity and inter-observer reliability of subjective hand-arm vibration assessments", "highlights": ["An easily applicable subjective hand-arm vibration assessment tool was presented.", "The assessment tool was developed for occupational safety and health practitioners.", "The tool is found to be fairly reliable among observers and moderately valid.", "The tool can be of use in future research and in field-based ergonomic assessments."], "abstract": ["Exposure to mechanical vibrations at work (e.g., due to handling powered tools) is a potential occupational risk as it may cause upper extremity complaints. However, reliable and valid assessment methods for vibration exposure at work are lacking. Measuring hand-arm vibration objectively is often difficult and expensive, while often used information provided by manufacturers lacks detail. Therefore, a subjective hand-arm vibration assessment method was tested on validity and inter-observer reliability.", "In an experimental protocol, sixteen tasks handling powered tools were executed by two workers. Hand-arm vibration was assessed subjectively by 16 observers according to the proposed subjective assessment method. As a gold standard reference, hand-arm vibration was measured objectively using a vibration measurement device. Weighted \u03ba's were calculated to assess validity, intra-class-correlation coefficients (ICCs) were calculated to assess inter-observer reliability.", "Inter-observer reliability of the subjective assessments depicting the agreement among observers can be expressed by an ICC of 0.708 (0.511\u20130.873). The validity of the subjective assessments as compared to the gold-standard reference can be expressed by a weighted \u03ba of 0.535 (0.285\u20130.785). Besides, the percentage of exact agreement of the subjective assessment compared to the objective measurement was relatively low (i.e., 52% of all tasks). This study shows that subjectively assessed hand-arm vibrations are fairly reliable among observers and moderately valid. This assessment method is a first attempt to use subjective risk assessments of hand-arm vibration. Although, this assessment method can benefit from some future improvement, it can be of use in future studies and in field-based ergonomic assessments."]},
{"title": "The assessment of material handling strategies in dealing with sudden loading: The effects of load handling position on trunk biomechanics", "highlights": ["Trunk biomechanics was assessed during sudden loading events.", "Smaller spinal loading was observed with the decrease of load handling height.", "Holding load 45\u00b0 to the side resulted in smaller peak spinal compression.", "Clear interaction effect between load handling height and asymmetry was revealed."], "abstract": ["Back injury caused by sudden loading is a significant risk among workers that perform manual handling tasks. The present study investigated the effects of load handling position on trunk biomechanics (flexion angle, L5/S1 joint moment and compression force) during sudden loading. Eleven subjects were exposed to a 6.8\u00a0kg sudden loading while standing upright, facing forward and holding load at three different vertical heights in the sagittal plane or 45\u00b0 left to the sagittal plane (created by arm rotation). Results showed that the increase of load holding height significantly elevated the peak L5/S1 joint compression force and reduced the magnitude of trunk flexion. Further, experiencing sudden loading from an asymmetric direction resulted in significantly smaller peak L5/S1 joint compression force, trunk flexion angle and L5/S1 joint moment than a symmetric posture. These findings suggest that handling loads in a lower position could work as a protective strategy during sudden loading."]},
{"title": "Shoulder muscle loading and task performance for overhead work on ladders versus Mobile Elevated Work Platforms", "highlights": ["High levels of discomfort were reported in the neck, shoulders and lower body.", "Almost 40% of the electricians' days were spent working at height.", "Cycle times of activities on MEWPs were double that, on average, than on ladders.", "Working on a MEWP did not affect task performance or discomfort.", "Shoulder muscle activity was lower for select tasks on the MEWP than ladder."], "abstract": ["A high incidence of Musculoskeletal Disorders (MSDs) has been reported in the construction sector. The use of ladders in the workplace has long been identified as a significant risk that can lead to workplace accidents. However, it is unclear if platform types have an effect on the physical risk factors for MSDs in overhead work. The aim of this study is to perform a pilot study on the effects of hand activity on both shoulder muscle loading and task performance while working on ladders versus Mobile Elevated Working Platforms (MEWPs). It is hypothesised that work on ladders would result in greater muscle loading demands, increased levels of discomfort, and reduced performance due to the restrictions on postures that could be adopted. A field study (", "\u00a0=\u00a019) of experienced electricians on a construction site found that workers spent approximately 28% of their working time on ladders versus 6% on MEWPs. However, the durations of individual tasks were higher on MEWPs (153\u00a0s) than on ladders (73\u00a0s). Additionally, maximum levels of perceived discomfort (on a VAS 0\u2013100) were reported for the shoulders (27), neck (23), and lower regions of the body (22). A simulated study (", "\u00a0=\u00a012) found that task performance and discomfort were not significantly different between platform types (ladder vs. MEWP) when completing either of three tasks: cabling, assembly and drilling. However, platform and task had significant effects (", "\u00a0<\u00a00.05) on median electromyographic (EMG) activity of the anterior deltoid and upper trapezius. EMG amplitudes were higher for the deltoid than the upper trapezius. For the deltoid, the peak amplitudes were, on average, higher for ladder work over MEWP work for the hand intensive cabling (32 vs. 27% Maximal Voluntary Exertion (MVE)) and the assembly task (19 vs. 6% MVE). Conversely, for drilling, the peak EMG amplitudes were marginally lower for ladder compared to the MEWP (3.9 vs. 5.1% MVE). The general implication was that working on the MEWP involved lower shoulder muscle loading\u00a0for cabling and assembly task. A difference due to platform type was not present for drilling work."]},
{"title": "Prevalence of carpal tunnel syndrome among employees at a poultry processing plant", "highlights": ["This cross-sectional evaluation offers detailed exposure measures of each participant.", "A multipart case definition that included nerve conduction testing was utilized.", "Increasing levels of hand activity and force were associated with increased CTS prevalence among participants.", "This evaluation demonstrates the need to reduce employee exposure to levels of hand activity and force."], "abstract": ["To determine prevalence of carpal tunnel syndrome (CTS) among poultry processing employees while taking into account non-occupational factors and assess any association between CTS prevalence and exposure groups.", "Performed a cross-sectional survey to assess CTS (", "\u00a0=\u00a0318). A CTS case was defined as an employee with self-reported CTS symptoms, an abnormal hand symptom diagram, and an abnormal nerve conduction study (NCS). Log-binomial regression was used to estimate prevalence ratios.", "Three hundred and one participants had sufficient symptom information or NCS data to be classified. 126 (42%) of 301 participants had evidence of CTS. In the adjusted analysis, the highest exposure group had CTS prevalence that was significantly higher than that for the lower exposure group [PR: 1.61; 95% CI\u00a0=\u00a0(1.20, 2.17)].", "Increasing levels of hand activity and force were associated with increased CTS prevalence among participants. Recommendations were provided to reduce exposure to these risk factors."]},
{"title": "A method superior to adding percentiles when only limited anthropometric data such as percentile tables are available for design models", "highlights": ["Severly limited data may force anthropometric modeling based on adding percentiles.", "Adding percentiles is shown to assume a perfect correlation between variables.", "A method superior to adding percentiles is demonstrated.", "The method is applicable when the correlation between variables is unknown.", "The method is applicable when the distribution of the data is unknown."], "abstract": ["Designers and ergonomists may occasionally be limited to using tables of percentiles of anthropometric data to model users. Design models that add or subtract percentiles produce unreliable estimates of the proportion of users accommodated, in part because they assume a perfect correlation between variables. Percentile data do not allow the use of more reliable modeling methods such as Principle Component Analysis. A better method is needed.", "A new method for modeling with limited data is described. It uses measures of central tendency (median or mean) of the range of possible correlation values to estimate the combined variance is shown to reduce error compared to combining percentiles. Second, use of the Chebyshev inequality allows the designer to more reliably estimate the percent accommodation when the distributions of the underlying anthropometric data are unknown than does combining percentiles.", "This paper describes a modeling method that is more accurate than combining percentiles when only limited data are available."]},
{"title": "Differences in typing forces, muscle activity, comfort, and typing performance among virtual, notebook, and desktop keyboards", "highlights": ["Typing biomechanics were compared between one virtual and two conventional keyboards.", "Finger flexor and extensor muscle activity was lower during the virtual keyboard use.", "Typing productivity and self-reported comfort declined during the virtual keyboard use."], "abstract": ["The present study investigated whether there were physical exposure and typing productivity differences between a virtual keyboard with no tactile feedback and two conventional keyboards where key travel and tactile feedback are provided by mechanical switches under the keys. The key size and layout were same across all the keyboards. Typing forces; finger and shoulder muscle activity; self-reported comfort; and typing productivity were measured from 19 subjects while typing on a virtual (0\u00a0mm key travel), notebook (1.8\u00a0mm key travel), and desktop keyboard (4\u00a0mm key travel). When typing on the virtual keyboard, subjects typed with less force (", "'s\u00a0<\u00a00.0001) and had lower finger flexor/extensor muscle activity (", "'s\u00a0<\u00a00.05). However, the lower typing forces and finger muscle activity came at the expense of a 60% reduction in typing productivity (", "\u00a0<\u00a00.0001), decreased self-reported comfort (", "'s\u00a0<\u00a00.0001), and a trend indicating an increase in shoulder muscle activity (", "'s\u00a0<\u00a00.10). Therefore, for long typing sessions or when typing productivity is at a premium, conventional keyboards with tactile feedback may be more suitable interface."]},
{"title": "Estimating oxygen consumption from heart rate using adaptive neuro-fuzzy inference system and analytical approaches", "highlights": ["We propose an individualized fuzzy model for the estimation of VO", " using HR measurements.", "We propose an individualized analytical model to estimate VO", " using HR measurements.", "We propose a general fuzzy model that does not require individual calibration for VO", " estimation of groups of workers."], "abstract": ["In new approaches based on adaptive neuro-fuzzy systems (ANFIS) and analytical method, heart rate (HR) measurements were used to estimate oxygen consumption (VO", "). Thirty-five participants performed Meyer and Flenghi's step-test (eight of which performed regeneration release work), during which heart rate and oxygen consumption were measured. Two individualized models and a General ANFIS model that does not require individual calibration were developed. Results indicated the superior precision achieved with individualized ANFIS modelling (RMSE\u00a0=\u00a01.0 and 2.8\u00a0ml/kg\u00a0min in laboratory and field, respectively). The analytical model outperformed the traditional linear calibration and Flex-HR methods with field data. The General ANFIS model's estimates of VO", " were not significantly different from actual field VO", " measurements (RMSE\u00a0=\u00a03.5\u00a0ml/kg\u00a0min). With its ease of use and low implementation cost, the General ANFIS model shows potential to replace any of the traditional individualized methods for VO", " estimation from HR data collected in the field."]},
{"title": "The effect of police cruiser restraint cage configuration on shoulder discomfort, muscular demands, upper limb postures, and task performance during simulated police patrol", "highlights": ["We compare current and modified police cruiser restraint cage configurations.", "Bilateral shoulder discomfort was reduced while typing directly in front of the body.", "Location of the laptop in front of the user allows more neutral shoulder postures.", "Reduced discomfort is likely driven by postures, not muscle demands.", "Reduced discomfort results may have implications for future police car designs."], "abstract": ["Advances in police-specific technology have led to changes in work layout and physical occupational demands of mobile police officers. This study investigated the influence of police cruiser compartment configuration on perceived discomfort, muscle activation, shoulder kinematics, and typing performance during simulated police patrol. Participants completed a one-hour session including simulated driving and 2-min typing trials in a standard compartment configuration with a fixed mobile data terminal (MDT) location (ST), and in a modified compartment configuration with an MDT in front of the user and a rearward translated seat (MOD). The MOD configuration resulted in reductions of 55\u201365% in perceived shoulder discomfort, up to 3.4% MVC in shoulder muscle demands, and more neutral humeral orientations (shoulder elevation reduced by 13\u201325\u00b0). These improvements associated with the MOD configuration may have ergonomic implications for future police car designs, particularly as new technology is introduced in the mobile environment and advanced solutions are sought."]},
{"title": "The application of SEAT values for predicting how compliant seats with backrests influence vibration discomfort", "highlights": ["Seat cushions affect vibration discomfort at the seat pan and the backrest.", "SEAT values show how cushions modify overall vibration discomfort.", "Predictions of vibration discomfort need adjustment for backrest inclination."], "abstract": ["The extent to which a seat can provide useful attenuation of vehicle vibration depends on three factors: the characteristics of the vehicle motion, the vibration transmissibility of the seat, and the sensitivity of the body to vibration. The \u2018seat effective amplitude transmissibility\u2019 (i.e., SEAT value) reflects how these three factors vary with the frequency and the direction of vibration so as to predict the vibration isolation efficiency of a seat. The SEAT value is mostly used to select seat cushions or seat suspensions based on the transmission of vertical vibration to the principal supporting surface of a seat. This study investigated the accuracy of SEAT values in predicting how seats with backrests influence the discomfort caused by multiple-input vibration. Twelve male subjects participated in a four-part experiment to determine equivalent comfort contours, the relative discomfort, the location of discomfort, and seat transmissibility with three foam seats and a rigid reference seat at 14 frequencies of vibration in the range 1\u201320\u00a0Hz at magnitudes of vibration from 0.2 to 1.6\u00a0ms", "\u00a0r.m.s. The \u2018measured seat dynamic discomfort\u2019 (MSDD) was calculated for each foam seat from the ratio of the vibration acceleration required to cause similar discomfort with the foam seat and with the rigid reference seat. Using the frequency weightings in current standards, the SEAT values of each seat were calculated from the ratio of overall ride values with the foam seat to the overall ride values with the rigid reference seat, and compared to the corresponding MSDD at each frequency. The SEAT values provided good predictions of how the foam seats increased vibration discomfort at frequencies around the 4-Hz resonance but reduced vibration discomfort at frequencies greater than about 6.3\u00a0Hz, with discrepancies explained by a known limitation of the frequency weightings."]},
{"title": "Exploring positive hospital ward soundscape interventions", "highlights": ["The subjective response to hospital ward sound was explored.", "Laboratory evaluation explored \u2018", "\u2019 sound interventions.", "Natural sound and written sound source information were most effective.", "Future work can explore if this can contribute to positively experienced environments."], "abstract": ["Sound is often considered as a negative aspect of an environment that needs mitigating, particularly in hospitals. It is worthwhile however, to consider how subjective responses to hospital sounds can be made more positive. The authors identified natural sound, steady state sound and written sound source information as having the potential to do this. Listening evaluations were conducted with 24 participants who rated their emotional (Relaxation) and cognitive (Interest and Understanding) response to a variety of hospital ward soundscape clips across these three interventions. A repeated measures ANOVA revealed that the \u2018Relaxation\u2019 response was significantly affected (", "\u00a0=\u00a00.05, ", "\u00a0=\u00a00.001) by the interventions with natural sound producing a 10.1% more positive response. Most interestingly, written sound source information produced a 4.7% positive change in response. The authors conclude that exploring different ways to improve the sounds of a hospital offers subjective benefits that move beyond sound level reduction. This is an area for future work to focus upon in an effort to achieve more positively experienced hospital soundscapes and environments."]},
{"title": "The effects of a moisture-wicking fabric shirt on the physiological and perceptual responses during acute exercise in the heat", "highlights": ["We explore the impact of clothing on the thermoregulatory responses.", "A moisture-wicking shirt lowers core temperature during exercise in the heat.", "Cotton shirts have higher regain properties than moisture-wicking fabric shirts.", "Shirts with higher air permeability can aid in lowering core temperature.", "Differing shirt characteristics do not alter the perceptual responses to exercise."], "abstract": ["This study investigated the effects that a form fitted, moisture-wicking fabric shirt, promoted to have improved evaporative and ventilation properties, has on the physiological and perceptual responses during exercise in the heat. Ten healthy male participants completed two heat stress tests consisting of 45\u00a0min of exercise (50% VO", ") in a hot environment (33\u00a0\u00b0C, 60% RH). One heat stress test was conducted with the participant wearing a 100% cotton short sleeved t-shirt and the other heat stress test was conducted with the participant wearing a short sleeved synthetic shirt (81% polyester and 19% elastane). Rectal temperature was significantly lower (", "\u00a0<\u00a00.05) in the synthetic condition during the last 15\u00a0min of exercise. Furthermore, the synthetic polyester shirt retained less sweat (", "\u00a0<\u00a00.05). As exercise duration increases, the ventilation and evaporation properties of the synthetic garment may prove beneficial in the preservation of body temperature during exercise in the heat."]},
{"title": "Metabolic rate of carrying added mass: A function of walking speed, carried mass and mass location", "highlights": ["We study metabolic rate required to carry a mass at ankle, knee and back.", "Equations for determining the metabolic rate of carrying mass at ankle, knee, back.", "Compared to previously reported equations, ours were 7% to 69% more accurate.", "Metabolic rate for mass at a distal location is higher than at a proximal location.", "Unlike on the back, leg mass cannot be modeled as an increase in body mass."], "abstract": ["The effort of carrying additional mass at different body locations is important in ergonomics and in designing wearable robotics. We investigate the metabolic rate of carrying a load as a function of its mass, its location on the body and the subject\u2019s walking speed. Novel metabolic rate prediction equations for walking while carrying loads at the ankle, knees and back were developed based on experiments where subjects walked on a treadmill at 4, 5 or 6", "\u00a0", "km/h bearing different amounts of added mass (up to 2", "\u00a0", "kg per leg and 22", "\u00a0", "kg for back). Compared to previously reported equations, ours are 7\u201369% more accurate. Results also show that relative cost for carrying a mass at a distal versus a proximal location changes with speed and mass. Contrary to mass carried on the back, mass attached to the leg cannot be modeled as an increase in body mass."]},
{"title": "A novel approach for fit analysis of thermal protective clothing using three-dimensional body scanning", "highlights": ["A novel approach was developed to determine the air gap in protective clothing.", "Both the overall and local air gap distributions were analyzed.", "The air gap unevenly distributed over the body with the largest air gap in legs.", "The air gap depended on the body parts, fabric properties, and garment size."], "abstract": ["The garment fit played an important role in protective performance, comfort and mobility. The purpose of this study is to quantify the air gap to quantitatively characterize a three-dimensional (3-D) garment fit using a 3-D body scanning technique. A method for processing of scanned data was developed to investigate the air gap size and distribution between the clothing and human body. The mesh model formed from nude and clothed body was aligned, superimposed and sectioned using Rapidform software. The air gap size and distribution over the body surface were analyzed. The total air volume was also calculated. The effects of fabric properties and garment size on air gap distribution were explored. The results indicated that average air gap of the fit clothing was around 25\u201330\u00a0mm and the overall air gap distribution was similar. The air gap was unevenly distributed over the body and it was strongly associated with the body parts, fabric properties and garment size. The research will help understand the overall clothing fit and its association with protection, thermal and movement comfort, and provide guidelines for clothing engineers to improve thermal performance and reduce physiological burden."]},
{"title": "Validating the Strategies Analysis Diagram: Assessing the reliability and validity of a formative method", "highlights": ["Evidence regarding the performance of Ergonomics methods is often not forthcoming.", "We evaluated the reliability and validity of the Strategies Analysis Diagram method.", "Individual results were average. Pooled group results were significantly better.", "A structured multi-analyst approach is proposed for the Strategies Analysis Diagram."], "abstract": ["The Strategies Analysis Diagram (SAD) is a recently developed method to model the range of possible strategies available for activities in complex sociotechnical systems. Previous applications of the new method have shown that it can effectively identify a comprehensive range of strategies available to humans performing activity within a particular system. A recurring criticism of Ergonomics methods is however, that substantive evidence regarding their performance is lacking. For a method to be widely used by other practitioners such evaluations are necessary. This article presents an evaluation of criterion-referenced validity and test-retest reliability of the SAD method when used by novice analysts. The findings show that individual analyst performance was average. However, pooling the individual analyst outputs into a group model increased the reliability and validity of the method. It is concluded that the SAD method's reliability and validity can be assured through the use of a structured process in which analysts first construct an individual model, followed by either another analyst pooling the individual results or a group process pooling individual models into an agreed group model."]},
{"title": "Delivering meat carcasses/cuts to craft-butcher shops: An investigation of work characteristics and manual handling hazards", "highlights": ["We gathered information about work techniques, and good practices, to characterise the handling practices.", "We gathered information about driver injury to determine underlying causes, and proneness to incidents.", "A number of factors that can increase the risk of injury are identified along with measures to mitigate them.", "An approach for risk assessment is suggested which could be applied in similar peripatetic delivery work situations."], "abstract": ["This study investigated delivery scenarios of service drivers working in the retail meat industry. The methodology included analysis of accident reports, and field investigations of deliveries at craft-butcher shop premises, including semi-structured interviews with managers and workers. The findings provide greater clarity about the hazards in this job, and suggest for peripatetic delivery activities, four main factors on which decisions about risk and good practice may be made: composition of the orders; characteristics of the delivery vehicle/truck; handling method most often used; and, the road/access conditions."]},
{"title": "Evaluation of two cooling systems under a firefighter coverall", "highlights": ["Two cooling systems were tested under a firefighter suit, exercising in the heat.", "The systems featured cooled pads or a cold water perfusion system.", "The cool pads initially improved thermal sensation, the perfused system continually.", "Neither system reduced core temperature, heart rate, perceived exertion or comfort.", "The tested cooling systems did not meet the cooling expectations."], "abstract": ["Firemen often suffer from heat strain. This study investigated two chest cooling systems for use under a firefighting suit. In nine male subjects, a vest with water soaked cooling pads and a vest with water perfused tubes were compared to a control condition. Subjects performed 30\u00a0min walking and 10\u00a0min recovery in hot conditions, while physiological and perceptual parameters were measured. No differences were observed in heart rate and rectal temperature, but scapular skin temperature and fluid loss were lower using the perfused vest. Thermal sensation was cooler for the perfused vest than for the other conditions, while the cool pad vest felt initially cooler than control. However, comfort and RPE scores were similar. We conclude that the cooling effect of both tested systems, mainly providing a (temporally) cooler thermal sensation, was limited and did not meet the expectations."]},
{"title": "The effect of pre-warming on performance during simulated firefighting exercise", "highlights": ["Physical activity in and around the firehouse may increase body temperature.", "An increased body temperature reduces speed during firefighting activities.", "An increased body temperature blurs the own perception of quality of performance.", "Careful planning of activities that increase body temperature is needed."], "abstract": ["This study examined the effect of active pre-warming on speed and quality of performance during simulated firefighting exercise. Twelve male firefighters performed two trials in counterbalanced order. They were either pre-warmed by 20-min cycling at 1.5\u00a0Watt\u00a0kg", " body mass (WARM) or remained thermoneutral (CON) prior to a simulated firefighting activity. After the pre-warming, gastrointestinal temperature (", "\u00a0<\u00a00.001), skin temperature (", "\u00a0=\u00a00.002), and heart rate (", "\u00a0<\u00a00.001) were higher in WARM than in CON. During the firefighting activity, rating of perceived exertion, thermal sensation and discomfort were higher for WARM than for CON. Finish time of the firefighting activity was similar, but the last task of the activity was completed slower in WARM than in CON (", "\u00a0=\u00a00.04). In WARM, self-reported performance quality was lower than in CON (", "\u00a0=\u00a00.04). It is concluded that pre-warming reduces the speed during the last part of simulated firefighting activity and reduces self-reported quality of performance."]},
{"title": "The impact of color combinations on the legibility of text presented on LCDs", "highlights": ["Effect of color combinations on the legibility of text on LCDs is significant.", "Le Courier and CRTs legibility tables are not appropriate for LCDs.", "New legibility table for LCDs is proposed.", "Dark text generally leads to greater legibility when contrast ratio is greatest."], "abstract": ["The paper investigates the impact of text and background color combinations on the legibility of text presented on LCDs. The legibility of 56 elementary color combinations was tested by 308 participants. The first task required the participants to identify 21 uppercase alphabetic characters selected and presented in conformance with a Snellen chart in various color combinations. For each color combination, the number of correctly identified characters was recorded as a visual performance measure. In the second task, participants subjectively rated the legibility of color combinations on a ten-point Likert scale. The results show that neither the Le Courier legibility table nor the CRT legibility table is appropriate for LCDs. A new legibility table is proposed for LCDs where the highest ranked are contrastive color combinations with positive polarity whereas for CRT displays, the highest ranked are contrastive color combinations with negative polarity. The findings of this study can be used to determine the best possible color combinations when developing content displayed on LCDs."]},
{"title": "How different types of users develop trust in technology: A qualitative analysis of the antecedents of active and passive user trust in a shared technology", "highlights": ["This paper investigated antecedents of trust in technology for active/passive users.", "A list of system features that influenced trust was derived from qualitative analysis.", "Different antecedents of trust related to trust/distrust differently.", "Active/passive users evaluated trust according to similar sets of system features."], "abstract": ["The aim of this study was to investigate the antecedents of trust in technology for active users and passive users working with a shared technology. According to the prominence-interpretation theory, to assess the trustworthiness of a technology, a person must first perceive and evaluate elements of the system that includes the technology. An experimental study was conducted with 54 participants who worked in two-person teams in a multi-task environment with a shared technology. Trust in technology was measured using a trust in technology questionnaire and antecedents of trust were elicited using an open-ended question. A list of antecedents of trust in technology was derived using qualitative analysis techniques. The following categories emerged from the antecedent: technology factors, user factors, and task factors. Similarities and differences between active users and passive user responses, in terms of trust in technology were discussed."]},
{"title": "Development of a frequency-separated knob with variable change rates by rotation speed", "highlights": ["We applied the principle of frequency separation to the design of knobs and related it to the study of control-display gain.", "We developed a prototype of a speed frequency-separated knob that shows change rates varying depending on its rotation speed.", "We conducted two experiments to examine whether the newly developed knob enhances users' task performance.", "The results showed that the newly designed speed frequency-separated knob was effective in enhancing task performance.", "The results can offer useful information when developing guidelines for designing a knob."], "abstract": ["The principle of frequency separation is a design method to display different information or feedback in accordance with the frequency of interaction between users and systems. This principle can be usefully applied to the design of knobs. Particularly, their rotation speed can be a meaningful criterion for applying the principle. Hence a knob can be developed, which shows change rates varying depending on its rotation speed. Such a knob would be more efficient than conventional knobs with constant change rate. We developed a prototype of frequency-separated knobs that has different combinations of the number of rotation speed steps and the size of the variation of change rate. With this prototype, we conducted an experiment to examine whether a speed frequency-separated knob enhances users' task performance. The results showed that the newly designed knob was effective in enhancing task performance, and that task efficiency was the best when its change rate increases exponentially and its rotation speed has three steps. We conducted another experiment to investigate how a more rapid exponential increase of change rate and a more number of steps of rotation speed influence users' task performance. The results showed that merely increasing both the size of the variation of change rates and the number of speed steps did not result in better task performance. Although two experimental results cannot easily be generalized to other contexts, they still offer practical information useful for designing a speed frequency-separated knob in various consumer electronics and control panels of industrial systems."]},
{"title": "Discomfort of seated persons exposed to low frequency lateral and roll oscillation: Effect of seat cushion", "highlights": ["A compliant cushion increased exposure to lateral and roll oscillation.", "The compliant cushion increased discomfort caused by lateral and roll oscillation.", "The study reveals frequency-dependence of vibration discomfort from 0.2 to 1\u00a0Hz."], "abstract": ["The discomfort caused by lateral oscillation, roll oscillation, and fully roll-compensated lateral oscillation has been investigated at frequencies between 0.25 and 1.0\u00a0Hz when sitting on a rigid seat and when sitting on a compliant cushion, both without a backrest. Judgements of vibration discomfort and the transmission of lateral and roll oscillation through the seat cushion were obtained with 20 subjects. Relative to the rigid seat, the cushion increased lateral acceleration and roll oscillation at the lower frequencies and also increased discomfort during lateral oscillation (at frequencies less than 0.63\u00a0Hz), roll oscillation (at frequencies less than 0.4\u00a0Hz), and fully roll-compensated lateral oscillation (at frequencies between 0.315 and 0.5\u00a0Hz). The root-sums-of-squares of the frequency-weighted lateral and roll acceleration at the seat surface predicted the greater vibration discomfort when sitting on the cushion. The frequency-dependence of the predicted discomfort may be improved by adjusting the frequency weighting for roll acceleration at frequencies between 0.25 and 1.0\u00a0Hz."]},
{"title": "Effects of a standing and three dynamic workstations on computer task performance and cognitive function tests", "highlights": ["Within-subjects comparison of three dynamic workstations and standing with sitting.", "Five standardised, common office tasks in an office-like laboratory setting.", "Objective: equal work performance on basic office tasks, precision mouse tasks excluded.", "Participant's perception of decreased performance might complicate acceptance.", "Dynamic workstations promising to increase physical activity during daily office work."], "abstract": ["Sedentary work entails health risks. Dynamic (or active) workstations, at which computer tasks can be combined with physical activity, may reduce the risks of sedentary behaviour. The aim of this study was to evaluate short term task performance while working on three dynamic workstations: a treadmill, an elliptical trainer, a bicycle ergometer and a conventional standing workstation. A standard sitting workstation served as control condition. Fifteen Dutch adults performed five standardised but common office tasks in an office-like laboratory setting. Both objective and perceived work performance were measured. With the exception of high precision mouse tasks, short term work performance was not affected by working on a dynamic or a standing workstation. The participant's perception of decreased performance might complicate the acceptance of dynamic workstations, although most participants indicate that they would use a dynamic workstation if available at the workplace."]},
{"title": "Stimulated recall methodology for assessing work system barriers and facilitators in family-centered rounds in a pediatric hospital", "highlights": ["Parents and healthcare team members who reviewed video records of their bedside rounds participated in the analysis of work system barriers and facilitators in family-centered rounds.", "The stimulated recall methodology was positively received by parents and healthcare team members.", "The stimulated recall methodology allowed the identification of a wide range of work system barriers and facilitators in family-centered rounds.", "Stimulated recall methodology can be used to improve healthcare work systems and processes."], "abstract": ["Human factors and ergonomics methods are needed to redesign healthcare processes and support patient-centered care, in particular for vulnerable patients such as hospitalized children. We implemented and evaluated a stimulated recall methodology for collective confrontation in the context of family-centered rounds. Five parents and five healthcare team members reviewed video records of their bedside rounds, and were then interviewed using the stimulated recall methodology to identify work system barriers and facilitators in family-centered rounds. The evaluation of the methodology was based on a survey of the participants, and a qualitative analysis of interview data in light of the work system model of ", ", ", ". Positive survey feedback from the participants was received. The stimulated recall methodology identified barriers and facilitators in all work system elements. Participatory ergonomics methods such as the stimulated recall methodology allow a range of participants, including parents and children, to participate in healthcare process improvement."]},
{"title": "An investigation of the performance of novel chorded keyboards in combination with pointing input devices", "highlights": ["We compared combinations of two novel chorded keyboards with various pointing devices.", "Usage performance was determined by input rate, accuracy, and user assessments.", "Input rates of both keyboards with stylus and mouse devices improved with training.", "The best combination observed was a novel cross-shaped key keyboard and a stylus."], "abstract": ["Rapid advances in computing power have driven the development of smaller and lighter technology products, with novel input devices constantly being produced in response to new user behaviors and usage contexts. The aim of this research was to investigate the feasibility of operating chorded keyboard control modules in concert with pointing devices such as styluses and mice. We compared combinations of two novel chorded keyboards with different pointing devices in hopes of finding a better combination for future electronic products. Twelve participants were recruited for simulation testing, and paired sample t testing was conducted to determine whether input and error rates for the novel keyboards were improved significantly over those of traditional input methods. The most efficient input device combination tested was the combination of a novel cross-shaped key keyboard and a stylus, suggesting the high potential for use of this combination with future mobile IT products."]},
{"title": "Sex differences in lifting strategies during a repetitive palletizing task", "highlights": ["We examine the difference between three groups of workers (expert males, novice males and females).", "The workers had to transfer 24 15-kg boxes from one pallet to another at a self-paced and then at an imposed pace.", "The patterns of interjoint coordination were different between expert males and females during lifting.", "The lifting strategy of females likely stretches spine passive tissues, which put them at greater risk of back injuries."], "abstract": ["Forty-five manual material handlers (15 females, 15 expert males and 15 novice males) performed series of box transfers under conditions similar to those of large distribution centers. The objective of the study was to verify whether sex differences in joint motions and in back loading variables (L5/S1 moments) exist during multiple box transfers. The task consisted in transferring 24 15-kg boxes from one pallet to another (4 layers of boxes; 6 boxes/layer: 3 in the front row, 3 in the back) at a self-determined pace and then at an imposed pace of 9 lifts/min. Full-body 3D kinematic data were collected as well as external foot forces. A dynamic 3D linked segment model was used to estimate the net moments at L5/S1. The results show that the peak L5/S1 moment during lifting for females was significantly lower than for males, but once normalized to body size the difference disappeared. In general, the female workers were very close to the posture adopted by the novice males at the instant of the peak resultant moment. However, females were closer to the box than the male workers. One major sex difference was seen when lifting from the ground, with the use of interjoint coordination analyses. Female workers showed a sequential motion initiated by the knees, followed by the hip and the back, while expert males showed a more synchronized motion. The lifting strategy of females likely stretches lumbar spine passive tissues, which in turn put them at greater risk of back injuries. As observed in our previous studies, these differences between expert males, novice males and females are especially notable when the box is lifted from the ground."]},
{"title": "Perceived density of road maps", "highlights": ["We model the perceived density (PD) of information as a property of road maps.", "The model predicted PD with three variables (", "\u00a0=\u00a0.923).", "The PD model was cross-validated with a different set of maps and participants.", "The validation experiment showed an effect of map configuration on PD."], "abstract": ["Maps should be designed so that users can comprehend and use the information. Display decisions, such as choosing the scale at which an area is shown, depend on properties of the displayed information such as the perceived density (PD) of the information. Taking a psychophysical approach we suggest that the PD of information in a road map is related to the scale and properties of the mapped area. 54 participants rated the PD of 60 maps from different regions. We provide a simple model that predicts the PD of electronic road map displays, using the logarithm of the number of roads, the logarithm of the number of junctions and the length of the shown roads. The PD model was cross-validated using a different set of 60 maps (", "\u00a0=\u00a044). The model can be used for automatically adjusting display scales and for evaluating map designs, considering the required PD to perform a map-related task."]},
{"title": "7 Themes for guiding situated ergonomic assessments of medical devices: A case study of an inpatient glucometer", "highlights": ["We report a case study of a situated ergonomic assessment of a modern inpatient blood glucose meter.", "We identify 19 issues with the design and use of the device.", "We identify 7 broad themes that can help guide future situated studies of medical devices.", "We highlight the need for more ergonomic case studies and guidance following the implementation of BS EN 62366."], "abstract": ["There is relatively little guidance on the situated ergonomic assessment of medical devices, and few case studies that detail this type of evaluation. This paper reports results of a detailed case study that focuses on the design and use of a modern blood glucose meter on an oncology ward. We spent approximately 150\u00a0h in-situ, over 11 days and 4 nights, performing observations and interviews with users. This was complemented by interviews with two staff with oversight and management responsibility related to the device. We identified 19 issues with the design and use of this device. These issues were grouped into 7 themes which can help guide the situated study of medical devices: usability, knowledge gaps and mental models, workarounds, wider tasks and equipment, the patient, connection between services, and policy."]},
{"title": "Evaluation of team lifting on work demands, workload and workers' evaluation: An observational field study", "highlights": ["The mass limit of 25", "\u00a0", "kg per person is regularly violated when team lifting must be applied.", "Load mass is not the main determinant for the appropriate number of workers during lifting.", "Work demands are not different when handling loads up to 50", "\u00a0", "kg or up to 100", "\u00a0", "kg."], "abstract": ["The objective of this study was to assess differences in work demands, energetic workload and workers\u2019 discomfort and physical effort in two regularly observable workdays in ironwork; one where loads up to 50", "\u00a0", "kg were handled with two persons manually (T50) and one where loads up to 100", "\u00a0", "kg were handled manually with four persons (T100). Differences between these typical workdays were assessed with an observational within-subject field study of 10 ironworkers. No significant differences were found for work demands, energetic workload or discomfort between T50 and T100 workdays. During team lifts, load mass exceeded 25", "\u00a0", "kg per person in 57% (T50 workday) and 68% (T100 workday) of the lifts. Seven ironworkers rated team lifting with two persons as less physically demanding compared with lifting with four persons. When loads heavier than 25", "\u00a0", "kg are lifted manually with a team, regulations of the maximum mass weight are frequently violated.", "Loads heavier than 25", "\u00a0", "kg are frequently lifted during concrete reinforcement work and should be lifted by a team of persons. However, the field study showed that loads above 25", "\u00a0", "kg are most of the time not lifted with the appropriate number of workers. Therefore, loads heavier than 25", "\u00a0", "kg should be lifted mechanically."]},
{"title": "Effects of portable computing devices on posture, muscle activation levels and efficiency", "highlights": ["Slate computers resulted in degraded wrist, elbow and neck postures.", "Degraded postures were magnified when working on the sofa.", "Performance on the slate computer was 4 times less than on the laptop or netbook.", "Muscle activity significantly lower on slate computer due to decreased performance."], "abstract": ["Very little research exists on ergonomic exposures when using portable computing devices. This study quantified muscle activity (forearm and neck), posture (wrist, forearm and neck), and performance (gross typing speed and error rates) differences across three portable computing devices (laptop, netbook, and slate computer) and two work settings (desk and computer) during data entry tasks. Twelve participants completed test sessions on a single computer using a test\u2013rest\u2013test protocol (30", "\u00a0", "min of work at one work setting, 15", "\u00a0", "min of rest, 30", "\u00a0", "min of work at the other work setting). The slate computer resulted in significantly more non-neutral wrist, elbow and neck postures, particularly when working on the sofa. Performance on the slate computer was four times less than that of the other computers, though lower muscle activity levels were also found. Potential or injury or illness may be elevated when working on smaller, portable computers in non-traditional work settings."]},
{"title": "Prior schemata transfer as an account for assessing the intuitive use of new technology", "highlights": ["An experiment is conducted for assessing the intuitive use of an interface.", "Intuitive use relies on the transfer of prior knowledge schemata.", "Familiar and new features yield distinct patterns of prior schemata transfer and of new schemata induction, respectively.", "Transfer and induction patterns were moderated by participants' cognitive style.", "Assessment of these patterns is reported for the evaluation and redesign of interfaces."], "abstract": ["New devices are considered intuitive when they allow users to transfer prior knowledge. Drawing upon fundamental psychology experiments that distinguish prior knowledge transfer from new schema induction, a procedure was specified for assessing intuitive use. This procedure was tested with 31 participants who, prior to using an on-board computer prototype, studied its screenshots in reading vs. schema induction conditions. Distinct patterns of transfer or induction resulted for features of the prototype whose functions were familiar or unfamiliar, respectively. Though moderated by participants' cognitive style, these findings demonstrated a means for quantitatively assessing transfer of prior knowledge as the operation that underlies intuitive use. Implications for interface evaluation and design, as well as potential improvements to the procedure, are discussed."]},
{"title": "A systematic review \u2013 physical activity in dementia: The influence of the nursing home environment", "highlights": ["Dementia patients (90%) who are residing in the nursing home environment remain physically passive for most of the day.", "Apathy negatively influences the residents' physical and cognitive well-being and therefore quality of life.", "The environmental elements of timed bright light and small-scale living concepts stimulate residents' physical activity.", "The intervention studies with background music differed strongly in scientific rigor, but showed promising results.", "More research is necessary to draw conclusions for a homelike interior, functional modifications, and building footprint."], "abstract": ["Most older persons with dementia living in nursing homes spend their days without engaging in much physical activity. This study therefore looked at the influence that the environment has on their level of physical activity, by reviewing empirical studies that measured the effects of environmental stimuli on the physical activity of nursing home residents suffering from dementia. The electronic databases PubMed, PsycINFO, EMBASE, CINAHL and the Cochrane Library were used for the search. The search covered studies published between January 1993 and December 2012, and revealed 3187 abstracts. 326 studies were selected as potentially relevant; of these, 24 met\u00a0all the inclusion criteria. Positive results on the residents' levels of physical activity were found for music, a homelike environment and functional modifications. Predominantly positive results were also found for the small-scale group living concepts. Mixed results were found for bright or timed light, the multisensory environment and differences in the building footprint."]},
{"title": "Psychometric properties evaluation of a new ergonomics-related job factors questionnaire developed for nursing workers", "highlights": ["We developed a new job factors questionnaire for nursing workers.", "It evaluates the perception about job factors and their contribution to disorders.", "Confirmatory factos analyses confirmed the pre-defined structure.", "Reliability and validity tests showed good results for using the new tool.", "ErgoEnf can assist in identifying problematic factors in nursing workplaces."], "abstract": ["The objectives of this study were to develop a questionnaire that evaluates the perception of nursing workers to job factors that may contribute to musculoskeletal symptoms, and to evaluate its psychometric properties. Internationally recommended methodology was followed: construction of domains, items and the instrument as a whole, content validity, and pre-test. Psychometric properties were evaluated among 370 nursing workers. Construct validity was analyzed by the factorial analysis, known-groups technique, and convergent validity. Reliability was assessed through internal consistency and stability. Results indicated satisfactory fit indices during confirmatory factor analysis, significant difference (", "\u00a0<\u00a00.01) between the responses of nursing and office workers, and moderate correlations between the new questionnaire and Numeric Pain Scale, SF-36 and WRFQ. Cronbach's alpha was close to 0.90 and ICC values ranged from 0.64 to 0.76. Therefore, results indicated that the new questionnaire had good psychometric properties for use in studies involving nursing workers."]},
{"title": "Office workers' computer use patterns are associated with workplace stressors", "highlights": ["Daily computer use duration was, on average, 30 minutes longer for workers with high overcommitment and perceived stress.", "The number of short computer breaks was about 20% lower for workers with high effort and for workers with low reward.", "Workers with high compared to low effort had, after adjusting for age, a significantly shorter key strike duration.", "No effects of overcommitment, reward, and perceived stress on the pace of input device usage were found."], "abstract": ["This field study examined associations between workplace stressors and office workers' computer use patterns. We collected keyboard and mouse activities of 93 office workers (68F, 25M) for approximately two work weeks. Linear regression analyses examined the associations between self-reported effort, reward, overcommitment, and perceived stress and software-recorded computer use duration, number of short and long computer breaks, and pace of input device usage. Daily duration of computer use was, on average, 30\u00a0min longer for workers with high compared to low levels of overcommitment and perceived stress. The number of short computer breaks (30\u00a0s\u20135\u00a0min long) was approximately 20% lower for those with high compared to low effort and for those with low compared to high reward. These outcomes support the hypothesis that office workers' computer use patterns vary across individuals with different levels of workplace stressors."]},
{"title": "Practitioner versus analyst methods: A nuclear decommissioning case study", "highlights": ["Are the hazard identification methods that have proven useful in one phase of the system lifecycle just as useful in another?", "Study undertaken in the nuclear decommissioning sector in which the risks have changed, challenging the adequacy of \u2018normal\u2019 risk identification methods.", "A formative risk identification approach was compared with a normative approach, with the latter detecting additional risks.", "The \u2018analyst method\u2019 (CWA) was comparable in application effort and time to an established \u2018practitioner method\u2019 (SWIFT)."], "abstract": ["A requirement arose during decommissioning work at a UK Magnox Nuclear Power Station to identify the hazards involved in removing High Dose Rate Items from a Cartridge Cooling Pond. Removing objects from the cooling pond under normal situations is a routine event with well understood risks but the situation described in this paper is not a routine event. The power station has shifted from an operational phase in its life-cycle to a decommissioning phase, and as such the risks, and procedures to deal with them, have become more novel and uncertain. This raises an important question. Are the hazard identification methods that have proven useful in one phase of the system lifecycle just as useful in another, and if not, what methods should be used? An opportunity arose at this site to put the issue to a direct test. Two methods were used, one practitioner focussed and in widespread use during the plant's operational phase (the Structured What-If method), the other was an analyst method (Cognitive Work Analysis). The former is proven on this site but might not be best suited to the novelty and uncertainty brought about by a shift in context from operations to decommissioning. The latter is not proven on this site but it is designed for novelty and uncertainty. The paper presents the outcomes of applying both methods to a real-world hazard identification task."]},
{"title": "The combined effect of physical, psychosocial/organisational and/or environmental risk factors on the presence of work-related musculoskeletal symptoms and its consequences", "highlights": ["Combined exposure to poor physical and poor psychosocial factors increased the odds of MSS.", "This combination also increased the odds of reduced activities and absenteeism due to MSS.", "Favourable psychosocial conditions reduced the odds of MSS due to poor physical conditions.", "To reduce MSS and its consequences, employers need to adopt a multifaceted approach."], "abstract": ["This study assessed the combined effect of physical and psychosocial/organisational and/or environmental factors on the presence of musculoskeletal symptoms (MSS) and its consequences (reduced activities and absenteeism due to MSS) in a random sample of 3003 workers in New Zealand. By telephone interview, participants reported their current workplace exposures and MSS (neck/shoulder, arm/elbow, wrist and low back) and its consequences. Data were analysed using multivariable logistic regression. Combined exposure to physical and psychosocial/organisational and/or environmental factors increased the odds of MSS in the neck/shoulder (OR 3.14, 95% CI 1.79\u20135.52), arms/elbow regions (OR 4.14, 95% CI 2.21\u20137.76) and low back (OR 1.74, 95% CI 1.28\u20132.37) and its consequences, i.e. reduced activities due to neck/shoulder symptoms (OR 5.45, 95% CI 2.28\u201313.00), absenteeism due to neck/shoulder symptoms (OR 5.19, 95% CI 2.24\u201312.01) and absenteeism due to low back symptoms (OR 4.37, 95% CI 2.92\u20136.53). In contrast, favourable psychosocial/organisational work conditions reduced the odds of wrist symptoms due to poor physical work conditions (OR 2.19, 95% CI 1.44\u20133.34). We conclude that to reduce MSS and its consequences, employers need to adopt a multifaceted approach: concentrate on improving physical conditions as well as the psychosocial/organisational and environmental aspects of the working environment."]},
{"title": "Social influence in a virtual tunnel fire \u2013 Influence of conflicting information on evacuation behavior", "highlights": ["Virtual Reality (VR) experiment on conflicting social information during a fire emergency in a road tunnel.", "Actions of a virtual agent (VA) affected participants evacuation destination choice and travel paths.", "If the VA stayed passive, participants waited longer and moved longer distances until they reached the emergency exit.", "If the VA stayed passive or moved away from the emergency exit, less participants moved to the emergency exit."], "abstract": ["Evacuation from a smoke filled tunnel requires quick decision-making and swift action from the tunnel occupants. Technical installations such as emergency signage aim to guide tunnel occupants to the closest emergency exits. However, conflicting information may come from the behavior of other tunnel occupants. We examined if and how conflicting social information may affect evacuation in terms of delayed and/or inadequate evacuation decisions and behaviors. To this end, forty participants were repeatedly situated in a virtual reality smoke filled tunnel with an emergency exit visible to one side of the participants. Four social influence conditions were realized. In the control condition participants were alone in the tunnel, while in the other three experimental conditions a virtual agent (VA) was present. In the no-conflict condition, the VA moved to the emergency exit. In the active conflict condition, the VA moved in the opposite direction of the emergency exit. In the passive conflict condition, the VA stayed passive. Participants were less likely to move to the emergency exit in the conflict conditions compared to the no-conflict condition. Pre-movement and movement times in the passive conflict condition were significantly delayed compared to all other conditions. Participants moved the longest distances in the passive conflict condition. These results support the hypothesis that social influence affects evacuation behavior, especially passive behavior of others can thwart an evacuation to safety."]},
{"title": "Understanding challenges in the front lines of home health care: A human-systems approach", "highlights": ["We use a human-systems perspective to understand home health care.", "We qualitatively assess challenges home health care providers encounter.", "Affect was a commonly mentioned challenge, as well as a high level of uncertainty.", "These findings inform a revised human-systems model of home health care."], "abstract": ["A human-systems perspective is a fruitful approach to understanding home health care because it emphasizes major individual components of the system \u2013 persons, equipment/technology, tasks, and environments \u2013 as well as the interaction between these components. The goal of this research was to apply a human-system perspective to consider the capabilities and limitations of the persons, in relation to the demands of the tasks and equipment/technology in home health care. Identification of challenges and mismatches between the person(s) capabilities and the demands of providing care provide guidance for human factors interventions. A qualitative study was conducted with 8 home health Certified Nursing Assistants and 8 home health Registered Nurses interviewed about challenges they encounter in their jobs. A systematic categorization of the challenges the care providers reported was conducted and human factors recommendations were proposed in response, to improve home health. The challenges inform a human-systems model of home health care."]},
{"title": "A longitudinal study of driving instructor guidance from an activity-oriented perspective", "highlights": ["This article aims to provide a better understanding of the scaffolding activity of instructors.", "Instructor scaffolding activity was analyzed throughout training.", "Results show that the instructors implemented the learning process using an integrative approach.", "Instructors transferred the responsibility of the driving components to the students in a similar order.", "Student autonomy and efficiency in driving increased as the training progressed."], "abstract": ["The aim of this study was to provide a better understanding of the scaffolding activity of instructors during driving lessons in a French urban traffic context. It focuses on three common and risky tasks: turning right, turning left and overtaking. Data were based on fine-grained longitudinal analyses of the records of five driving lessons involving four student-instructor dyads. The instructor scaffolding activity was analyzed throughout training \u2013 an original approach in the sphere of driving. The results show that the instructors implemented the learning process using an integrative approach based on \u2018cutting\u2019 and \u2018decoupling\u2019 the driving task rather than the step-by-step method recommended in the curriculum. They transferred the responsibility of the driving components to the students in a similar order: 1) technical maneuvers, 2) situation identification and 3) goals focusing on other road-users. As expected, student autonomy and efficiency in driving increased as the training progressed. However, at the end of training, uncertainties remained with regard to the execution of basic sub-goals in complex situation; moreover, the instructors were still in charge of the navigational task. The results were discussed and suggestions were made to improve instructor training with a view to increasing their efficiency in teaching students."]},
{"title": "Whole-body vibration exposure of haul truck drivers at a surface coal mine", "highlights": ["Haul truck drivers are exposed to WBV amplitudes within the HGCZ defined by ISO2631-1.", "Maintained roadways were associated with substantially lower WBV amplitudes.", "Larger trucks were associated with lower vibration levels than small trucks.", "A WBV management plan should be in place at surface mine sites."], "abstract": ["Haul truck drivers at surface mines are exposed to whole-body vibration for extended periods. Thirty-two whole-body vibration measurements were gathered from haul trucks under a range of normal operating conditions. Measurements taken from 30 of the 32 trucks fell within the health guidance caution zone defined by ISO2631-1 for an 8\u00a0h daily exposure suggesting, according to ISO2631-1, that \u201ccaution with respect to potential health risks is indicated\u201d. Maintained roadways were associated with substantially lower vibration amplitudes. Larger trucks were associated with lower vibration levels than small trucks. The descriptive nature of the research, and small sample size, prevents any strong conclusion regarding causal links. Further investigation of the variables associated with elevated vibration levels is justified.", "The operators of mining equipment such as haul trucks are exposed to whole-body vibration amplitudes which have potential to lead to long term health effects. Systematic whole-body vibration measurements taken at frequent intervals are required to provide an understanding of the causes of elevated vibration levels and hence determine appropriate control measures."]},
{"title": "Comparing the physiological and perceptual responses of construction workers (bar benders and bar fixers) in a hot environment", "highlights": ["In a hot environment, energy expenditure of rebar work is 2.57\u00a0Kcal/min.", "Bar fixing induced higher physiological responses as compared to bar bending.", "Bar fixing induced non-significant but high perceptual response than bar bending."], "abstract": ["This study aimed to (1) quantify the respective physical workloads of bar bending and fixing; and (2) compare the physiological and perceptual responses between bar benders and bar fixers. Field studies were conducted during the summer in Hong Kong from July 2011 to August 2011 over six construction sites. Synchronized physiological, perceptual, and environmental parameters were measured from construction rebar workers. The average duration of the 39 field measurements was 151.1\u00a0\u00b1\u00a022.4\u00a0min under hot environment (WBGT\u00a0=\u00a031.4\u00a0\u00b1\u00a02.2\u00a0\u00b0C), during which physiological, perceptual and environmental parameters were synchronized. Energy expenditure of overall rebar work, bar bending, and bar fixing were 2.57, 2.26 and 2.67\u00a0Kcal/min (179, 158 and 186\u00a0W), respectively. Bar fixing induced significantly higher physiological responses in heart rate (113.6 vs. 102.3\u00a0beat/min, ", "\u00a0<\u00a00.05), oxygen consumption (9.53 vs. 7.14\u00a0ml/min/kg, ", "\u00a0<\u00a00.05), and energy expenditure (2.67 vs. 2.26\u00a0Kcal/min, ", "\u00a0<\u00a00.05) (186 vs. 158\u00a0W, ", "\u00a0<\u00a00.05) as compared to bar bending. Perceptual response was higher in bar fixing but such difference was not statistically significant. Findings of this study enable the calculation of daily energy expenditure of rebar work."]},
{"title": "Measuring job quality: A study with bus drivers", "highlights": ["Measuring job quality is important politically and economically.", "Ergonomics is well placed to contribute to this discussion.", "The DGB-Index is an effective tool for assessing job quality.", "It is not inevitable that bus driving should be low quality work."], "abstract": ["There is growing interest in the contribution which job design can make to worker health; also a desire to better understand the multidimensional notion of \u2018job quality\u2019 and to develop approaches to measuring this. This paper reviews concepts of \u2018job quality\u2019 and \u2018good jobs\u2019 and examines these issues in the work of bus drivers, an occupational group commonly reported as having poor health and poor working conditions. The DGB-Index (Deutscher Gewerkschaftsbund Good Work Index), a tool used recently in Germany for measuring job quality, was translated and administered to a sample of UK bus drivers (", "\u00a0=\u00a0381). It found job quality to be significantly lower than that for a group of non-drivers in the same organisation; and better than that for a sample of German bus drivers. We conclude that the DGB-Index is an effective tool for measuring job quality and providing feedback to employers; and could be used to compare job quality between organisations or internationally."]},
{"title": "Developing a comprehensive approach to risk management of musculoskeletal disorders in non-nursing health care sector employees", "highlights": ["High prevalence of discomfort levels are evident in the health care sector.", "WMSD risk in the health care is predicted by physical and psychosocial hazards.", "Risk management of WMSDs must address physical and psychosocial hazards.", "A toolkit approach is proposed to improve risk management of WMSDs in health care."], "abstract": ["This study of selected jobs in the health care sector explored a range of physical and psychosocial factors to identify those that most strongly predicted work-related musculoskeletal disorders (WMSD) risk. A self-report survey was used to collect data on physical and psychosocial risk factors from employees in three health care organisations in Victoria, Australia. Multivariate analyses demonstrated the importance of both psychosocial and physical hazards in predicting WMSD risk and provides evidence for risk management of WMSDs to incorporate a more comprehensive and integrated approach. Use of a risk management toolkit is recommended to address WMSD risk in the workplace."]},
{"title": "Typing performance and body discomfort among overweight and obese office workers: A pilot study of keyboard modification", "highlights": ["Alternative keyboards can receive positive gains without reducing worker productivity.", "Obese participants reported reductions wrist, upper back, and lower back discomfort.", "It took participants approximately 11 days to adjust to the alternative keyboard style."], "abstract": ["Obesity in the workplace is associated with loss of productivity, high medical care expenses, and increased rates of work-related injuries and illness. Thus, effective, low-cost interventions are needed to accommodate the size of today's obese office worker while alleviating potential physical harm associated with musculoskeletal disorders. Utilizing a sample of 22 overweight and obese office workers, this pilot study assessed the impact of introducing an alternative, more ergonomically-sound keyboard on perceptions about design, acceptability, and usability; self-reported body discomfort; and typing productivity. Data were collected using self-reported questionnaires and objective typing tests administered before and after the intervention. The intervention duration was six weeks. After switching from their standard work keyboard to an alternative keyboard, all participants reported significant decreases in lower back discomfort (", "\u00a0=\u00a02.14, ", "\u00a0=\u00a00.044); although obese participants reported significant decreases in both upper (", "\u00a0=\u00a02.46, ", "\u00a0=\u00a00.032) and lower (", "\u00a0=\u00a02.39, ", "\u00a0=\u00a00.036) back discomfort. No significant changes were observed in overall typing performance scores from baseline to follow-up. Findings suggest that such interventions may be introduced into the workforce with positive gains for workers without reducing short-term worker productivity."]},
{"title": "Impacts of different types of insoles on postural stability in older adults", "highlights": ["Four different types of insoles were examined in terms of their effects on postural stability in older adults.", "Static postural stability was not affected by insoles.", "Cupped insoles improved dynamic postural stability.", "Rigid insole was associated with better dynamic postural stability compared to soft insoles.", "The findings can aid in better understanding the insole design features that could improve postural stability in older adults."], "abstract": ["The objective of this study was to examine the effects of different types of insoles on postural stability in older adults. Four types of commercially available insoles were selected including the cupped insoles, textured insoles, rigid insoles, and soft insoles. The experiment included a static stance session and a walking session. In the static stance session, the participants stood upright on a force platform as still as possible, with feet together, arms by the side and looking straight ahead. The mean velocity of center-of-pressure time series obtained from the force platform was used to assess static postural stability. In the walking session, the participants walked on a treadmill at their self-selected comfortable speed for 4.5\u00a0min in each insole condition. Dynamic postural stability was assessed using the margin of stability. It was found that static postural stability was not affected by insoles, but cupped insoles improved dynamic postural stability, and rigid insole was associated with better dynamic postural stability compared to soft insoles. These findings can aid in better understanding the insole design features associated with improved postural stability in older adults."]},
{"title": "Intense illumination in the morning hours improved mood and alertness but not mental performance", "highlights": ["High intensity illumination negatively impacted sustained attention.", "Melatonin suppression was not altered as a result of the brief bright light exposure.", "Bright light induced an increase in subjective mood and alertness, which dim light did not."], "abstract": ["Cognitive performance and alertness are two determinants for work efficiency, varying throughout the day and depending on bright light. We conducted a prospective crossover study evaluating the impacts of exposure to an intense, early morning illumination on sustained attention, alertness, mood, and serum melatonin levels in 33 healthy individuals. Compared with a dim illumination, the intense illumination negatively impacted performance requiring sustained attention; however, it positively impacted subjective alertness and mood and had no impact on serum melatonin levels. These results suggest that brief exposure to bright light in the morning hours can improve subjective measures of mood and alertness, but can also have detrimental effects on mental performance as a result of visual distraction. Therefore, it is important that adequate lighting should correspond to both non-visual and visual demands."]},
{"title": "Systematic engineering tools for describing and improving medication administration processes at rural healthcare facilities", "highlights": ["Providing a clear road map to identify potential issues in a clinical activity.", "Medical staff not exchanging the up-to-date MARs promptly.", "Nurses spent a lot of time seeking tools, supplies, or missing drugs.", "Nurses prepare more than one patient's drugs at a time out a convenience.", "Some drugs or supplies are stored in inappropriate locations due to limited space."], "abstract": ["This study demonstrates a series of systematic methods for mapping medication administration processes and for elaborating violations of work standards at two rural hospitals. Thirty-four observational periods were conducted to capture the details of clinical activities, and hierarchical task analysis (HTA) was used to demonstrate the current medication administration process. Facility nurse managers in five units across the two facilities participated in focus group discussions to validate the observational data and to generate a reliable context-appropriate medication administration process. The potential errors or misconduct when passing the drugs were identified, such as unsafe storage and transportation of drugs from room to room. Those hazards would cause drug contamination, loss, or access by unauthorized individuals. Hospitals without 24-hour pharmacy coverage and other interruptions would hinder the medication administration process. Preparing drugs for more than one patient at a time would increase the risk of passing the drugs to the wrong patient. This study shows the use of observation and focus groups to describe and identify violations in the medication administration process. A clear road map for continuous clinical process improvement obtained from the current study could be used to help future health information technology implementation."]},
{"title": "Validation of standard ASTM F2732 and comparison with ISO 11079 with respect to comfort temperature ratings for cold protective clothing", "highlights": ["Both standards predict similar temperature ratings for 1.89\u00a0clo and 2 MET activity.", "The prediction is consistent with thermophysiological and thermal comfort responses.", "For 4 MET activity, the whole body thermal responses are on the cold side."], "abstract": ["American standard ASTM F2732 estimates the lowest environmental temperature for thermal comfort for cold weather protective clothing. International standard ISO 11079 serves the same purpose but expresses cold stress in terms of required clothing insulation for a given cold climate. The objective of this study was to validate and compare the temperature ratings using human subject tests at two levels of metabolic rates (2 and 4 MET corresponding to 116.4 and 232.8\u00a0W/m", "). Nine young and healthy male subjects participated in the cold exposure at 3.4 and\u00a0\u221230.6\u00a0\u00b0C. The results showed that both standards predict similar temperature ratings for an intrinsic clothing insulation of 1.89\u00a0clo and for 2 MET activity. The predicted temperature rating for 2 MET activity is consistent with test subjects' thermophysiological responses, perceived thermal sensation and thermal comfort. For 4 MET activity, however, the whole body responses were on the cold side, particularly the responses of the extremities. ASTM F2732 is also limited due to its omission and simplification of three climatic variables (air velocity, radiant temperature and relative humidity) and exposure time in the cold which are of practical importance."]},
{"title": "Assessment model for perceived visual complexity of automotive instrument cluster", "highlights": ["In-vehicle instrument cluster was divided into component for developing perceived visual complexity estimation model for each.", "Estimation model was developed based on objective measurement variables, questionnaires, and user experiment.", "Analysis on the influence of each component on in-vehicle instrument cluster was studied.", "Quantitative assessment model of instrument cluster developed based on estimated perceived visual complexity of components."], "abstract": ["This research proposes an assessment model for quantifying the perceived visual complexity (PVC) of an in-vehicle instrument cluster. An initial study was conducted to investigate the possibility of evaluating the PVC of an in-vehicle instrument cluster by estimating and analyzing the complexity of its individual components. However, this approach was only partially successful, because it did not take into account the combination of the different components with random levels of complexity to form one visual display. Therefore, a second study was conducted focusing on the effect of combining the different components. The results from the overall research enabled us to suggest a basis for quantifying the PVC of an in-vehicle instrument cluster based both on the PVCs of its components and on the integration effect."]},
{"title": "Design method for multi-user workstations utilizing anthropometry and preference data", "highlights": ["A method for sizing workstations with both personal and shared spaces is presented.", "The accommodation of pairs (not simply individuals) is considered.", "The method incorporates the anthropometric distributions of a target population.", "The method is applied to examples for sizing polygonal and circular workstations."], "abstract": ["Past efforts have been made to design single-user workstations to accommodate users' anthropometric and preference distributions. However, there is a lack of methods for designing workstations for ", " interaction. This paper introduces a method for sizing workstations to allow for a personal work area for each user and a shared space for adjacent users. We first create a virtual population with the same anthropometric and preference distributions as an intended demographic of college-aged students. Members of the virtual population are randomly paired to test if their extended reaches overlap but their normal reaches do not. This process is repeated in a Monte Carlo simulation to estimate the total percentage of groups in the population that will be accommodated for a workstation size. We apply our method to two test cases: in the first, we size polygonal workstations for two populations and, in the second, we dimension circular workstations for different group sizes."]},
{"title": "The effects of obesity, age, and relative workload levels on handgrip endurance", "highlights": ["The impacts of obesity and age on handgrip endurance across relative workload levels were examined.", "Obesity was associated with increased fatigability and perception of effort in younger adults.", "Current prediction models overestimate endurance times for the younger obese adults."], "abstract": ["The purpose of the study was to examine obesity and age effects on handgrip endurance across a range of relative workload levels. Forty-five non-obese and obese younger and older females performed fatiguing handgrip exercises at 20, 40, 60, and 80% of relative handgrip strength. The younger obese group demonstrated \u223c7% greater strength, 32% shorter endurance times, and \u223c34% faster rate of strength loss, accompanied by heightened perception of effort, than the younger non-obese group. However, these obesity-related differences were not observed in the older age group. Moreover, there were no interactions between relative workload levels, obesity, and age on any of the fatigue measures. Findings obtained here suggest that work-rest schedules computed from existing force endurance prediction models may not be protective of the younger obese working population."]},
{"title": "Ergonomics and sustainable development in the past two decades (1992\u20132011): Research trends and how ergonomics can contribute to sustainable development", "highlights": ["We review 10,000+ papers in the fields of ergonomics and sustainable development.", "The frequently researched areas in ergonomics have been identified.", "Ergonomics can contribute its knowledge in 4 categories of sustainable development.", "How to contribute ergonomics expertise to sustainable development are discussed."], "abstract": ["The need for sustainable development has been widely recognized and sustainable development has become a hot topic of various disciplines even though the role of ergonomics in it is seldom reported or considered. This study conducts a systematic survey of research publications in the fields of ergonomics and sustainable development over the past two decades (1992\u20132011), in order to identify their research trends and convergent areas where ergonomics can play an important role in sustainable development. The results show that \u2018methods and techniques\u2019, \u2018human characteristics\u2019, \u2018work design and organization\u2019, \u2018health and safety\u2019 and \u2018workplace and equipment design\u2019 are the top five frequently researched areas in ergonomics. Ergonomics has an opportunity to contribute its knowledge especially to \u2018industrial and product design\u2019, \u2018architecture\u2019, \u2018health and safety\u2019 and \u2018HCI\u2019 (especially for energy reduction issues) categories of sustainable development. Typical methodologies and general guidance on how to contribute the expertise of ergonomist to sustainable development are also discussed."]},
{"title": "Work debate spaces: A tool for developing a participatory safety management", "highlights": ["Studies emphasize the importance of initiating a discussion about actual work in organizations.", "There are few studies that demonstrate the practical effects of this discussion.", "We demonstrate how a structured debate can contribute to the persons and organization.", "We noted improvements not only on the safety but also the health and competences of the agents."], "abstract": ["In recent years, various studies have shown the importance of instituting work debate space within companies in order to address constraints within the organization. However, few of these studies demonstrate the implementation methods of discussion spaces and their contributions. Based on the action research developed in an electric company, this article demonstrates how work debate space (WDS) contribute to the development of an integrated safety culture. After describing the establishment methods and function of WDS within a technical group, we will present the main benefits of these spaces for the organization and its employees, and then discuss the minimal conditions for their implementation."]},
{"title": "Should electric fans be used during a heat wave?", "highlights": ["The effect of electric fan use on human heat balance during heat waves was modelled.", "Fans increase critical air temperature for elevated physiological strain by 3\u20134\u00a0\u00b0C.", "Model suggests fans would not have been harmful during any recent major heat wave.", "Current public health guidance seem to underestimate the evaporative power of fans.", "A simple guidance chart on electric fan use for healthcare practitioners is given."], "abstract": ["Heat waves continue to claim lives, with the elderly and poor at greatest risk. A simple and cost-effective intervention is an electric fan, but public health agencies warn against their use despite no evidence refuting their efficacy in heat waves. A conceptual human heat balance model can be used to estimate the evaporative requirement for heat balance, the potential for evaporative heat loss from the skin, and the predicted sweat rate, with and without an electrical fan during heat wave conditions. Using criteria defined by the literature, it is clear that fans increase the predicted critical environmental limits for both the physiological compensation of endogenous/exogenous heat, and the onset of cardiovascular strain by an air temperature of \u223c3\u20134\u00a0\u00b0C, irrespective of relative humidity (RH) for the young and elderly. Even above these critical limits, fans would apparently still provide marginal benefits at air temperatures as high as 51.1\u00a0\u00b0C at 10%RH for young adults and 48.1\u00a0\u00b0C at 10%RH for the elderly. Previous concerns that dehydration would be exacerbated with fan use do not seem likely, except under very hot (>40\u00a0\u00b0C) and dry (<10%RH) conditions, when predicted sweat losses are only greater with fans by a minor amount (\u223c20\u201330\u00a0mL/h). Relative to the peak outdoor environmental conditions reported during ten of the most severe heat waves in recent history, fan use would be advisable in all of these situations, even when reducing the predicted maximum sweat output for the elderly. The protective benefit of fans appears to be underestimated by current guidelines."]},
{"title": "Multi-stakeholder collaboration in the redesign of family-centered rounds process", "highlights": ["We explored stakeholder experience with collaboration in healthcare system redesign.", "We proposed a model of stakeholder collaboration in healthcare system redesign.", "We identified challenges to stakeholder collaboration in healthcare system redesign."], "abstract": ["A human factors approach to healthcare system redesign emphasizes the involvement of multiple healthcare stakeholders (e.g., patients and families, healthcare providers) in the redesign process. This study explores the experience of multiple stakeholders with collaboration in a healthcare system redesign project. Interviews were conducted with ten stakeholder representatives who participated in the redesign of the family-centered rounds process in a pediatric hospital. Qualitative interview data were analyzed using a phenomenological approach. A model of collaborative healthcare system redesign was developed, which defined four phases (i.e., setup of the redesign team, preparation for meetings, collaboration in meetings, follow-up after meetings) and two outcomes (i.e., team outcomes, redesign outcomes) of the collaborative process. Challenges to multi-stakeholder collaboration in healthcare system redesign, such as need to represent all relevant stakeholders, scheduling of meetings and managing different perspectives, were identified."]},
{"title": "Understanding the complex needs of automotive training at final assembly lines", "highlights": ["We performed user studies of assembly training for automotive assembly lines.", "We conducted interviews and observations at two automotive industries.", "There is a strong case for virtual training implementation.", "Virtual training is viewed as a complement of current training."], "abstract": ["Automobile final assembly operators must be highly skilled to succeed in a low automation environment where multiple variants must be assembled in quick succession. This paper presents formal user studies conducted at OPEL and VOLVO Group to identify assembly training needs and a subset of requirements; and to explore potential features of a hypothetical game-based virtual training system. Stakeholder analysis, timeline analysis, link analysis, Hierarchical Task Analysis and thematic content analysis were used to analyse the results of interviews with various stakeholders (17 and 28 participants at OPEL and VOLVO, respectively). The results show that there is a strong case for the implementation of virtual training for assembly tasks. However, it was also revealed that stakeholders would prefer to use a virtual training to complement, rather than replace, training on pre-series vehicles."]},
{"title": "Time to onset of pain: Effects of magnitude and location for static pressures applied to the plantar foot", "highlights": ["We introduced \u201ctime to pain onset,\u201d a measurement of sensitivity to static pressure.", "Time to pain onset was sensitive to magnitude and location of static pressure.", "Pain onset occurs earlier for higher levels of pressure and earlier at the midfoot.", "During standing, pain most often originated at the foot region of greatest pressure."], "abstract": ["Mechanisms that cause foot discomfort during prolonged standing are poorly understood. There is currently no method for evaluating discomfort associated with low levels of static pressure that are typical during standing. Pain thresholds were measured for 20 healthy participants by applying five levels of static pressure at different plantar foot locations. A survival analysis was performed to determine the effects of pressure magnitude and foot location on the time until pain onset. Time to pain onset was significantly affected by pressure magnitude (", "\u00a0<\u00a00.001); time decreased as pressure increased. Foot location was also significant (", "\u00a0<\u00a00.001); greatest times to pain onset (least sensitive) were observed under the heel and fifth metatarsal head, shortest times (most sensitive) were found under the midfoot. This research presents a novel methodology for evaluating static pressure that may be applicable to product design."]},
{"title": "Evaluating the physical demands on firefighters using track-type stair descent devices to evacuate mobility-limited occupants from high-rise buildings", "highlights": ["Arm and back muscle forces differ across track-type stair descent devices (SDDs).", "Track type SDD's have egress speeds similar to pedestrian egress speeds.", "Smaller track-type chairs had faster egress and were perceived as easier to use.", "On the landings, the number of wheels affects shoulders and arm muscle use."], "abstract": ["The physical demands on firefighting personnel were investigated when using different types of track-type stair descent devices designed for the emergency evacuation of high rise buildings as a function of staircase width and evacuation urgency. Twelve firefighters used five track-type stair descent devices during simulated urgent and non-urgent evacuations. The devices were evaluated under two staircase width conditions (1.12, and 1.32\u00a0m), and three devices were also evaluated under a narrower staircase condition (0.91\u00a0m). Dependent measures included electromyographic (EMG) data, spine motion, heart rates, Borg Scale ratings, task durations and descent velocities. Stair descent speeds favored the devices that had shorter fore/aft dimensions when moving through the landing. EMG results indicated that there were tradeoffs due to design features, particularly on the landings where the physical demands tended to be greater. On the landings, devices that could be rolled on four wheels reduced the deltoid and bicep activation levels."]},
{"title": "Use of body armor protection with fighting load impacts soldier performance and kinematics", "highlights": ["Large increases in body armor protection decrease soldier performance.", "Increased body armor protection did not decrease soldier mobility.", "Adding the fighting load carrier was detrimental to mobility.", "The fighting load carrier increased trunk extension during soldier specific tasks."], "abstract": ["The purpose of this evaluation was to examine how increasing body armor protection with and without a fighting load impacted soldiers' performance and mobility. Thirteen male soldiers performed one performance (repeated 30-m rushing) and three mobility tasks (walk, walk over and walk under) with three different body armor configurations and an anterior fighting load. Increasing body armor protection, decreased soldier performance, as individual and total 30-m rush times were significantly longer with greater protection. While increasing body armor protection had no impact on mobility, i.e. significant effect on trunk and lower limb biomechanics, during the walk and walk over tasks, greater protection did significantly decrease maximum trunk flexion during the walk under task. Adding fighting load may negatively impact soldier mobility, as greater maximum trunk extension was evident during the walk and walk over tasks, and decreased maximum trunk flexion exhibited during the walk under task with the fighting load."]},
{"title": "A socio-technical approach to improving retail energy efficiency behaviours", "highlights": ["This research explores how energy efficiency improvements within a large retail company are influenced by job design and goal setting.", "The study describes a longitudinal qualitative case study examining efforts to improve behavioural energy efficiency over a two- year period.", "Our findings highlight a set of socio-technical and goal-setting factors which both impede and/or enable energy efficient behaviours.", "Insights concerning management of energy tasks secondary goals in order to reduce multiple goal conflict are provided."], "abstract": ["In recent years, the UK retail sector has made a significant contribution to societal responses on carbon reduction. We provide a novel and timely examination of environmental sustainability from a systems perspective, exploring how energy-related technologies and strategies are incorporated into organisational life. We use a longitudinal case study approach, looking at behavioural energy efficiency from within one of the UK's leading retailers. Our data covers a two-year period, with qualitative data from a total of 131 participants gathered using phased interviews and focus groups. We introduce an adapted socio-technical framework approach in order to describe an existing organisational behavioural strategy to support retail energy efficiency. Our findings point to crucial socio-technical and goal-setting factors which both impede and/or enable energy efficient behaviours, these include: tensions linked to store level perception of energy management goals; an emphasis on the importance of technology for underpinning change processes; and, the need for feedback and incentives to support the completion of energy-related tasks. We also describe the evolution of a practical operational intervention designed to address issues raised in our findings. Our study provides fresh insights into how sustainable workplace behaviours can be achieved and sustained over time. Secondly, we discuss in detail a set of issues arising from goal conflict in the workplace; these include the development of a practical energy management strategy to facilitate secondary organisational goals through job redesign."]},
{"title": "Influence of different shoulder-elbow configurations on steering precision and steering velocity in automotive context", "highlights": ["Influence of posture on driving precision and steering velocity was investigated.", "Arm posture influences steering precision and steering velocity.", "Steering precision and velocity are significantly increased in mid-positions.", "Driver safety can be enhanced by implementing these data in the design process.", "Subjective comfort rating confirmed experimental results."], "abstract": ["Ergonomic design requirements are needed to develop optimum vehicle interfaces for the driver. The majority of the current specifications consider only anthropometric conditions and subjective evaluations of comfort. This paper examines specific biomechanical aspects to improve the current ergonomic requirements. Therefore, a research which involved 40 subjects was carried out to obtain more knowledge in the field of steering movement while driving a car. Five different shoulder-elbow joint configurations were analyzed using a driving simulator to find optimum posture for driving in respect of steering precision and steering velocity. Therefore, a 20\u00a0s precision test and a test to assess maximum steering velocity over a range of 90\u00b0 steering motion have been conducted. The results show that driving precision, as well as maximum steering velocity, are significantly increased in mid-positions (elbow angles of 95\u00b0 and 120\u00b0) compared to more flexed (70\u00b0) or extended (145\u00b0 and 160\u00b0) postures. We conclude that driver safety can be enhanced by implementing these data in the automotive design process because faster and highly precise steering can be important during evasive actions and in accident situations. In addition, subjective comfort rating, analyzed with questionnaires, confirmed experimental results."]},
{"title": "Interaction between physical and psychosocial work risk factors for low back symptoms and its consequences amongst Indonesian coal mining workers", "highlights": ["Interaction between physical and psychosocial exposures was present and significant for reduced activities due to LBS.", "Interactions between physical and psychosocial exposures were also present and for LBS and absenteeism, but not significant.", "Workers with physical and psychosocial exposures were most likely to report LBS and its consequences.", "High psychosocial exposure appears to play a role in reporting LBS and particularly of its consequences.", "Current smokers increased the odds of reduced activities and absenteeism due to LBS."], "abstract": ["This study assessed the interaction between physical and psychosocial factors for low back symptoms (LBS) and its consequences (reduced activities and absenteeism) in a developing country. A sample of 1294 Indonesian coal mining workers reported occupational exposures, LBS and its consequences using a self-administered questionnaire. Respondents were placed into one of four combination exposure groups: high physical and high psychosocial (HPhyHPsy); high physical and low psychosocial (HPhyLPsy); low physical and high psychosocial (LPhyHPsy), and; low physical and low psychosocial (LPhyLPsy). The attributable proportion due to interaction between physical and psychosocial factors was examined. Individuals in the HPhyHPsy group were most likely to report LBS (OR 5.42, 95% CI 3.30\u20138.89), reduced activities (OR 4.89, 95% CI 3.09\u20137.74), and absenteeism (OR 4.96, 95% CI 3.05\u20138.06). Interactions between physical and psychosocial factors were present for LBS, reduced activities, and absenteeism; although for LBS and absenteeism the interactions were not significant. Current smokers were more likely to report LBS consequences. Permanent employment and night shift work increased the odds of LBS and its consequences. We conclude that interventions aimed at reducing LBS and its consequences should address both physical and psychosocial factors, with a focus on smokers, permanent employment and night shift work."]},
{"title": "Impact of automation: Measurement of performance, workload and behaviour in a complex control environment", "highlights": ["We compared three levels of automation in an ecologically valid rail signalling environment.", "We measured performance, subjective workload and operator activity for each level.", "Workload decreased as automation increased for both normal and disrupted conditions.", "Performance became more consistent across participants as automation increased.", "Individual operators engage in different strategies when working with automation."], "abstract": ["This paper describes an experiment that was undertaken to compare three levels of automation in rail signalling; a high level in which an automated agent set routes for trains using timetable information, a medium level in which trains were routed along pre-defined paths, and a low level where the operator (signaller) was responsible for the movement of all trains. These levels are described in terms of a Rail Automation Model based on previous automation theory (Parasuraman et\u00a0al., 2000). Performance, subjective workload, and signaller activity were measured for each level of automation running under both normal operating conditions and abnormal, or disrupted, conditions. The results indicate that perceived workload, during both normal and disrupted phases of the experiment, decreased as the level of automation increased and performance was most consistent (i.e. showed the least variation between participants) with the highest level of automation. The results give a strong case in favour of automation, particularly in terms of demonstrating the potential for automation to reduce workload, but also suggest much benefit can achieved from a mid-level of automation potentially at a lower cost and complexity."]},
{"title": "The influence of body mass on foot dimensions during pregnancy", "highlights": ["This study used a time-series approach to measure women\u2019s feet during pregnancy.", "After 20 weeks of pregnancy, occurrence of pregnant women\u2019s foot discomfort generally increased.", "During pregnancy, women\u2019s foot size increased, whereas the height of arch decreased.", "Body mass accounted for more than 90% of the variation (", ") in foot dimensions during pregnancy."], "abstract": ["In this study, a time-series approach was used to measure women's feet to accurately analyze changes in foot size and body mass during pregnancy. One-hundred women who were pregnant for the first time were asked to respond to questions on subjective complaints of foot discomfort listed in a questionnaire. Among these 100 women, a sample of 30 was obtained and used to measure the women's feet from the twentieth week of the gestation period until labor. The data (from 5 of the 30 women) were used to establish a prediction model for the influence of body mass on changes in foot size during pregnancy. The results indicate that the women subjectively complained that their shoes were too tight, resulting in foot discomfort. From the twentieth to the thirty-eighth week of pregnancy, the average increase in foot length, width, and back foot surface was 0.86\u00a0cm (3.6%), 0.25\u00a0cm (2.6%), and 18.36\u00a0cm", " (11.9%), respectively. The height of the arch decreased by an average of 0.52\u00a0cm (\u221224.2%). Body mass accounted for more than 90% of the variation (", ") in foot dimensions during pregnancy and, thus indicated satisfactory predictive ability. The prediction model developed in this study can serve as a reference for clinical applications and shoe design to prevent women from experiencing extreme discomfort in their feet during pregnancy."]},
{"title": "Analysis of the most relevant anthropometric dimensions for school furniture selection based on a study with students from one Chilean region", "highlights": ["The Chilean student population has a different body proportion regarding age and gender.", "The levels of mismatch, especially Seat Height, are influenced by body proportion.", "Popliteal Height is the most accurate anthropometric measure for the furniture selection purposes."], "abstract": ["Most of the worldwide standards used for furniture selection suggest the use of the Stature of the school children, assuming that all the other anthropometric characteristics will also be appropriate. However, it is important to consider that students' growth differ with age. The aim of this study is to determine if Popliteal Height can be used as a better, or more adequate, measure for classroom furniture selection when comparing with Stature. This study involved a representative group of 3046 students from the Valpara\u00edso Region, in Chile. Regarding the methodology, eight anthropometric measures were gathered, as well as six furniture dimensions from the Chilean standard. After assigning the level of school furniture using Stature and Popliteal Height to each of the students, six mismatch equations were applied. The results show that when using Popliteal Height, higher levels of match were obtained for the two more important furniture dimensions. Additionally, it also presents a better cumulative fit than Stature. In conclusion, it seems that Popliteal Height can be the most accurate anthropometric measure for classroom furniture selection purposes."]},
{"title": "A week in the life of full-time office workers: Work day and weekend light exposure in summer and winter", "highlights": ["Measured 24-h light exposure in full-time office workers in summer & winter weeks.", "Consistent workday/weekend schedules and circadian phase between summer & winter.", "Morning light exposure was greater on workdays versus weekends in both seasons.", "Early evening light exposure was greater in summer versus winter on all days.", "In the 2\u00a0h before sleep onset, light exposure was consistently low (<20\u00a0lux)."], "abstract": ["Little is known about the light exposure in full-time office workers, who spend much of their workdays indoors. We examined the 24-h\u00a0light exposure patterns of 14 full-time office workers during a week in summer, and assessed their dim light melatonin onset (DLMO, a marker of circadian timing) at the end of the working week. Six workers repeated the study in winter. Season had little impact on the workers' schedules, as the timing of sleep, commute, and work did not vary by more than 30\u00a0min in the summer and winter. In both seasons, workers received significantly more morning light on workdays than weekends, due to earlier wake times and the morning commute. Evening light in the two hours before bedtime was consistently dim. The timing of the DLMO did not vary between season, and by the end of the working week, the workers slept at a normal circadian phase."]},
{"title": "The Threat-Strategy Interview", "highlights": ["A structured interview (TSI) elicited strategies to combat threats.", "Threats obstruct safe task completion.", "Strategies are plans to achieve goals.", "The TSI can be applied to uncover strategic thinking in dynamic environments."], "abstract": ["Operators in dynamic work environments use strategies to manage threats in order to achieve task goals. We introduce a structured interview method, the Threat-Strategy Interview (TSI), and an accompanying qualitative analysis to induce operator-level threats, strategies, and the cues that give rise to them. The TSI can be used to elicit knowledge from operators who are on the front line of managing threats to provide an understanding of strategic thinking, which in turn can be applied toward a variety of problems."]},
{"title": "Risk factors for carpal tunnel syndrome related to the work organization: A prospective surveillance study in a large working\u00a0population", "highlights": ["The study showed the association of CTS with some factors related to the work organization.", "Payment on a piecework basis was associated with increased risk of CTS.", "Work pace dependent on automatic rate was associated with increased risk of CTS.", "Work organization should be an important target for the prevention of CTS."], "abstract": ["The study aimed to determine the risk factors for incident carpal tunnel syndrome (CTS) in a large working population, with a special focus on factors related to work organization. In 2002\u20132005, 3710 workers were assessed and, in 2007\u20132010, 1611 were re-examined. At baseline all completed a self-administered questionnaire about personal/medical factors and work exposure. CTS symptoms and physical examination signs were assessed by a standardized medical examination at baseline and follow-up. The risk of \u201csymptomatic CTS\u201d was higher for women (OR\u00a0=\u00a02.9 [1.7\u20135.2]) and increased linearly with age (OR\u00a0=\u00a01.04 [1.00\u20131.07] for 1-year increment). Two work organizational factors remained in the multivariate risk model after adjustment for the personal/medical and biomechanical factors: payment on a piecework basis (OR\u00a0=\u00a02.0, 95% CI 1.1\u20133.5) and work pace dependent on automatic rate (OR\u00a0=\u00a01.9, 95% CI 0.9\u20134.1). Several factors related to work organization were associated with incident CTS after adjustment for potential confounders."]},
{"title": "A user study of auditory, head-up and multi-modal displays in vehicles", "highlights": ["The users were asked to perform several tasks with the aid of a menu while driving.", "The menu was presented through a visual head-up, an audio and a multi-modal display.", "The visual and multi-modal displays were more efficient than the audio display.", "All displays had a similar impact on driving performance.", "The majority of users selected the multi-modal display as the easiest to use."], "abstract": ["This paper describes a user study on the interaction with an in-vehicle information system (IVIS). The motivation for conducting this research was to investigate the subjectively and objectively measured impact of using a single- or multi-modal IVIS while driving. A hierarchical, list-based menu was presented using a windshield projection (head-up display), auditory display and a combination of both interfaces. The users were asked to navigate a vehicle in a driving simulator and simultaneously perform a set of tasks of varying complexity. The experiment showed that the interaction with visual and audio-visual head-up displays is faster and more efficient than with the audio-only display. All the interfaces had a similar impact on the overall driving performance. There was no significant difference between the visual only and audio-visual displays in terms of their efficiency and safety; however, the majority of test subjects clearly preferred to use the multi-modal interface while driving."]},
{"title": "Energy expenditure at work in physical education teachers", "highlights": ["Energy expenditure (EE) was measured in physical educators during 2 workdays.", "Physical education teachers showed a high level of fitness level.", "Average EE was low-to-moderate (kcal\u00a0min", ") and low in relative values (% VO", "max).", "However, physical educators reached very high work intensities for significant periods.", "Physical educators should be prepared to perform tasks with different levels of EE."], "abstract": ["The objective of this study was to quantify work energy expenditure (EE) in physical education (PE) teachers. Sixty-four (64) physical educators (49 men, 15 women) had their individualized linear function between heart rate (HR) and oxygen consumption measured by laboratory testing. HR was then recorded on 2 different days at work to estimate EE, correlated with a diary of daily tasks. Average absolute EE was low-to-moderate (2.7\u00a0\u00b1\u00a01.4 to 4.6\u00a0\u00b1\u00a02.5\u00a0kcal\u00b7min", ") and low when expressed in relative values (15.3\u00a0\u00b1\u00a06.1% to 24.8\u00a0\u00b1\u00a07.6% of VO", "max). However, these physical educators often reached very high intensities (from 7.5\u00a0\u00b1\u00a07.9% to 23.8\u00a0\u00b1\u00a022.3% of work time at 100\u00a0bpm and more). PE teaching requires a\u00a0light-to-moderate EE with more intense periods of physical activity. The variety of tasks performed (office work, supervision and monitoring, mixed participation and active participation) significantly influenced EE."]},
{"title": "Breakdowns in coordinated decision making at and above the incident management team level: An analysis of three large scale Australian wildfires", "highlights": ["Breakdowns and disconnects occur during emergency management.", "There are three main types of disconnects: operational, informational and evaluative.", "Some disconnects occur in a temporal sequence.", "Informational and operational disconnects were often unresolved.", "Unresolved disconnects may lead to impaired team functioning."], "abstract": ["Emergency situations are by their nature difficult to manage and success in such situations is often highly dependent on effective team coordination. Breakdowns in team coordination can lead to significant disruption to an operational response. Breakdowns in coordination were explored in three large-scale bushfires in Australia: the Kilmore East fire, the Wangary fire, and the Canberra Firestorm. Data from these fires were analysed using a top-down and bottom-up qualitative analysis technique. Forty-four breakdowns in coordinated decision making were identified, which yielded 83 disconnects grouped into three main categories: operational, informational and evaluative. Disconnects were specific instances where differences in understanding existed between team members. The reasons why disconnects occurred were largely consistent across the three sets of data. In some cases multiple disconnects occurred in a temporal manner, which suggested some evidence of disconnects creating states that were conducive to the occurrence of further disconnects. In terms of resolution, evaluative disconnects were nearly always resolved however operational and informational disconnects were rarely resolved effectively. The exploratory data analysis and discussion presented here represents the first systematic research to provide information about the reasons why breakdowns occur in emergency management and presents an account of how team processes can act to disrupt coordination and the operational response."]},
{"title": "The effect of sustained static kneeling on kinetic and kinematic knee joint gait parameters", "highlights": ["Altered gait profiles are known to predispose the onset of knee osteoarthritis.", "A model is proposed for the link between static kneeling and knee osteoarthritis.", "The study shows that 30\u00a0min of static kneeling alters young adult gait profiles.", "The findings support the proposed model."], "abstract": ["Despite epidemiological evidence for kneeling as an occupational risk factor for knee osteoarthritis, biomechanical evidence is lacking. Gait knee joint mechanics, a common measure used to study knee osteoarthritis initiation, were used in the present study to investigate the effect of sustained static kneeling on the knee. Ten healthy male subjects (24.1\u00a0years\u00a0\u00b1\u00a03.5) performed ten baseline walking trials, followed by a 30-min kneeling protocol and a second set of walking trials. Knee joint moments and angles were calculated during the stance phase. Within-subject root mean squared differences were compared within and between the pre- and post-kneeling gait trials. Differences were observed between the pre-kneeling and post-kneeling walking trails for flexion and adduction knee moments (0.12\u00a0Nm/kg\u00a0\u00b1\u00a00.03, 0.07\u00a0Nm/kg\u00a0\u00b1\u00a00.02) and angles (3.18\u00b0\u00a0\u00b1\u00a01.22 and 1.64\u00b0\u00a0\u00b1\u00a01.15), indicating that sustained static deep-knee flexion kneeling does acutely alter knee joint gait parameters."]},
{"title": "An implementation evaluation of a qualitative culture assessment tool", "highlights": ["Evaluates a qualitative tool for assessing safety culture as it impacts on MSD.", "Cultural advancement was assessed across nine aspects in two organisations.", "Different safety culture aspects develop at different levels of advancement.", "There are important prerequisites for using the tool to assess MSD culture.", "The tool could be considered as part of a suite of methods to manage MSD risk."], "abstract": ["Safety culture has been identified as a critical element of healthy and safe workplaces and as such warrants the attention of ergonomists involved in occupational health and safety (OHS). This study sought to evaluate a tool for assessing organisational safety culture as it impacts a common OHS problem: musculoskeletal disorders (MSD). The level of advancement across nine cultural aspects was assessed in two implementation site organisations. These organisations, in residential healthcare and timber processing, enabled evaluation of the tool in contrasting settings, with reported MSD rates also high in both sectors. Interviews were conducted with 39 managers and workers across the two organisations. Interview responses and company documentation were compared by two researchers to the descriptor items for each MSD culture aspect. An assignment of the level of advancement, using a five stage framework, was made for each aspect. The tool was readily adapted to each implementation site context and provided sufficient evidence to assess their levels of advancement. Assessments for most MSD culture aspects were in the mid to upper levels of advancement, although the levels differed within each organisation, indicating that different aspects of MSD culture, as with safety culture, develop at a different pace within organisations. Areas for MSD culture improvement were identified for each organisation. Reflections are made on the use and merits of the tool by ergonomists for addressing MSD risk."]},
{"title": "The impact of different types of textile liners used in protective footwear on the subjective sensations of firefighters", "highlights": ["A tool for evaluating the comfort of use of firefighter footwear is presented.", "New construction of liners was tested in terms of the subjective sensations of firefighters.", "Higher comfort was observed for the liner with a superabsorbent and a ventilation system."], "abstract": ["The paper presents ergonomic evaluation of footwear used with three types of textile liners differing in terms of design and material composition. Two novel textile composite liners with enhanced hygienic properties were compared with a standard liner used in firefighter boots. The study involved 45 healthy firefighters from fire and rescue units who wore protective footwear with one of the three types of liners. The study was conducted in a laboratory under a normal atmosphere. The ergonomic properties of the protective footwear and liners were evaluated according to the standard ", " as well as using an additional questionnaire concerning the thermal and moisture sensations experienced while wearing the footwear. The study was conducted on a much larger group of subjects (45) than that required by the ISO standard (3) to increase the reliability of subjective evaluations. Some statistically significant differences were found between the different types of textile liners used in firefighter boots. It was confirmed that the ergonomic properties of protective footwear worn in the workplace may be improved by the use of appropriate textile components."]},
{"title": "Is what you see what you get? Standard inclinometry of set upper arm elevation angles", "highlights": ["Meticulously set upper arm elevation angles were measured by standard inclinometry.", "A downward bias was found in the inclinometer results, particularly at angles\u00a0>\u00a060\u00b0.", "A 2-point, subject-specific, linear calibration was effective at removing bias.", "Calibration equations were developed for studies lacking 2-point calibration data.", "Elevations gauged by inclinometry and observation need adjustment to a common scale."], "abstract": ["Previous research suggests inclinometers (INC) underestimate upper arm elevation. This study was designed to quantify possible bias in occupationally relevant postures, and test whether INC performance could be improved using calibration.", "Participants were meticulously positioned in set arm flexion and abduction angles between 0\u00b0 and 150\u00b0. Different subject-specific and group-level regression models comprising linear and quadratic components describing the relationship between set and INC-registered elevation were developed using subsets of data, and validated using additional data.", "INC measured arm elevation showed a downward bias, particularly above 60\u00b0. INC data adjusted using the regression models were superior to unadjusted data; a subject-specific, two-point calibration based on measurements at 0\u00b0 and 90\u00b0 gave results closest to the \u2018true\u2019 set angles.", "Thus, inclinometer measured arm elevation data required calibration to arrive at \u2018true\u2019 elevation angles. Calibration to a common measurement scale should be considered when comparing arm elevation data collected using different methods."]},
{"title": "Scalable interrogation: Eliciting human pheromone responses to deception in a security interview setting", "highlights": ["For the first time a stress pheromone was identified in deception interrogations.", "Participants concealing information responded differently to innocent participants.", "Biological, physiological, psychological and behavioural stress data were collected.", "This research validated a unique scalable interrogation paradigm."], "abstract": ["Individuals trying to conceal knowledge from interrogators are likely to experience raised levels of stress that can manifest itself across biological, physiological, psychological and behavioural factors, providing an opportunity for detection. Using established research paradigms an innovative scalable interrogation was designed in which participants were given a \u2018token\u2019 that represented information they had to conceal from interviewers. A control group did not receive a token and therefore did not have to deceive the investigators. The aim of this investigation was to examine differences between deceivers and truth-tellers across the four factors by collecting data for cortisol levels, sweat samples, heart-rate, respiration, skin temperature, subjective stress ratings and video and audio recordings. The results provided an integrated understanding of responses to interrogation by those actively concealing information and those acting innocently. Of particular importance, the results also suggest, for the first time in an interrogation setting, that stressed individuals may secrete a volatile steroid based marker that could be used for stand-off detection. The findings are discussed in relation to developing a scalable interrogation protocol for future research in this area."]},
{"title": "Heat strain evaluation of overt and covert body armour in a hot and humid environment", "highlights": ["We examine the thermophysiological effects of wearing lightweight non-military body armour.", "Particular focus is paid to wearing different types of armour in a hot and humid environment.", "Heat strain encountered was negligible compared to no armour."], "abstract": ["The aim of this study was to elucidate the thermophysiological effects of wearing lightweight non-military overt and covert personal body armour (PBA) in a hot and humid environment. Eight healthy males walked on a treadmill for 120\u00a0min\u00a0at 22% of their heart rate reserve in a climate chamber simulating 31\u00a0\u00b0C (60%RH) wearing either no armour (control), overt or covert PBA in addition to a security guard uniform, in a randomised controlled crossover design. No significant difference between conditions at the end of each trial was observed in core temperature, heart rate or skin temperature (", "\u00a0>\u00a00.05). Covert PBA produced a significantly greater amount of body mass change (\u22121.81\u00a0\u00b1\u00a00.44%) compared to control (\u22121.07\u00a0\u00b1\u00a00.38%, ", "\u00a0=\u00a00.009) and overt conditions (\u22121.27\u00a0\u00b1\u00a00.44%, ", "\u00a0=\u00a00.025). Although a greater change in body mass was observed after the covert PBA trial; based on the physiological outcome measures recorded, the heat strain encountered while wearing lightweight, non-military overt or covert PBA was negligible compared to no PBA.", "The wearing of bullet proof vests or body armour is a requirement of personnel engaged in a wide range of occupations including police, security, customs and even journalists in theatres of war. This randomised controlled crossover study is the first to examine the thermophysiological effects of wearing lightweight non-military overt and covert personal body armour (PBA) in a hot and humid environment. We conclude that the heat strain encountered while wearing both overt and covert lightweight, non-military PBA was negligible compared to no PBA."]},
{"title": "The influence of officer equipment and protection on short sprinting performance", "highlights": ["We examined officer equipment weight and a lateral focal point influences on a short sprint.", "The belt averaged 11.47\u00a0\u00b1\u00a01.64% of subject body mass.", "Significant decreases in velocity and acceleration were found for all weight belt trials.", "No performance decreases were observed with a focal point."], "abstract": ["As advances in protective equipment are made, it has been observed that the weight law enforcement officers must carry every day is greatly increasing. Many investigations have noted the health risks of these increases, yet none have looked at its effects on officer mobility. The primary purpose of this study was to examine the influence of both the weight of officer safety equipment, as well as a lateral focal point (FP), on the stride length, stride velocity, and acceleration of the first six strides of a short sprint. Twenty male law enforcement students performed two maximal effort sprint trials, in the participating college's gymnasium, from each of four starting positions: forwards (control position), backwards, 90\u00b0 left, and 90\u00b0 right. Subjects placed in the FP group (", "\u00a0=\u00a09) were required to maintain focus on lateral FP during the 90\u00b0 left and 90\u00b0 right trials, and a forwards FP during the backwards trials. On a second testing date, subjects repeated the sprint tests while wearing a 9.07\u00a0kg weight belt, simulating officer equipment and protective gear. The belt averaged 11.47\u00a0\u00b1\u00a01.64% of subject body mass. A significant main effect of weight belt trials was found (", "\u00a0=\u00a020.494, ", "\u00a0<\u00a00.01), in which significant decreases were found for velocity and acceleration. No other significant effects were found as a result of starting position or focal point and no significant interactions were found between independent variables. Conclusively, the results of this study show the increasing weights of duty gear and protective equipment have detrimental effects on officer velocity and acceleration, impeding their mobility, which may be dangerous in use of force or threatening situations."]},
{"title": "Designing and optimizing a healthcare kiosk for the community", "highlights": ["We outline the development of an interactive self-service healthcare kiosk.", "We apply a formal methodology to guarantee the measurement accuracy.", "The formal, generalizable and rigorous approach shows its practical efficiency.", "We know of no other studies that apply such methods in designing interaction.", "There is globally an increasing need for the health technologies outlined herein."], "abstract": ["Investigating new ways to deliver care, such as the use of self-service kiosks to collect and monitor signs of wellness, supports healthcare efficiency and inclusivity. Self-service kiosks offer this potential, but there is a need for solutions to meet acceptable standards, e.g. provision of accurate measurements. This study investigates the design and optimization of a prototype healthcare kiosk to collect vital signs measures. The design problem was decomposed, formalized, focused and used to generate multiple solutions. Systematic implementation and evaluation allowed for the optimization of measurement accuracy, first for individuals and then for a population. The optimized solution was tested independently to check the suitability of the methods, and quality of the solution. The process resulted in a reduction of measurement noise and an optimal fit, in terms of the positioning of measurement devices. This guaranteed the accuracy of the solution and provides a general methodology for similar design problems."]},
{"title": "Work with prolonged arm elevation as a risk factor for shoulder pain: A longitudinal study among young adults", "highlights": ["Longitudinal association between work with arm elevation and shoulder pain in the first years of working life.", "An association between prolonged arm elevation and shoulder pain was found among women.", "Low levels of shoulder pain are found. The associations may nevertheless be early indicators of more severe shoulder pain."], "abstract": ["This prospective study aimed at examining if work with prolonged arm elevation predicts shoulder pain among 41 young adults in their first years of working life. Fifteen hairdressers, 15 electricians, 5 students and 6 with various work were followed over a 2.5-year period (2006/7\u20132009). Arm elevation was measured with inclinometers during a full working day at baseline. Shoulder pain was reported at baseline and twice in the follow-up period. Data were analyzed by generalized estimating equations (GEE-analysis), stratified by gender and adjusted for time, mechanical workload, work demand, physical activity, tobacco use and prior shoulder pain. Work with prolonged arm elevation with angles >60\u00b0 and >90\u00b0 were associated with shoulder pain among women. Even though the shoulder pain levels are low the study suggests work with arms elevated as an early work-related risk factor among women, and indicates the importance of early prevention strategies."]},
{"title": "Optimizing the physical ergonomics indices for the use of partial pressure suits", "highlights": ["The results showed that 12 pairs of ergonomics indices were linearly correlated.", "An optimal ergonomics index system for partial pressure suits (PPSs) was determined.", "It includes manipulative mission, operational reach and operational strength indices.", "The ergonomics performance of PPSs can be conveniently and quickly evaluated."], "abstract": ["This study developed an ergonomic evaluation system for the design of high-altitude partial pressure suits (PPSs). A total of twenty-one Chinese males participated in the experiment which tested three types of ergonomics indices (manipulative mission, operational reach and operational strength) were studied using a three-dimensional video-based motion capture system, a target-pointing board, a hand dynamometer, and a step-tread apparatus. In total, 36 ergonomics indices were evaluated and optimized using regression and fitting analysis. Some indices that were found to be linearly related and redundant were removed from the study. An optimal ergonomics index system was established that can be used to conveniently and quickly evaluate the performance of different pressurized/non-pressurized suit designs. The resulting ergonomics index system will provide a theoretical basis and practical guidance for mission planners, suit designers and engineers to design equipment for human use, and to aid in assessing partial pressure suits."]},
{"title": "The impact of work time control on physicians' sleep and well-being", "highlights": ["We report a cross-sectional survey of hospital physicians in Sweden.", "More work time control predicted less fatigue after work.", "Work time control buffered some of the negative effects of night work."], "abstract": ["Physicians' work schedules are an important determinant of their own wellbeing and that of their patients. This study considers whether allowing physicians control over their work hours ameliorates the effects of demanding work schedules. A questionnaire was completed by hospital physicians regarding their work hours (exposure to long shifts, short inter-shift intervals, weekend duties, night duties, unpaid overtime; and work time control), sleep (quantity and disturbance) and wellbeing (burnout, stress and fatigue). Work time control moderated the negative impact that frequent night working had upon sleep quantity and sleep disturbance. For participants who never worked long shifts, work time control was associated with fewer short sleeps, but this was not the case for those who did work long shifts. Optimizing the balance between schedule flexibility and patient needs could enhance physicians\u2019 sleep when working the night shift, thereby reducing their levels of fatigue and enhancing patient care."]},
{"title": "Strategic threat management: An exploration of nursing strategies in the pediatric intensive care unit", "highlights": ["Strategic threat management of ICU nurses was studied with structured interviews.", "We classified and mapped strategies to threats and to the cues that evoked them.", "Most threats involved technology, staff, or organization.", "Most strategies involved staff and patient, or were cocktails of work facets (treatment, exam).", "Most threats were managed by strategies that reflected the threat's work facet."], "abstract": ["Part of the work of a critical care nurse is to manage the threats that arise that could impede efficient and effective job performance. Nurses manage threats by employing various strategies to keep performance high and workload manageable. We investigated strategic threat management by using the Threat-Strategy Interview. Threats frequently involved technology, staff, or organizational components. The threats were managed by a toolbox of multifaceted strategies, the most frequent of which involved staff-, treatment- (patient\u00a0+\u00a0technology), examination- (patient\u00a0+\u00a0clinician), and patient-oriented strategies. The profile of strategies for a particular threat often leveraged work facets similar to the work facet that characterized the threat. In such cases, the nurse's strategy was directed at eliminating the threat (not working around it). A description at both a domain invariant level \u2013 useful for understanding strategic threat management generally \u2013 and a description at an operational, specific level \u2013 useful for guiding interventions-- are presented. A structural description of the relationship among threats, strategies, and the cues that trigger them is presented in the form of an evidence accumulation framework of strategic threat management."]},
{"title": "The relationship between physical load and musculoskeletal complaints among Brazilian dentists", "highlights": ["We examine musculoskeletal complaints in nine joints of dentists.", "Clearly association between complaints of pain with awkward working positions.", "Complaints of pain do not generate absenteeism or a loss of productivity at work."], "abstract": ["The aim of the present study was to assess the relationship between physical load and musculoskeletal complaints in dentistry and to analyze the prevalence and severity of such complaints in nine anatomical regions using a cross-sectional study of 387 dentists from Natal, Brazil. The highest prevalence of complaints was related to the lower back (58.4%) and the lowest prevalence was found in the elbow (10.3%). In general, symptoms were classified as mild because they did not cause absence due to illness. Pain complaints were associated with the following characteristics: awkward posture at work; prolonged standing or sitting; strenuous position of the upper limbs; excessive tightening of the hands during clinical treatment; and the use of vibrating tools. The results of the present study suggest a high prevalence of musculoskeletal complaints in dentists that are significantly associated with variables related to their physical workload."]},
{"title": "A vertical mouse and ergonomic mouse pads alter wrist position but do not reduce carpal tunnel pressure in patients with carpal tunnel syndrome", "highlights": ["Computer use increases carpal tunnel pressure in patients with carpal tunnel syndrome.", "Ergonomic devices alter wrist and forearm positions during computer use.", "None of the tested ergonomic devices reduced carpal tunnel pressure.", "A strong recommendation for or against ergonomic devices cannot be endorsed.", "Selection will depend on the patient's preference until further evidence is available."], "abstract": ["Non-neutral wrist positions and external pressure leading to increased carpal tunnel pressure during computer use have been associated with a heightened risk of carpal tunnel syndrome (CTS). This study investigated whether commonly used ergonomic devices reduce carpal tunnel pressure in patients with CTS. Carpal tunnel pressure was measured in twenty-one patients with CTS before, during and after a computer mouse task using a standard mouse, a vertical mouse, a gel mouse pad and a gliding palm support. Carpal tunnel pressure increased while operating a computer mouse. Although the vertical mouse significantly reduced ulnar deviation and the gel mouse pad and gliding palm support decreased wrist extension, none of the ergonomic devices reduced carpal tunnel pressure. The findings of this study do therefore not endorse a strong recommendation for or against any of the ergonomic devices commonly recommended for patients with CTS. Selection of ergonomic devices remains dependent on personal preference."]},
{"title": "The collective construction of safety: A trade-off between \u201cunderstanding\u201d and \u201cdoing\u201d in managing dynamic situations", "highlights": ["Analyses of anesthesia simulations enable understanding of how risks are managed collectively.", "Three modes of management for dynamic risky situations are described.", "Safety rests on teams' ability to make trade-offs between ", " and ", ".", "Safety implies being able to accept suboptimal performance and approximations.", "Performance improvement for teams requires specific training sessions to develop meta-knowledge."], "abstract": ["This exploratory research aims to understand how teams organize themselves and collectively manage risky dynamic situations. The objective is to assess the plausibility of a model of a collective trade-off between \u201cunderstanding\u201d and \u201cdoing\u201d. The empirical study, conducted in the pediatric anesthesia service of a French university hospital, was supported by a \u201chigh fidelity\u201d simulation with six teams. Data on the teams' behavior and on the verbal communications were collected through video recordings. The results highlight three modes for management of dynamic situations (determined management, cautious management, and overwhelmed management). These modes are related to the way in which teams manage their cognitive resources. More precisely, they are related to the teams' ability to collectively elaborate a trade-off between \u201cunderstanding\u201d and \u201cdoing\u201d. These results question existing perspectives on safety and suggest improvements in the design of crisis management training (concerning for example the recommendation of \u201ccalling for help\u201d)."]},
{"title": "Driver behavior in use of guide and logo signs under distraction and complex roadway conditions", "highlights": ["Visual processing times for logo signs are slightly higher than for guide signs.", "Increased visual processing times for logo signs do not result in any vehicle control degradation.", "There are no attention allocation or driving performance differences between the use of six- and nine-panel logo signs.", "Drivers exhibit high levels of lane and speed control in navigating a work zone as compared to driving on an open freeway.", "Driving performance did not degrade during driver use of an in-car navigation aid."], "abstract": ["White-on-blue logo signs on the sides of highways are typically used to notify drivers of food, gas, and lodging at an upcoming interchange. The current research assessed driver performance and attention allocation in a simulated freeway driving task when exposed to six-panel logo signs, nine-panel logo signs, mileage guide signs, and roadway work zones both with and without an in-car navigation device. The objective was to identify the impact of signage types on driver behavior under realistic driving conditions. Results revealed glance durations and fixation frequencies to guide signs to be significantly lower than with six-panel and nine-panel logo signs, but no differences were found between six-panel and nine-panel logo signs. There were also statistical differences among the independent variables for speed deviation and lane deviation, but magnitudes were not large enough to be considered practically significant in terms of driving safety. Overall, there were minor differences in sign processing time between logo signs and mileage guide signs, but such differences did not translate to degradations in vehicle control."]},
{"title": "Cycling at varying load: How are experiences of perceived exertion integrated in a single measurement?", "highlights": ["Participants scaled perceived exertion (PE) for systematically varied cycling intensities (steady, ascending or descending).", "Scalings were altered by the order of the intensity changes.", "Alterations were more pronounced for ascending than for descending intensities.", "Practitioners should mind that scalings for ascending work intensities can be overestimated."], "abstract": ["How are experiences of perceived exertion (PE), associated with varying load, integrated in a single measurement? In search of an integrating pattern of scalings of PE, 209 participants were randomly assigned to 7.5-min pedalling trials on a bicycle-ergometer. Intensities were either kept constant at 25\u00a0W, 50\u00a0W, 75\u00a0W, 100\u00a0W, 125\u00a0W, or were systematically varied after 2.5 and 5\u00a0min whereby the overall load was kept constant at either 50\u00a0W, 75\u00a0W or 100\u00a0W. Systematically varied intensities were either continuously increased or decreased by steps of 25\u00a0W. A nearly linear relationship between steady presented intensities and session scalings of PE confirmed the validity of the Category partitioning (CP) procedure. Scalings obtained in sessions with systematically varied loads were altered by the order of the intensity changes. The influence was more pronounced in sessions with increasing loads than in sessions with loads in decreasing order."]},
{"title": "Healthcare workers' perceptions of lean: A context-sensitive, mixed methods study in three Swedish hospitals", "highlights": ["Hospital workers' perceptions of lean vary by hospital, unit, and role.", "Perceptions in Hospital A were unique compared to those in Hospitals B and C.", "Higher-acuity units typically reported more favorable perceptions.", "Nurses typically reported more favorable perceptions than physicians.", "Implementation differences explained some of the context-specific effects."], "abstract": ["As the application of lean in healthcare expands, further research is needed in at least two areas: first, on the role of context in shaping lean and its consequences and second, on how healthcare workers perceive lean. Accordingly, this context-sensitive, mixed methods study addressed how hospital workers' perceptions of lean varied across contexts in three Swedish hospitals. Registered nurses and physicians at the hospitals and across units differing in acuity completed standardized surveys (", "\u00a0=\u00a0236, 57% response rate) about their perceptions of hospital-wide lean implementation. Perceptions varied by: hospital context, with one hospital's employees reporting the least favorable perceptions; unit acuity, with higher-acuity units reporting more favorable perceptions; and professional role, with nurses reporting more favorable perceptions than physicians. Individual interviews, group interviews, and observations provided insight about these dissimilar contexts and possible explanations for context-specific variability. Findings are discussed with respect to strategies for implementing lean in healthcare; the importance of attending to levels, context, and worker consequences of lean; and directions for future research."]},
{"title": "The patient work system: An analysis of self-care performance barriers among elderly heart failure patients and their informal caregivers", "highlights": ["Patients' and informal caregivers' work performance was shaped by system factors.", "Person factors included biomedical, psychological, and physical characteristics.", "Task factors included task difficulty, complexity, timing, and consequences.", "Tool factors included tool accessibility, usability, impact, and design.", "Context domains were physical\u2013spatial, social\u2013cultural, and organizational factors."], "abstract": ["Human factors and ergonomics approaches have been successfully applied to study and improve the work performance of healthcare professionals. However, there has been relatively little work in \u201cpatient-engaged human factors,\u201d or the application of human factors to the health-related work of patients and other nonprofessionals. This study applied a foundational human factors tool, the systems model, to investigate the barriers to self-care performance among chronically ill elderly patients and their informal (family) caregivers. A Patient Work System model was developed to guide the collection and analysis of interviews, surveys, and observations of patients with heart failure (", "\u00a0=\u00a030) and their informal caregivers (", "\u00a0=\u00a014). Iterative analyses revealed the nature and prevalence of self-care barriers across components of the Patient Work System. Person-related barriers were common and stemmed from patients' biomedical conditions, limitations, knowledge deficits, preferences, and perceptions as well as the characteristics of informal caregivers and healthcare professionals. Task barriers were also highly prevalent and included task difficulty, timing, complexity, ambiguity, conflict, and undesirable consequences. Tool barriers were related to both availability and access of tools and technologies and their design, usability, and impact. Context barriers were found across three domains\u2014physical\u2013spatial, social\u2013cultural, and organizational\u2014and multiple \u201cspaces\u201d such as \u201cat home,\u201d \u201con the go,\u201d and \u201cin the community.\u201d Barriers often stemmed not from single factors but from the interaction of several work system components. Study findings suggest the need to further explore multiple actors, contexts, and interactions in the patient work system during research and intervention design, as well as the need to develop new models and measures for studying patient and family work."]},
{"title": "Experimental investigation of personal air supply nozzle use in aircraft cabins", "highlights": ["Passengers' use of individual air supply nozzles in aircraft cabins was simulated.", "Human thermal and draft sensations change over time in an aircraft cabin.", "The reasons of opening nozzles and opening adjustment position were illustrated.", "Nozzle utilization rate and opening degree increased initially and then decreased."], "abstract": ["To study air passengers' use of individual air supply nozzles in aircraft cabins, we constructed an experimental chamber which replicated the interior of a modern passenger aircraft. A series of experiments were conducted at different levels of cabin occupancy. Survey data were collected focused on the reasons for opening the nozzle, adjusting the level of air flow, and changing the direction of the air flow. The results showed that human thermal and draft sensations change over time in an aircraft cabin. The thermal sensation response was highest when the volunteers first entered the cabin and decreased over time until it stablized. Fifty-one percent of volunteers opened the nozzle to alleviate a feeling of stuffiness, and more than 50% adjusted the nozzle to improve upper body comfort. Over the period of the experiment the majority of volunteers chose to adjust their the air flow of their personal system. This confirms airline companies' decisions to install the individual aircraft ventilation systems in their aircraft indicates that personal air systems based on nozzle adjustment are essential for cabin comfort. These results will assist in the design of more efficient air distribution systems within passenger aircraft cabins where there is a need to optimize the air flow in order to efficiently improve aircraft passengers' thermal comfort and reduce energy use."]},
{"title": "A practical approach to glare assessment for train cabs", "highlights": ["Glare is key design consideration for train cabs.", "There are no established standardised approaches for assessing glare within a rail context.", "We propose a new approach for assessing the impact of glare using subjective ratings.", "Initial findings indicate that this approach may be of wider use.", "Further validation of the described approach is needed."], "abstract": ["The assessment of glare is a key consideration in the design of a railway driver's cab. However, unlike assessment of other factors, such as forward visibility, there are no standardised approaches for performing assessments of glare. This paper describes an approach for assessing the impact of glare in a full size mock-up of a railway cab. While it is unrealistic to evaluate every possible lighting condition that may potentially occur in the vehicle cab in service, a pragmatic and practical approach is taken to provide a good level of indicative information about the cab design's likely glare performance. This involves assessing internal light sources, such as internal lights and illuminated controls, and simulating external light sources (e.g. the sun, other trains' headlights) by illuminating the cab mock up windscreen, side and door windows with a single light source manually located in a sequence of discrete positions and orientations and assessing the resulting glare impacts. The paper describes a structured process for assessing and recording the impact of glare and recommending mitigations."]},
{"title": "Work exposure and vigilance decrements in closed circuit television surveillance", "highlights": ["Detection rates for inconspicuous events in real time surveillance are low.", "Surveillance background contributes to differences in detection.", "The nature of previous visual analysis experience contributes to detection levels.", "The type of sample influences the results of ecologically valid vigilance tasks."], "abstract": ["The aim of this study was to examine operator effectiveness in terms of detection rates and potential vigilance decrements in a proactive or real time CCTV surveillance task. The study was conducted in two stages. During stage one, 42 operators who were employed full-time in CCTV surveillance observed a 90-min\u00a0video and were required to detect four types of target behaviours. No vigilance decrement was found for this sample as a whole. Stage two involved collecting additional data from 31 novices and dividing the existing operators into two sub-samples, consisting of generalists and specialists depending on the type of surveillance they performed at work (total ", "\u00a0=\u00a073). Fifty percent of target behaviours were detected and false alarms were high. Vigilance decrements were found for novices and generalists, but specialists maintained their performance for the first hour and then increased it. Results are discussed in terms of surveillance background, work exposure, transfer of learning, selection, training and motivation and the impact of these on vigilance and CCTV performance."]},
{"title": "The influence of active seating on car passengers' perceived comfort and activity levels", "highlights": ["Active seating is a promising concept to interrupt static sitting.", "Participants felt significantly more challenged, more fit and more refreshed.", "Heart rate indicated a light intensity, but nevertheless non-sedentary, activity.", "EMG variability in several muscles increased during active seating.", "Active seating might stimulate movements and hence help to reduce sedentary behaviour."], "abstract": ["New technologies have led to an increasingly sedentary lifestyle. Sedentary behaviour is characterised by physical inactivity and is associated with several health risks. This excessive sitting does not only take place in the office or at home, but also during daily commute. Therefore, BMW AG developed an active seating system for the back seat of a car, consisting of sensors in the back rest that register upper body movements of the passenger, with which the passenger controls a game. This study evaluated three different aspects of active seating compared to other tasks (reading, working on laptop, and gaming on tablet). First, discomfort and comfort perception were measured in a 30-minute driving test. Discomfort was very low for all activities and participants felt significantly more challenged, more fit and more refreshed during active seating. Second, heart rate was measured, indicating a light intensity, but nevertheless non-sedentary, activity. Third, average and variability in activity of six postural muscles was measured by electromyography (EMG), showing a higher muscle activity and higher muscle variability for active seating compared to other activities. Active seating might stimulate movements, thereby increasing comfort and well-being."]},
{"title": "Self-rostering and psychosocial work factors \u2013 A mixed methods intervention study", "highlights": ["Implementing self-rostering has a potential to improve the psychosocial factors at work.", "There were large differences of implementing self-rostering on the effects of psychosocial factors at work.", "The effect on psychosocial factor seemed to be dependent on the aim and content of intervention."], "abstract": ["This study aims at 1) examining the effect of self-rostering on emotional demands, quantitative demands, work pace, influence, social community at work, social support from leaders and colleagues, job satisfaction, and negative acts, 2) examining whether this effect was mediated through increased influence on the scheduling of working hours, and interpreting the results in light of the different implementation processes that emerged in the study and by including qualitative data. We conducted a 12 months follow-up, quasi-experimental study of self-rostering among 28 workplaces out of which 14 served as reference workplaces. We also interviewed 26 employees and 14 managers about their expectations of introducing self-rostering. In the present study implementation of self-rostering had a positive effect on job demands and the social environment of the workplace, especially if the intervention does not comprise drastic changes of the organisation of the employees' work and private life."]},
{"title": "Usability in product development practice; an exploratory case study comparing four markets", "highlights": ["Research goal: identifying barriers and enablers for usability in product development practice.", "Multiple case study across four different markets (FMCG, automotive, coffee machines, printers).", "Product-market combination influences prioritization of usability.", "Attitude towards usability influences team composition and choice of user involvement methods.", "Method choice influenced by product/market combination, prioritization, development stage."], "abstract": ["This study explored how usability was dealt with in four product development organizations active in different sectors: high-end automotive, professional printers and copiers, office coffee makers and fast moving consumer goods. The primary differentiators of the selected cases were whether they were targeting businesses or consumers and the degree of product complexity. Interviews with 19 product development practitioners were conducted, focussing on three topics: 1) the product development process and the integration of user involvement, 2) multidisciplinary teamwork, and 3) organizational attitude towards usability. Based on the interviews, context descriptions of the companies were created and barriers and enablers for usability were identified. To verify the findings and to discuss remaining issues a feedback workshop was held in which the primary contact from each company participated. The results indicate that differences in product\u2013market combination lead to differences in organizational attitude towards usability. The prioritization of usability in an organization seems to be influenced by the degree of product complexity (complex products are more prone to suffer from usability issues) and whether developers think that usability is a purchase consideration for their clients. The product\u2013market combination a company targets also affects the methods for user-centred design that a company can apply and that are relevant. What methods for user-centred design are used also seems to be influenced by the attitude towards usability: if usability is considered more important, methods that require more resources can be applied."]},
{"title": "An anthropometric data bank for the Iranian working population with\u00a0ethnic diversity", "highlights": ["We measured 37 body dimensions of 3720 Iranian workers with different ethnicities.", "Comparing anthropometric data of Iranian with four Asian population.", "Anthropometric data of Iranian ethnical groups were statistically different.", "Observing anthropometric data differences of Iranian with the four nations."], "abstract": ["This study constructed an anthropometric data bank for the Iranian working population. In total, thirty-seven body dimensions were measured among 3720 Iranian workers with different ethnicities (3000 male and 720 female; aged 20\u201360\u00a0years). Statistical analysis revealed significant differences for most of body dimensions among the ethnical groups. Moreover, the authors compared Iranian anthropometric characteristics with those of four Asian populations: Taiwanese, Chinese, Japanese, and Korean. Overall, 16 body dimensions for the five Asian populations were selected and compared. Accordingly, different morphological characteristics of these five populations were observed. The Iranian population showed wide shoulders and hips and long legs; the Chinese population showed narrow hips and shoulders and a short height relative to the other populations. The Korean sample recorded moderate body size comparing the other populations. The Taiwanese had large hands, relatively wide shoulders and short upper limbs. These differences in population dimensions should be taken into consideration for product and process design when expanding regional markets."]},
{"title": "Greater physical fitness is associated with better air ventilation efficiency in firefighters", "highlights": ["Firefighters who performed the simulated work circuit (SWC) in less time ventilated less air at a given work intensity.", "The fastest firefighters on the SWC performed longer on the graded walking test.", "Firefighters with better peak oxygen consumption also performed better on the SWC.", "Firefighters who performed the SWC in less time had a greater oxygen extraction rate in the ", " muscle."], "abstract": ["Firefighting is a hazardous task associated with a heavy workload where task duration may be limited by air cylinder capacity. Increased fitness may lead to better air ventilation efficiency and task duration at a given heavy work intensity. This study compared performance, air ventilation and skeletal muscle oxygen extraction during a maximal graded walking test (GWT), a 10\u00a0METS (metabolic equivalent) treadmill test (T10) and a simulated work circuit (SWC). Participants (", "\u00a0=\u00a013) who performed the SWC in a shorter time had significantly lower air cylinder ventilation values on the T10 (", "\u00a0=\u00a0\u22120.495), better peak oxygen consumption (", "\u00a0=\u00a0\u22120.924) during the GWT and significantly greater skeletal muscle oxygen extraction during the SWC (HbDiff, ", "\u00a0=\u00a00.768). These results demonstrate that the fastest participants on the SWC had better air ventilation efficiency that could prolong interventions in difficult situations requiring air cylinder use."]},
{"title": "Thermal discomfort and hypertension in bus drivers and chargers in the metropolitan region of Belo Horizonte, Brazil", "highlights": ["This study was conducted with a sample of Brazilian collective transportation workers.", "Prevalence of hypertension was higher in workers that reported thermal discomfort.", "Our findings call for reflection about social protection needed for these workers.", "Our findings encourage transport policies about bus climatic conditions."], "abstract": ["This study aimed to assess the relationship between perception of temperature inside the bus and hypertension among 1126 collective transportation workers in metropolitan region of Belo Horizonte, Brazil. Thermal discomfort was determined based on the perception of temperature inside the bus. Hypertension was determined if participant had a medical diagnosis of this disease. Prevalence ratios (PR) for hypertension and their respective 95% confidence intervals (95% CI) were adjusted using multivariate Poisson regression analysis. The perceptions of temperature inside the bus were tolerable (26.5%), disturbs a little (28.6%), disturbs a lot (34.8%) and unbearable (10.2%). The prevalence of hypertension was 14.3%. The thermal discomfort categories of disturbs a lot (PR\u00a0=\u00a01.41; 95% CI\u00a0=\u00a01.02\u20131.95) and unbearable (PR\u00a0=\u00a01.75; 95% CI\u00a0=\u00a01.16\u20132.63) were independently related to hypertension. Thermal discomfort was associated with a higher prevalence of hypertension. This finding should be considerate in new policies for public transportation."]},
{"title": "Subjective responses to display bezel characteristics", "highlights": ["It has been suggested that glossy screen surrounds (bezels) could cause eyestrain, or adverse symptoms to users.", "We studied the responses of twenty naive office workers who each used six different bezels, which were changed weekly.", "None of the participants were disturbed by the glossiness of the bezels.", "Although preferences were found, there was no relationship between glossiness and adverse symptom.", "We conclude that there are no sound reasons for restricting bezel glossiness."], "abstract": ["High quality flat panel computer displays (FPDs) with high resolution screens are now commonplace, and black, grey, white, beige and silver surrounds (\u2018bezels\u2019), matt or glossy, are in widespread use. It has been suggested that bezels with high reflectance, or with a high gloss, could cause eyestrain, and we have investigated this issue. Twenty office workers (unaware of the study purpose) used six different FPDs, for a week each, at their own desk. These displays were identical apart from the bezel colour (black, white or silver) and shininess (matt or glossy). Participants completed questionnaires about their visual comfort at the end of each week, and were fully debriefed in lunch-time focus groups at the end of the study. For the white and the silver bezels, the glossiness of the bezel was not an issue of concern. The participants were significantly less content with the glossy black surround than with the matt black surround, and in general the glossy black bezel was the least-liked of all those used. With the possible exception of this surround, there was no evidence of significantly increased visual discomfort, indicative of eyestrain, as a result of high or low bezel reflectance, or of high glossiness."]},
{"title": "Evaluating the effect of four different pointing device designs on upper extremity posture and muscle activity during mousing tasks", "highlights": ["We created a specific hand posture metric to assess interaction between hand and pointing devices.", "Pointing devices that do not require holding induced more neutral hand postures and lower forearm muscle load.", "Centrally located pointing devices induced more neutral shoulder postures.", "Device familiarity did not affect user's perception of pointing devices."], "abstract": ["The goal of this study was to evaluate the effect of different types of computer pointing devices and placements on posture and muscle activity of the hand and arm. A repeated measures laboratory study with 12 adults (6 females, 6 males) was conducted. Participants completed two mouse-intensive tasks while using a conventional mouse, a trackball, a stand-alone touchpad, and a rollermouse. A motion analysis system and an electromyography system monitored right upper extremity postures and muscle activity, respectively. The rollermouse condition was associated with a more neutral hand posture (lower inter-fingertip spread and greater finger flexion) along with significantly lower forearm extensor muscle activity. The touchpad and rollermouse, which were centrally located, were associated with significantly more neutral shoulder postures, reduced ulnar deviation, and lower forearm extensor muscle activities than other types of pointing devices. Users reported the most difficulty using the trackball and touchpad. Rollermouse was not more difficult to use than any other devices. These results show that computer pointing device design and location elicit significantly different postures and forearm muscle activities during use, especially for the hand posture metrics."]},
{"title": "Verification and validation of a Work Domain Analysis with turing machine task analysis", "highlights": ["Work domain models produced by Work Domain Analysis need to be validated and verified.", "A method based on the Turing Machine formalism was proposed.", "An application to two domains allowed us to highlight some required changes.", "Over or underspecification, omission or false inclusion of objects were noticed."], "abstract": ["While the use of Work Domain Analysis as a methodological framework in cognitive engineering is increasing rapidly, verification and validation of work domain models produced by this method are becoming a significant issue. In this article, we propose the use of a method based on Turing machine formalism named \u201cTuring Machine Task Analysis\u201d to verify and validate work domain models. The application of this method on two work domain analyses, one of car driving which is an \u201cintentional\u201d domain, and the other of a ship water system which is a \u201ccausal domain\u201d showed the possibility of highlighting improvements needed by these models. More precisely, the step by step analysis of a degraded task scenario in each work domain model pointed out unsatisfactory aspects in the first modelling, like overspecification, underspecification, omission of work domain affordances, or unsuitable inclusion of objects in the work domain model."]},
{"title": "Lean production tools and decision latitude enable conditions for innovative learning in organizations: A multilevel analysis", "highlights": ["Innovative learning climate and sharing ideas is necessary for innovative learning.", "The use of lean tools can facilitate questioning, new thinking, and sharing ideas.", "Also decision latitude can enable a climate for innovative learning.", "Lean tools enable sharing of ideas especially when decision latitude is low.", "Especially value flow analysis can create an arena for shared understanding."], "abstract": ["The effect of lean production on conditions for learning is debated. This study aimed to investigate how tools inspired by lean production (standardization, resource reduction, visual monitoring, housekeeping, value flow analysis) were associated with an innovative learning climate and with collective dispersion of ideas in organizations, and whether decision latitude contributed to these associations. A questionnaire was sent out to employees in public, private, production and service organizations (", "\u00a0=\u00a04442). Multilevel linear regression analyses were used. Use of lean tools and decision latitude were positively associated with an innovative learning climate and collective dispersion of ideas. A low degree of decision latitude was a modifier in the association to collective dispersion of ideas. Lean tools can enable shared understanding and collective spreading of ideas, needed for the development of work processes, especially when decision latitude is low. Value flow analysis played a pivotal role in the associations."]},
{"title": "Can a glass cockpit display help (or hinder) performance of novices in simulated flight training?", "highlights": ["Flight na\u00efve participants performed more poorly with a simulated glass cockpit display than a traditional flight display.", "This was largely the case irrespective of whether they had previous experience with the glass cockpit display or not.", "Participants preferred the glass cockpit display and associated it with better performance."], "abstract": ["The analog dials in traditional GA aircraft cockpits are being replaced by integrated electronic displays, commonly referred to as glass cockpits. Pilots may be trained on glass cockpit aircraft or encounter them after training on traditional displays. The effects of glass cockpit displays on initial performance and potential transfer effects between cockpit display configurations have yet to be adequately investigated. Flight-na\u00efve participants were trained on either a simulated traditional display cockpit or a simulated glass display cockpit. Flight performance was measured in a test flight using either the same or different cockpit display. Loss of control events and accuracy in controlling altitude, airspeed and heading, workload, and situational awareness were assessed. Preferences for cockpit display configurations and opinions on ease of use were also measured. The results revealed consistently poorer performance on the test flight for participants using the glass cockpit compared to the traditional cockpit. In contrast the post-flight questionnaire data revealed a strong subjective preference for the glass cockpit over the traditional cockpit displays. There was only a weak effect of prior training. The specific glass cockpit display used in this study was subjectively appealing but yielded poorer flight performance in participants with no previous flight experience than a traditional display. Performance data can contradict opinion data. The design of glass cockpit displays may present some difficulties for pilots in the very early stages of training."]},
{"title": "Applications of integrated human error identification techniques on the chemical cylinder change task", "highlights": ["A number of execution procedures and frameworks of HEI techniques were integrated to assess latent human errors.", "Some latent human errors and error mechanism of a chemical cylinder change task have been identified.", "The human error probability of the case system has been calculated.", "Some strategies to prevent latent human errors of the case system have been proposed."], "abstract": ["This paper outlines the human error identification (HEI) techniques that currently exist to assess latent human errors. Many formal error identification techniques have existed for years, but few have been validated to cover latent human error analysis in different domains. This study considers many possible error modes and influential factors, including external error modes, internal error modes, psychological error mechanisms, and performance shaping factors, and integrates several execution procedures and frameworks of HEI techniques. The case study in this research was the operational process of changing chemical cylinders in a factory. In addition, the integrated HEI method was used to assess the operational processes and the system's reliability. It was concluded that the integrated method is a valuable aid to develop much safer operational processes and can be used to predict human error rates on critical tasks in the plant."]},
{"title": "The effect of volumetric (3D) tactile symbols within inclusive tactile maps", "highlights": ["We compare two tactile maps, one of them includes Volumetric (3D) tactile symbols.", "Improving the interaction between users and tactile maps using 3D symbols together with 2D ones.", "3D symbols can be located in less time and, generally, cause fewer errors than flat relief symbols-2D.", "3D printing opens new horizons for the design and production of tactile maps for blind users."], "abstract": ["Point, linear and areal elements, which are two-dimensional and of a graphic nature, are the morphological elements employed when designing tactile maps and symbols for visually impaired users.", "However, beyond the two-dimensional domain, there is a fourth group of elements \u2013 volumetric elements \u2013 which mapmakers do not take sufficiently into account when it comes to designing tactile maps and symbols.", "This study analyses the effect of including volumetric, or 3D, symbols within a tactile map. In order to do so, the researchers compared two tactile maps. One of them uses only two-dimensional elements and is produced using thermoforming, one of the most popular systems in this field, while the other includes volumetric symbols, thus highlighting the possibilities opened up by 3D printing, a new area of production.", "The results of the study show that including 3D symbols improves the efficiency and autonomous use of these products."]},
{"title": "An objective method for screening and selecting personal cooling systems based on cooling properties", "highlights": ["Presentation of a personal cooling system (PCS) selection method.", "Provides ability to rank systems based on projected cooling ability.", "Integrates times, weights, and metabolic rates in a cooling effectiveness score.", "Method decreases testing costs by allowing better selection of systems for testing.", "Validation is performed against human subject PCS testing."], "abstract": ["A method is proposed for evaluation and selection of a personal cooling system (PCS) incorporating PCS, subject, and equipment weights; PCS run time; user task time; PCS cooling power; and average metabolic rate. The cooling effectiveness method presented is derived from first principles and allows those who select PCSs for specific applications to compare systems based on their projected use. This can lower testing costs by screening for the most applicable system. Methods to predict cooling power of PCSs are presented and are compared to data taken through standard manikin testing. The cooling effectiveness ranking is presented and validated against human subject test data. The proposed method provides significant insight into the application of PCS on humans. However, the interaction a humans with a PCS is complex, especially considering the range of clothing ensembles, physiological issues, and end use scenarios, and requires additional analysis."]},
{"title": "A theoretical framework for negotiating the path of emergency management multi-agency coordination", "highlights": ["We explore human\u2013environment interaction in multi-agency coordination.", "We offer a framework facilitating emergency management multi-agency coordination.", "We extend the theory of core-task analysis in a new high reliability environment."], "abstract": ["Multi-agency coordination represents a significant challenge in emergency management. The need for liaison officers working in strategic level emergency operations centres to play organizational boundary spanning roles within multi-agency coordination arrangements that are enacted in complex and dynamic emergency response scenarios creates significant research and practical challenges. The aim of the paper is to address a gap in the literature regarding the concept of multi-agency coordination from a human\u2013environment interaction perspective. We present a theoretical framework for facilitating multi-agency coordination in emergency management that is grounded in human factors and ergonomics using the methodology of core-task analysis. As a result we believe the framework will enable liaison officers to cope more efficiently within the work domain. In addition, we provide suggestions for extending the theory of core-task analysis to an alternate high reliability environment."]},
{"title": "Development and evaluation of one-hand drivable manual wheelchair device for hemiplegic patients", "highlights": ["Existing manual wheelchairs require considerable use and control of both hands for operation.", "Hemiplegic patients encounter difficulty in propelling the wheelchair with one hand.", "Most hemiplegic patients performed incorrectly in driving a manual wheelchair without using their feet.", "Adaptation of \u2018One-hand drivable manual wheelchair\u2019 can be an effective solution."], "abstract": ["This study was conducted for one-hand users including hemiplegic clients currently using standard manual wheelchairs, so as to analyze their specific problems and recommend solutions regarding usage. Thirty hemiplegic clients who were admitted to rehabilitation and convalescent hospitals participated as subjects. The research tools were standard manual wheelchairs commonly used by people with impaired gait and a \u201cone-hand drivable manual wheelchair,\u201d which was developed for this study. The Wheelchair Skills Test (WST) was adopted for the objective assessment tool, while drivability, convenience, difference, and acceptability were developed for the subjective evaluation tools. The assessment procedures comprise two phases of pre-assessment and post-assessment. In the pre-assessment phase, the WST and subjective evaluation (drivability, convenience) were conducted using the existing standard manual wheelchair and with/without use of a foot to control the wheelchair. In the post-assessment phase, the WST and subjective evaluation (drivability, convenience, difference, acceptability) were also carried out using the developed one-hand drivable manual wheelchair. The results showed that the highest pass rate recorded for the WST items was 3.3% when the participants drove standard manual wheelchairs without the use of either foot and 96.7% when using the manual wheelchairs equipped with developed device. As compared to the existing wheelchair, statistical results showed significant effects on the WST, drivability, convenience, difference and acceptability when the participants drove wheelchairs equipped with the developed device. These findings imply that the one-hand drivable wheelchair equipped with the developed device can be an active and effective solution for hemiplegic clients using existing manual wheelchairs to increase their mobility and occupational performance."]},
{"title": "Effect of firefighters' personal protective equipment on gait", "highlights": ["Wearing firefighting boots is a major contributor to firefighters' gait change.", "An increase in flexing resistance of the boots restricts normal foot motions.", "Increased flexing resistance of the boots elevates gait instability.", "Increased gait instability in stiff boots can increase traumatic injury risk."], "abstract": ["The biomechanical experiment with eight male and four female firefighters demonstrates that the effect of adding essential equipment: turnout ensemble, self-contained breathing apparatus, and boots (leather and rubber boots), significantly restricts foot pronation. This finding is supported by a decrease in anterior-posterior and medial-lateral excursion of center of plantar pressure (COP) trajectory during walking. The accumulation of this equipment decreases COP velocity and increases foot-ground contact time and stride time, indicating increased gait instability. An increase in the flexing resistance of the boots is the major contributor to restricted foot pronation and gait instability as evidenced by the greater decrease in excursion of COP in leather boots (greater flexing resistance) than in rubber boots (lower resistance). The leather boots also shows the greatest increase in foot contact time and stride time. These negative impacts can increase musculoskeletal injuries in unfavorable fire ground environments."]},
{"title": "Effects of air bottle design on postural control of firefighters", "highlights": ["Firefighters often use a self-contained breathing apparatus (SCBA).", "No systematic assessment on the effect of SCBA design on firefighter's balance has been done.", "Heavy SCBA increased excursion and randomness of postural sway in medial-lateral direction.", "Heavy and large SCBA can threaten postural stability in anterior-posterior direction.", "New designs of SCBA may be needed for the enhanced postural stability of the firefighters."], "abstract": ["The purpose of this study was to investigate the effect of firefighter's self-contained breathing apparatus (SCBA) air bottle design and vision on postural control of firefighters. Twenty-four firefighters were tested using four 30-minute SCBA bottle designs that varied by mass and size. Postural sway measures were collected using a forceplate under two visual conditions (eyes open and closed) and two stance conditions (quiet and perturbed stances). For perturbed stance, a mild backward impulsive pull at the waist was applied. In addition to examining center of pressure postural sway measures for both stance conditions, a robustness measure was assessed for the perturbation condition. The results suggest that wearing heavy bottles significantly increased excursion and randomness of postural sway only in medial-lateral direction but not in anterior-posterior direction. This result may be due to stiffening of plantar-flexor muscles. A significant interaction was obtained between SCBA bottle design and vision in anterior-posterior postural sway, suggesting that wearing heavy and large SCBA air bottles can significantly threaten postural stability in AP direction in the absence of vision. SCBA bottle should be redesigned with reduced weight, smaller height, and COM closer to the body of the firefighters. Firefighters should also widen their stance width when wearing heavy PPE with SCBA."]},
{"title": "Comparative evaluation of six quantitative lifting tools to estimate spine loads during static activities", "highlights": ["Predictions of six lifting analysis tools for the spine loads were compared.", "26 symmetric/asymmetric lifting tasks in upright/flexed postures were considered.", "Significantly different spinal loads were predicted by the tools.", "The potential risk of injury could vary depending on the tool used.", "The shortcomings of each tool and its domain of applications were identified."], "abstract": ["Different lifting analysis tools are commonly used to assess spinal loads and risk of injury. Distinct musculoskeletal models with various degrees of accuracy are employed in these tools affecting thus their relative accuracy in practical applications. The present study aims to compare predictions of six tools (HCBCF, LSBM, 3DSSPP, AnyBody, simple polynomial, and regression models) for the L4-L5 and L5-S1 compression and shear loads in twenty-six static activities with and without hand load. Significantly different spinal loads but relatively similar patterns for the compression (", "\u00a0>\u00a00.87) were computed. Regression models and AnyBody predicted intradiscal pressures in closer agreement with available ", " measurements (RMSE\u00a0\u2248\u00a00.12\u00a0MPa). Due to the differences in predicted spinal loads, the estimated risk of injury alters depending on the tool used. Each tool is evaluated to identify its shortcomings and preferred application domains."]},
{"title": "Hearing the way: Requirements and preferences for technology-supported navigation aids", "highlights": ["Technology-supported navigation aids can support users with sight problems.", "Users should have control over cues provided by the technology.", "The cues provided should be assimilated with the environment.", "The technology should support the environmental cues rather than replacing them.", "Information when users are not on the correct path is particularly important."], "abstract": ["Many systems have been developed to assist wayfinding for people with sight problems. There is a need for user requirements for such systems to be defined. This paper presents a study which aimed to determine such user requirements. An experiment was also conducted to establish the best way of guiding users between locations. The focus group results indicated that users require systems to provide them with information about their surroundings, to guide them along their route and to provide progress information. They also showed that users with sight conditions interact with systems differently to sighted users, thereby highlighting the importance of designing systems for the needs of these users. Results of the experiment found that the preferred method of guiding users was a notification when they were both on and off track. However, performance was best when only provided with the off track notification, implying that this cue is particularly important. Technology has the potential to support navigation for people with sight problems. Users should have control over cues provided and for these cues should supplement environmental cues rather than replacing them."]},
{"title": "Development of a two-step touch method for website navigation on\u00a0smartphones", "highlights": ["A two-step touch method for website navigation on smartphones has been developed.", "The new touch method was superior to the current touch method in error rate and satisfaction score.", "Superior method in touch time was subjected to change depending on the occurrence of touch error.", "The new touch method is recommended when hyperlinks are smaller than the size of finger contact area."], "abstract": ["The touch method for hyperlink selection in smartphones can often create usability problems because a hyperlink is universally smaller than a finger contact area as well as visually occluded by a finger while pressing. In this study, we developed a two-step touch method (called ", " method) and comprehensively examined its effectiveness using the goals, operators, methods, and selection rules (GOMS) model and user testing. The two-step touch method consisted of finger press and flick motions; a target hyperlink was selected by a finger press motion, and a finger flick method was subsequently conducted for error correction if the initial interaction (press) failed. We compared the two-step touch method with the current touch method through the GOMS model and user testing. As a result, the two-step touch method was significantly superior to the current touch method in terms of error rate and subjective satisfaction score; however, its superiority in terms of number of interactions and touch time was vulnerably affected by error rate. The two-step touch method developed in this study can improve the usability and user experience of website navigation using smartphones."]},
{"title": "Did a brief nap break have positive benefits on information processing among nurses working on the first 8-h night shift?", "highlights": ["Shift workers frequently experience acute sleep deprivation on first night shift.", "We investigated impact of a nap on the visual attention ability on first night shift.", "There were no differences between those taking and not taking a nap.", "The effect size was medium in the information process between groups.", "Need increase sample size to draw the conclusion regarding the benefits of nap."], "abstract": ["Shift workers frequently experience acute sleep deprivation on first night shift. This study compared the efficacy of 30-min nap (between 2 and 3 a.m.) on the visual attention ability of the nurses working at first 8-h night shift at the time of maximum fatigue (between 3 and 4 a.m.). In addition, we measured cognitive function (between 9 and 10 a.m.) in nurses working on daytime shift, which we defined as baseline wakefulness. The results showed that working on the night shift groups was associated with sleep loss, leading to a decrease in visual attention performance compared to the daytime shift group. There was no statistically significant difference in the visual attention performance between those taking and not taking a nap during the night shift, however the effect size was medium in the information process. It was still needed increase sample size to draw the conclusion regarding a 30-min nap break have positive benefits on perceptual speed during the first night shift."]},
{"title": "The effect of hair density on the coupling between the tactor and the skin of the human head", "highlights": ["Obtained vibration detection thresholds for seven locations on the head.", "Obtained vibration detection thresholds for those with and without hair.", "Hair density slightly impedes vibration signals from reaching the scalp.", "Those without hair are most sensitive to vibration at the back/sides of the head.", "Those with hair are also most sensitive to vibration at the back/sides of the head."], "abstract": ["The purpose of this study was to determine the effect of hair density on vibration detection thresholds associated with the perception of low frequency vibration stimuli applied to the head. A host of tactile sensitivity information exists for other parts of the body, however the same information is lacking for the head. Thirty-three college students, age 18-35, were recruited for the study. A mixed design was used to evaluate the effect of hair density, head location, and frequency on vibration detection thresholds. Results suggest that hair density might slightly impede vibration signals from reaching the scalp and reduce vibration sensitivity, for the least sensitive locations on the head. This research provides design recommendations for head-mounted tactile displays for women and those with hair that can be used to convey directional cues for navigation and as alerts to critical events in the environment."]},
{"title": "Evaluation of an anthropometric shape model of the human scalp", "highlights": ["The human head shape can be predicted using a statistical shape model.", "Anthropometric measurements can be used as intuitive parameters for shape models.", "At least 90 scans are required to build a representative shape model.", "Measurement errors for the arc length have the least effect on the prediction.", "Shape models have applications for research and for industrial design."], "abstract": ["This paper presents the evaluation a 3D shape model of the human head. A statistical shape model of the head is created from a set of 100 MRI scans. The ability of the shape model to predict new head shapes is evaluated by considering the prediction error distributions. The effect of using intuitive anthropometric measurements as parameters is examined and the sensitivity to measurement errors is determined. Using all anthropometric measurements, the average prediction error is 1.60\u00a0\u00b1\u00a00.36\u00a0mm, which shows the feasibility of the new parameters. The most sensitive measurement is the ear height, the least sensitive is the arc length. Finally, two applications of the anthropometric shape model are considered: the study of the male and female population and the design of a brain-computer interface headset. The results show that an anthropometric shape model can be a valuable tool for both research and design."]},
{"title": "Work\u2013family conflict and enrichment from the perspective of psychosocial resources: Comparing Finnish healthcare workers by\u00a0working schedules", "highlights": ["3-shiftwork may imply less managers' support relating to high WFC.", "Lacking co-workers' support may be more detrimental for non-shiftworkers associating with high WFC.", "Certain resources may be equally beneficial in different schedule arrangements in terms of WFC and WFE.", "Shiftwork organizations should pay more attention to family\u2013friendly management.", "All organizations should improve their employees' job control and schedule satisfaction."], "abstract": ["We examined work\u2013family conflict (WFC) and work\u2013family enrichment (WFE) by comparing Finnish nurses, working dayshifts (non-shiftworkers, ", "\u00a0=\u00a0874) and non-dayshifts. The non-dayshift employees worked either two different dayshifts (2-shiftworkers, ", "\u00a0=\u00a0490) or three different shifts including nightshifts (3-shiftworkers, ", "\u00a0=\u00a0270). Specifically, we investigated whether different resources, i.e. job control, managers' work\u2013family support, co-workers' work\u2013family support, control at home, personal coping strategies, and schedule satisfaction, predicted differently WFC and WFE in these three groups. Results showed that lower managers' work\u2013family support predicted higher WFC only among 3-shiftworkers, whereas lower co-workers' support associated with increased WFC only in non-shiftworkers. In addition, shiftworkers reported higher WFC than non-shiftworkers. However, the level of WFE did not vary by schedule types. Moreover, the predictors of WFE varied only very little across schedule types. Shiftwork organizations should pay more attention to family\u2013friendly management in order to reduce WFC among shiftworkers."]},
{"title": "Effects of inter-stimulus interval and intensity on the perceived urgency of tactile patterns", "highlights": ["The feasibility of coding urgency into tactile patterns was examined.", "A stationary laboratory study and a dynamic field study were conducted.", "Patterns at the high 23.5 dB intensity had higher urgency and identification rates.", "Varying ISI at the high 23.5 dB intensity is suitable for providing urgency."], "abstract": ["This research examines the feasibility of coding urgency into tactile patterns. Four tactile patterns were presented at either, 12 or 23.5\u00a0dB above mean threshold, with an ISI of either 0 (no interval) or 500\u00a0msec. Measures included pattern identification and urgency rating on a scale of 1 (least urgent) to 10 (most urgent). Two studies were conducted, a laboratory study and a field study. In the laboratory study, participants received the tactile patterns while seated in front of a computer. For the field study, participants performed dismounted Soldier maneuvers while receiving the tactile patterns. Higher identification rates were found for the 23.5\u00a0dB intensity. Patterns presented at the 23.5\u00a0dB intensity and no ISI were rated most urgent. No differences in urgency ratings were found for 12\u00a0dB based on ISI. Findings support the notion of coding urgency into tactile patterns as a way of augmenting tactile communication."]},
{"title": "Evaluation of thermal comfort in university classrooms through objective approach and subjective preference analysis", "highlights": ["Fanger and Adaptive models are compared to occupants' questionnaires results.", "Analysis on thermal preference is performed, in terms of acceptability, neutrality and preference.", "Gender influence and relationship between microclimate control and thermal sensation has been investigated.", "A different thermal response can be appreciated comparing women and men thermal response."], "abstract": ["Assessing thermal comfort becomes more relevant when the aim is to maximise learning and productivity performances, as typically occurs in offices and schools. However, if, in the offices, the Fanger model well represents the thermal occupant response, then on the contrary, in schools, adaptive mechanisms significantly influence the occupants' thermal preference. In this study, an experimental approach was performed in the Polytechnic University of Bari, during the first days of March, in free running conditions. First, the results of questionnaires were compared according to the application of the Fanger model and the adaptive model; second, using a subjective scale, a complete analysis was performed on thermal preference in terms of acceptability, neutrality and preference, with particular focus on the influence of gender. The user possibility to control the indoor plant system produced a significant impact on the thermal sensation and the acceptability of the thermal environment. Gender was also demonstrated to greatly influence the thermal judgement of the thermal environment when an outdoor cold climate occurs."]},
{"title": "Novel ventilation design of combining spacer and mesh structure in sports T-shirt significantly improves thermal comfort", "highlights": ["Spacer and mesh structures were combined to increase ventilation of sports T-shirt.", "Effects of the ventilation design are examined by thermal manikin and wearer trials.", "Ventilation design of sports T-shirt increases wearer's thermal comfort."], "abstract": ["This paper reports on novel ventilation design in sports T-shirt, which combines spacer and mesh structure, and experimental evidence on the advantages of design in improving thermal comfort. Evaporative resistance (", ") and thermal insulation (", ") of T-shirts were measured using a sweating thermal manikin under three different air velocities. Moisture permeability index (", ") was calculated to compare the different designed T-shirts. The T-shirts of new and conventional designs were also compared by wearer trials, which were comprised of 30\u00a0min treadmill running followed by 10\u00a0min rest. Skin temperature, skin relative humidity, heart rate, oxygen inhalation and energy expenditure were monitored, and subjective sensations were asked. Results demonstrated that novel T-shirt has 11.1% significant lower ", " than control sample under windy condition. The novel T-shirt contributes to reduce the variation of skin temperature and relative humidity up to 37% and 32%, as well as decrease 3.3% energy consumption during exercise."]},
{"title": "An investigation of pointing postures in a 3D stereoscopic environment", "highlights": ["We examined pointing postures while interacting with virtual targets.", "Hand-directed pointing contributed to better overall performance and usability.", "Gaze-directed pointing provided higher accuracy in tracking tasks.", "Targets with negative parallax were easier for users to manipulate.", "Pointing postures affected both task performance and muscular fatigue."], "abstract": ["Many object pointing and selecting techniques for large screens have been proposed in the literature. There is a lack of quantitative evidence suggesting proper pointing postures for interacting with stereoscopic targets in immersive virtual environments. The objective of this study was to explore users' performances and experiences of using different postures while interacting with 3D targets remotely in an immersive stereoscopic environment. Two postures, hand-directed and gaze-directed pointing methods, were compared in order to investigate the postural influences. Two stereo parallaxes, negative and positive parallaxes, were compared for exploring how target depth variances would impact users' performances and experiences. Fifteen participants were recruited to perform two interactive tasks, tapping and tracking tasks, to simulate interaction behaviors in the stereoscopic environment. Hand-directed pointing is suggested for both tapping and tracking tasks due to its significantly better overall performance, less muscle fatigue, and better usability. However, a gaze-directed posture is probably a better alternative than hand-directed pointing for tasks with high accuracy requirements in home\u2013in phases. Additionally, it is easier for users to interact with targets with negative parallax than with targets with positive parallax. Based on the findings of this research, future applications involving different pointing techniques should consider both pointing performances and postural effects as a result of pointing task precision requirements and potential postural fatigue."]},
{"title": "3D skin length deformation of lower body during knee joint flexion for the practical application of functional sportswear", "highlights": ["Visualization of skin deformation including lines of non-extension was provided.", "Skin deformation along the dermatome directions of lower body was investigated.", "Optimal positions of seam for the various lengths of functional pants were suggested.", "Unit area deformation can be used for the attachment of various wearing devices."], "abstract": ["With the advent of 3D technology in the design process, a tremendous amount of scanned data is available. However, it is difficult to trace the quantitative skin deformation of a designated location on the 3D body surface data during movement. Without identical landmarks or reflective markers, tracing the same reference points on the different body postures is not easy because of the complex shape change of the body. To find the least deformed location on the body, which is regarded as the optimal position of seams for the various lengths of functional compression pants, landmarks were directly marked on the skin of six subjects and scanned during knee joint flexion. Lines of non-extension (LoNE) and maximum stretch (LoMS) were searched for, both by tracing landmarks and newly drawn guidelines based on ratio division in various directions. Considering the waist as the anchoring position of the pants, holistic changes were quantified and visualized from the waistline in lengthwise and curvilinear deformation along the dermatomes of the lower body for various lengths of pants. Widthwise and unit area skin deformation data of the skin were also provided as guidelines for further use such as streamlined pants or design of other local wearing devices."]},
{"title": "The development and assessment of behavioural markers to support counter-IED training", "highlights": ["Improvised explosive devices (IEDs) present significant risk of harm to military personnel.", "A behavioural marker checklist was developed for use in counter-IED training activities.", "A new method to capture and assess operationally relevant behavioural markers is presented.", "Testing of the checklist with military personnel showed good content validity and user acceptance.", "Behavioural markers have the potential to improve performance and reduce risk in military operations."], "abstract": ["This article describes the method used to develop and test a checklist of behavioural markers designed to support UK military forces during Counter-Improvised Explosive Device (C-IED) training. IEDs represent a significant threat to UK and allied forces. Effective C-IED procedures and techniques are central to reducing risk to life in this safety critical role. Behavioural markers have been developed to characterise and assess non-technical skills which have been shown to be important in maintaining high performance in other safety critical domains.", "The aims of this study were two-fold. Firstly to develop a method which could be used to capture and assess operationally relevant behavioural markers for use in C-IED training relating primarily to non-technical skills. Secondly, to test the user acceptance of the behavioural marker checklist during military training activities.", "Through engagement with military subject matter experts, operationally relevant and observable behaviours seen in C-IED training have been identified and their links to stronger and weaker performance have been established. Using a card-sort technique, the content validity of each of the markers was assessed in addition to their detectability in an operational context. Following this assessment, a selection of the most operationally relevant and detectable behaviours were assimilated into a checklist and this checklist was tested in C-IED training activities.", "The results of the study show that the method used was effective in generating and assessing the behavioural markers using military subject matter experts. The study also broadly supports the utility and user-acceptance of the use of behavioural markers during training activities.", "The checklist developed using this methodology will provide those responsible for delivering instruction in C-IED techniques and procedures with a straightforward process for identifying good and poor performance with respect to non-technical skills. In addition it will provide a basis for the provision of focussed feedback to trainees during debrief."]},
{"title": "An investigation of thermal comfort inside a bus during heating period within a climatic chamber", "highlights": ["A general and combined testing and calculation model for thermal comfort assessment of a bus HVAC design was described.", "Thermal comfort investigation inside a coach during heating period within a climatic chamber was carried out.", "Transient Energy Balance Model by Gagge et\u00a0al. was used to calculate changes in thermal parameters.", "The model developed for use of HVAC engineers and scientists is valid for all vehicle types.", "Iterations to reach to the desired thermal comfort condition are possible with the given model."], "abstract": ["By this study, it was aimed to define a testing and calculation model for thermal comfort assessment of a bus HVAC design and to compare effects of changing parameters on passenger's thermal comfort. For this purpose, a combined theoretical and experimental work during heating period inside a coach was carried out. The bus was left under 20\u00a0\u00b0C for more than 7\u00a0h\u00a0within a climatic chamber and all heat sources were started at the beginning of a standard test. To investigate effects of fast transient conditions on passengers' physiology and thermal comfort, temperatures, air humidity and air velocities were measured. Human body was considered as one complete piece composed of core and skin compartments and the Transient Energy Balance Model developed by Gagge et\u00a0al. in 1971 was used to calculate changes in thermal parameters between passenger bodies and bus interior environment. Depending on the given initial and environmental conditions, the graphs of passengers Thermal Sensation and Thermal Discomfort Level were found. At the end, a general mathematical model supported with a related experimental procedure was developed for the use of automotive HVAC engineers and scientists working on thermal comfort as a human dimension."]},
{"title": "Sleep patterns of offshore day-workers in relation to overtime work and age", "highlights": ["Offshore personnel work extended tours (14\u00a0\u00d7\u00a012\u00a0h shifts) in a stressful environment.", "About half offshore day-workers also work overtime (on average, 16\u00a0h/week).", "Overtime workers report impaired sleep relative to those who do no overtime.", "Longer overtime hours predicted less sleep in a linear dose\u2013response pattern.", "During leave weeks, sleep was \u223c1\u00a0h/day longer than during offshore weeks."], "abstract": ["In addition to long contractual hours during offshore weeks (14\u00a0\u00d7\u00a012\u00a0h shifts), many personnel on North Sea oil/gas installations also work overtime, but little is known about the implications of overtime for sleep patterns offshore. In this study, the additive and interactive effects of overtime and age were analysed as predictors of sleep duration and sleep quality among offshore day-workers (", "\u00a0=\u00a0551), 54% of whom reported overtime. Sleep duration and quality were impaired among personnel who worked overtime, relative to those who worked only standard shifts; there was also an inverse dose\u2013response relationship between overtime hours and sleep duration. Although the sleep measures were more favourable during shore leave than during offshore weeks, there was little evidence of compensatory sleep patterns. These findings are discussed with reference to known performance and health effects of short sleep hours; formal guidance on overtime work offshore is noted; and methodological issues are considered."]},
{"title": "Age-related differences in processing visual device and task characteristics when using technical devices", "highlights": ["With aging actions are increasingly controlled by vision.", "Middle-aged participants using a touch screen should be less efficient than young ones.", "Visual task difficulty stronger affected middle-aged than young participants.", "The impact of display size depended on psychomotor abilities.", "The results emphasize the need for a user-specific design for touch screen devices."], "abstract": ["With aging visual feedback becomes increasingly relevant in action control. Consequently, visual device and task characteristics should more and more affect tool use. Focussing on late working age, the present study aims to investigate age-related differences in processing task irrelevant (display size) and task relevant visual information (task difficulty). Young and middle-aged participants (20\u201335 and 36\u201364 years of age, respectively) sat in front of a touch screen with differently sized active touch areas (4\u2033 to 12\u2033) and performed pointing tasks with differing task difficulties (1.8\u20135 bits). Both display size and age affected pointing performance, but the two variables did not interact and aiming duration moderated both effects. Furthermore, task difficulty affected the pointing durations of middle-aged adults moreso than those of young adults. Again, aiming duration accounted for the variance in the data. The onset of an age-related decline in aiming duration can be clearly located in middle adulthood. Thus, the fine psychomotor ability \u201caiming\u201d is a moderator and predictor for age-related differences in pointing tasks. The results support a user-specific design for small technical devices with touch interfaces."]},
{"title": "A correlation linking the predicted mean vote and the mean thermal vote based on an investigation on the human thermal comfort in short-haul domestic flights", "highlights": ["An experimental investigation on the human thermal comfort inside the aircrafts was carried out.", "A correlation among the PMV, the equivalent temperature and MTV was defined.", "The correlation linking the thermal sensation scales and zones used by the PMV and the MTV resulted quite accurate."], "abstract": ["The results of an experimental investigation on the human thermal comfort inside the cabin of some Airbus A319 aircrafts during 14 short-haul domestic flights, linking various Italian cities, are presented and used to define a correlation among the predicted mean vote (PMV), a procedure which is commonly used to assess the thermal comfort in inhabited environments, and the equivalent temperature and mean thermal vote (MTV), which are the parameters suggested by the European Standard EN ISO 14505-2 for the evaluation of the thermal environment in vehicles.", "The measurements of the radiant temperature, air temperature and relative humidity during flights were performed. The air temperature varied between 22.2\u00a0\u00b0C and 26.0\u00a0\u00b0C; the relative humidity ranged from 8.7% to 59.2%. The calculated values of the PMV varied from\u00a0\u22120.16 to 0.90 and were confirmed by the answers of the passengers. The equivalent temperature was evaluated using the equations of Fanger or on the basis of the values of the skin temperature measured on some volunteers.", "The correlation linking the thermal sensation scales and zones used by the PMV and the MTV resulted quite accurate because the minimum value of the absolute difference between such environmental indexes equalled 0.0073 and the maximum difference did not exceed the value of 0.0589. Even though the equivalent temperature and the MTV were specifically proposed to evaluate the thermal sensation in vehicles, their use may be effectively extended to the assessment of the thermal comfort in airplanes or other occupied places."]},
{"title": "A comparison of instrumentation methods to estimate thoracolumbar motion in field-based occupational studies", "highlights": ["Inertial measurement unit estimates were comparable to Lumbar Motion Monitor estimates.", "Use of two inertial measurement units increased comparability.", "Fusion of accelerometer and gyroscope measurements increased comparability."], "abstract": ["The performance of an inertial measurement unit (IMU) system for directly measuring thoracolumbar trunk motion was compared to that of the Lumbar Motion Monitor (LMM). Thirty-six male participants completed a simulated material handling task with both systems deployed simultaneously. Estimates of thoracolumbar trunk motion obtained with the IMU system were processed using five common methods for estimating trunk motion characteristics. Results of measurements obtained from IMUs secured to the sternum and pelvis had smaller root-mean-square differences and mean bias estimates in comparison to results obtained with the LMM than results of measurements obtained solely from a sternum mounted IMU. Fusion of IMU accelerometer measurements with IMU gyroscope and/or magnetometer measurements was observed to increase comparability to the LMM. Results suggest investigators should consider computing thoracolumbar trunk motion as a function of estimates from multiple IMUs using fusion algorithms rather than using a single accelerometer secured to the sternum in field-based studies."]},
{"title": "Persistence of threat-induced errors in police officers' shooting decisions", "highlights": ["We experimentally tested the impact of threat on police officers' shooting decisions.", "We examined if representative practice could prevent threat-induced shooting errors.", "High threat caused increases in anxiety and false-positive (shooting) responses.", "Practice did not lead to a reduction in threat-induced shooting errors.", "The impact of threat on police officers' shooting decisions is robust."], "abstract": ["This study tested whether threat-induced errors in police officers' shooting decisions may be prevented through practice. Using a video-based test, 57 Police officers executed shooting responses against a suspect who rapidly appeared with (shoot) or without (don't shoot) a firearm. Threat was manipulated by switching on (high-threat) or switching off (low-threat) a \u201cshootback canon\u201d that could fire small plastic bullets at the officers. After an initial pretest, officers were divided over four different practice groups and practiced their shooting decisions for three consecutive weeks. Effects of practice were evaluated on a posttest. On the pretest, all groups experienced more anxiety and executed more false-positive responses under high-threat. Despite practice, these effects persisted on the posttest and remained equally strong for all practice groups. It is concluded that the impact of threat on police officers' shooting decisions is robust and may be hard to prevent within the limits of available practice."]},
{"title": "Passing crisis and emergency risk communications: The effects of communication channel, information type, and repetition", "highlights": ["Written communication channels resulted in more accurate message transmission than spoken channels.", "Message repetition will increase the accuracy of message transmission, particularly for spoken messages.", "If people are free to choose how to communicate, they prefer to use the telephone.", "If people are unable to use the telephone, they prefer to communicate using email.", "Using the telephone speeds up communication but is less accurate."], "abstract": ["Three experiments explore several factors which influence information transmission when warning messages are passed from person to person. In Experiment 1, messages were passed down chains of participants using five different modes of communication. Written communication channels resulted in more accurate message transmission than verbal. In addition, some elements of the message endured further down the chain than others. Experiment 2 largely replicated these effects and also demonstrated that simple repetition of a message eliminated differences between written and spoken communication. In a final field experiment, chains of participants passed information however they wanted to, with the proviso that half of the chains could not use telephones. Here, the lack of ability to use a telephone did not affect accuracy, but did slow down the speed of transmission from the recipient of the message to the last person in the chain. Implications of the findings for crisis and emergency risk communication are discussed."]},
{"title": "Practicing universal design to actual hand tool design process", "highlights": ["The context and concept of UD which evolves with time.", "Appropriate UD evaluation scales are varied on product attributes.", "User evaluation results are converted into product design factors.", "Examination of the importance of design factors by design verification.", "The proposal of new UD principles."], "abstract": ["UD evaluation principles are difficult to implement in product design. This study proposes a methodology for implementing UD in the design process through user participation. The original UD principles and user experience are used to develop the evaluation items. Difference of product types was considered. Factor analysis and Quantification theory type I were used to eliminate considered inappropriate evaluation items and to examine the relationship between evaluation items and product design factors. Product design specifications were established for verification. The results showed that converting user evaluation into crucial design verification factors by the generalized evaluation scale based on product attributes as well as the design factors applications in product design can improve users' UD evaluation. The design process of this study is expected to contribute to user-centered UD application."]},
{"title": "Are pressure measurements effective in the assessment of office chair comfort/discomfort? A review", "highlights": ["The relation between pressure measures and office chair comfort has been analysed.", "The method used for evaluating subjective comfort seems to be critically important.", "Pressure parameters potentially capable of describing comfort has been identified.", "Future work is needed due to the controversial results of the few available studies."], "abstract": ["Nowadays, the majority of jobs in the western world involves sitting in an office chair. As a result, a comfortable and supported sitting position is essential for employees. In the literature, various objective methods (e.g. pressure measurements, measurements of posture, EMG etc.) have been used to assess sitting comfort/discomfort, but their validity remains unknown. This review therefore examines the relationship between subjective comfort/discomfort and pressure measurements while sitting in office chairs.", "The literature search resulted in eight papers that met\u00a0all our requirements. Four studies identified a relationship between subjective comfort/discomfort and pressure distribution parameters (including correlations of up to r\u00a0=\u00a00.7\u00a0\u00b1\u00a00.13). However, the technique for evaluating subjective comfort/discomfort seems to play an important role on the results achieved, therefore placing their validity into question.", "The peak pressure on the seat pan, the pressure distribution on the backrest and the pressure pattern changes (seat pan and backrest) all appear to be reliable measures for quantifying comfort or discomfort."]},
{"title": "Influence of musculoskeletal pain on workers' ergonomic risk-factor assessments", "highlights": ["This study compares the workstation assessments of workers with and without musculoskeletal pain.", "Workers reporting pain assessed their workstations more negatively with respect to certain aspects related to ergonomic risk factors.", "Findings indicate that from an MSD prevention point of view, an ergonomics expert assessment may be more suitable to detect at risk workstations."], "abstract": ["This study compares the ergonomic risk-factor assessments of workers with and without musculoskeletal pain. A questionnaire on the musculoskeletal pain experienced in various body regions during the 12 months and seven days preceding the data collection was administered to 473 workers from three industrial sectors. The ", " method, developed by the Finnish Institute of Occupational Health (FIOH), was then used by the workers and an ergonomics expert to assess the workstations. The ergonomic quality of the workstations and the need for change were also assessed by the expert and the workers at the workstation, using visual analog\u00a0scales (VAS). Results show that the workers in this study were exposed to significant musculoskeletal disorder (MSD) risk factors, according to the FIOH assessment and the high percentages of reported pain. The results also show that those who reported pain in the seven days prior to the assessment evaluated their workstations more negatively than subjects who reported no pain, while the expert found no difference between the two groups' exposure to MSD risk factors."]},
{"title": "Reduction in predicted survival times in cold water due to wind and waves", "highlights": ["Recent marine tragedies have called into question immersion suit standards.", "Our participants were immersed in harsh environments consisting of wind and waves.", "We compared heat loss in calm water to that in wind and waves.", "Wind and waves caused significantly greater heat loss.", "Predicted survival times were lower in wind and waves compared to calm water."], "abstract": ["Recent marine accidents have called into question the level of protection provided by immersion suits in real (harsh) life situations. Two immersion suit studies, one dry and the other with 500\u00a0mL of water underneath the suit, were conducted in cold water with 10\u201312 males in each to test body heat loss under three environmental conditions: calm, as mandated for immersion suit certification, and two combinations of wind plus waves to simulate conditions typically found offshore. In both studies mean skin heat loss was higher in wind and waves vs. calm; deep body temperature and oxygen consumption were not different. Mean survival time predictions exceeded 36\u00a0h for all conditions in the first study but were markedly less in the second in both calm and wind and waves. Immersion suit protection and consequential predicted survival times under realistic environmental conditions and with leakage are reduced relative to calm conditions."]},
{"title": "Dutch police officers' preparation and performance of their arrest and self-defence skills: A questionnaire study", "highlights": ["Officers are unsatisfied with the training of their arrest and self-defence skills.", "More experienced officers are more critical towards the taught skills.", "More experienced officers reported better performance but less use of taught skills.", "Results plead for more training, reality-based practice, and teaching other skills."], "abstract": ["We investigated how Dutch police officers perceive their preparation for arrest and self-defence skills (ASDS) and their ability to manage violence on duty. Furthermore, we assessed whether additional experience (i.e., by having encountered violence on duty or by practicing martial arts) and self-perceived anxiety have an influence on these perceptions. Results of an online questionnaire (", "\u00a0=\u00a0922) showed that having additional experience was associated with self-perceived better performance. Officers who experience anxiety more often, on the other hand, reported more problems. Although most officers report sufficiently effective performance on duty, they, especially those with additional experience, feel that training frequency is too low and that the currently taught ASDS are only moderately usable (at least with the current amount of training). Based on the results, we suggest that increasing officers' ASDS experience, teaching officers to perform with high anxiety, or reconsidering the taught skills, may be necessary to further improve performance of police officers on duty."]},
{"title": "The use of virtual reality and physical tools in the development and validation of ease of entry and exit in passenger vehicles", "highlights": ["Inconsistent entry strategies were observed in a CAVE virtual environment.", "The CAVE needs development to support design for ease of entry and exit.", "Representation of vehicular hard points is necessary for optimising entry and exit.", "Various combinations of methods are required at different stages of vehicle development."], "abstract": ["Ease of entry and exit is important for creating a positive first impression of a car and increasing customer satisfaction. Several methods are used within vehicle development to optimise ease of entry and exit, including CAD reviews, benchmarking and buck trials. However, there is an industry trend towards digital methods to reduce the costs and time associated with developing physical prototypes. This paper reports on a study of entry strategy in three properties (buck, car, CAVE) in which inconsistencies were demonstrated by people entering a vehicle representation in the CAVE. In a second study industry practitioners rated the CAVE as worse than physical methods for identifying entry and exit issues, and having lower perceived validity and reliability. However, the resource issues associated with building bucks were recognised. Recommendations are made for developing the CAVE and for combinations of methods for use at different stages of a vehicle's development."]},
{"title": "The validity of the first and second generation Microsoft Kinect\u2122 for identifying joint center locations during static postures", "highlights": ["This study proposed a method to align Kinect sensor with a motion tracking system.", "The accuracy of Kinect sensor-identified coordinates of joint locations was examined.", "The accuracy level is posture-dependent and joint-dependent.", "Standing postures can be identified with better accuracy than sitting postures."], "abstract": ["The Kinect\u2122 sensor released by Microsoft is a low-cost, portable, and marker-less motion tracking system for the video game industry. Since the first generation Kinect sensor was released in 2010, many studies have been conducted to examine the validity of this sensor when used to measure body movement in different research areas. In 2014, Microsoft released the computer-used second generation Kinect sensor with a better resolution for the depth sensor. However, very few studies have performed a direct comparison between all the Kinect sensor-identified joint center locations and their corresponding motion tracking system-identified counterparts, the result of which may provide some insight into the error of the Kinect-identified segment length, joint angles, as well as the feasibility of adapting inverse dynamics to Kinect-identified joint centers. The purpose of the current study is to first propose a method to align the coordinate system of the Kinect sensor with respect to the global coordinate system of a motion tracking system, and then to examine the accuracy of the Kinect sensor-identified coordinates of joint locations during 8 standing and 8 sitting postures of daily activities. The results indicate the proposed alignment method can effectively align the Kinect sensor with respect to the motion tracking system. The accuracy level of the Kinect-identified joint center location is posture-dependent and joint-dependent. For upright standing posture, the average error across all the participants and all Kinect-identified joint centers is 76\u00a0mm and 87\u00a0mm for the first and second generation Kinect sensor, respectively. In general, standing postures can be identified with better accuracy than sitting postures, and the identification accuracy of the joints of the upper extremities is better than for the lower extremities. This result may provide some information regarding the feasibility of using the Kinect sensor in future studies."]},
{"title": "Effects of wearing gumboots and leather lace-up boots on lower limb muscle activity when walking on simulated underground coal mine surfaces", "highlights": ["Males walked on simulated underground coal mine surfaces in two different boots.", "Boot structure influenced thigh muscle activity when walking on uneven surfaces.", "Leather lace-up boots caused increased thigh muscle activity compared to gumboots.", "Increased thigh muscle activity appears to be a slip and/or trip prevention strategy."], "abstract": ["This study aimed to investigate the effects of wearing two standard underground coal mining work boots (a gumboot and a leather lace-up boot) on lower limb muscle activity when participants walked across simulated underground coal mining surfaces. Quadriceps (rectus femoris, vastus medialis, vastus lateralis) and hamstring (biceps femoris, semitendinosus) muscle activity were recorded as twenty male participants walked at a self-selected pace around a circuit while wearing each boot type. The circuit consisted of level, inclined and declined surfaces composed of rocky gravel and hard dirt. Walking in a leather lace-up boot, compared to a gumboot, resulted in increased vastus lateralis and increased biceps femoris muscle activity when walking on sloped surfaces. Increased muscle activity appears to be acting as a slip and/or trip prevention strategy in response to challenging surfaces and changing boot features."]},
{"title": "Driving performance and driver discomfort in an elevated and standard driving position during a driving simulation", "highlights": ["An elevated driving posture does not adversely affect comfort.", "An elevated driving posture does not adversely affect driver performance.", "Accelerator pedal stiffness is associated with ankle discomfort."], "abstract": ["The primary purposes of a vehicle driver's seat, is to allow them to complete the driving task comfortably and safely. Within each class of vehicle (e.g. passenger, commercial, industrial, agricultural), there is an expected driving position to which a vehicle cabin is designed. This paper reports a study that compares two driving positions, in relation to Light Commercial Vehicles (LCVs), in terms of driver performance and driver discomfort. In the \u2018elevated\u2019 driving position, the seat is higher than usually used in road vehicles; this is compared to a standard driving position replicating the layout for a commercially available vehicle. It is shown that for a sample of 12 drivers, the elevated position did not, in general, show more discomfort than the standard position over a 60\u00a0min driving simulation, although discomfort increased with duration. There were no adverse effects shown for emergency stop reaction time or for driver headway for the elevated posture compared to the standard posture. The only body part that showed greater discomfort for the elevated posture compared to the standard posture was the right ankle. A second experiment confirmed that for 12 subjects, a higher pedal stiffness eliminated the ankle discomfort problem."]},
{"title": "Effect of gait on formation of thermal environment inside footwear", "highlights": ["The relation between the gait conditions and foot temperature distributions inside footwear was studied.", "The metabolic heat generation was found to have a basic impact on the temperature profile.", "High-temperature-elevation regions coincided with high-contact-load regions.", "The relation between the temperature elevation and contact load was clarified.", "The key aspects of a thermally comfortable design concept and applications were described."], "abstract": ["In this study, the relationship between the gait condition and foot temperature distributions inside footwear was investigated using subject experiments. Mechanical, physical, and physiological variables such as the foot contact force, landing speed, and metabolic heat generation were also measured. Gait motion measurements showed that a large contact force was concentrated in the small area of the heel at the initial contact and later at the forefoot. A faster gait produced a larger contact force, higher landing velocity, higher skin temperature, and larger metabolism during gait. The temperature at the bottom of the foot increased, and the temperature on the upper side decreased. The metabolic heat generation had a basic impact on the temperature profile, and skin temperatures tended to increase gradually. In addition, high-temperature-elevation regions such as the big toe and heel coincided with regions with high-contact loads, which suggested a relationship between the temperature elevation and contact load."]},
{"title": "Anthropometry of external auditory canal by non-contactable measurement", "highlights": ["We use the non-invasive computed tomography technology to measure the geometric shape of external auditory canal.", "The length and width of ear canal openings for men are longer, wider than those for women.", "The difference between the length and width of the ear canal opening is about 40%.", "The circular cross-section shape of the earplugs should be replaced with elliptical cross-section shape for better fitting."], "abstract": ["Human ear canals cannot be measured directly with existing general measurement tools. Furthermore, general non-contact optical methods can only conduct simple peripheral measurements of the auricle and cannot obtain the internal ear canal shape-related measurement data. Therefore, this study uses the computed tomography (CT) technology to measure the geometric shape of the ear canal and the shape of the ear canal using a non-invasive method, and to complete the anthropometry of external auditory canal. The results of the study show that the average height and width of ear canal openings, and the average depth of the first bend for men are generally longer, wider and deeper than those for women. In addition, the difference between the height and width of the ear canal opening is about 40% (p\u00a0<\u00a00.05). Hence, the circular cross-section shape of the earplugs should be replaced with an elliptical cross-section shape during manufacturing for better fitting."]},
{"title": "New technical design of food packaging makes the opening process easier for patients with hand disorders", "highlights": ["It is possible for manufacturers today to produce easy-to-open food packaging.", "Opening of easy-to-open food packages increases the consumer satisfaction.", "Food packages of the future are a benefit for the entire population."], "abstract": ["Opening packaged food is a complex daily activity carried out worldwide. Peelable packaging, as used for cheese or meat, causes real problems for many consumers, especially elderly people and those with hand disorders.", "Our aim was to investigate the possibility of producing meat packaging that is easier for patients with hand disorders to open. One hundred patients with hand osteoarthritis were asked to open a meat package currently available in supermarkets (Type A) and a modified, newly designed version (Type B), and rate their experiences with a consumer satisfaction index (CSI).", "The mean CSI of the Type B packs was 68.9%, compared with 41.9% for Type A (p\u00a0<\u00a00.0001). These results show that manufacturers today can produce easy-to-open food packages that afford greater consumer satisfaction. Such future packaging would benefit not only people with hand disorders but also the population as a whole."]},
{"title": "Partly visible periods in posture observation from video: Prevalence and effect on summary estimates of postures in the job", "highlights": ["We compared observation of fully and partly visible working postures from video.", "Postures were rated to differ between fully and partly visible video frames.", "Postural angles tended to be lower for fully visible frames than partly visible.", "Findings were consistent for both trunk and arm postures.", "Including partly visible postures resulted in different daily exposure summaries."], "abstract": ["This paper investigated the extent to which observers rated clearly visible postures on video differently from partly visible postures, and whether visibility affected full-shift posture summaries. Trunk and upper arm postures were observed from 10,413 video frames representing 80 shifts of baggage handling; observers reported postures as fully or only partly visible. Postures were summarized for each shift into several standard metrics using all available data, only fully visible frames, or only partly visible frames. 78% of trunk and 70% of upper arm postural observations were inferred. When based on all data, mean and 90th percentile trunk postures were 1.8\u00b0 and 5.6\u00b0 lower, respectively, than when based only on fully visible situations. For the arm; differences in mean and 90th percentile were 0.7\u00b0 and 8.2\u00b0. Daily posture summaries were significantly influenced by whether partly visible postures are included or not."]},
{"title": "The influence of gait cadence on the ground reaction forces and plantar pressures during load carriage of young adults", "highlights": ["High ground reaction forces and plantar pressures are observed during load carriage at high gait cadences.", "Special attention should be paid in the medial forefoot and medial rearfoot regions during load carriage at high gait cadences.", "The pattern of force distribution is different between load carriage and normal walking during low and high gait cadences.", "During load carriage changes on gait pattern appears to occur to protect the musculoskeletal system from high vertical forces."], "abstract": ["Biomechanical gait parameters\u2014ground reaction forces (GRFs) and plantar pressures\u2014during load carriage of young adults were compared at a low gait cadence and a high gait cadence. Differences between load carriage and normal walking during both gait cadences were also assessed. A force plate and an in-shoe plantar pressure system were used to assess 60 adults while they were walking either normally (unloaded condition) or wearing a backpack (loaded condition) at low (70 steps per minute) and high gait cadences (120 steps per minute). GRF and plantar pressure peaks were scaled to body weight (or body weight plus backpack weight). With medium to high effect sizes we found greater anterior-posterior and vertical GRFs and greater plantar pressure peaks in the rearfoot, forefoot and hallux when the participants walked carrying a backpack at high gait cadences compared to walking at low gait cadences. Differences between loaded and unloaded conditions in both gait cadences were also observed."]},
{"title": "Sonification of in-vehicle interface reduces gaze movements under dual-task condition", "highlights": ["We studied the impact of using a sonified or silent interface while performing a visual primary task.", "We measured participants gaze and performance to both primary and secondary tasks.", "Participants used the sonified interface nearly exclusively be ear while performing the primary task.", "The reaction times in the primary task were increased in both sonified and silent conditions."], "abstract": ["In-car infotainment systems (ICIS) often degrade driving performances since they divert the driver's gaze from the driving scene. Sonification of hierarchical menus (such as those found in most ICIS) is examined in this paper as one possible solution to reduce gaze movements towards the visual display. In a dual-task experiment in the laboratory, 46 participants were requested to prioritize a primary task (a continuous target detection task) and to simultaneously navigate in a realistic mock-up of an ICIS, either sonified or not. Results indicated that sonification significantly increased the time spent looking at the primary task, and significantly decreased the number and the duration of gaze saccades towards the ICIS. In other words, the sonified ICIS could be used nearly exclusively by ear. On the other hand, the reaction times in the primary task were increased in both silent and sonified conditions. This study suggests that sonification of secondary tasks while driving could improve the driver's visual attention of the driving scene."]},
{"title": "Designers' and users' roles in participatory design: What is actually co-designed by participants?", "highlights": ["We explore the actual forms of participation in a participatory design process.", "Actual roles and profiles are analyzed in relation to levels of abstraction.", "The artifact ", " its concept are shown to be co-designed by users and designers.", "Certain roles and profiles are shown to be key to this co-design.", "This gives clues to improve the teaching and practice of participatory design."], "abstract": ["This research deals with an analysis of forms of participation in a participatory design (PD) process of a software that assesses the sustainability of agricultural cropping systems. We explore the actual forms of participation of designers and users by adapting an Actual Role Analysis in Design approach (", ") to capture the levels of abstraction (conceptual, functional and operational) of participants' discussions. We show that: (1) the process does not only concern the design of the artifact itself, but also the design of the concept of sustainability; (2) all participants (users & designers) have a role in co-designing the concept (in our case, sustainability); (3) some roles and profiles are key to this co-design. We discuss our contributions to both the research and the practices of participatory design. These contributions deal with the production of a method and related knowledge about actual activities in participatory design situations. They may support the development of relevant training programs regarding participatory situations, or be reflexive activities that can help those who are involved in designing and leading in participatory situations, to make improvements."]},
{"title": "The influence of occupation and age on maximal and rapid lower extremity strength", "highlights": ["We examined age-related maximal and rapid strength capacities in workers.", "Physical workload was quantified and examined in relation to strength capacities.", "Occupation did not influence the age-related decline in any strength variables.", "A negative correlation was shown between workload and rapid strength in older men."], "abstract": ["The aims of this study were to 1) examine the influence of age and occupation on maximal and rapid strength of the lower-extremity muscles and 2) examine the relationship between maximal and rapid strength and physical workload (work index (WI)) in the blue-collar (BC) cohort. Peak torque (PT) and peak rate of torque development (peakRTD) of the leg extensors (LE), leg flexors (LF), and plantar flexors (PF) were assessed in 47 young (age\u00a0=\u00a024.1\u00a0\u00b1\u00a02.4 years) and 41 middle-aged (52.4\u00a0\u00b1\u00a05.2 years) white-collar (WC) and BC men. Middle-aged workers exhibited lower PT for all muscles, and peakRTD for the LF and PF muscles. A positive relationship (", "\u00a0=\u00a00.59; ", "\u00a0<\u00a00.01) was observed between WI and peakRTD for the PF in the young BC workers, however, this relationship was negative (", "\u00a0=\u00a0\u22120.45; ", "\u00a0=\u00a00.053) for the LF of the middle-aged BC workers. Lowering physical work demands and/or incorporating effective health-related practices for employees may be appealing strategies to enhance aging workers' productivity and longevity in the workforce."]},
{"title": "Evaluating the physical demands when using sled-type stair descent devices to evacuate mobility-limited occupants from high-rise buildings", "highlights": ["We simulated evacuating people with motor disabilities from high-rise buildings.", "This study compared 6 sled-type devices designed for stair descent.", "Surface EMG, heart rate, trunk motion, and perceived exertion data were collected.", "There are significant differences across devices, particularly on the landings.", "Two-evacuator devices, did well on biomechanics, evacuation speed, and usability."], "abstract": ["The physical demands on evacuators were investigated when using different types of sled-type stair descent devices designed for the emergency evacuation of high rise buildings. Twelve firefighters used six sled-type stair descent devices during simulated evacuations. The devices were evaluated under two staircase width conditions (1.12, and 1.32\u00a0m). Dependent measures included electromyographic (EMG) data, heart rates, Borg Scale ratings, and descent velocities. All stair descent speeds were below those reported during pedestrian egress trials. With the exception of the inflatable device, the devices operated by two evacuators had higher descent speeds than those operated by a single evacuator. High friction materials under the sleds facilitated control and reduced the muscle demands on stairs but increased physical demands on the landings. Usability assessments found devices with shorter overall lengths had fewer wall contacts on the landing, and handles integrated in the straps were preferred by the evacuators."]},
{"title": "Complex socio-technical systems: Characterization and management guidelines", "highlights": ["A framework to operationalize the attribute view of complexity.", "Description of attributes and assessment of management guidelines.", "Attributes can be irreducible/manageable complexity and liabilities/assets.", "Relationships between attributes are discussed.", "The case of an emergency department illustrates the use of the framework."], "abstract": ["Although ergonomics has paid increasing attention to the perspective of complexity, methods for its operationalization are scarce. This study introduces a framework for the operationalization of the \u201cattribute view\u201d of complexity, which involves: (i) the delimitation of the socio-technical system (STS); (ii) the description of four complexity attributes, namely a large number of elements in dynamic interactions, a wide diversity of elements, unexpected variability, and resilience; (iii) the assessment of six management guidelines, namely design slack, give visibility to processes and outcomes, anticipate and monitor the impacts of small changes, monitor the gap between prescription and practice, encourage diversity of perspectives when making decisions, and create an environment that supports resilience; and (iv) the identification of leverage points for improving the STS design, based on both the analysis of relationships among the attributes and their classification as irreducible/manageable complexity, and liability/asset. The use of the framework is illustrated by the study of an emergency department of a University hospital. Data collection involved analysis of documents, observations of work at the front-line, interviews with employees, and the application of questionnaires."]},
{"title": "An experiment with content distribution methods in touchscreen mobile devices", "highlights": ["An experiment about usability for reading in mobile web pages has been carried out.", "Vertical scrolling is the most efficient for reading in webpages for mobile devices.", "Mobile users prefer vertical scrolling instead of paging or internal links.", "If the content to be showed is very large then paging is preferred by users."], "abstract": ["This paper compares the usability of three different content distribution methods (scrolling, paging and internal links) in touchscreen mobile devices as means to display web documents. Usability is operationalized in terms of effectiveness, efficiency and user satisfaction. These dimensions are then measured in an experiment (N\u00a0=\u00a023) in which users are required to find words in regular-length web documents. Results suggest that scrolling is statistically better in terms of efficiency and user satisfaction. It is also found to be more effective but results were not significant. Our findings are also compared with existing literature to propose the following guideline: \u201ctry to use vertical scrolling in web pages for mobile devices instead of paging or internal links, except when the content is too large, then paging is recommended\u201d. With an ever increasing number of touchscreen web-enabled mobile devices, this new guideline can be relevant for content developers targeting the mobile web as well as institutions trying to improve the usability of their content for mobile platforms."]},
{"title": "Lumbar compression forces while lifting and carrying with two and four workers", "highlights": ["Carrying while stepping up a platform results in higher compression forces compared with lifting.", "100-kg four-worker lifts result in lower compression forces than 50-kg two-worker lifts.", "Increasing the number of workers does not result in higher variability of the compression force."], "abstract": ["Team lifting and carrying is advised when loads exceed 25\u00a0kg and mechanical lifting is not feasible. The aim of this study was to assess mean, maximum and variability of peak lumbar compression forces which occur daily at construction sites. Therefore, 12 ironworkers performed 50-kg two-worker and 100-kg four-worker lifting and carrying tasks in a laboratory experiment. The 50-kg two-worker lifts resulted in significantly higher mean (\u0394 537 N) and maximum (\u0394 586 N) peak lumbar compression forces compared with the 100-kg four-worker lifts. The lowest mean and maximum peak lumbar compression forces were found while carrying on level ground and increased significantly when stepping over obstacles and up platforms. Lifting 100\u00a0kg with four workers in a rectangular line up resulted in lower compression forces compared with lifting 50\u00a0kg with two workers standing next to each other. When loads are carried manually routes should be free of any obstacles to be overcome."]},
{"title": "Adaptive neuro-fuzzy inference systems with ", "highlights": ["We present a practical approach based on neuro-fuzzy systems to estimate energy expenditure using heart rate monitoring.", "The proposed approach improves the standard Flex-HR method in that it does not require individual calibration.", "The proposed approach treats the uncertainty in human physiological systems and in various workplaces by using fuzzy logic."], "abstract": ["This paper presents a new model based on adaptive neuro-fuzzy inference systems (ANFIS) to predict oxygen consumption ", " from easily measured variables. The ANFIS prediction model consists of three ANFIS modules for estimating the Flex\u2013HR parameters. Each module was developed based on clustering a training set of data samples relevant to that module and then the ANFIS prediction model was tested against a validation data set. Fifty-eight participants performed the Meyer and Flenghi step-test, during which heart rate (HR) and ", " were measured. Results indicated no significant difference between observed and estimated Flex\u2013HR parameters and between measured and estimated ", " in the overall HR range, and separately in different HR ranges. The ANFIS prediction model (MAE\u00a0=\u00a03\u00a0ml\u00a0kg", "\u00a0min", ") demonstrated better performance than Rennie et\u00a0al.'s (MAE\u00a0=\u00a07\u00a0ml\u00a0kg", "\u00a0min", ") and Keytel et\u00a0al.'s (MAE\u00a0=\u00a06\u00a0ml\u00a0kg", "\u00a0min", ") models, and comparable performance with the standard Flex\u2013HR method (MAE\u00a0=\u00a02.3\u00a0ml\u00a0kg", "\u00a0min", ") throughout the HR range. The ANFIS model thus provides practitioners with a practical, cost- and time-efficient method for ", " estimation without the need for individual calibration."]},
{"title": "Mobile input device type, texting style and screen size influence upper extremity and trapezius muscle activity, and cervical posture while texting", "highlights": ["Input device type and texting style affect ergonomic exposures when texting.", "Touch screen texting requires less muscle activity than a physical keypad.", "2-Handed texting elicits less wrist extensor muscle activity than 1 thumb/hand.", "Muscle activity and cervical flexion increase as touch screen size increases.", "Fewer choose to use thumbs only when texting with larger touch screen sizes."], "abstract": ["This study aimed to determine the effects of input device type, texting style, and screen size on upper extremity and trapezius muscle activity and cervical posture during a short texting task in college students. Users of a physical keypad produced greater thumb, finger flexor, and wrist extensor muscle activity than when texting with a touch screen device of similar dimensions. Texting on either device produced greater wrist extensor muscle activity when texting with 1 hand/thumb compared with both hands/thumbs. As touch screen size increased, more participants held the device on their lap, and chose to use both thumbs less. There was also a trend for greater finger flexor, wrist extensor, and trapezius muscle activity as touch screen size increased, and for greater cervical flexion, although mean differences for cervical flexion were small. Future research can help inform whether the ergonomic stressors observed during texting are associated with musculoskeletal disorder risk."]},
{"title": "Movement transformation on multi-touch devices: Intuition or instructional preparation?", "highlights": ["We evaluate the usability of a touch pad device.", "An action-regulatory framework is introduced.", "Users cannot generalize gestures between touch pads and touch screens."], "abstract": ["Multi-touch technology is a key part of computer interaction today, yet little is known about the distinction between direct and indirect input devices in terms of intuitive interaction. An experimental study aims to identify the difficulties of interaction with indirect multi-touch devices by applying the action regulation theory and the principle of movement transformation to common computer tasks involving gesture utilization. An analysis of the data acquired from 54 subjects working with an Apple Magic Trackpad implies that gestures on indirect multi-touch devices are not utilized intuitively without instructions that bypass conceptual difficulties of indirect gesture usage. It is shown that gesture use influences product assessment measured by User Experience questionnaires and that prior experience with direct multi-touch devices does not influence gesture usage or product assessment. We advise that product developers utilize video instructions to create a sense of intuitive interaction."]},
{"title": "Effects of flooring on required coefficient of friction: Elderly adult vs. middle-aged adult barefoot gait", "highlights": ["Barefoot gait on carpet revealed higher required coefficient of friction than vinyl flooring.", "The results of the present study showed different behavior between EA and MA subjects with respect to RCOF.", "Females presented higher RCOF values during heel contact.", "Friction on barefoot gait is affected by flooring types, gender and age.", "Carpet was the safer flooring in terms of required coefficient of friction."], "abstract": ["The aim of this study was to investigate the effect of flooring on barefoot gait according to age and gender. Two groups of healthy subjects were analyzed: the elderly adult group (EA; 10 healthy subjects) and the middle-aged group (MA; 10 healthy subjects). Each participant was asked to walk at his or her preferred speed over two force plates on the following surfaces: 1) homogeneous vinyl (HOV), 2) carpet, 3) heterogeneous vinyl (HTV) and 4) mixed (in which the first half of the pathway was covered by HOV and the second by HTV). Two force plates (Kistler 9286BA) embedded in the data collection room floor measured the ground reaction forces and friction. The required coefficient of friction (RCOF) was analyzed. For the statistical analysis, a linear mixed-effects model for repeated measures was performed. During barefoot gait, there were differences in the RCOF among the flooring types during the heel contact and toe-off phases. Due to better plantar proprioception during barefoot gait, the EA and MA subjects were able to distinguish differences among the flooring types. Moreover, when the EA were compared with the MA subjects, differences could be observed in the RCOF during the toe-off phase, and gender differences in the RCOF could also be observed during the heel contact phase in barefoot gait."]},
{"title": "Human error identification for laparoscopic surgery: Development of a motion economy perspective", "highlights": ["The paper develops motion economy principles for operating room environment.", "Our approach identifies deficiencies that predispose experienced surgeons to errors.", "We use patient positioning for hand-assisted nephrectomy as illustrative case.", "Observations were conducted to test the validity of our approach.", "The developed set of principles can be useful for training surgeons on motion economy."], "abstract": ["This study postulates that traditional human error identification techniques fail to consider motion economy principles and, accordingly, their applicability in operating theatres may be limited. This study addresses this gap in the literature with a dual aim. First, it identifies the principles of motion economy that suit the operative environment and second, it develops a new error mode taxonomy for human error identification techniques which recognises motion economy deficiencies affecting the performance of surgeons and predisposing them to errors. A total of 30 principles of motion economy were developed and categorised into five areas. A hierarchical task analysis was used to break down main tasks of a urological laparoscopic surgery (hand-assisted laparoscopic nephrectomy) to their elements and the new taxonomy was used to identify errors and their root causes resulting from violation of motion economy principles. The approach was prospectively tested in 12 observed laparoscopic surgeries performed by 5 experienced surgeons. A total of 86 errors were identified and linked to the motion economy deficiencies. Results indicate the developed methodology is promising. Our methodology allows error prevention in surgery and the developed set of motion economy principles could be useful for training surgeons on motion economy principles."]},
{"title": "The effect of user's perceived presence and promotion focus on\u00a0usability for interacting in virtual environments", "highlights": ["Involvement and promotion focus are driver factors for interacting in flow effect.", "An extended framework is proposed to examine usability in VR.", "Immersion of presence has not a significant influence on learning."], "abstract": ["Technological advance in human\u2013computer interaction has attracted increasing research attention, especially in the field of virtual reality (VR). Prior research has focused on examining the effects of VR on various outcomes, for example, learning and health. However, which factors affect the final outcomes? That is, what kind of VR system design will achieve higher usability? This question remains largely. Furthermore, when we look at VR system deployment from a human\u2013computer interaction (HCI) lens, does user's attitude play a role in achieving the final outcome? This study aims to understand the effect of immersion and involvement, as well as users' regulatory focus on usability for a somatosensory VR learning system. This study hypothesized that regulatory focus and presence can effectively enhance user's perceived usability. Survey data from 78 students in Taiwan indicated that promotion focus is positively related to user's perceived efficiency, whereas involvement and promotion focus are positively related to user's perceived effectiveness. Promotion focus also predicts user satisfaction and overall usability perception."]},
{"title": "Adapting the force characteristics of a staple gun to the human hand", "highlights": ["Three modified staple guns are compared with a standard staple gun.", "A linearly increasing staple gun force curve is not optimal for a human hand.", "The muscular exertion is lowered with an adaption of a staple gun force curve.", "The tool efficiency is increased with an adapted force curve."], "abstract": ["Three prototype staple guns with modified force characteristics were compared with a commercially available standard staple gun with a linearly increasing force resistance during squeezing. The force characteristics of the prototypes were more or less adapted to the force characteristics of the human hand, and in one of the staple guns the general force level was also reduced by one third. Evaluation instruments were electromyography of the forearm flexors and extensors, subjective rating of forearm exertion and subjects' free comments about the four tools. Twelve professional craftsmen were recruited as test subjects. The results show significantly lower readings for two of the three prototypes compared with the standard gun in electromyography as well as subjective ratings. The squeezing times are also reduced for two of the prototypes. It is concluded that the choice of force characteristics of a staple gun is important both to minimize forearm muscular exertion and to increase tool efficiency."]},
{"title": "Postural dynamism during computer mouse and keyboard use: A pilot study", "highlights": ["Postural dynamism related to keyboard and mouse use is explored.", "Postural dynamism and spinal positions in three dimensions are used as parameters.", "Mouse use is associated with a generally fixated spinal posture.", "Keyboard use demonstrates a more dynamic spine than mouse use."], "abstract": ["Prolonged sedentary computer use is a risk factor for musculoskeletal pain. The aim of this study was to explore postural dynamism during two common computer tasks, namely mouse use and keyboard typing. Postural dynamism was described as the total number of postural changes that occurred during the data capture period. Twelve participants were recruited to perform a mouse and a typing task. The data of only eight participants could be analysed. A 3D motion analysis system measured the number of cervical and thoracic postural changes as well as, the range in which the postural changes occurred. The study findings illustrate that there is less postural dynamism of the cervical and thoracic spinal regions during computer mouse use, when compared to keyboard typing."]},
{"title": "Driver discomfort in vehicle seats \u2013 Effect of changing road conditions and seat foam composition", "highlights": ["Exposure to whole-body vibration accelerates the onset of discomfort when driving.", "Improving chemical composition of polyurethane foam improves seat comfort when driving for 40 min and exposed to vibration.", "Cessation of vibration exposure caused an acute improvement in feelings of comfort in a car seat.", "Discomfort increased with time whilst driving with and without vibration."], "abstract": ["Discomfort in vehicle seats is a multi-factorial problem with contributions occurring from effects of sitting duration, seat design, and the dynamic environment to which the occupant is exposed. This paper reports laboratory studies investigating the extent to which reports of discomfort are affected by vibration commencing or ceasing, and whether methods of assessment are sensitive enough to detect small changes in foam composition. Study 1 measured discomfort ratings for two conditions of 60\u00a0min each, comprising 30\u00a0min of vibration exposure followed by 30\u00a0min of static sitting in a car seat, and vice-versa. Study 2 measured discomfort ratings for three conditions over a period of 40\u00a0min each, whilst participants were sitting in one of two car seat compositions, and either exposed to vibration or not. In both studies participants operated a driving simulator. It is shown that exposure to vibration increases the rate of discomfort onset in comparison to periods of static sitting. When vibration stopped, there was an acute improvement in comfort but discomfort did not drop to the levels reported by those who had been unexposed. When vibration started after 30\u00a0min of static sitting, there was an acute increase in discomfort but not to the levels reported by those who had been exposed to 30\u00a0min of vibration. After 40\u00a0min of continuous exposure it was possible to detect significant differences in overall discomfort between the two seat compositions, although trends could be observed in less time."]},
{"title": "Quantitative evaluation of the impact of night shifts and alcohol consumption on construction tiling quality", "highlights": ["We designed five tiling tasks to evaluate the impact of night shift and alcohol.", "Participants executed the tasks in four statuses (e.g., inebriation, night shift).", "Night shift had a larger impact than did alcohol.", "Tiling quality was decreased, by 31.31% (night shift) and 23.82% (alcohol).", "Both factors potently affected the ability to execute basic and advanced tasks."], "abstract": ["The adverse effects of night-shift work and alcohol consumption on performance have received considerable attention. However, how night shifts and alcohol affect productivity in workers has not been quantified. This paper describes the experiments featuring multiple tiling tasks and patterns. The tiling quality performed by the graduate student participants in four different statuses was objectively evaluated by an edge-detection computer program. The results indicate that both night shift and alcohol significantly reduce the quality in general, and the effects of the factors on position and alignment-angle qualities were dissimilar in distinct areas due to tile patterns and size. Both night-shift and alcohol conditions affected the basic (\u221234.01% and\u00a0\u221225.79%) and advanced tiling abilities (\u221240.14% and\u00a0\u221226.16%), and night shift had a larger impact than alcohol. These results provide jobsite managers with usable information regarding how night shifts and alcohol affect workers' abilities to execute basic and advanced tasks."]},
{"title": "Use of adaptive cruise control functions on motorways and urban roads: Changes over time in an on-road study", "highlights": ["Mastering each ACC system function was a process completed in a few trials.", "Changes registered over time were dependent on the road environment.", "Usage rate discrepancy between road environments faded away significantly with time.", "Shortest time headway significantly more used over time on the urban road.", "Existence of distinct overriding strategy pattern for each road environment."], "abstract": ["The study aimed at investigating how drivers use Adaptive Cruise Control and its functions in distinct road environments and to verify if changes occur over time. Fifteen participants were invited to drive a vehicle equipped with a Stop & Go Adaptive Cruise Control system on nine occasions. The course remained the same for each test run and included roads on urban and motorway environments. Results showed significant effect of experience for ACC usage percentage, and selection of the shortest time headway value in the urban road environment. This indicates that getting to know a system is not a homogenous process, as mastering the use of all the system's functions can take differing lengths of time in distinct road environments. Results can be used not only for the development of the new generation of systems that integrate ACC functionalities but also for determining the length of training required to operate an ACC system."]},
{"title": "Masking of thresholds for the perception of fore-and-aft vibration of seat backrests", "highlights": ["A component of vehicle vibration may not be easily perceptible to passengers.", "Thresholds for perceiving a vibration can be impaired (\u2018masked\u2019) by other vibrations.", "Masking may depend on whether the vibrations cause sensations at the same locations."], "abstract": ["The detection of a vibration may be reduced by the presence of another vibration: a phenomenon known as \u2018masking\u2019. This study investigated how the detection of one frequency of vibration is influenced by vibration at another frequency. With nine subjects, thresholds for detecting fore-and-aft backrest vibration were determined (for 4, 8, 16, and 31.5-Hz sinusoidal vibration) in the presence of a masker vibration (4-Hz random vibration, 1/3-octave bandwidth at six intensities). The masker vibration increased thresholds for perceiving vibration at each frequency by an amount that reduced with increasing difference between the frequency of the sinusoidal vibration and the frequency of the masker vibration. The 4-Hz random vibration almost completely masked 4-Hz sinusoidal vibration, partially masked 8- and 16-Hz vibration, and only slightly masked 31.5-Hz vibration. The findings might be explained by the involvement of different sensory systems and different body locations in the detection of different frequencies of vibration."]},
{"title": "Developing brain-computer interfaces from a user-centered perspective: Assessing the needs of persons with amyotrophic lateral sclerosis, caregivers, and professionals", "highlights": ["We identified the needs of BCI end-users (persons with ALS and their contacts).", "BCI end-users need more information on BCIs and everyday applications.", "A BCI system should support different users through different stages of ALS.", "BCIs should monitor and account for affective states.", "Retaining the sense of agency is crucial for end-users with ALS."], "abstract": ["By focus group methodology, we examined the opinions and requirements of persons with ALS, their caregivers, and health care assistants with regard to developing a brain-computer interface (BCI) system that fulfills the user's needs. Four overarching topics emerged from this analysis: 1) lack of information on BCI and its everyday applications; 2) importance of a customizable system that supports individuals throughout the various stages of the disease; 3) relationship between affectivity and technology use; and 4) importance of individuals retaining a sense of agency. These findings should be considered when developing new assistive technology. Moreover, the BCI community should acknowledge the need to bridge experimental results and its everyday application."]},
{"title": "Ergonomics Climate Assessment: A measure of operational performance and employee well-being", "highlights": ["The Ergonomics Climate Assessment assesses a value for performance and well-being.", "A discrepancy between values is associated with more work-related pain.", "An equal value for performance and well-being is associated with less pain.", "Organizations should show a value for performance and well-being equally."], "abstract": ["Ergonomics interventions have the potential to improve operational performance and employee well-being. We introduce a framework for ", ", the extent to which an organization emphasizes and supports the design and modification of work to maximize both performance and well-being outcomes. We assessed ergonomics climate at a large manufacturing facility twice during a two-year period. When the organization used ergonomics to promote performance and well-being equally, and at a high level, employees reported less work-related pain. A larger discrepancy between measures of operational performance and employee well-being was associated with increased reports of work-related pain. The direction of this discrepancy was not significantly related to work-related pain, such that it didn't matter which facet was valued more. The Ergonomics Climate Assessment can provide companies with a baseline assessment of the overall value placed on ergonomics and help prioritize areas for improving operational performance and employee well-being."]},
{"title": "A field study on thermal comfort in an Italian hospital considering differences in gender and age", "highlights": ["Comparison between AMV and PMV values in the hospital taking into account factors as gender and age.", "Correlation diagrams of PMV and AMV values are concordant for medical staff, discordant for patients.", "The association between AMV and PMV is stronger for medical staff than for patients.", "PMV finds its best correlation with AMV values among male medical staff under 65 years of age.", "Gender and age are factors that must be taken into account in the assessment of thermal comfort in the hospital."], "abstract": ["The hospital is a thermal environment where comfort must be calibrated by taking into account two different groups of people, that is, patients and medical staff. The study involves 30 patients and 19 medical staff with a view to verifying if Predicted Mean Vote (PMV) index can accurately predict thermal sensations of both groups also taking into account any potential effects of age and gender. The methodology adopted is based on the comparison between PMV values (calculated according to ISO 7730 after having collected environmental data and estimated personal parameters) and perceptual judgments (Actual Mean Vote, AMV), expressed by the subjects interviewed. Different statistical analyses show that PMV model finds his best correlation with AMV values in a sample of male medical staff under 65 years of age. It has been observed that gender and age are factors that must be taken into account in the assessment of thermal comfort in the hospital due to very weak correlation between AMV and PMV values."]},
{"title": "Assessing the performance of winter footwear using a new maximum achievable incline method", "highlights": ["We assess footwear slip resistance on ice by the maximum slope angles subjects could stand and walk.", "The maximum slope angles were objective and ecologically valid measures of slip resistance.", "We tested footwear performance while standing and walking uphill, downhill, and on cross-slopes.", "This human-centred measure of slip resistance did not require controlling of individuals' gait characteristics.", "One outsole design significantly outperformed the others on the smooth, wet ice."], "abstract": ["More informative tests of winter footwear performance are required in order to identify footwear that will prevent injurious slips and falls on icy conditions. In this study, eight participants tested four styles of winter boots on smooth wet ice. The surface was progressively tilted to create increasing longitudinal and cross-slopes until participants could no longer continue standing or walking. Maximum achievable incline angles provided consistent measures of footwear slip resistance and demonstrated better resolution than mechanical tests. One footwear outsole material and tread combination outperformed the others on wet ice allowing participants to successfully walk on steep longitudinal slopes of 17.5\u00b0\u00a0\u00b1\u00a01.9\u00b0 (mean\u00a0\u00b1\u00a0SD). By further exploiting the methodology to include additional surfaces and contaminants, such tests could be used to optimize tread designs and materials that are ideal for reducing the risk of slips and falls."]},
{"title": "The effects of social interactions with in-vehicle agents on a driver's anger level, driving performance, situation awareness, and perceived workload", "highlights": ["An in-vehicle agent can improve a driver's psychological state and road safety.", "Participants drove in a simulator with induced anger or neutral affect.", "Speech-based agents enhanced driver situation awareness and driving performance.", "Speech-based agents reduced participants' anger level and perceived workload.", "A driver's situation awareness mediated anger effects on driving performance."], "abstract": ["Research has suggested that interaction with an in-vehicle software agent can improve a driver's psychological state and increase road safety. The present study explored the possibility of using an in-vehicle software agent to mitigate effects of driver anger on driving behavior. After either anger or neutral mood induction, 60 undergraduates drove in a simulator with two types of agent intervention. Results showed that both speech-based agents not only enhance driver situation awareness and driving performance, but also reduce their anger level and perceived workload. Regression models show that a driver's anger influences driving performance measures, mediated by situation awareness. The practical implications include design guidelines for the design of social interaction with in-vehicle software agents."]},
{"title": "The influence of footwear tread groove parameters on available friction", "highlights": ["The available friction provided by a shoe outsole is influenced by many factors, including the design of tread grooves.", "We investigated the influence of three tread groove parameters (width, depth, and orientation) on available friction.", "Groove orientation had the greatest impact on available friction."], "abstract": ["The purpose of this study was to determine how footwear tread groove parameters influence available friction (COF). Utilizing a whole shoe tester (SATRA STM 603), 3 groove parameters (width, depth and orientation) were evaluated. Groove orientation had 3 levels (parallel, oblique and perpendicular), width had 3 levels (3, 6 and 9\u00a0mm) and depth had 3 levels (2, 4 and 6\u00a0mm). In total, the COF of 27 shoes, each with a distinct groove combination, was assessed on wet porcelain tile. The 27 groove combinations produced a wide range of COF values (0.080\u20130.344). Groove orientation had the greatest impact on COF, explaining the greatest variance in observed COF values (\u014b", "\u00a0=\u00a00.81). The most slip resistant groove combination was an oblique orientation, with 3\u00a0mm width and 2\u00a0mm depth. The least slip resistant groove combination was a parallel orientation, with a 6\u00a0mm width and 6\u00a0mm depth."]},
{"title": "Driver behaviour at roadworks", "highlights": ["Road networks around the world are reaching a critical stage in their lifecycle.", "Transport authorities are planning significant maintenance activities with associated roadworks and traffic management.", "Traffic microsimulation is used to plan these roadworks but modelled drivers are not behaving in the same way as real drivers.", "A range of psychological explanations for this difference are reviewed.", "Guidance for incorporating these psychological factors into future models is proposed."], "abstract": ["There is an incompatibility between how transport engineers think drivers behave in roadworks and how they actually behave. As a result of this incompatibility we are losing approximately a lane's worth of capacity in addition to those closed by the roadworks themselves. The problem would have little significance were it not for the fact a lane of motorway costs approx. \u00a330\u00a0m per mile to construct and \u00a343\u00a0k a year to maintain, and that many more roadworks are planned as infrastructure constructed 40 or 50 years previously reaches a critical stage in its lifecycle. Given current traffic volumes, and the sensitivity of road networks to congestion, the effects of roadworks need to be accurately assessed. To do this requires a new ergonomic approach. A large-scale observational study of real traffic conditions was used to identify the issues and impacts, which were then mapped to the ergonomic knowledge-base on driver behaviour, and combined to developed practical guidelines to help in modelling future roadworks scenarios with greater behavioural accuracy. Also stemming from the work are novel directions for the future ergonomic design of roadworks themselves."]},
{"title": "Impact of online training on delivering a difficult medical diagnosis: Acquiring communication skills", "highlights": ["Design and evaluation of a web-based self-training package.", "Web-based training for the acquisition of communication skills.", "Web-based training that enriches how physicians view diagnosis delivery.", "Interviews showed how physicians' mental picture of diagnosis delivery evolved.", "The exercises identify changes in practice and acquisition of new skills."], "abstract": ["This paper deals with developing and assessing the training of physicians to deliver a difficult diagnosis to patients. The training is provided by a web-based self-training package. This online training emphasizes the structural, functional and relational dimensions of interviews delivering a serious diagnosis, and a logical set of recommendations for behavior towards the patient. The content is illustrated by numerous delivery interview sequences that are described and for which commentary is provided. This online package was expected to enable physicians to acquire new skills and change their mental picture of diagnosis delivery. Here we discuss the assessment of training in managing the delivery of a serious diagnosis. The approach taken and the methods used to measure knowledge and skills are presented."]},
{"title": "Multi-parameter prediction of drivers' lane-changing behaviour with neural network model", "highlights": ["We conducted a lane change experiment under real road environment.", "Lane changing intent time window is about 5\u00a0s.", "Vehicle motion states, driving conditions and head movements information were chosen to predict lane changing behaviours.", "The improved neural network detects 85% of lane changes 1.5\u00a0s in advance."], "abstract": ["Accurate prediction of driving behaviour is essential for an active safety system to ensure driver safety. A model for predicting lane-changing behaviour is developed from the results of naturalistic on-road experiment for use in a lane-changing assistance system. Lane changing intent time window is determined via visual characteristics extraction of rearview mirrors. A prediction index system for left lane changes was constructed by considering drivers' visual search behaviours, vehicle operation behaviours, vehicle motion states, and driving conditions. A back-propagation neural network model was developed to predict lane-changing behaviour. The lane-change-intent time window is approximately 5\u00a0s long, depending on the subjects. The proposed model can accurately predict drivers' lane changing behaviour for at least 1.5\u00a0s in advance. The accuracy and time series characteristics of the model are superior to the use of turn signals in predicting lane-changing behaviour."]},
{"title": "Managing children's postural risk when using mobile technology at home: Challenges and strategies", "highlights": ["RULA was used to assess a child's postural risk when using mobile ICT in the home.", "Disparity between raters was found dependent on the side of the body assessed.", "RULA has limitations in screening unconventional postures commonly used by children.", "Most postures in the home were identified as requiring immediate or timely action.", "Risk management strategies for children and parents should use accessible language."], "abstract": ["Maintaining the musculoskeletal health of children using mobile information and communication technologies (ICT) at home presents a challenge. The physical environment influences postures during ICT use and can contribute to musculoskeletal complaints. Few studies have assessed postures of children using ICT in home environments. The present study investigated the Rapid Upper Limb Assessment (RULA) scores determined by 16 novice and 16 experienced raters. Each rater viewed 11 videotaped scenarios of a child using two types of mobile ICT at home. The Grand Scores and Action Levels determined by study participants were compared to those of an ergonomist experienced in postural assessment. All postures assessed were rated with an Action Level of 2 or above; representing a postural risk that required further investigation and/or intervention. The sensitivity of RULA to assess some of the unconventional postures adopted by children in the home is questioned."]},
{"title": "Ergonomics and comfort in lawn mower handle positioning: An evaluation of handle geometry", "highlights": ["User-centric handle position defined for a lawn mower control system (MCS).", "Required forces were higher with larger grip spans.", "Forces in a nontraditional position were lower than vertical and pistol positions.", "The MCS must reduce force requirements or grip spans for user health and comfort."], "abstract": ["Hand operation accompanied with any combination of large forces, awkward positions and repetition may lead to upper limb injury or illness and may be exacerbated by vibration. Commercial lawn mowers expose operators to these factors during actuation of hand controls and therefore may be a health concern. A nontraditional lawn mower control system may decrease upper limb illnesses and injuries through more neutral hand and body positioning. This study compared maximum grip strength in twelve different orientations (3 grip spans and 4 positions) and evaluated self-described comfortable handle positions. The results displayed force differences between nontraditional (X) and both vertical (V) and pistol (P) positions (p\u00a0<\u00a00.0001) and among the different grip spans (p\u00a0<\u00a00.0001). Based on these results, recommended designs should incorporate a tilt between 45 and 70\u00b0, handle rotations between 48 and 78\u00b0, and reduced force requirements or decreased grip spans to improve user health and comfort."]},
{"title": "Seat and seatbelt accommodation in fire apparatus: Anthropometric aspects", "highlights": ["New firefighter anthropometry data for fire truck seat and seatbelt design are presented.", "Seating space specifications are proposed to foster greater use of seatbelts.", "Adequate seatbelt web length is suggested for fire apparatus.", "Results are being used for updating national fire apparatus standards."], "abstract": ["This study developed anthropometric information on U.S. firefighters to guide fire-apparatus seat and seatbelt designs and future standards development. A stratified sample of 863 male and 88 female firefighters across the U.S. participated in the study. The study results suggested 498\u00a0mm in width, 404\u00a0mm in depth, and 365\u2013476\u00a0mm in height for seat pans; 429\u2013522\u00a0mm in width and 542\u00a0mm in height for seat back; 871\u00a0mm in height for head support; a seat space of 733\u00a0mm at shoulder and 678\u00a0mm at hip; and a knee/leg clearance of 909\u00a0mm in fire truck cab. Also, 1520\u00a0mm of lap belt web effective length and 2828\u00a0mm of lap-and-shoulder belt web effective length were suggested. These data for fire-truck seats and seatbelts provide a foundation for fire apparatus manufacturers and standards committees to improve firefighter seat designs and seatbelt usage compliance."]},
{"title": "Divide and rule: A qualitative analysis of the debriefing process in elite team sports", "highlights": ["We presented a model of the debriefing process used by coaches in elite team sports.", "Coaches were shown to divide the labor within the staff and team.", "Results showed that debriefing consisted of two steps: preparation and presentation.", "Coaches were transformational and transactional leaders."], "abstract": ["This article aimed to gain an understanding of the process of debriefing during major competitions in elite team sports. Debrief interviews were conducted with 9 head coaches. The interview data were used to identify how head coaches divided up the tasks given to staff and team members prior to, and during the post-match debriefing. Results showed that debriefing consisted of two steps: preparation and presentation. Preparation referred to four successive tasks. Presentation to the team of players consisted of eight tasks relating to transformational and transactional styles of leadership. Coaches were shown to divide the labor within the staff and team. The data tend to support the view that in elite team sports, coaches are both transformational and transactional leaders, adapting their style of leadership to the situation, athletes and time available. This study provides insights into the task-work and team-work underlying team functioning and division of labor."]},
{"title": "Effect of an auditory feedback substitution, tactilo-kinesthetic, or visual feedback on kinematics of pouring water from kettle into cup", "highlights": ["Motion capture provides auditory feedback during blindfolded water pouring.", "Motor learning is observed by reduced movement time, travel distance and jerk.", "Body kinematics are similar for auditory, tactile-kinesthetic or visual feedbacks.", "Auditory feedback in smart homes may prevent burn injuries of the visually-impaired."], "abstract": ["Pouring hot water from a kettle into a cup may prove a hazardous task, especially for the elderly or the visually-impaired. Individuals with deteriorating eyesight may endanger their hands by performing this task with both hands, relaying on tactilo-kinesthetic feedback (TKF). Auditory feedback (AF) may allow them to perform the task singlehandedly, thereby reducing the risk for injury. However since relying on an AF is not intuitive and requires practice, we aimed to determine if AF supplied during the task of pouring water can be used naturally as visual feedback (VF) following practice. For this purpose, we quantified, in young healthy sighted subjects (n\u00a0=\u00a020), the performance and kinematics of pouring water in the presence of three isolated feedbacks: visual, tactilo-kinesthetic, or auditory. There were no significant differences between the weights of spilled water in the AF condition compared to the TKF condition in the first, fifth or thirteenth trials. The subjectively-reported difficulty levels of using the TKF and the AF were significantly reduced between the first and thirteenth trials for both TKF (p\u00a0=\u00a00.01) and AF (p\u00a0=\u00a00.001). Trunk rotation during the first trial using the TKF was significantly lower than the trunk rotation while using VF. Also, shoulder adduction during the first trial using the TKF was significantly higher than the shoulder adduction while using the VF. During the AF trials, the median travel distance of the tip of the kettle was significantly reduced in the first trials so that in the thirtieth trial it did not differ significantly from the median travel distance during the thirtieth trial using TKF and VF. The maximal velocity of the tip of the kettle was constant for each of the feedback conditions but was higher in 10\u00a0cm\u00a0s", " using VF than TKF, which was higher in 10\u00a0cm\u00a0s", " from using AF. The smoothness of movement of the TKF and AF conditions, expressed by the normalized jerk score (NJSM), was one and two orders of magnitude higher from the VF, respectively. The median NJSM then decreased significantly by the fifth trial. Monitoring in-house activity via motion capture and classification of movements, i.e. liquid pouring, can assist with daily activities via AF. As a built-in feature in a smart home, this task-specific AF may prevent burn injuries of the visually-impaired."]},
{"title": "Applying riding-posture optimization on bicycle frame design", "highlights": ["The concept of \u201cfitting object to the body\u201d is designed in the bicycle.", "The important feature points of riding posture can be detected automatically.", "The dimensions of bicycle frame can be captured from the measurement data.", "The frame size table for common bicycle types is further proposed.", "The result of this study can be applied to bicycle design and development."], "abstract": ["Customization design is a trend for developing a bicycle in recent years. Thus, the comfort of riding a bike is an important factor that should be paid much attention to while developing a bicycle. From the viewpoint of ergonomics, the concept of \u201cfitting object to the human body\u201d is designed into the bicycle frame in this study. Firstly, the important feature points of riding posture were automatically detected by the image processing method. In the measurement process, the best riding posture was identified experimentally, thus the positions of feature points and joint angles of human body were obtained. Afterwards, according to the measurement data, three key points: the handlebar, the saddle and the crank center, were identified and applied to the frame design of various bicycle types. Lastly, this study further proposed a frame size table for common bicycle types, which is helpful for the designer to design a bicycle."]},
{"title": "Efficacy of a rubber outsole with a hybrid surface pattern for preventing slips on icy surfaces", "highlights": ["Hybrid rubber outsole was designed to provide high slip resistance without use of protruding studs or asperities.", "Slip resistance of hybrid rubber sole was examined on dry and wet icy surfaces.", "Drag tests and gait trials were performed on dry and wet icy surfaces.", "Hybrid rubber sole exhibited high SCOF and DCOF values on both icy surfaces.", "Hybrid rubber sole showed comparable slip resistance to anti-slip winter footwear devices."], "abstract": ["Conventional winter-safety footwear devices, such as crampons, can be effective in preventing slips on icy surfaces but the protruding studs can lead to other problems such as trips. A new hybrid (rough and smooth) rubber outsole was designed to provide high slip resistance without use of protruding studs or asperities. In the present study, we examined the slip resistance of the hybrid rubber outsole on both dry (\u221210\u00a0\u00b0C) and wet (0\u00a0\u00b0C) icy surfaces, in comparison to three conventional strap-on winter anti-slip devices: 1) metal coils (\u201cYaktrax Walker\u201d), 2) gritted (sandpaper-like) straps (\u201cRough Grip\u201d), and 3) crampons (\u201cAltagrips-Lite\u201d). Drag tests were performed to measure static (SCOF) and dynamic (DCOF) coefficients of friction, and gait trials were conducted on both level and sloped ice surfaces (16 participants). The drag-test results showed relatively high SCOF (\u22670.37) and DCOF (\u22670.31) values for the hybrid rubber sole, at both temperatures. The other three footwear types exhibited lower DCOF values (0.06\u20130.20) when compared with the hybrid rubber sole at 0\u00a0\u00b0C (", "\u00a0<\u00a00.01). Slips were more frequent when wearing the metal coils, in comparison to the other footwear types, when descending a slope at\u00a0\u221210\u00a0\u00b0C (6% of trials vs 0%; ", "\u00a0<\u00a00.05). There were no other significant footwear-related differences in slip frequency, distance or velocity. These results indicate that the slip-resistance of the hybrid rubber sole on icy surfaces was comparable to conventional anti-slip footwear devices. Given the likely advantages of the hybrid rubber sole (less susceptibility to tripping, better slip resistance on non-icy surfaces), this type of sole should contribute to a decrease in fall accidents; however, further research is needed to confirm its effectiveness under a wider range of test conditions."]},
{"title": "Thermal environment in eight low-energy and twelve conventional Finnish houses", "highlights": ["Perceived IEQ was as slightly better in the low-energy than conventional houses.", "The occupants want lower room temperatures than ISO 7730 optimal temperature.", "The occupants adjust room air temperatures to their liking in single house.", "Significant difference between the mean temperature and humidity was not found."], "abstract": ["We assessed the thermal environment of eight recently built low-energy houses and twelve conventional Finnish houses. We monitored living room, bedroom and outdoor air temperatures and room air relative humidity from June 2012 to September 2013. Perceived thermal environment was evaluated using a questionnaire survey during the heating, cooling and interim seasons. We compared the measured and perceived thermal environments of the low-energy and conventional houses. The mean air temperature was 22.8\u00a0\u00b0C (21.9\u201323.8\u00a0\u00b0C) in the low-energy houses, and 23.3\u00a0\u00b0C (21.4\u201326.5\u00a0\u00b0C) in the conventional houses during the summer (1. June 2013\u201331. August 2013). In the winter (1. December 2012\u201328. February 2013), the mean air temperature was 21.3\u00a0\u00b0C (19.8\u201322.5\u00a0\u00b0C) in the low-energy houses, and 21.6\u00a0\u00b0C (18.1\u201326.4\u00a0\u00b0C) in the conventional houses. The variation of the air temperature was less in the low-energy houses than that in the conventional houses. In addition, the occupants were on average slightly more satisfied with the indoor environment in the low-energy houses. However, there was no statistically significant difference between the mean air temperature and relative humidity of the low-energy and conventional houses. Our measurements and surveys showed that a good thermal environment can be achieved in both types of houses."]},
{"title": "Ergonomic evaluation of the operating characteristics of the 6MF-30 portable pneumatic extinguisher", "highlights": ["We measured HR, EMG and the degree of fatigue of 14 subjects operating the 6MF-30.", "The effects of carrying posture on HR and EMG data were significant.", "The effects of carrying posture on the degree of fatigue were significant.", "The strap of the 6MF-30 lessens physical stress.", "The optimal carrying posture adopted when using 6MF-30 includes the oblique strap."], "abstract": ["The 6MF-30 portable pneumatic extinguisher, which is one of the most widely used pieces of equipment for fighting forest fires in China, can produce great physical discomfort for the wearer. To mitigate the physical discomfort associated with the use of the 6MF-30, the operating characteristics of this machine were ergonomically evaluated. Fourteen subjects were instructed to operate the 6MF-30 portable pneumatic extinguisher using three different carrying postures (oblique strap, vertical strap and no strap) two different motions (stationary and swinging) during a simulated firefighting task. Dependent measures included heart rate (HR), electromyography (EMG) data and a subjective assessment (measured as the degree of fatigue in the left arm, right arm and waist). The EMG data were acquired from the ", " and the ", " of the left arm of each subject. Variance analysis indicated that the effects of the carrying posture on the HR (p\u00a0<\u00a00.001), the EMG data of the left arm (the p value of the ", " is 0.001 and the p value of the ", " is 0.015), and the degree of fatigue of the left and right arms (p\u00a0<\u00a00.001) were significant, while the effects of motion on all of the dependent measures, and the effects of carrying posture on the degree of fatigue of the waist were not significant. The effect of an oblique strap on the whole-body load is minimal, and the use of the equipment without a strap produced significantly greater physical discomfort for the wearer than did the oblique strap and the vertical strap. The results suggest that the strap of the 6MF-30 can help Chinese forest firefighters to lessen physical stress when operating the 6MF-30, and the use of the oblique strap should be adopted as the standard position."]},
{"title": "An ergonomics based design research method for the arrangement of\u00a0helicopter flight instrument panels", "highlights": ["Integrated value of a display is better understood by users than the degree of importance and degree of frequency of use.", "UCIAM\u2013", " is a new technique for user-interface arrangement.", "Locational Value of a Display, Sub-Areas of the Display Panel are new concepts to analyze paper prototypes.", "Functional Grouping of Displays is an intuitive concept for expert users.", "Sequence of use is not applicable in the context of display reading."], "abstract": ["In this paper, we study the arrangement of displays in flight instrument panels of multi-purpose civil helicopters following a user-centered design method based on ergonomics principles. Our methodology can also be described as a user-interface arrangement methodology based on user opinions and preferences. This study can be outlined as gathering user-centered data using two different research methods and then analyzing and integrating the collected data to come up with an optimal instrument panel design. An interview with helicopter pilots formed the first step of our research. In that interview, pilots were asked to provide a quantitative evaluation of basic interface arrangement principles. In the second phase of the research, a paper prototyping study was conducted with same pilots. The final phase of the study entailed synthesizing the findings from interviews and observational studies to formulate an optimal flight instrument arrangement methodology. The primary results that we present in our paper are the methodology that we developed and three new interface arrangement concepts, namely ", ", ", " and ", ". An optimum instrument panel arrangement is also proposed by the researchers."]},
{"title": "The effectiveness of using pictures in teaching young children about burn injury accidents", "highlights": ["Preventing burns injuries can reduce home accidents for children under 6 in Taiwan.", "Sequence diagrams are an image type appropriate to children's cognitive ability.", "Burn stories using sequence diagrams aid a child's memory of burn injury severity."], "abstract": ["This study utilized the \u201cstory grammar\u201d approach (Stein and Glenn, 1979) to analyze the within-corpus differences in recounting of sixty 6- and 7-year-old children, specifically whether illustrations (5-factor accident sequence) were or were not resorted to as a means to assist their narration of a home accident in which a child received a burn injury from hot soup. Our investigation revealed that the message presentation strategy \u201ccombining oral and pictures\u201d better helped young children to memorize the story content (sequence of events leading to the burn injury) than \u201coral only.\u201d Specifically, the content of \u201cthe dangerous objects that caused the injury\u201d, \u201cthe unsafe actions that people involved took\u201d, and \u201chow the people involved felt about the severity of the accident\u201d differed significantly between the two groups."]},
{"title": "Modeling people with motor disabilities to empower the automatic accessibility and ergonomic assessment of new products", "highlights": ["We model motor disabilities of people through regression analysis.", "We propose a novel hybrid regression method able to handle small sample sizes.", "We estimate the probability density function of each disability parameter.", "We examine the validity of the regression results using new measurements.", "The proposed analysis leads to the development of accurate virtual user models."], "abstract": ["Virtual User Models (VUMs) can be a valuable tool for accessibility and ergonomic evaluation of designs in simulation environments. As increasing the accessibility of a design is usually translated into additional costs and increased development time, the need for specifying the percentage of population for which the design will be accessible is crucial. This paper addresses the development of VUMs representing specific groups of people with disabilities. In order to create such VUMs, we need to know the functional limitations, i.e. disability parameters, caused by each disability and their variability over the population. Measurements were obtained from 90 subjects with motor disabilities and were analyzed using both parametric and nonparametric regression methods as well as a proposed hybrid regression method able to handle small sample sizes. Validation results showed that in most cases the proposed regression analysis can produce valid estimations on the variability of each disability parameter."]},
{"title": "Association of individual and work-related risk factors with musculoskeletal symptoms among Iranian sewing machine operators", "highlights": ["Prevalence and severity of musculoskeletal symptoms (MSDs) was high among operators.", "The relatively high RULA scores highlighted a poor sewing workstation design.", "Both physical and psychosocial job factors were associated with MSDs."], "abstract": ["This cross-sectional study evaluated working conditions and the occurrence of self-reported musculoskeletal symptoms among 251 Iranian sewing machine operators. A questionnaire and direct observations of working postures using the rapid upper limb assessment (RULA) method were used. A high prevalence of musculoskeletal symptoms, particularly in the neck/shoulders, back and hands/wrists were found. The mean RULA grand score of 5.7 highlighted a poor sewing workstation design and indicated that most operators (with posture assessed at action level 3) needed an investigation and changes in their working habits soon. Work-related factors (including number of years worked as an operator, prolonged working hours per shift, long duration of sitting work without a break, feeling pressure due to work and working postures) and individual factors (including age, gender, BMI and regular sport/physical activities) were associated with musculoskeletal symptoms in multiple logistic regression models. The findings add to the understanding of working conditions of those jobs involving sewing activities and emphasise the need for ergonomic interventions to reduce musculoskeletal symptoms in the future."]},
{"title": "The inter-rater reliability of Strain Index and OCRA Checklist task assessments in cheese processing", "highlights": ["Inter-rater reliability was good to excellent for OCRA Checklist risk assessments.", "Inter-rater reliability was moderate to good for Strain Index risk assessments.", "Individual assessment task variables were less reliable than risk indexes.", "The SI and OCRA Checklist are reliable methods for practitioners and researchers."], "abstract": ["The purpose of this study was to characterize the inter-rater reliability of two physical exposure assessment methods of the upper extremity, the Strain Index (SI) and Occupational Repetitive Actions (OCRA) Checklist. These methods are commonly used in occupational health studies and by occupational health practitioners. Seven raters used the SI and OCRA Checklist to assess task-level physical exposures to the upper extremity of workers performing 21 cheese manufacturing tasks. Inter-rater reliability was characterized using a single-measure, agreement-based intraclass correlation coefficient (ICC). Inter-rater reliability of SI assessments was moderate to good (ICC\u00a0=\u00a00.59, 95% CI: 0.45\u20130.73), a similar finding to prior studies. Inter-rater reliability of OCRA Checklist assessments was excellent (ICC\u00a0=\u00a00.80, 95% CI: 0.70\u20130.89). Task complexity had a small, but non-significant, effect on inter-rater reliability SI and OCRA Checklist scores. Both the SI and OCRA Checklist assessments possess adequate inter-rater reliability for the purposes of occupational health research and practice. The OCRA Checklist inter-rater reliability scores were among the highest reported in the literature for semi-quantitative physical exposure assessment tools of the upper extremity. The OCRA Checklist however, required more training time and time to conduct the risk assessments compared to the SI."]},
{"title": "Work demands and health consequences of organizational and technological measures introduced to enhance the quality of home care services \u2013 A subgroup analysis", "highlights": ["Production system rationalization generates diverse worker responses.", "Management should be aware of diversity, stimulate worker feedback and implement individually-based corrective actions.", "Age and perception of tension were early indicators of worker problems."], "abstract": ["This study of home care workers in a Norwegian municipality aimed to examine the effect of two measures involving organizational (job checklists) and technological (personal digital assistants) job aids on perceived work demands and musculoskeletal health. Questionnaire data was collected in 2009 (n\u00a0=\u00a0138, response rate 76.2%) and 2011 (n\u00a0=\u00a080, response rate 54%). Forty-six home care workers responded at both waves. Respondents were assigned into \u2018high\u2019, \u2018moderate\u2019 and \u2018low\u2019 strain groups based on their responses to open and closed survey questions regarding impact of the two measures. One-way ANOVA with post-hoc t-tests and regression analyses investigated group differences and examined development in variables. Perceived work demands and health effects over the two-year study period were unchanged overall, yet significant differences between subgroups were highlighted. Work demands and shoulder-neck pain remained high for high-strain workers, but were reduced for low and moderate strain workers. Management should be aware of diversity in worker responses to rationalizations and give priority to supplementary, targeted measures to counteract adverse effects."]},
{"title": "Job hindrances, job resources, and safety performance: The mediating role of job engagement", "highlights": ["We examine job engagement as a mediator in the relationships between job characteristics and safety performance.", "Job resources had positive effects on safety performance directly and indirectly through job engagement.", "Job insecurity was negatively related to compliance whereas role overload was positively related to participation.", "Employee engagement can translate into better safety performance."], "abstract": ["Job engagement has received widespread attention in organizational research but has rarely been empirically investigated in the context of safety. In the present study, we examined the mediating role of job engagement in the relationships between job characteristics and safety performance using self-reported data collected at a coal mining company in China. Most of our study hypotheses were supported. Job engagement partially mediated the relationships between job resources and safety performance dimensions. Theoretical and practical implications and directions for future research are also discussed."]},
{"title": "Evaluating change in user error when using ruggedized handheld devices", "highlights": ["There are no significant differences between user error and age.", "Lack of corrective software may not impact user error as much as expected.", "Keypad devices had more character errors while touchscreen devices had more word."], "abstract": ["The increasing number of handheld mobile devices used today and the increasing dependency on them in the workplace makes understanding how users interact with these devices critical. This study seeks to understand how user error changes based on user age as well as input content type on ruggedized handheld devices. Participants completed data entry tasks of word and character input on two different devices, a physical keypad and touchscreen device. The number of errors and types of error, corrected and permanent were collected for each participant. Based on results on the study, touchscreen devices proved to be the optimal ruggedized handheld device to minimize user error."]},
{"title": "Objective classification of performance in the use of a piercing saw in jewellery making", "highlights": ["Objective measures of tool use defined.", "Data collected from sensors fitted to tool used ", "Performance classified in terms of control, regulatory and functional parameters.", "Methods and analysis for the study of skilled tool use developed and illustrated."], "abstract": ["Data from 15 jewellery students, in their 1st and 3rd years of training, were analysed to show how data collected from work settings can be used to objectively evaluate performance in the use of tools. Participants were asked to use a piercing saw to cut 5 lines in a piece of metal. Performance was categorised in terms of functional dynamics. Data from strain gauges and a tri-axial accelerometer (built into the handle of the saw) were recorded and thirteen metrics derived from these data. The key question for this paper is which metrics could be used to distinguish levels of ability. Principal Components Analysis identified five components: sawing action; grasp of handle; task completion time; lateral deviation of strokes; and quality of lines cut. Using representative metrics for these components, participants could be ranked in terms of performance (low, medium, high) and statistical analysis showed significant differences between participants on key metrics."]},
{"title": "Global drivers, sustainable manufacturing and systems ergonomics", "highlights": ["Concentrates on the manufacturing sector, though findings apply to other domains such as smart cities.", "Outlines role of cyber-physical systems in sustainability, and the role of systems ergonomics in these systems.", "Develops knowledge extensions to enable systems ergonomists to work in CPS.", "Ends with an action plan to disseminate these knowledge extensions."], "abstract": ["This paper briefly explores the expected impact of the \u2018Global Drivers\u2019 (such as population demographics, food security; energy security; community security and safety), and the role of sustainability engineering in mitigating the potential effects of these Global Drivers. The message of the paper is that sustainability requires a significant input from Ergonomics/Human Factors, but the profession needs some expansion in its thinking in order to make this contribution.", "Creating a future sustainable world in which people experience an acceptable way of life will not happen without a large input from manufacturing industry into all the Global Drivers, both in delivering products that meet sustainability criteria (such as durability, reliability, minimised material requirement and low energy consumption), and in developing sustainable processes to deliver products for sustainability (such as minimum waste, minimum emissions and low energy consumption). Appropriate changes are already being implemented in manufacturing industry, including new business models, new jobs and new skills.", "Considerable high-level planning around the world is in progress and is bringing about these changes; for example, there is the US \u2018Advanced Manufacturing National Program\u2019 (AMNP)\u2019, the German \u2018Industrie 4.0\u2019 plan, the French plan \u2018la nouvelle France industrielle\u2019 and the UK Foresight publications on the \u2018Future of Manufacturing\u2019.", "All of these activities recognise the central part that humans will continue to play in the new manufacturing paradigms; however, they do not discuss many of the issues that systems ergonomics professionals acknowledge. This paper discusses a number of these issues, highlighting the need for some new thinking and knowledge capture by systems ergonomics professionals. Among these are ethical issues, job content and skills issues.", "Towards the end, there is a summary of knowledge extensions considered necessary in order that systems ergonomists can be fully effective in this new environment, together with suggestions for the means to acquire and disseminate the knowledge extensions."]},
{"title": "Development of safety incident coding systems through improving coding reliability", "highlights": ["Factors affecting incident coding system reliability are tested and analysed.", "Domain-specific coding systems are more reliable than generic, multi-domain systems.", "Systems with abstract and/or loaded terms affected coding reliability negatively.", "Coder profession, rather than training provided, influenced coding reliability.", "System improvement and design recommendations are provided based on these results."], "abstract": ["This paper reviews classification theory sources to develop five research questions concerning factors associated with incident coding system development and use and how these factors affect coding reliability. Firstly, a method was developed to enable the comparison of reliability results obtained using different methods. Second, a statistical and qualitative review of reliability studies was conducted to investigate the influence of the identified factors on the reliability of incident coding systems. As a result several factors were found to have a statistically significant effect on reliability. Four recommendations for system development and use are provided to assist researchers in improving the reliability of incident coding systems in high hazard industries."]},
{"title": "Prevention of musculoskeletal disorders within management systems: A scoping review of practices, approaches, and techniques", "highlights": ["There was little information on the integration of MSD prevention into management systems in the peer-reviewed literature.", "Incorporating MSD prevention into organizational level approaches could improve production in addition to preserving workers' health in workplaces.", "MSD hazard assessment and risk prevention are partially outside of the main management processes.", "Bringing ergonomics as a means of preventing MSD into organizations' management systems appears to be highly desirable."], "abstract": ["The purpose of this study was to identify and summarize the current research evidence on approaches to preventing musculoskeletal disorders (MSD) within Occupational Health and Safety Management Systems (OHSMS). Databases in business, engineering, and health and safety were searched and 718 potentially relevant publications were identified and examined for their relevance. Twenty-one papers met the selection criteria and were subjected to thematic analysis. There was very little literature describing the integration of MSD risk assessment and prevention into management systems. This lack of information may isolate MSD prevention, leading to difficulties in preventing these disorders at an organizational level. The findings of this review argue for further research to integrate MSD prevention into management systems and to evaluate the effectiveness of the approach."]},
{"title": "Anthropometric study of farm workers on Java Island, Indonesia, and\u00a0its implications for the design of farm tools and equipment", "highlights": ["We collected anthropometric data of farm-workers from 3 diverse regions on Java Island.", "The anthropometry was statistically different among regional groups and gender.", "We compared the anthropometry to the groups of people from eight other nations.", "We analysed the suitability of farm tools design to the anthropometry of farmers.", "Ergonomic consideration may improve the tools' design to suit them to the users."], "abstract": ["Anthropometric data are a prerequisite for designing agricultural tools and equipment that enable workers to achieve better performance and productivity while providing better safety and comfort. A set of thirty anthropometric dimensions was collected from a total sample of 371 male and female farm-workers from three different regions (west, central and east) of Java Island, Indonesia. The mean stature is 162.0\u00a0cm and 152.5\u00a0cm, the sitting height is 82.9\u00a0cm and 77.4\u00a0cm, and the body weight is 57.1\u00a0kg and 52.3\u00a0kg for male and female subjects, respectively. The index of relative sitting height (RSH) was 0.51 on average for both male and female subjects. Significant differences are found in most of the anthropometric dimensions between gender and regional data groups as well. Compared with groups of people from several other countries, the anthropometric dimensions of Indonesian people are quite similar to Indian people, but are relatively smaller than Filipino, Chinese, Japanese, British, and American people. An attempt was conducted to illustrate the use of this anthropometric database and ergonomic considerations in refining the design of traditional tools and equipment commonly in use for rice farming operations."]},
{"title": "A better way of fitting clips? A comparative study with respect to physical workload", "highlights": ["The study compared muscular activity and external force for 4 clip fitting methods.", "Muscle activity of the dominant limb was lower when using the unpowered tools.", "Non-dominant limb muscle activity was lower with bare-hand or powered-tool fitting.", "However, fitting clips with the bare hand required a higher external force.", "Thus, measuring the external force alone may give rise to an erroneous interpretation."], "abstract": ["The clip fitting task is a frequently encountered assembly operation in the car industry. It can cause upper limb pain. During task laboratory simulations, upper limb muscular activity and external force were compared for 4 clip fitting methods: with the bare hand, with an unpowered tool commonly used at a company and with unpowered and powered prototype tools. None of the 4 fitting methods studied induced a lower overall workload than the other three. Muscle activity was lower at the dominant limb when using the unpowered tools and at the non-dominant limb with the bare hand or with the powered tool. Fitting clips with the bare hand required a higher external force than fitting with the three tools. Evaluation of physical workload was different depending on whether external force or muscle activity results were considered. Measuring external force only, as recommended in several standards, is insufficient for evaluating physical workload."]},
{"title": "When is job rotation perceived useful and easy to use to prevent work-related musculoskeletal complaints?", "highlights": ["We interview employers and workers from the construction industry.", "We examine facilitators and barriers to usefulness and ease of use of job rotation.", "Job and organizational factors influence usefulness and ease of use of job rotation.", "Individual factors influence usefulness and ease of use of job rotation."], "abstract": ["Job rotation is often recommended to optimize physical work demands and prevent work-related musculoskeletal complaints, but little is known about possible facilitators and barriers to its usefulness and ease of use. Following a qualitative research design, semi-structured interviews with employers (n\u00a0=\u00a012) and workers (n\u00a0=\u00a011) from the construction industry were conducted. Organizational climate, job autonomy, job characteristics and work processes were mentioned as either facilitators or barriers on an organizational level. Worker characteristics, work behavior and attitude were mentioned as either facilitators or barriers on an individual level. Following a structured approach to assess usefulness of job rotation to optimize physical work exposures and identifying barriers to usefulness and ease of use in relevant stakeholder groups is necessary in order to select or develop strategies to overcome these barriers, or to reject job rotation as a useful or easy to use intervention in the given context."]},
{"title": "Virtual Sliding QWERTY: A new text entry method for smartwatches using Tap-N-Drag", "highlights": ["We developed Virtual Sliding QWERTY (VSQ) for enhancing text entry tasks with smartwatches.", "For the verification of VSQ, we also conducted usability tests.", "As a result, VSQ showed an average of 11.9 words per minute which was higher than previous studies."], "abstract": ["A smaller screen of smartwatches compare to conventional mobile devices such as PDAs and smartphones is one of the main factors that makes users to input texts difficult. However, several studies have only proposed a concept for entering texts for smartwatches without usability tests while other studies showed low text input performance. In this study, we proposed a new text entry method called Virtual Sliding QWERTY (VSQ) which utilizes a virtual qwerty-layout keyboard and a \u2018Tap-N-Drag\u2019 method to move the keyboard to the desired position. In addition, to verify VSQ we conducted a usability test with 20 participants for a combination of 5 key sizes and 4 CD-gains. As a result, VSQ achieved an average of 11.9 Words per Minute which was higher than previous studies. In particular, VSQ at 5\u00a0\u00d7\u00a05 key size and 2\u00d7, or 3\u00d7 CD-gain had the highest performance in terms of the quantitative and qualitative usability test."]},
{"title": "Schoolbag carriage and schoolbag-related musculoskeletal discomfort among primary school children", "highlights": ["A novel approach was used to investigate schoolbag-related musculoskeletal discomfort.", "Schoolbag weight generally exceeded the 10% body weight guideline.", "The prevalence of baseline musculoskeletal discomfort was high.", "Schoolbag-related shoulder discomfort was more prevalent than back discomfort.", "Risk factors for site-specific schoolbag-related musculoskeletal discomfort were identified."], "abstract": ["Schoolbag carriage is a common occurrence and has been associated with musculoskeletal discomfort in children. The current study investigated the relationship between schoolbag-related musculoskeletal discomfort and individual, physical and psychosocial risk factors in primary school children in Ireland. A cross-sectional survey and pretest\u2013posttest quasi-experimental design was used. The site and intensity of musculoskeletal discomfort was assessed before and after schoolbag carriage to provide a dose-response assessment of schoolbag-related discomfort for the first time. Objective measurements of the children, schoolbags and other additional items were made, and a researcher assisted questionnaire was completed on arrival at school. A total of 529 children (male 55.8%: female 44.2%) with a mean age of 10.6 years\u00a0\u00b1\u00a07.14 months were included. The majority had backpacks (93.8%) and 89.7% (n\u00a0=\u00a0445) carried the backpack over 2 shoulders. The mean schoolbag weight (4.8\u00a0\u00b1\u00a01.47\u00a0kgs) represented a mean % body weight (%BW) of 12.6\u00a0\u00b1\u00a04.29%. Only 29.9% carried schoolbags that were \u226410%BW. A significantly greater proportion of normal weight children carried schoolbags that were >10%BW compared to overweight/obese children (p\u00a0<\u00a00.001). The mean %BW carried was 18.3\u00a0\u00b1\u00a05.03 for those who had an additional item. The majority (77.5%) carried schoolbags to school for \u226410\u00a0min. The prevalence of baseline musculoskeletal discomfort was high (63.4%). Schoolbag-related discomfort was reported more frequently in the shoulders (27.3%) than in the back (15%). The dose\u2013response assessment indicated that both statistically and meaningfully significant increases in discomfort were observed following schoolbag carriage. Multiple logistic regression models indicated that psychosocial factors and a history of discomfort were predictors of schoolbag-related back discomfort, while gender (being female) and a history of discomfort were predictors of schoolbag-related shoulder discomfort. None of the physical factors (absolute/relative schoolbag weight, carrying an additional item, duration of carriage, method of travel to school) were associated with schoolbag-related discomfort. This study highlights the need to consider the multi-factorial nature of schoolbag-related discomfort in children, and also the need to identify background pain as its presence can inadvertently influence the reporting of 'schoolbag-related' discomfort if it is not accounted for."]},
{"title": "Analysis of foot clearance in firefighters during ascent and descent of\u00a0stairs", "highlights": ["Following firefighting, increased risk of falls may exist while traversing stairs.", "Foot clearance decreased during ascent and increased during descent after activity.", "Medial-lateral asymmetric load carriage significantly affected stair clearance."], "abstract": ["Slips, trips, and falls are a leading cause of injury to firefighters with many injuries occurring while traversing stairs, possibly exaggerated by acute fatigue from firefighting activities and/or asymmetric load carriage. This study examined the effects that fatigue, induced by simulated firefighting activities, and hose load carriage have on foot clearance while traversing stairs. Landing and passing foot clearances for each stair during ascent and descent of a short staircase were investigated. Clearances decreased significantly (", "\u00a0<\u00a00.05) post-exercise for nine of 12 ascent parameters and increased for two of eight descent parameters. Load carriage resulted in significantly decreased (", "\u00a0<\u00a00.05) clearance over three ascent parameters, and one increase during descent. Decreased clearances during ascent caused by fatigue or load carriage may result in an increased trip risk. Increased clearances during descent may suggest use of a compensation strategy to ensure stair clearance or an increased risk of over-stepping during descent."]},
{"title": "A coupling system to predict the core and skin temperatures of human wearing protective clothing in hot environments", "highlights": ["A coupling system to predict the human core and skin temperatures was developed.", "Experiments were conducted in hot environment to get human temperatures.", "Comparison between the predicted results and experimental data was conducted.", "The coupling system has shown accurate compared with the experimental results."], "abstract": ["The aim of this study is to predict the core and skin temperatures of human wearing protective clothing in hot environments using the coupling system. The coupling system consisted of a sweating manikin Newton controlled by a multi-node human thermal model, and responded dynamically to the thermal environment as human body. Validation of the coupling system results was conducted by comparison with the subject tests. Five healthy men wearing protective clothing were exposed to the thermal neutral and high temperature environments. The skin temperatures of seven body segments and the rectal temperatures were recorded continuously. The predictions of core temperatures made by the coupling system showed good agreement with the experimental data, with maximum difference of 0.19\u00a0\u00b0C and RMSD of 0.12\u00a0\u00b0C. The predicted mean skin temperatures fell outside of the 95% CI for most points, whereas the difference between the simulated results and measured data was no more than 1\u00a0\u00b0C which is acceptable. The coupling system predicted the local skin temperatures reasonably with the maximum local skin temperature of 1.30\u00a0\u00b0C. The coupling system has been validated and exhibited reasonable accuracy compared with the experimental results."]},
{"title": "A systematic review of mixed methods research on human factors and ergonomics in health care", "highlights": ["Mixed methods research is increasingly used in healthcare HFE research.", "About two-thirds of mixed methods studies on healthcare HFE use the convergent parallel design.", "A large variety of methods is used to collect qualitative and quantitative data.", "The most frequent combination of qualitative and quantitative data collection involved interview and survey respectively.", "Formal mixed methods research approaches should be used in healthcare HFE research."], "abstract": ["This systematic literature review provides information on the use of mixed methods research in human factors and ergonomics (HFE) research in health care. Using the PRISMA methodology, we searched four databases (PubMed, PsycInfo, Web of Science, and Engineering Village) for studies that met the following inclusion criteria: (1) field study in health care, (2) mixing of qualitative and quantitative data, (3) HFE issues, and (4) empirical evidence. Using an iterative and collaborative process supported by a structured data collection form, the six authors identified a total of 58 studies that primarily address HFE issues in health information technology (e.g., usability) and in the work of healthcare workers. About two-thirds of the mixed methods studies used the convergent parallel study design where quantitative and qualitative data were collected simultaneously. A variety of methods were used for collecting data, including interview, survey and observation. The most frequent combination involved interview for qualitative data and survey for quantitative data. The use of mixed methods in healthcare HFE research has increased over time. However, increasing attention should be paid to the formal literature on mixed methods research to enhance the depth and breadth of this research."]},
{"title": "Participatory ergonomics simulation of hospital work systems: The influence of simulation media on simulation outcome", "highlights": ["Simulation media fidelity and affordance impact ergonomics evaluation.", "Difference in fidelity and affordance show difference in simulation outcomes.", "Full-scale mock-ups enable evaluation of workspace and technologies & tools.", "Table-top models enable evaluation of work organization."], "abstract": ["Current application of work system simulation in participatory ergonomics (PE) design includes a variety of different simulation media. However, the actual influence of the media attributes on the simulation outcome has received less attention. This study investigates two simulation media: full-scale mock-ups and table-top models. The aim is to compare, how the media attributes of ", " and ", " influence the ergonomics identification and evaluation in PE design of hospital work systems. The results illustrate, how the full-scale mock-ups\u2019 high fidelity of room layout and affordance of tool operation support ergonomics identification and evaluation related to the work system entities ", " and ", ". The table-top models\u2019 high fidelity of function relations and affordance of a helicopter view support ergonomics identification and evaluation related to the entity ", ". Furthermore, the study addresses the form of the identified and evaluated conditions, being either identified challenges or tangible design criteria."]},
{"title": "Lower limb flexion posture relates to energy absorption during drop landings with soldier-relevant body borne loads", "highlights": ["When landing with body borne load, greater hip and knee flexion may promote knee energy absorption.", "Landing with body borne load increased the demand placed on the lower extremity.", "Energy absorption by the lower limb did not increase when landing with body borne load.", "Participants relied upon extended hip and knee angle when landing with load."], "abstract": ["Fifteen military personnel performed 30-cm drop landings to quantify how body borne load (light, \u223c6\u00a0kg, medium, \u223c20\u00a0kg, and heavy, \u223c40\u00a0kg) impacts lower limb kinematics and knee joint energy absorption during landing, and determine whether greater lower limb flexion increases energy absorption while landing with load. Participants decreased peak hip (", "\u00a0=\u00a00.002), and knee flexion (", "\u00a0=\u00a00.007) posture, but did not increase hip (", "\u00a0=\u00a00.796), knee (", "\u00a0=\u00a00.427) or ankle (", "\u00a0=\u00a00.161) energy absorption, despite exhibiting greater peak hip (", "\u00a0=\u00a00.003) and knee (", "\u00a0=\u00a00.001) flexion, and ankle (", "\u00a0=\u00a00.003) dorsiflexion angular impulse when landing with additional load. Yet, when landing with the light and medium loads, greater hip (R", "\u00a0=\u00a00.500, ", "\u00a0=\u00a00.003 and R", "\u00a0=\u00a00.314, ", "\u00a0=\u00a00.030) and knee (R", "\u00a0=\u00a00.431, ", "\u00a0=\u00a00.008 and R", "\u00a0=\u00a00.342, ", "\u00a0=\u00a00.022) flexion posture predicted larger knee joint energy absorption. Thus, military training that promotes hip and knee flexion, and subsequently greater energy absorption during landing, may potentially reduce risk of musculoskeletal injury and optimize soldier performance."]},
{"title": "Interruptions in the wild: Development of a sociotechnical systems model of interruptions in the emergency department through a systematic review", "highlights": ["A review and synthesis of the literature on interruptions in the ED was conducted.", "Gaps were identified in conceptualizing sociotechnical system factors of interruptions.", "Although interruption outcomes were noted, few were measured.", "A sociotechnical model of interruptions in complex settings is presented."], "abstract": ["Interruptions are unavoidable in the \u201cinterrupt driven\u201d Emergency Department (ED). A critical review and synthesis of the literature on interruptions in the ED can offer insight into the nature of interruptions in complex real-world environments. Fifteen empirical articles on interruptions in the ED were identified through database searches. Articles were reviewed, critiqued, and synthesized. There was little agreement and several gaps in conceptualizing sociotechnical system factors, process characteristics, and interruption outcomes. While multiple outcomes of interruptions were mentioned, few were measured, and the relationship between multiple outcomes was rarely assessed. Synthesizing the literature and drawing on ergonomic concepts, we present a sociotechnical model of interruptions in complex settings that motivates new directions in research and design. The model conceptualizes interruptions as a process, not a single event, that occurs within and is shaped by an interacting socio-technical system and that results in a variety of interrelated outcomes."]},
{"title": "A comparison of linear and logarithmic auditory tones in pulse oximeters", "highlights": ["Pulse oximeters are used throughout medicine but pitch-to-value mappings are not consistent.", "Logarithmic mappings led to both more accurate estimations of saturation, and more accurate estimations of saturation rate change.", "Use of logarithmic tone scales, for example a semitone scale, could be used more extensively in practice."], "abstract": ["This study compared the ability of forty anaesthetists to judge absolute levels of oxygen saturation, direction of change, and size of change in saturation using auditory pitch and pitch difference in two laboratory-based studies that compared a linear pitch scale with a logarithmic scale. In the former the differences in saturation become perceptually closer as the oxygenation level becomes higher whereas in the latter the pitch differences are perceptually equivalent across the whole range of values. The results show that anaesthetist participants produce significantly more accurate judgements of both absolute oxygenation values and size of oxygenation level difference when a logarithmic, rather than a linear, scale is used. The line of best fit for the logarithmic function was also closer to x\u00a0=\u00a0y than for the linear function. The results of these studies can inform the development and standardisation of pulse oximetry tones in order to improve patient safety."]},
{"title": "Effects of two ergonomic improvements in brazing coils of air-handler units", "highlights": ["We address physically loading, quality and productivity problems in coil brazing.", "Intervention I uses twin brazing torch (TBT) to replace the single brazing gun.", "Intervention II uses TBT in sitting position.", "Results: 58.9% quality and 140% productivity improvements and 113 times ROI.", "Include reductions in bending of left and right wrists, neck and back."], "abstract": ["The research aims to address the physically loading task and quality and productivity problems in the brazing of coils of air-handler units. Eight operators participated in two intervention studies conducted in a factory in Malaysia to compare the status quo brazing with (1) the use of a new twin-brazing torch that replaced the single-brazing gun and (2) brazing in a sitting position. The outcome measures are related to quality, productivity, monetary costs, body postures and symptoms. After baseline, Interventions I and II were applied for 3 months respectively. The results show a 58.9% quality improvement, 140% productivity increase and 113 times ROI. There was also a reduction in poor work postures e.g. in the raising of the arms and shoulders; bending, twisting and extending of the neck; and bending of left and right wrists, and the back. This research can be replicated in other factories that share similar processes."]},
{"title": "Tallman lettering as a strategy for differentiation in look-alike, sound-alike drug names: The role of familiarity in differentiating drug\u00a0doppelgangers", "highlights": ["Tallman lettering resulted in quicker detection of confusable pairs.", "Tallman lettering resulted in more frequent detection of confusable pairs.", "Were more pronounced for healthcare providers.", "Prior familiarity discussed within an attentional allocation framework."], "abstract": ["Tallman lettering, capitalizing the dissimilar portions of easily confused drug names, is one strategy for reducing medication errors. We assessed the efficacy of Tallman lettering in a visually complex environment using a change detection method with healthcare providers and laypeople. In addition, the effect of familiarity with the drug name was assessed using a subset of responses collected from healthcare providers.", "Both healthcare providers and laypeople detected changes in confusable pairs of drug names more often (P\u00a0<\u00a00.0001) and more quickly (P\u00a0<\u00a00.05) when changes were presented in Tallman lettering, though the benefits were more pronounced for healthcare providers (p\u00a0<\u00a00.05). Familiarity with both drug names in a confusable pair mitigated the benefit of Tallman lettering. Results are discussed in terms of bottom-up and top-down attentional systems for processing of information in the context of the varied healthcare environments."]},
{"title": "Detailed assessment of low-back loads may not be worth the effort: A\u00a0comparison of two methods for exposure-outcome assessment of low-back pain", "highlights": ["We assessed the effects of differing measurement accuracy on exposure estimates.", "Also the effect on associations with low-back pain was assessed.", "Inaccuracy can be unacceptably large at the individual level.", "Collecting more accurate data does not necessarily lead to higher predictive values.", "This was especially the case for cumulative loads."], "abstract": ["The trade-off between feasibility and accuracy of measurements of physical exposure at the workplace has often been discussed, but is unsufficiently understood. We therefore explored the effect of two low-back loading measurement tools with different accuracies on exposure estimates and their associations with low-back pain (LBP).", "Low-back moments of 93 workers were obtained using two methods: a moderately accurate observation-based method and a relatively more accurate video-analysis method. Group-based exposure metrics were assigned to a total of 1131 workers who reported on their LBP status during three follow-up years. The two methods were compared regarding individual and group-based moments and their predictive value for LBP.", "Differences between the two methods for peak moments were high at the individual level and remained substantial at group level. For cumulative moments, differences between the two methods were attenuated as random inaccuracies cancelled out. Peak moments were not predictive for LBP in any method while cumulative moments were, suggesting comparable predictive values of the two methods. While assessment of low-back load improves from investing in collecting relatively more accurate individual-based data, this does not necessarily lead to better predictive values on a group level, especially not for cumulative loads."]},
{"title": "Integrating user centered design, universal design and goal, operation, method and selection rules to improve the usability of DAISY player for persons with visual impairments", "highlights": ["This study integrated and synergized the UCD process, UD rules, and GOMS method as a new product development process.", "Redesigned DAISY player has better usability according to subjective and objective performance of participants.", "The proposed methodology can design user friendly products for both visual impaired and normal individuals."], "abstract": ["The Digital Accessible Information SYstem (DAISY) player is an assistive reading tool developed for use by persons with visual impairments. Certain problems have persisted in the operating procedure and interface of DAISY players, especially for their Chinese users. Therefore, the aim of this study was to redesign the DAISY player with increased usability features for use by native Chinese speakers. First, a User Centered Design (UCD) process was employed to analyze the development of the prototype. Next, operation procedures were reorganized according to GOMS (Goals, Operators, Methods, and Selection rules) methodology. Then the user interface was redesigned according to specific Universal Design (UD) principles. Following these revisions, an experiment involving four scenarios was conducted to compare the new prototype to other players, and it was tested by twelve visually impaired participants. Results indicate the prototype had the quickest operating times, the fewest number of operating errors, and the lowest mental workloads of all the compared players, significantly enhancing the prototype's usability. These findings have allowed us to generate suggestions for developing the next generation of DAISY players for people, especially for Chinese audience."]},
{"title": "Effects of culture (China vs. US) and task on perceived hazard: Evidence from product ratings, label ratings, and product to label matching", "highlights": ["Chinese participants perceived less hazard in response to warning labels than did US participants.", "Perceived hazard in response to products did not differ as a function of culture.", "Likelihood of injury and control affected matching performance differently across culture.", "Warning design should take into account that risk perceptions vary with culture and context."], "abstract": ["In the current study, 44 Chinese and 40 US college students rated their perceived hazard in response to warning labels and products and attempted to match products with warning labels communicating the same level of hazard. Chinese participants tended to provide lower ratings of hazard in response to labels, but hazard perceived in response to products did not significantly differ as a function of culture. When asked to match a product with a warning label, Chinese participants' hazard perceptions appeared to be better calibrated, than did US participants', across products and labels. The results are interpreted in terms of constructivist theory which suggests that risk perceptions vary depending on the \u201cframe of mind\u201d evoked by the environment/context. Designers of warnings must be sensitive to the fact that product users' cognitive representations develop within a culture and that risk perceptions will vary based on the context in which they are derived."]},
{"title": "Overload depending on driving experience and situation complexity: Which strategies faced with a pedestrian crossing?", "highlights": ["We examine the influence of situation complexity and driving experience on subjective workload and driving performance.", "We identify effective avoidance maneuvers faced with a hazard pedestrian appearance.", "A driving simulation study is carried out.", "The situation complexity and the lack of experience increase the subjective workload.", "The swerving maneuver is the most effective strategy to avoid a pedestrian."], "abstract": ["The purpose of this study was to identify the influence of situation complexity and driving experience on subjective workload and driving performance, and the less costly and the most effective strategies faced with a hazard pedestrian crossing. Four groups of young drivers (15 traditionally trained novices, 12 early-trained novices, 15 with three years of experience and 15 with a minimum of five years of experience) were randomly assigned to three situations (simple, moderately complex and very complex) including unexpected pedestrian crossings, in a driving simulator. The subjective workload was collected by the NASA-TLX questionnaire after each situation. The main results confirmed that the situation complexity and the lack of experience increased the subjective workload. Moreover, the subjective workload, the avoidance strategies and the reaction times influenced the number of collisions depending on situation complexity and driving experience. These results must be taken into account to target the prevention actions."]},
{"title": "The influence of age in usability testing", "highlights": ["There is a need to consider age more strongly in usability testing.", "Older adult users (52\u201379\u00a0yrs) showed no decrement in effectiveness compared to younger adults (19\u201329\u00a0yrs).", "Older adult users showed deteriorations in task efficiency compared to younger adults.", "It may be necessary to distinguish between speed and accuracy in performance."], "abstract": ["The effects of age in usability testing were examined in an experiment. Sixty users from two age groups (M\u00a0=\u00a023.0\u00a0yrs, M\u00a0=\u00a058.1\u00a0yrs) operated two technical devices (keyboard-based and touchscreen-based smartphones). In addition to various performance measures (e.g.\u00a0task completion time, task completion rate), several subjective measures were taken (e.g.\u00a0perceived usability, affect, and workload). The results showed better performance scores for younger adults than older adults for task completion time. For older adult users there was a mismatch between usability ratings and task completion time but not between usability ratings and task completion rate. Age-related differences in the importance of speed and accuracy in task completion point to the need to consider more strongly the factor user age in usability research and practice."]},
{"title": "The implementation of ergonomics advice and the stage of change approach", "highlights": ["This paper investigates the implementation of ergonomics advice tailored according to the Stage of Change approach.", "Companies were randomly allocated to receive tailored or standard ergonomics advice.", "Companies in receipt of tailored advice implemented significantly more changes.", "The implementation of ergonomics advice may be improved by the incorporation of Stage of Change principles."], "abstract": ["This paper investigates the implementation of injury prevention advice tailored according to the Stage of Change (SOC) approach. The managers of 25 workgroups, drawn from medium to large companies across a wide range of occupational sectors were allocated to receive either standard ergonomics advice or ergonomics advice tailored according to the workgroup SOC. Twelve months after the advice was provided, semi-structured interviews were conducted with each manager. In a multivariate model, managers who had received tailored advice were found to have implemented significantly more of the recommended changes (IRR\u00a0=\u00a01.68, 95% CI 1.07\u20132.63) and more \u201cadditional\u201d changes (IRR\u00a0=\u00a01.90, 95% CI 1.12\u20133.20). Qualitative analysis identified that the key barriers and facilitators to the implementation of changes were largely related to worker resistance to change and the attitudes of senior managers towards health and safety. The findings from this study suggest that the implementation of ergonomics recommendations may be improved by the tailoring of advice according to SOC principles."]},
{"title": "Pre-flight safety briefings, mood and information retention", "highlights": ["Mood is a moderating factor that is known to affect performance.", "The delivery of the pre-flight safety briefing on aircraft is an opportunity to affect one's mood.", "Results suggest that mood can be affected by a humorous safety briefing."], "abstract": ["Mood is a moderating factor that is known to affect performance. For airlines, the delivery of the pre-flight safety briefing prior to a commercial flight is not only an opportunity to inform passengers about the safety features on-board the aircraft they are flying, but an opportunity to positively influence their mood, and hence performance in the unlikely event of an emergency. The present research examined whether indeed the pre-flight safety briefing could be used to positively impact passengers' mood. In addition, the present research examined whether the recall of key safety messages contained within the pre-flight safety briefing was influenced by the style of briefing. Eighty-two participants were recruited for the research and divided into three groups; each group exposed to a different pre-flight cabin safety briefing video (standard, humorous, movie theme). Mood was measured prior and post safety briefing. The results revealed that pre-flight safety briefing videos can be used to manipulate passengers' mood. Safety briefings that are humorous or use movie themes to model their briefing were found to positively affect mood. However, there was a trade-off between entertainment and education, the greater the entertainment value, the poorer the retention of key safety messages. The results of the research are discussed from both an applied and theoretical perspective."]},
{"title": "Safety challenges of medical equipment in nurse anaesthetist training in Haiti", "highlights": ["Performance obstacles of nurse anaesthetists in training in Haiti are described.", "Various environment-related problems hinder safety of use of medical equipment.", "Coping strategies are a key characteristic of practice in low-resource settings.", "Using a systems approach is relevant in technology transfer.", "Human Factors research in low-resource settings requires adjusted methods."], "abstract": ["Safety challenges related to the use of medical equipment were investigated during the training of nurse anaesthetists in Haiti, using a systems approach to Human Factors and Ergonomics (HFE). The Observable Performance Obstacles tool, based on the Systems Engineering Initiative for Patient Safety (SEIPS) model, was used in combination with exploratory observations during 13 surgical procedures, to identify performance obstacles created by the systemic interrelationships of medical equipment. The identification of performance obstacles is an effective way to study the accumulation of latent factors and risk hazards, and understand its implications in practice and behaviour of healthcare practitioners. In total, 123 performance obstacles were identified, of which the majority was related to environmental and organizational aspects. These findings show how the performance of nurse anaesthetists and their relation to medical equipment is continuously affected by more than user-related aspects. The contribution of systemic performance obstacles and coping strategies to enrich system design interventions and improve healthcare system is highlighted. In addition, methodological challenges of HFE research in low-resource settings related to professional culture and habits, and the potential of community ergonomics as a problem-managing approach are described."]},
{"title": "Body size and ability to pass through a restricted space: Observations from 3D scanning of 210 male UK offshore workers", "highlights": ["Offshore workers have greater shoulder breadth and chest depth than UK males.", "Increased body size results in a reduced safety clearance in restricted spaces.", "Facilities designed for offshore use need to be larger than for typical UK males.", "Wearing a survival suit dramatically enlarges body dimensions and space needs.", "Ability to pass in restricted space is commensurately reduced."], "abstract": ["Offshore workers are subjected to a unique physical and cultural environment which has the ability to affect their size and shape. Because they are heavier than the UK adult population we hypothesized they would have larger torso dimensions which would adversely affect their ability to pass one another in a restricted space. A sample of 210 male offshore workers was selected across the full weight range, and measured using 3D body scanning for shape. Bideltoid breadth and maximum chest depth were extracted from the scans and compared with reference population data. In addition a size algorithm previously calculated on 44 individuals was applied to adjust for wearing a survival suit and re-breather device. Mean bideltoid breadth and chest depth was 51.4\u00a0cm and 27.9\u00a0cm in the offshore workers, compared with 49.7\u00a0cm and 25.4\u00a0cm respectively in the UK population as a whole. Considering the probability of two randomly selected people passing within a restricted space of 100\u00a0cm and 80\u00a0cm, offshore workers are 28% and 34% less likely to pass face to face and face to side respectively, as compared with UK adults, an effect which is exacerbated when wearing personal protective equipment."]},
{"title": "The future flight deck: Modelling dual, single and distributed crewing options", "highlights": ["Contrasting different crewing options with CWA and SNA.", "Identification of functional loadings on agents.", "Using data from SOCA-CAT as input into SNA.", "Analysis of networked in interactions between agents.", "Using network analysis to identify system resilience."], "abstract": ["It is argued that the barrier to single pilot operation is not the technology, but the failure to consider the whole socio-technical system. To better understand the socio-technical system we model alternative single pilot operations using Cognitive Work Analysis (CWA) and analyse those models using Social Network Analysis (SNA). Four potential models of single pilot operations were compared to existing two pilot operations. Using SOCA-CAT from CWA, we were able to identify the potential functional loading and interactions between networks of agents. The interactions formed the basis on the SNA. These analyses potentially form the basis for distributed system architecture for the operation of a future aircraft. The findings from the models suggest that distributed crewing option could be at least as resilient, in network architecture terms, as the current dual crewing operations."]},
{"title": "More than meets the eye: Using cognitive work analysis to identify design requirements for future rail level crossing systems", "highlights": ["Future transport systems will be more diverse and complex.", "Rail level crossings are a key safety issue that will likely degenerate.", "We used cognitive work analysis to analyse rail level crossings.", "Key flaws and design requirements for future systems were identified.", "Modifications outside of rail level crossings are also required."], "abstract": ["An increasing intensity of operations means that the longstanding safety issue of rail level crossings is likely to become worse in the transport systems of the future. It has been suggested that the failure to prevent collisions may be, in part, due to a lack of systems thinking during design, crash analysis, and countermeasure development. This paper presents a systems analysis of current active rail level crossing systems in Victoria, Australia that was undertaken to identify design requirements to improve safety in future rail level crossing environments. Cognitive work analysis was used to analyse rail level crossing systems using data derived from a range of activities. Overall the analysis identified a range of instances where modification or redesign in line with systems thinking could potentially improve behaviour and safety. A notable finding is that there are opportunities for redesign outside of the physical rail level crossing infrastructure, including improved data systems, in-vehicle warnings and modifications to design processes, standards and guidelines. The implications for future rail level crossing systems are discussed."]},
{"title": "Factors affecting the risk of developing lower back musculoskeletal disorders (MSDs) in experienced and inexperienced rodworkers", "highlights": ["Inexperienced preferred raising to upright more frequently, possibly increasing the risk of back muscle fatigue.", "Experienced preferred to remain in full trunk flexion for longer periods, potentially putting them at risk of ligament creep.", "Both groups seemed to be at risk of decreased back stability by different means."], "abstract": ["Injury and dropout rates during rodwork training appear to reflect difficulties encountered by apprentices adapting to increased physical demands of tying on slab, one of the rodworking tasks with the highest injury risk. Because experience influences work strategies, and consequently the risk of developing musculoskeletal disorders (MSDs), this study aimed to identify differences in work practices associated with tying rebar on slab, potentially relevant to back MSD development, in experienced and inexperienced rodworkers.", "Fourteen male rodworkers were recruited from either experienced (>2 years experience post apprenticeship), or inexperienced (<6 months experience) groups. Both tied an area with rebar laid on the ground. Trunk flexion/extension angles were measured. L4/L5 moments were estimated from T9 Erector Spinae EMG.", "Experienced workers were found to spend longer periods of time in trunk flexed postures, with lower peak L4/L5 moments. Our findings revealed practices associated with each group might have different implications on back health."]},
{"title": "Human factors engineering approaches to patient identification armband design", "highlights": ["Patient identification is performed many times each day with armbands used for verification and barcode scanning.", "The information layout of patient identification armbands is critical to reducing patient misidentification and workarounds.", "Healthcare systems lack guides to armband designs: Current requirements are insufficient.", "Two healthcare systems followed human factors design approaches on new patient identification armbands.", "Paper shows ways to standardize armband designs that incorporate human factors principles."], "abstract": ["The task of patient identification is performed many times each day by nurses and other members of the care team. Armbands are used for both direct verification and barcode scanning during patient identification. Armbands and information layout are critical to reducing patient identification errors and dangerous workarounds. We report the effort at two large, integrated healthcare systems that employed human factors engineering approaches to the information layout design of new patient identification armbands. The different methods used illustrate potential pathways to obtain standardized armbands across healthcare systems that incorporate human factors principles. By extension, how the designs have been adopted provides examples of how to incorporate human factors engineering into key clinical processes."]},
{"title": "Keep the driver in control: Automating automobiles of the future", "highlights": ["Driver-initiated automation leads to increased workload and decreased trust.", "Driver protocols and behavioural observations highlight systems design issues.", "Recommendations for future systems development discussed."], "abstract": ["Automated automobiles will be on our roads within the next decade but the role of the driver has not yet been formerly recognised or designed. Rather, the driver is often left in a passive monitoring role until they are required to reclaim control from the vehicle. This research aimed to test the idea of driver-initiated automation, in which the automation offers decision support that can be either accepted or ignored. The test case examined a combination of lateral and longitudinal control in addition to an auto-overtake system. Despite putting the driver in control of the automated systems by enabling them to accept or ignore behavioural suggestions (e.g. overtake), there were still issues associated with increased workload and decreased trust. These issues are likely to have arisen due to the way in which the automated system has been designed. Recommendations for improvements in systems design have been made which are likely to improve trust and make the role of the driver more transparent concerning their authority over the automated system."]},
{"title": "Two-handed grip on a mobile phone affords greater thumb motor performance, decreased variability, and a more extended thumb posture than a one-handed grip", "highlights": ["Thumb tapping performance is greater for a two-compared to a one-handed grip.", "Tapping with a two-handed grip involved different wrist and thumb postures compared to a one-handed grip.", "Variability of the device's movement while tapping was more pronounced in a two compared to a one-handed grip."], "abstract": ["Holding a mobile computing device with two hands may affect thumb motor performance, joint postures, and device stability compared to holding the device and tapping the touchscreen with the thumb of the holding hand. We tested the hypotheses that holding a touchscreen mobile phone with two hands lead to increased thumb motor performance, different thumb postures, and decreased device movement relative to using one hand. Ten right-handed participants completed reciprocal thumb tapping tasks between emulated keys on a smartphone in either a one- (portrait) or two-handed (landscape) grip configuration. Effective index of performance measured from Fitts' Law was 9% greater (p\u00a0<\u00a00.001), movement time 7% faster (p\u00a0<\u00a00.001), and taps were 4% more precise (p\u00a0<\u00a00.016) for the two-handed grip. Tapping with a two-handed grip involved significantly different wrist and thumb postures than a one-handed grip. Variability of the computing device's movement was 36\u201363% lower for the two-handed grip compared to the one-handed grip condition (p\u00a0<\u00a00.001). The support for our hypotheses suggests that a two-handed grip results in increased performance and more extended wrist and thumb postures than a single-handed grip. Device designs that allow two-handed grips may afford increased performance relative to a one-handed grip."]},
{"title": "Normative static grip strength of population of Turkey, effects of various factors and a comparison with international norms", "highlights": ["Normative static grip strength of normal population of Turkey was estimated.", "Effects of individual and job factors on grip strength were investigated.", "Grip strength was significantly affected by gender, age-group and hand.", "There exist cross-national grip strength variations among some nations but not all."], "abstract": ["Normative data are of importance in ergonomics and clinical settings. Applying normative data internationally is questionable. To this end, this study aimed to establish gender- and age-specific reference values for static (isometric) hand grip strength of normal population of Turkey with special regard to occupational demand, and compare them with the international norms. The secondary aims were to investigate the effects of gender, age-group, weight-group, job-group, hand and several anthropometric variables on static grip strength. A sample of 211 (128 male and 83 female) volunteers aged between 18 and 69 with various occupations participated in the study. Grip strength data were collected using a Jamar dynamometer with standard testing position, protocol and instructions. The mean and std deviation of maximum voluntary static grip strength values (in N) for dominant and non-dominant hands respectively were 455.2\u00a0\u00b1\u00a073.6 and 441.5\u00a0\u00b1\u00a072.6 for males, and 258\u00a0\u00b1\u00a046.1 and 246.2\u00a0\u00b1\u00a049.1 for females. The mean female strength was about 57% of the mean male strength value for both dominant and non-dominant hands. There was a curvilinear relationship of grip strength to age, significant differences between genders, hands, and some age-groups, and a correlation to height, body-mass, BMI and hand dimensions depending on the gender. The comparisons with the norms of other world populations indicate that there are cross-national grip strength variations among some nations but not all."]},
{"title": "21st century trucking: A trajectory for ergonomics and road freight", "highlights": ["Addressing the \u2018triple bottom line\u2019 (TBL) is key for sustainable road freight.", "Ergonomics and logistics have rarely intersected in research.", "Results present a technology & human factors roadmap for commercial vehicles to 2050.", "Technology alone is not enough to achieve future CO", " targets.", "Ergonomics research in commercial driver behaviour is crucial in meeting the TBL."], "abstract": ["Over the past decade there has been significant pressure to minimise emissions and safety risks related to commercial driving. This pressure to meet the triple bottom line of cost, environment, and society has often resulted in the rapid application of vehicle technologies designed to mitigate undesired effects. Often the cognitive and behavioural effects of technologies on the commercial driver have not received in-depth analysis to determine comprehensive viability. As such, this paper aims to identify a timescale for implementation for future technologies for UK road freight, and likely associated human factors issues, improving upon the currently employed \u2018trial-and-error\u2019 approach to implementation which may carry high economic, environmental, safety-related risk. Thought experiments are carried out to broadly explore these future systems. Furthermore, this work aims to examine whether technology alone will be enough to meet future CO", " reduction targets, and assess the role of behavioural and systems interventions for future research."]},
{"title": "Future directions for the development of virtual reality within an automotive manufacturer", "highlights": ["Opportunities exist to optimise vehicle development using Virtual Reality.", "Several issues engineers face in car design are being addressed in VR research.", "Depth perception in VR and haptic/multimodal feedback are priority areas.", "Markerless tracking, networked VR and fluids/flexibles also offer opportunities."], "abstract": ["Virtual Reality (VR) can reduce time and costs, and lead to increases in quality, in the development of a product. Given the pressure on car companies to reduce time-to-market and to continually improve quality, the automotive industry has championed the use of VR across a number of applications, including design, manufacturing, and training. This paper describes interviews with 11 engineers and employees of allied disciplines from an automotive manufacturer about their current physical and virtual properties and processes. The results guided a review of research findings and scientific advances from the academic literature, which formed the basis of recommendations for future developments of VR technologies and applications. These include: develop a greater range of virtual contexts; use multi-sensory simulation; address perceived differences between virtual and real cars; improve motion capture capabilities; implement networked 3D technology; and use VR for market research."]},
{"title": "Dynamic push\u2013pull characteristics at three hand-reach envelopes: Applications for the workplace", "highlights": ["We investigated dynamic push pull strength and work measures during a seated task, at different reach envelopes.", "Dynamic pushing work capability was best at medium reach envelopes.", "Pulling was best at maximum, followed by medium, reach positions.", "A recommendation was made to avoid strenuous push\u2013pull tasks at neutral reach envelopes.", "Push/pull strength ratio was approximately 1.00."], "abstract": ["Pushing and pulling are common tasks in the workplace. Overexertion injuries related to manual pushing and pulling are often observed, and therefore the understanding of work capacity is important for efficient and safe workstation design. The purpose of the present study was to describe workloads obtained during different reach envelopes during a seated push\u2013pull task. Forty-five women performed an isokinetic push\u2013pull sequence at two velocities. Strength, work and agonist/antagonist muscle ratio were calculated for the full range of motion (ROM). We then divided the ROM into three reach envelopes \u2013 neutral, medium, and maximum reach. The work capacity for each direction was determined and the reach envelope work data were compared. Push capability was best at medium reach envelope and pulling was best at maximum reach envelope. Push/pull strength ratio was approximately 1. A recommendation was made to avoid strenuous push\u2013pull tasks at neutral reach envelopes."]},
{"title": "Extending helicopter operations to meet future integrated transportation needs", "highlights": ["Work Domain Analysis was used for concept design.", "A Head Up Display (HUD) was developed to assist pilots landing in degraded visual conditions.", "The HUD increased awareness and reduced workload in degraded conditions."], "abstract": ["Helicopters have the potential to be an integral part of the future transport system. They offer a means of rapid transit in an overly populated transport environment. However, one of the biggest limitations on rotary wing flight is their inability to fly in degraded visual conditions in the critical phases of approach and landing. This paper presents a study that developed and evaluated a Head up Display (HUD) to assist rotary wing pilots by extending landing to degraded visual conditions. The HUD was developed with the assistance of the Cognitive Work Analysis method as an approach for analysing the cognitive work of landing the helicopter. The HUD was tested in a fixed based flight simulator with qualified helicopter pilots. A qualitative analysis to assess situation awareness and workload found that the HUD enabled safe landing in degraded conditions whilst simultaneously enhancing situation awareness and reducing workload. Continued development in this area has the potential to extend the operational capability of helicopters in the future."]},
{"title": "Observed use of voluntary controls to reduce physical exposures among sheet metal workers of the mechanical trade", "highlights": ["Our observations showed incomplete transfer of trade-specific stakeholder recommendations into work practice.", "Tools and practices to reduce physical exposures among construction workers, a high risk group for MSD, are not fully adopted.", "More work is needed to disseminate means to reduce exposures in this decentralized industry with slow adoption of technology."], "abstract": ["Little is known about the transfer into the workplace of interventions designed to reduce the physical demands of sheet metal workers.", "We reviewed videos from a case series of 15 sheet metal worksite assessments performed in 2007\u20132009 to score postures and physical loads, and to observe the use of recommended interventions to reduce physical exposures in sheet metal activities made by a NIOSH stakeholder meeting in 2002.", "Workers showed consistent use of material handling devices, but we observed few uses of recommended interventions to reduce exposures during overhead work. Workers spent large proportions of time in awkward shoulder elevation and low back rotation postures.", "In addition to the development of new technologies and system designs, increased adoption of existing tools and practices could reduce time spent in awkward postures and other risks for musculoskeletal disorders in sheet metal work."]},
{"title": "Reductions in self-reported stress and anticipatory heart rate with the use of a semi-automated parallel parking system", "highlights": ["Self-reported stress was significantly lower with assistive parking technology.", "Heart rate (HR) was on average 13\u00a0bpm lower during assisted parks vs. manual parks.", "Anticipatory stress, measured by HR, was significantly lower with the technology.", "Turn signal usage was significantly lower during the assistive technology trials."], "abstract": ["Drivers' reactions to a semi-autonomous technology for assisted parallel parking system were evaluated in a field experiment. A sample of 42 drivers balanced by gender and across three age groups (20\u201329, 40\u201349, 60\u201369) were given a comprehensive briefing, saw the technology demonstrated, practiced parallel parking 3 times each with and without the assistive technology, and then were assessed on an additional 3 parking events each with and without the technology. Anticipatory stress, as measured by heart rate, was significantly lower when drivers approached a parking space knowing that they would be using the assistive technology as opposed to manually parking. Self-reported stress levels following assisted parks were also lower. Thus, both subjective and objective data support the position that the assistive technology reduced stress levels in drivers who were given detailed training. It was observed that drivers decreased their use of turn signals when using the semi-autonomous technology, raising a caution concerning unintended lapses in safe driving behaviors that may occur when assistive technologies are used."]},
{"title": "Predicting physiological capacity of human load carriage \u2013 A review", "highlights": ["Physiological limits in occupational settings have previously been investigated.", "Existing workload limit guidelines are likely not appropriate for load carriage.", "A model to predict occupational load carriage performance has been proposed.", "Able to provide rapid assessments to inform task and personnel management.", "The predictive tool would be invaluable for planning in military and firefighting."], "abstract": ["This review article aims to evaluate a proposed maximum acceptable work duration model for load carriage tasks. It is contended that this concept has particular relevance to physically demanding occupations such as military and firefighting. Personnel in these occupations are often required to perform very physically demanding tasks, over varying time periods, often involving load carriage. Previous research has investigated concepts related to physiological workload limits in occupational settings (e.g. industrial). Evidence suggests however, that existing (unloaded) workload guidelines are not appropriate for load carriage tasks. The utility of this model warrants further work to enable prediction of load carriage durations across a range of functional workloads for physically demanding occupations. If the maximum duration for which personnel can physiologically sustain a load carriage task could be accurately predicted, commanders and supervisors could better plan for and manage tasks to ensure operational imperatives were met whilst minimising health risks for their workers."]},
{"title": "Temporal uncertainty analysis of human errors based on interrelationships among multiple factors: A case of Minuteman III missile accident", "highlights": ["A system-dynamics-based method is proposed to highlight interrelationships among the aspects contributing to human errors.", "Quantitatively estimating the human error probability (HEP) and its related variables changing over time in a long period.", "In a missile accident case, the HEP is assessed during 50-year operations, with the variables\u2019 impacts on risks determined."], "abstract": ["In traditional approaches of human reliability assessment (HRA), the definition of the error producing conditions (EPCs) and the supporting guidance are such that some of the conditions (especially organizational or managerial conditions) can hardly be included, and thus the analysis is burdened with incomprehensiveness without reflecting the temporal trend of human reliability. A method based on system dynamics (SD), which highlights interrelationships among technical and organizational aspects that may contribute to human errors, is presented to facilitate quantitatively estimating the human error probability (HEP) and its related variables changing over time in a long period. Taking the Minuteman III missile accident in 2008 as a case, the proposed HRA method is applied to assess HEP during missile operations over 50 years by analyzing the interactions among the variables involved in human-related risks; also the critical factors are determined in terms of impact that the variables have on risks in different time periods. It is indicated that both technical and organizational aspects should be focused on to minimize human errors in a long run."]},
{"title": "Evaluation of the effectiveness of a multi-skill program for training younger drivers on higher cognitive skills", "highlights": ["The current study measures the effectiveness of an integrated training program that takes only a third as long to complete compared to three individual training programs for hazard anticipation, hazard mitigation, and attention maintenance skills.", "Drivers in the SAFE-T-trained group were more likely to anticipate hazards, quicker and more effective at responding to hazards, and more likely to keep glance durations under a critical threshold of 2\u00a0s as compared to drivers in the Placebo-trained group.", "Drivers in the SAFE-T trained group showed the same effect of attention maintenance training as drivers who had been trained using FOCAL when compared with prior studies."], "abstract": ["Training programs exist that prove effective at teaching novice drivers to anticipate latent hazards (RAPT), mitigate hazards (ACT) and maintain attention (FOCAL). The current study (a) measures the effectiveness of a novel integrated training program (SAFE-T) that takes only a third as long to complete compared to the three individual training programs and (b) determines if integrating the training of all the three higher cognitive skills would yield results comparable to the existing programs. Three groups were evaluated: SAFE-T, RAPT and Placebo. The results show that the drivers in the SAFE-T-trained group were more likely to anticipate hazards, quicker and more effective at responding to hazards, and more likely to maintain glance durations under a critical threshold of 2\u00a0s as compared to drivers in the Placebo-trained group who received a control program that does not actively train on any of the three cognitive skills. Moreover, the results show that the drivers in the SAFE-T trained group were just as likely to anticipate hazards as the drivers in the RAPT trained group. Finally, when compared with prior studies, the drivers in the SAFE-T trained group showed similar effects of attention maintenance training."]},
{"title": "The application of an industry level participatory ergonomics approach in developing MSD interventions", "highlights": ["A participatory ergonomics approach was applied at an industry level to develop MSD interventions.", "This approach was required to help overcome industry scepticism and to deal with industry level MSD risks.", "Key stakeholders developed collective ownership of the interventions and increased their own awareness of MSD prevention.", "An industry-supported key stakeholder group, and a mandate for the initiative, are important prerequisites for success."], "abstract": ["Participatory ergonomics projects are traditionally applied within one organisation. In this study, a participative approach was applied across the New Zealand meat processing industry, involving multiple organisations and geographical regions. The purpose was to develop interventions to reduce musculoskeletal disorder (MSD) risk. This paper considers the value of an industry level participatory ergonomics approach in achieving this. The main rationale for a participative approach included the need for industry credibility, and to generate MSD interventions that address industry level MSD risk factors. An industry key stakeholder group became the primary vehicle for formal participation. The study resulted in an intervention plan that included the wider work system and industry practices. These interventions were championed across the industry by the key stakeholder group and have extended beyond the life of the study. While this approach helped to meet the study aim, the existence of an industry-supported key stakeholder group and a mandate for the initiative are important prerequisites for success."]},
{"title": "A longitudinal investigation of work environment stressors on the performance and wellbeing of office workers", "highlights": ["We measure responses to indoor environmental quality in real office workers.", "Environmental stress reduces work performance by 2.4\u20135.8% in most situations.", "Stressors reduce motivation, and increased tiredness and distractibility.", "Stressors reduce not only cognitive performance, but the rate of work.", "Stress factors appear to be additive, not multiplicative."], "abstract": ["This study uses a longitudinal within-subjects design to investigate the effects of inadequate Indoor Environmental Quality (IEQ) on work performance and wellbeing in a sample of 114 office workers over a period of 8 months. Participants completed a total of 2261 online surveys measuring perceived thermal comfort, lighting comfort and noise annoyance, measures of work performance, and individual state factors underlying performance and wellbeing. Characterising inadequate aspects of IEQ as environmental stressors, these stress factors can significantly reduce self-reported work performance and objectively measured cognitive performance by between 2.4% and 5.8% in most situations, and by up to 14.8% in rare cases. Environmental stressors act indirectly on work performance by reducing state variables, motivation, tiredness, and distractibility, which support high-functioning work performance. Exposure to environmental stress appears to erode individuals' resilience, or ability to cope with additional task demands. These results indicate that environmental stress reduces not only the cognitive capacity for work, but the rate of work (i.e. by reducing motivation). Increasing the number of individual stress factors is associated with a near linear reduction in work performance indicating that environmental stress factors are additive, not multiplicative. Environmental stressors reduce occupant wellbeing (mood, headaches, and feeling \u2018off\u2019) causing indirect reductions in work performance. Improving IEQ will likely produce small but pervasive increases in productivity."]},
{"title": "Evaluation of jar lid design characteristics by older women with hand use limitations", "highlights": ["We studied jar lid design characteristics to aid people with hand dysfunction.", "Study participants provided ratings of perceived effort and discomfort.", "Advantageous combinations of jar lid design characteristics include.", "For 42\u00a0mm dia. lids: taller height, hexagonal top shape, and convex side shape.", "For 28\u00a0mm dia. lids: taller height and hexagonal top shape."], "abstract": ["The study evaluated several lid design characteristics (diameter, height, top shape, side shape, and surface texture) by means of controlled laboratory testing with older women with hand function limitations. A subjective evaluation process was applied to examine main effects and interactions of lid design characteristics on usability, determined by participants' perceptions of effort and discomfort. Results showed that lid height was the most important design characteristic associated with usability. For 42\u00a0mm diameter lids, designs perceived as best were ones with taller height, hexagonal top shape, and convex side shape. For 28\u00a0mm diameter lids, the best designs were ones with taller height and hexagonal top shape. Additionally, when the smaller lid's side shape was flat, a serrate surface texture provided some advantages, particularly for subjects with more severe hand dysfunction. This information could be used by package designers to improve jar lid usability for a growing sector of consumers."]},
{"title": "Effects of mental workload on physiological and subjective responses during traffic density monitoring: A field study", "highlights": ["This study evaluated operators' MWL using physiological indices and the NASA-TLX in real working conditions.", "Increasing traffic density may affect operators' physiological responses and subjective experience of MWL.", "The shift work probably did not have any effect on physiological responses.", "Conducting an ergonomic program to manage mental health may be suitable for detecting at-risk operators."], "abstract": ["This study evaluated operators' mental workload while monitoring traffic density in a city traffic control center. To determine the mental workload, physiological signals (ECG, EMG) were recorded and the NASA-Task Load Index (TLX) was administered for 16 operators. The results showed that the operators experienced a larger mental workload during high traffic density than during low traffic density. The traffic control center stressors caused changes in heart rate variability features and EMG amplitude, although the average workload score was significantly higher in HTD conditions than in LTD conditions. The findings indicated that increasing traffic congestion had a significant effect on HR, RMSSD, SDNN, LF/HF ratio, and EMG amplitude. The results suggested that when operators' workload increases, their mental fatigue and stress level increase and their mental health deteriorate. Therefore, it maybe necessary to implement an ergonomic program to manage mental health. Furthermore, by evaluating mental workload, the traffic control center director can organize the center's traffic congestion operators to sustain the appropriate mental workload and improve traffic control management."]},
{"title": "System reliability, performance and trust in adaptable automation", "highlights": ["We examined the effects of reduced reliability levels in an adaptable automatic system.", "We took measures of trust, automation reliance, performance, and workload.", "Operator trust was affected by reduced automation reliability.", "Operator reliance on automation was however unaffected by reliability."], "abstract": ["The present study examined the effects of reduced system reliability on operator performance and automation management in an adaptable automation environment. 39 operators were randomly assigned to one of three experimental groups: low (60%), medium (80%), and high (100%) reliability of automation support. The support system provided five incremental levels of automation which operators could freely select according to their needs. After 3\u00a0h of training on a simulated process control task (AutoCAMS) in which the automation worked infallibly, operator performance and automation management were measured during a 2.5-h testing session. Trust and workload were also assessed through questionnaires. Results showed that although reduced system reliability resulted in lower levels of trust towards automation, there were no corresponding differences in the operators' reliance on automation. While operators showed overall a noteworthy ability to cope with automation failure, there were, however, decrements in diagnostic speed and prospective memory with lower reliability."]},
{"title": "Center of pressure and total force analyses for amputees walking with a backpack load over four surfaces", "highlights": ["Examined amputee gait with and without a backpack load, over multiple surfaces.", "COP and total force were similar when walking with and without a backpack load.", "Transtibial amputees had more deviations in COP on the intact limb.", "High functioning amputees are not limited by a backpack load on the tested tasks."], "abstract": ["Understanding how load carriage affects walking is important for people with a lower extremity amputation who may use different strategies to accommodate to the additional weight. Nine unilateral traumatic transtibial amputees (K4-level) walked over four surfaces (level-ground, uneven ground, incline, decline) with and without a 24.5\u00a0kg backpack. Center of pressure (COP) and total force were analyzed from F-Scan insole pressure sensor data. COP parameters were greater on the intact limb than on the prosthetic limb, which was likely a compensation for the loss of ankle control. Double support time (DST) was greater when walking with a backpack. Although longer DST is often considered a strategy to enhance stability and/or reduce loading forces, changes in DST were only moderately correlated with changes in peak force. High functioning transtibial amputees were able to accommodate to a standard backpack load and to maintain COP progression, even when walking over different surfaces."]},
{"title": "Reflecting on Jens Rasmussen's legacy (2) behind and beyond, a \u2018constructivist turn\u2019", "highlights": ["This article is the second part of a study on the legacy of Jens Rasmussen.", "The first article looks back on his 30 years of scientific contribution, from 1969 to 2000.", "This second article explores and investigates some of his intellectual roots.", "It uses them as a basis to understand some limits and move forward."], "abstract": ["This article is the second part of a study on the legacy of Jens Rasmussen. The first article, subtitled \u2018A Strong Program for a Hard Problem\u2019, looks back on his 30 years of scientific contribution, from 1969 to 2000. This second article explores and investigates some of the intellectual roots which influenced his thinking, using them as a basis to understand some limits and move forward. Indeed, historically oriented studies such as this one are not only tributes to researchers, but a way to differentiate and contrast our present situation with the past in order to integrate contemporary trends, be they theoretical or empirical, or oriented towards research and new models.", "In the first section of this article, I offer a synthesis of the background covered in the previous article, but I use a tree here as a graphical complement. Branches of the tree show the many fruitful directions opened by Jens Rasmussen, directions which inspired many researchers. In the second part, I address what I believe to be behind this wealth of engineering legacy: cybernetics. I contend that cybernetics has had a profound influence on his thinking and provided him key principles for his inspiring and successful models. To develop the tree image, one might say that cybernetics is the trunk of the tree. Finally, in the third part, I take the opportunity to explore the relevance of extending and sensitising his program to constructivist discourses. After an introduction to this discourse, identifying four types of constructivisms (cognitive, social, epistemological and anthropological), I characterise this move as a \u2018constructivist turn\u2019."]},
{"title": "Lumbar postures, seat interface pressures and discomfort responses to a novel thoracic support for police officers during prolonged simulated driving exposures", "highlights": ["A novel thoracic support for police officers was developed and tested.", "The thoracic support reduced seat back contact pressure area at the low back.", "The thoracic support increased seat back contact pressure area at the upper back.", "Normalized lumbar postures were more flexed with the thoracic support.", "Frequent secondary tasks (MDT usage) may reduce the rate of discomfort development."], "abstract": ["A high prevalence of low back pain has been reported among professional drivers, including mobile police officers. The purpose of this investigation was to develop and evaluate a novel thoracic support designed for mobile police officers. Fourteen participants (7 male, 7 female) attended two 120-min driving simulations using a Crown Victoria Interceptor seat and the same seat equipped with a surface mounted thoracic support. Time-varying spine postures, seat pressures and ratings of discomfort were measured. Averaged discomfort values were low (less than 10\u00a0mm of a possible 100\u00a0mm) for both seating conditions. The postures in the thoracic support condition were more similar to non-occupational driving without occupational equipment than the Crown Victoria seating condition. The reduction in pressure area at the low back with the thoracic support has the potential to reduce discomfort reporting in officers compared to a standard vehicle package."]},
{"title": "Overweight, obesity and work functioning: The role of working-time arrangements", "highlights": ["We examine how obese workers function at work and the role of working-time arrangements.", "We assesse work functioning with the Work-Role Functioning Questionnaire.", "Obesity is associated with lower work functioning scores for physical demands in all workers.", "In shift workers, obesity is also associated with lower work functioning for output demands.", "In day and on-call workers, obesity is not associated with lower work functioning and work demands."], "abstract": ["Obesity is associated with productivity loss, but little is known about how obese workers function at work and also the role of working-time arrangements on this association is lacking. Therefore, the aim of this study was to examine the association of overweight and obesity with work functioning (WF), and to determine whether the associations differ between workers with different working-time arrangements.", "A cross-sectional study was conducted within the sampling frame of the \u2018Shift Your Work\u2019 study that examined the effect of irregular working-times in relation to health and functioning at work. We included N\u00a0=\u00a0622 Dutch employees, of which N\u00a0=\u00a0384 (62%) were shift-workers, N\u00a0=\u00a0171 (27%) on-call workers and N\u00a0=\u00a067 (11%) day-workers. Overweight and obesity were defined as BMI 25\u201330 and \u226530, respectively. WF was assessed using the Work-Role Functioning Questionnaire.", "The prevalences of overweight and obesity were 48% and 10% in all workers, 49% and 11% in shift-workers, 45% and 10% in on-call workers, and 49% and 6% in day workers, respectively. In all workers, obesity was associated with lower WF scores for physical demands (adjusted estimate, aB\u00a0=\u00a0\u22125.5). In shift-workers, obesity was associated with lower WF scores for output and physical demands (aB\u00a0=\u00a0\u22128.8 and\u00a0\u22126.8, respectively). In day and on-call workers, overweight and obesity were not associated with WF.", "Overweight and obesity are highly prevalent in the working population. Obesity might reduce the executive function performance beyond physical limitations, and limit the ability to accomplish tasks successfully, especially in shift workers."]},
{"title": "Rasmussen's legacy in the great outdoors: A new incident reporting and learning system for led outdoor activities", "highlights": ["Rasmussen's risk management framework and Accimap method are popular in academic circles.", "However, widespread adoption in practice has not yet been achieved.", "We present a new incident reporting and learning system underpinned by the framework and Accimap.", "An analysis of the data from the first 3 months use of the system is presented.", "The analysis shows contributory factors and relationships across 5 out of the 6 levels of the framework."], "abstract": ["Jens Rasmussen's seminal risk management framework and accompanying Accimap method have become highly popular in safety science circles. Despite this, widespread adoption of the model and method in practice has not yet been achieved. This paper describes a project involving the development and implementation of an incident reporting and learning system underpinned by Rasmussen's risk management framework and Accimap method. The system was developed for the led outdoor activity sector in Australia to enable reporting and analysis of injuries and near miss incidents, with the aim of supporting the development of more effective countermeasures. An analysis of the data derived from the first 3 months use of the system by 43 organisations is presented. The outputs provide an in-depth Accimap-based analysis of all incidents reported by participating organisations over the 3 month period. In closing, the importance of developing usable domain specific tools to support translation of Ergonomics theory and methods in practice is discussed."]},
{"title": "Application of a human factors classification framework for patient safety to identify precursor and contributing factors to adverse clinical incidents in hospital", "highlights": ["The HFCF for patient safety identified patterns in the causation of clinical incidents.", "Human error played a leading causal role in clinical incidents, with error type varying by type of action.", "Targeted approaches to prevention of incidents, based on an understanding of how and why they occur, are needed."], "abstract": ["This study aimed to identify temporal precursor and associated contributing factors for adverse clinical incidents in a hospital setting using the Human Factors Classification Framework (HFCF) for patient safety. A random sample of 498 clinical incidents were reviewed. The framework identified key precursor events (PE), contributing factors (CF) and the prime causes of incidents. Descriptive statistics and correspondence analysis were used to examine incident characteristics. Staff action was the most common type of PE identified. Correspondence analysis for all PEs that involved staff action by error type showed that rule-based errors were strongly related to performing medical or monitoring tasks or the administration of medication. Skill-based errors were strongly related to misdiagnoses. Factors relating to the organisation (66.9%) or the patient (53.2%) were the most commonly identified CFs. The HFCF for patient safety was able to identify patterns of causation for the clinical incidents, highlighting the need for targeted preventive approaches, based on an understanding of how and why incidents occur."]},
{"title": "The science behind codes and standards for safe walkways: Changes in level, stairways, stair handrails and slip resistance", "highlights": ["Walkway safety standards should be substantially based on scientific research.", "We identify portions of such standards that are supported by empirical data.", "We identify parts of such standards that could benefit from more empirical data."], "abstract": ["Walkway codes and standards are often created through consensus by committees based on a number of factors, including historical precedence, common practice, cost, and empirical data. The authors maintain that in the formulation of codes and standards that impact pedestrian safety, the results of pertinent scientific research should be given significant weight. This article examines many elements of common walkway codes and standards related to changes in level, stairways, stair handrails, and slip resistance. It identifies which portions are based on or supported by empirical data; and which could benefit from additional scientific research. This article identifies areas in which additional research, codes, and standards may be beneficial to enhance pedestrian safety."]},
{"title": "Cultural ergonomics in interactional and experiential design: Conceptual framework and case study of the Taiwanese twin cup", "highlights": ["The study proposes a framework for cultural ergonomics in product design.", "We study the cultural ergonomics of the ", " as a case study.", "The ", " is used to demonstrate how to design cultural products in the study."], "abstract": ["Cultural ergonomics is an approach that considers interaction- and experience-based variations among cultures. Designers need to develop a better understanding of cultural ergonomics not just to participate in cultural contexts but also to develop interactive experiences for users. Cultural ergonomics extends our understanding of cultural meaning and our ability to utilize such understanding for design and evaluate everyday products. This study aims to combine cultural ergonomics and interactive design to explore human\u2013culture interaction in user experiences. The ", " is a typical Taiwanese aboriginal cultural object. This study examined the cultural meaning and operational interface of the ", ", as well as the scenarios in which it is used in interaction and user experiences. The results produced a cultural ergonomics interface for examining the manner in which designers communicate across cultures as well as the interweaving of design and culture in the design process."]},
{"title": "Context-based presets for lighting setup in residential space", "highlights": ["Multilateral relationship among activity, affect, and lighting setup was explored.", "Twenty context-based lighting setup presets were derived for residential space.", "Seven affect factors were derived for describing emotions of colored illumination.", "A set of testbed was prototyped: a smartphone application and a model house."], "abstract": ["This study aims to derive context-based lighting setup presets in residential space by exploring the multilateral relationships among household activities, affects, and lighting setups. Three procedures were involved: First, sixty affective words were evaluated through which seven affect factors were extracted to facilitate the evaluation of colored illumination in the subsequent experiment. Second, in the user study, seven affect factors and thirty household activities were used to evaluate 147 lighting setups extracted from combinations of twelve hues, six illuminance levels, and three purity levels. As a result, twenty lighting setup presets were derived that were not only activity-based, but affect-based as well. Lastly, the twenty presets were prototyped as a set of testbed to further explore potentials and limitations. This study demonstrates that intuitive, context-based presets can help users explore the effects of colored illumination in creating a diverse range of user experiences."]},
{"title": "The role of organisational support in teleworker wellbeing: A socio-technical systems approach", "highlights": ["Organisational social support was associated with positive wellbeing outcomes.", "Teleworker support was associated with increased job satisfaction and reduced psychological strain.", "Both forms of organisational support reduced teleworker social isolation.", "Social isolation mediated the relationship between organisational support and telework outcomes.", "Differences in some of these relationships were observed between low-intensity and hybrid teleworkers."], "abstract": ["The prevalence of telework and other forms of mobile working enabled by digital technology is increasing markedly. Following a socio-technical systems approach, this study aims to examine the role of organisational social support and specific support for teleworkers in influencing teleworker wellbeing, the mediating role of social isolation, potentially resulting from a person-environment mismatch in these relationships, and possible differences in these relationships between low-intensity and hybrid teleworkers. Teleworkers' (n\u00a0=\u00a0804) perceptions of support and telework outcomes (psychological strain, job satisfaction, and social isolation) were collected using an on-line survey of teleworking employees distributed within 28 New Zealand organisations where knowledge work was undertaken. Organisational social support and teleworker support was associated with increased job satisfaction and reduced psychological strain. Social isolation mediated the relationship between organisational social support and the two outcome variables, and some differences were observed in the structural relationships for hybrid and low-intensity teleworker sub-samples. These findings suggest that providing the necessary organisational and teleworker support is important for enhancing the teleworker-environment fit and thereby ensuring desirable telework outcomes."]},
{"title": "Feedback has a positive effect on cognitive function during total sleep deprivation if there is sufficient time for it to be effectively processed", "highlights": ["We examined performance on the psychomotor vigilance task\u2014a serial simple reaction time test\u2014during 28 h of sustained wake.", "The provision of feedback and the length of the inter-stimulus interval (ISI) interacted to affect performance.", "Feedback after each response resulted in better performance when the ISI ranged from 2\u201310 s.", "Feedback after each response did not affect performance when the ISI ranged from 1\u20135 s.", "When the ISI is long, it is possible that feedback provides motivation.", "When the ISI is short, it is likely that there is not time to both attend to feedback and prepare for the next stimulus."], "abstract": ["This study examined whether the provision of feedback and the interval between successive stimuli interact to affect performance on a serial simple reaction time test during sleep deprivation. Sixteen participants (9 female, 7 male, aged 18\u201327\u00a0yr) completed four versions of the 5-min psychomotor vigilance task for a handheld personal digital assistant (PalmPVT) every 2\u00a0h during 28\u00a0h of sustained wakefulness. The four versions differed in terms of whether or not they provided feedback immediately after each response, and whether the inter-stimulus intervals (ISIs) were long (2\u201310\u00a0s) or short (1\u20135\u00a0s). Cognitive function was assessed using reciprocal response time and percentage of responses that were lapses (i.e., had a response time\u00a0\u2265\u00a0500\u00a0ms). Data were analysed using repeated measures ANOVA with three within-subjects factors: test session, feedback, and ISI. For both measures, the only significant interaction was between feedback and ISI. Cognitive function was enhanced by feedback when the ISIs were long because it provided motivation. Cognitive function was not affected by feedback when the ISIs were short because there was insufficient time to both attend to the feedback and prepare for the subsequent stimulus."]},
{"title": "Age-related differences in dynamic balance control during stair descent and effect of varying step geometry", "highlights": ["Longer run length and shorter riser height reduces forward upper body tilt angles.", "Longer run length provides larger margins of stability for young and older adults.", "Results explain biomechanical factors underlying increased falls risk on shorter steps.", "Results highlight need for safer stair standards to minimize risk of falls."], "abstract": ["The incidence of stairway falls and related injuries remains persistently high; however, the risk of stair injuries could be reduced through improved stairway design. The current study investigated dynamic balance control during stair descent and the effects of varying the step geometry. Data were collected from 20 healthy young and 20 older adults as they descended three staircases (riser heights of 7, 7.5 and 8 inches (178, 190 and 203\u00a0mm, respectively)). At each riser height, the tread run length was varied between 8 and 14 inches (203\u00a0mm and 356\u00a0mm) in one-inch (25\u00a0mm) increments. Kinematic data provided measures of segmental and whole-body dynamic control. Results demonstrated that older adults had greater lateral tilt of the upper body than young adults, but actually had larger margins of stability than the young in the antero-posterior direction as a result of their slower cadence. Nonetheless, for both age groups, the longer run lengths were found to provide the largest margins of stability. In addition, increase in run length and decrease in riser height tended to reduce forward upper body tilt. These results help to explain the underlying biomechanical factors associated with increased risk of falls and the relationship with step geometry. Considering the importance of stair ambulation in maintaining independence and activity in the community, this study highlights the definite need for safer stair design standards to minimize the risk of falls and increase stair safety across the lifespan."]},
{"title": "Photograph-based ergonomic evaluations using the Rapid Office Strain Assessment (ROSA)", "highlights": ["We examined the use of photographs to perform ROSA evaluations of office workstations.", "Photo assessment interrater reliability ranged from fairly good to excellent, and was comparable to previous results.", "Photo-assessments had more \u201cfalse-positives\u201d, which could lead to unnecessary additional ergonomic interventions.", "Sources of error include the parallax effect, and boundary errors in postural binning."], "abstract": ["The Rapid Office Strain Assessment (ROSA) was developed to assess musculoskeletal disorder (MSD) risk factors for computer workstations. This study examined the validity and reliability of remotely conducted, photo-based assessments using ROSA. Twenty-three office workstations were assessed on-site by an ergonomist, and 5 photos were obtained. Photo-based assessments were conducted by three ergonomists. The sensitivity and specificity of the photo-based assessors' ability to correctly classify workstations was 79% and 55%, respectively. The moderate specificity associated with false positive errors committed by the assessors could lead to unnecessary costs to the employer. Error between on-site and photo-based final scores was a considerable \u223c2 points on the 10-point ROSA scale (RMSE\u00a0=\u00a02.3), with a moderate relationship (\u03c1\u00a0=\u00a00.33). Interrater reliability ranged from fairly good to excellent (ICC\u00a0=\u00a00.667\u20130.856) and was comparable to previous results. Sources of error include the parallax effect, poor estimations of small joint (e.g. hand/wrist) angles, and boundary errors in postural binning. While this method demonstrated potential validity, further improvements should be made with respect to photo-collection and other protocols for remotely-based ROSA assessments."]},
{"title": "Aircraft passenger comfort experience: Underlying factors and differentiation from discomfort", "highlights": ["The eight experiential aspects of passenger comfort are validated including \u2018peace of mind\u2019, \u2018physical wellbeing\u2019, \u2018proxemics\u2019, etc.", "The factors impacting passenger comfort experience may be considered the same as those impacting discomfort.", "Aircraft passengers may not experience privacy and control on a conscious level.", "It is suggested to evaluate the overall comfort experience of passengers using one single rating scale ranging from extreme discomfort to extreme comfort."], "abstract": ["Previous studies defined passengers' comfort based on their concerns during the flight and a set of eight experiential factors such as \u2018peace of mind\u2019, \u2018physical wellbeing\u2019, \u2018pleasure\u2019, etc. One Objective of this paper was to determine whether the factors underlying the passengers' experience of comfort differ from those of discomfort. Another objective was to cross-validate those factors. In the first study, respondents provided written reports of flight comfort and discomfort experiences separately and gave ratings on the impact of the eight factors on each experience. Follow up interviews were also conducted. Significant difference was found between comfort and discomfort ratings for two factors of \u2018pleasure\u2019, denoted by one's concern for stimulation, ambience and exceeded expectations, and \u2018physical wellbeing\u2019 characterized in terms of bodily support and energy. However, there were no significant differences between the comfort and discomfort ratings on the other six factors. The evidence does not support the proposition that passenger comfort and discomfort are underline by different sets of factors. It is therefore suggested that the evaluation of overall passenger comfort experience, as a whole, employ one spectrum ranging from extreme comfort to discomfort. In study two, a pool of comfort descriptors was collected. Those that were less relevant to passenger comfort were eliminated in a number of steps. Factor analysis was used to classify the remaining descriptors, using respondents' ratings on their potential impact on passenger comfort. Seven factors corresponded to the pre-determined passenger comfort factors from previous research, validating those with an exception of \u2018proxemics\u2019 (concerning one's privacy and control over their situation) but it was argued that this is due to the nature of the factor itself, which is context dependent and generally perceived unconsciously."]},
{"title": "Ecological interface design and system safety: One facet of Rasmussen's legacy", "highlights": ["The focus of this manuscript is on CSE/EID and the role\u00a0that it may play in improving system safety.", "The decision making and problem solving literatures are reviewed for insights regarding effective decision support design.", "Analytical tools (i.e., abstraction and aggregation hierarchies) for conducting work domain analyses are described.", "Concrete examples are used to illustrate the need for ecological displays to span all levels of the abstraction hierarchy.", "The role of graphical displays in supporting decision making, problem solving and system safety is explored in detail."], "abstract": ["The focus of this manuscript is on cognitive systems engineering/ecological interface design (CSE/EID) and the role that this framework may play in improving system safety. First, the decision making and problem solving literatures are reviewed with an eye towards informational needs that are required to support these activities. The utility of two of Rasmussen's analytical tools (i.e., the abstraction and aggregation hierarchies) in conducting work domain analyses to identify associated information (i.e., categories and relationships) is discussed. The importance of designing ecological displays and interfaces that span the informational categories in the abstraction hierarchy is described and concrete examples are provided. The potential role that ecological interfaces can play in providing effective decision making (i.e., preventing accidents) and problem solving (i.e., dealing with accidents) support, thereby improving the safety of our socio-technical systems, is explored."]},
{"title": "Using analytic network process for evaluating mobile text entry methods", "highlights": ["Presents a multi-criteria decision-making approach for preference evaluation of text entry methods.", "QWERTY is the most preferred method and arrangement of keys is the most preferred criterion for evaluation.", "The proposed method can be extended to user experience evaluations."], "abstract": ["This paper highlights a preference evaluation methodology for text entry methods in a touch keyboard smartphone using analytic network process (ANP). Evaluation of text entry methods in literature mainly considers speed and accuracy. This study presents an alternative means for selecting text entry method that considers user preference. A case study was carried out with a group of experts who were asked to develop a selection decision model of five text entry methods. The decision problem is flexible enough to reflect interdependencies of decision elements that are necessary in describing real-life conditions. Results showed that QWERTY method is more preferred than other text entry methods while arrangement of keys is the most preferred criterion in characterizing a sound method. Sensitivity analysis using simulation of normally distributed random numbers under fairly large perturbation reported the foregoing results reliable enough to reflect robust judgment. The main contribution of this paper is the introduction of a multi-criteria decision approach in the preference evaluation of text entry methods."]},
{"title": "Assessments of risky driving: A Go/No-Go simulator driving task to evaluate risky decision-making and associated behavioral patterns", "highlights": ["High-risk and low-risk drivers in G/NG-SDT were characterized by the number of Go decisions.", "High-risk drivers showed more violations, as well as higher DBQ-violations scores, and more BART pumps.", "High-risk drivers demonstrated smaller safety margins, and more reckless longitudinal and lateral controls.", "Velocity, accelerator and rotation were consistently distinguishable between high and low risk drivers in all situations."], "abstract": ["This study sought to develop and validate a Go/No-Go Simulator Driving Task (G/NG-SDT) to evaluate driver risky decision-making and associated behavioral assessments at a situation-specific level. Eighty-four participants were instructed to complete a route in as short time as possible, but avoiding any violations or crashes. To achieve this aim, they had to decide to go or wait in the dilemma scenes, paired with the baseline scenes in several scenarios. High-risk drivers with more Go decisions demonstrated more violations, in both simulator tasks and real road driving, as well as higher scores of Driving Behavior Questionnaire (DBQ) violations and more Balloon Analogue Risk Task (BART) pumps. These high-risk drivers also showed distinguishable behavioral patterns in simulator driving, moderated by the specific driving situations (e.g. scenario and scene). Several behavior assessments were consistently distinguishable in all tested situations, qualified as robust indictors to predict risk-taking in more general driving situations."]},
{"title": "Variable Message Signs for road tunnel emergency evacuations", "highlights": ["The Theory of Affordances is proposed to evaluate Variable Message Signs (VMS).", "A questionnaire study is used to investigate VMS for tunnel evacuation.", "VMS including amber text, flashing lights and emergency exit symbol results as best."], "abstract": ["This paper investigates the design of Variable Message Signs (VMS) as a way-finding aid for road tunnel emergency evacuations. The use of the Theory of Affordances is suggested to provide recommendations on the design of VMS. A preliminary evaluation of 11 selected VMS systems was performed and 6 of them were further evaluated using an affordance-based within subject stated-preference questionnaire administered to a sample of 62 participants. Results are used to provide recommendations on the characteristics of the VMS systems, such as (1) size of the sign (large or small); (2) use of flashing lights; (3) colour scheme; (4) message coding (i.e., text, pictograms or a combination of them). The best performing VMS features for road tunnel emergency evacuation included the use of larger signs, flashing lights, the combination of emergency exit pictorial symbol in green in one panel and text in amber in the other panel."]},
{"title": "Concentration on performance with P300-based BCI systems: A\u00a0matter of interface features", "highlights": ["The user's capacity to concentrate can affect his/her performance with a P300-based speller.", "This study was to test this hypothesis using three different interfaces.", "The d2 test was applied to assess attention and concentration.", "The performance of users with lower concentration level can be improved by providing BCIs with more interactive interfaces."], "abstract": ["People who suffer from severe motor disabilities have difficulties to communicate with others or to interact with their environment using natural, i.e., muscular channels. These limitations can be overcome to some extent by using brain\u2013computer interfaces (BCIs), because such systems allow users to communicate on the basis of their brain activity only. Among the several types of BCIs for spelling purposes, those that rely on the P300 event related potential\u2014P300-based spellers\u2014are chosen preferentially due to their high reliability. However, they demand from the user to sustain his/her attention to the desired character over a relatively long period of time. Therefore, the user's capacity to concentrate can affect his/her performance with a P300-based speller. The aim of this study was to test this hypothesis using three different interfaces: one based on the classic P300 speller paradigm, another also based on that speller but including a word predictor, and a third one that was based on the T9 interface developed for mobile phones. User performance was assessed by measuring the time to complete a spelling task and the accuracy of character selection. The d2 test was applied to assess attention and concentration. Sample (N\u00a0=\u00a014) was divided into two groups basing on of concentration scores. As a result, performance was better with the predictor-enriched interfaces: less time was needed to solve the task and participants made fewer errors (p\u00a0<\u00a0.05). There were also significant effects of concentration (p\u00a0<\u00a0.05) on performance with the standard P300 speller. In conclusion, the performance of those users with lower concentration level can be improved by providing BCIs with more interactive interfaces. These findings provide substantial evidence in order to highlight the impact of psychological features on BCI performance and should be taken into account for future assistive technology systems."]},
{"title": "Seat pan and backrest pressure distribution while sitting in office chairs", "highlights": ["Seating pressure distribution is strongly influenced by the surface area properties.", "Chair-specific calibration of the sensor mat is essential.", "The pressure distribution of the seat pan can be explained using four parameters.", "The pressure distribution of the backrest can be explained using three parameters."], "abstract": ["Nowadays, an increasing amount of time is spent seated, especially in office environments, where sitting comfort and support are increasingly important due to the prevalence of musculoskeletal disorders. The aim of this study was to develop a methodology for chair-specific sensor mat calibration, to evaluate the interconnections between specific pressure parameters and to establish those that are most meaningful and significant in order to differentiate pressure distribution measures between office chairs.", "The shape of the exponential calibration function was highly influenced by the material properties and geometry of the office chairs, and therefore a chair-specific calibration proved to be essential. High correlations were observed between the eight analysed pressure parameters, whereby the pressure parameters could be reduced to a set of four and three parameters for the seat pan and the backrest respectively. In order to find significant differences between office chairs, gradient parameters should be analysed for the seat pan, whereas for the backrest almost all parameters are suitable."]},
{"title": "A human factors approach to snowsport safety: Novel research on pediatric participants' behaviors and head injury risk", "highlights": ["Interdisciplinary, human factors research exploring real-time: snowsport behaviors and head acceleration frequency and severity.", "Data collection via the combination of technologies incorporating GPS data, tri-axial accelerometers, and supported by survey data.", "Results indicate that pediatric snowsport participants regularly exceed the design and test parameters of snowsport helmets."], "abstract": ["This study applied a human factors approach to snowsport resort systems to contribute to the understanding of the incidence and severity of pediatric snowsport head accelerations.", "Previous research indicates low magnitude head accelerations are common among snowsport participants. This study adds to the knowledge of snowsport safety by measuring aspects of participants' snowsport behavior and linking this with head acceleration data.", "School-aged students (n\u00a0=\u00a0107) wore telemetry-fitted helmets and Global Positioning System (GPS) devices during snowsport activity. Data was collected over 159 sessions (total hours 701). Head accelerations recorded by the telemetry units were compared with GPS-generated data.", "This study found speeds attained normally exceed the testing rating for which helmets are designed; lower rates of head accelerations compared to earlier studies and that when head accelerations did occur they were generally below the threshold for concussions.", "Pediatric snowsport head accelerations are rare and are generally of low magnitude. Those most at risk of a head acceleration >40\u00a0g were male snowboarders. Given the recorded speeds in first time participants, increased targeting of novice snowsport participants to encourage education about the use of protective equipment, including helmets, is warranted. Post event recall was not a good indicator of having experienced a head impact. Consideration should be given to raising the standard design speed testing for snowsport helmet protective devices to reflect actual snowsport behaviors."]},
{"title": "Correction factors for assessing immersion suits under harsh conditions", "highlights": ["Immersion performance in marine accidents may differ from that measured in the lab.", "Our participants were immersed in harsh environments consisting of wind and waves.", "We measured immersion suit insulation change from calm water to wind and waves.", "Wind and waves caused significantly greater decrease in insulation.", "Correction factors for harsh environments were developed to test suits in calm water."], "abstract": ["Many immersion suit standards require testing of thermal protective properties in calm, circulating water while these suits are typically used in harsher environments where they often underperform. Yet it can be expensive and logistically challenging to test immersion suits in realistic conditions. The goal of this work was to develop a set of correction factors that would allow suits to be tested in calm water yet ensure they will offer sufficient protection in harsher conditions. Two immersion studies, one dry and the other with 500\u00a0mL of water within the suit, were conducted in wind and waves to measure the change in suit insulation. In both studies, wind and waves resulted in a significantly lower immersed insulation value compared to calm water. The minimum required thermal insulation for maintaining heat balance can be calculated for a given mean skin temperature, metabolic heat production, and water temperature. Combining the physiological limits of sustainable cold water immersion and actual suit insulation, correction factors can be deduced for harsh conditions compared to calm. The minimum in-situ suit insulation to maintain thermal balance is 1.553\u20130.0624\u00b7T", "\u00a0+\u00a00.00018\u00b7T", " for a dry calm condition. Multiplicative correction factors to the above equation are 1.37, 1.25, and 1.72 for wind\u00a0+\u00a0waves, 500\u00a0mL suit wetness, and both combined, respectively. Calm water certification tests of suit insulation should meet or exceed the minimum in-situ requirements to maintain thermal balance, and correction factors should be applied for a more realistic determination of minimum insulation for harsh conditions."]},
{"title": "Designing Fatigue Warning Systems: The perspective of professional drivers", "highlights": ["Both truck and taxi drivers are experiencing heavy driving fatigue, and they have a positive attitude toward Fatigue Warning Systems (FWSs).", "Professional drivers hope FWSs could not only monitor and warn their fatigue but also somewhat relieve their fatigue before they could stop and rest.", "Participants prefer auditory warnings, as opposed to visual, vibrotactile or electric stimuli. Specifically, truck drivers prefer alarm warnings, while taxi drivers prefer verbal warnings.", "Warning patterns, concerns regarding FWSs are also discussed. The results would provide valuable information for companies who wish to develop FWSs for professional drivers."], "abstract": ["Professional drivers have been characterized as experiencing heavy fatigue resulting from long driving time in their daily work. This study aimed to explore the potential demand of Fatigue Warning Systems (FWSs) among professional drivers as a means of reducing the danger of fatigue driving and to examine their opinions regarding the design of FWSs. Six focus groups with 35 participants and a questionnaire survey with 600 respondents were conducted among Chinese truck and taxi drivers to collect qualitative and quantitative data concerning the current situation of fatigue driving and opinions regarding the design of FWSs. The results revealed that both truck and taxi drivers had a positive attitude toward FWSs, and they hoped this system could not only monitor and warn them regarding their fatigue but also somewhat relieve their fatigue before they could stop and rest. As for warning signals, participants preferred auditory warnings, as opposed to visual, vibrotactile or electric stimuli. Interestingly, it was proposed that verbal warnings involving the information regarding consequences of fatigue driving or the wishes of drivers' family members would be more effective. Additionally, different warning patterns, including graded, single and continuous warnings, were discussed in the focus group. Finally, the participants proposed many other suggestions, as well as their concerns regarding FWSs, which will provide valuable information for companies who wish to develop FWSs for professional drivers."]},
{"title": "Evaluation of load carriage systems used by active duty police officers: Relative effects on walking patterns and perceived comfort", "highlights": ["We compared two different load carriage designs for uniformed police.", "Gait kinematics in police are significantly altered with different load carriage designs.", "Police have a personal preference for a load bearing vest design.", "Knowledge gained from this research may help in improving design of load carriage systems for police."], "abstract": ["This study aimed to examine the effects of two different load carriage systems on gait kinematics, temporospatial gait parameters and self-reported comfort in Swedish police.", "21 active duty police officers were recruited for this crossover study design. Biomechanical and self-report data was collected on two testing occasions. On occasion 1, three dimensional kinematic data was collected while police wore a/no equipment (control), b/their standard issues belt and ballistic protection vest and c/a load bearing vest with ballistic protection vest. Police then wore the load bearing vest for a minimum of 3 months before the second testing occasion.", "The load bearing vest was associated with a significant reduction in range of motion of the trunk, pelvis and hip joints. Biomechanical changes associated with the load bearing vest appeared to reduce with increased wear time. In both the standard issue belt condition and the load bearing vest condition, police walked with the arms held in a significantly greater degree of abduction. Self-report data indicated a preference for the load bearing vest.", "The two load carriage designs tested in this study were found to significantly alter gait kinematics. The load bearing vest design was associated with the greatest number of kinematic compensations however these reduced over time as police became more accustomed to the design. Results from this study do not support selection of one load carriage design over the other and providing individuals with the option to choose a load carriage design is considered appropriate."]},
{"title": "Effects of overhead work configuration on muscle activity during a simulated drilling task", "highlights": ["We examine influences on muscular demand in a drilling task.", "Encourage bilateral tasks.", "Forward force exertions require less muscular demand.", "During upwards exertions avoid reaching.", "Complete tasks seated when possible."], "abstract": ["Overhead work is a known catalyst for occupational shoulder injury. Industrial workers must often adopt awkward overhead postures and loading profiles to complete required tasks, potentially elevating injury risk. This research examined the combined influence of multiple overhead working parameters on upper extremity muscular demands for an industrial drilling application.", "Twenty-two right-handed males completed 24 unilateral and bilateral overhead work exertions stratified by direction (upward, forward), point of force application (15, 30 and 45\u00a0cm in front of the body), and whole-body posture (seated, standing).", "The dependency of electromyographic (EMG) activity on several factors was established. Significant two-way interactions existed between point of force application and direction (p\u00a0<\u00a00.0001) and direction and whole body posture (p\u00a0<\u00a00.0001). An average increase in muscular activity of 6.5% maximal voluntary contraction (MVC) occurred for the contralateral limb when the bilateral task was completed, compared to unilateral tasks, with less than a 1% MVC increase for the active limb.", "These findings assist evidence-based approaches to overhead tasks, specifically in the construction industry. A bilateral task configuration is recommended to reduce glenohumeral stability demands. As well, particularly for tasks with a far reach distance, design tasks to promote a forward directed exertion. The considerable inter-subject variability suggests that fixed heights are not ideal, and should be avoided, and where this is not possible reaches should be reduced."]},
{"title": "Validity and reliability of pressure-measurement insoles for vertical ground reaction force assessment in field situations", "highlights": ["Validity and reliability of medilogic", " insoles for force measurements were tested.", "Static tests showed a high reliability, but also a creeping of output force signal.", "RMSE's in field situations were in average from 6.6% to 17.7% (besides kneeling).", "Strongly bending of medilogic", " insoles (r\u00a0<\u00a020\u00a0mm) leads to high measurement errors.", "Frequent calibration and correction algorithms are necessary for high validity."], "abstract": ["This study aimed to test the validity and reliability of pressure-measurement insoles (medilogic", " insoles) when measuring vertical ground reaction forces in field situations. Various weights were applied to and removed from the insoles in static mechanical tests. The force values measured simultaneously by the insoles and force plates were compared for 15 subjects simulating work activities. Reliability testing during the static mechanical tests yielded an average interclass correlation coefficient of 0.998. Static loads led to a creeping pattern of the output force signal. An individual load response could be observed for each insole. The average root mean square error between the insoles and force plates ranged from 6.6% to 17.7% in standing, walking, lifting and catching trials and was 142.3% in kneeling trials. The results show that the use of insoles may be an acceptable method for measuring vertical ground reaction forces in field studies, except for kneeling positions."]},
{"title": "The effect of rest break schedule on acute low back pain development in pain and non-pain developers during seated work", "highlights": ["Pain and non-pain developers engaged in prolonged sitting under four conditions.", "Pain developers showed clinically relevant increases in pain in all conditions.", "Non-pain developers remained at stable low-levels of pain in all conditions.", "Productivity was maintained in both groups in all four sessions.", "Changes in muscle activation were not observed in pain or non pain developers."], "abstract": ["A significant portion of the population (25\u201350%) is known to develop acute low back pain (LBP) within a bout of prolonged sitting. Previous research has supported the use of frequent rest breaks, from seated office work, in order to reduce self-reported LBP, however, there is limited consensus about the recommended frequency and duration of rest breaks. This may be due to the limited consideration of individual differences in acute LBP development. The purpose of this study was to examine the effect of three different standing rest-break conditions on a group of pain developers (PD) and non-pain developers (NPD) engaged in prolonged seated work. Twenty participants completed four one-hour-long bouts of seated typing: Condition A \u2013 no rest; Condition B \u2013 5\u00a0min\u00a0of standing rest every 30\u00a0min; Condition C \u2013 2.5\u00a0min\u00a0of standing rest every 15\u00a0min; Condition D \u2013 50\u00a0s of standing rest every 5\u00a0min. Self-reported LBP, self-reported mental fatigue and 30-s\u00a0samples of EMG were collected every 10\u00a0min\u00a0throughout each session. Eight out of 20 participants (40%) reported LBP during Condition A (classified as PD). Only PD demonstrated clinically relevant increases in LBP across conditions where Conditions B, C, or D provided some relief, but did not restore pain scores to their original level, prior to sitting. PD and NPD developed mental fatigue equally, with Conditions B and D helping to reduce fatigue. No differences in productivity were observed between conditions or groups and no main effects were observed for muscle activity, median power frequency or co-contraction. These data suggests that frequent, short, standing rest breaks may help to reduce symptoms of LBP, however they are only a temporary solution as PD still developed clinically important LBP, even with frequent rest breaks."]},
{"title": "An integrated biomechanical modeling approach to the ergonomic evaluation of drywall installation", "highlights": ["Work sampling, computer simulation, biomechanical modeling were integrated.", "A sensitivity analysis was conducted to demonstrate robustness.", "The integrated approach provided an applicable examination of physical loads."], "abstract": ["Three different methodologies: work sampling, computer simulation and biomechanical modeling, were integrated to study the physical demands of drywall installation. PATH (Posture, Activity, Tools, and Handling), a work-sampling based method, was used to quantify the percent of time that the drywall installers were conducting different activities with different body segment (trunk, arm, and leg) postures. Utilizing Monte-Carlo simulation to convert the categorical PATH data into continuous variables as inputs for the biomechanical models, the required muscle contraction forces and joint reaction forces at the low back (L4/L5) and shoulder (glenohumeral and sternoclavicular joints) were estimated for a typical eight-hour workday. To demonstrate the robustness of this modeling approach, a sensitivity analysis was conducted to examine the impact of some quantitative assumptions that have been made to facilitate the modeling approach. The results indicated that the modeling approach seemed to be the most sensitive to both the distribution of work cycles for a typical eight-hour workday and the distribution and values of Euler angles that are used to determine the \u201cshoulder rhythm.\u201d Other assumptions including the distribution of trunk postures did not appear to have a significant impact on the model outputs. It was concluded that the integrated approach might provide an applicable examination of physical loads during the non-routine construction work, especially for those operations/tasks that have certain patterns/sequences for the workers to follow."]},
{"title": "Intrinsic movement variability at work. How long is the path from motor control to design engineering?", "highlights": ["Intrinsic movement variability is an essential feature of human motion.", "Motor control theories can explain the existence of intrinsic movement variability.", "Task and personal characteristics influence intrinsic movement variability.", "Intrinsic movement variability should be taken into account in workstation design."], "abstract": ["For several years, increasing numbers of studies have highlighted the existence of movement variability. Before that, it was neglected in movement analysis and it is still almost completely ignored in workstation design. This article reviews motor control theories and factors influencing movement execution, and indicates how intrinsic movement variability is part of task completion. These background clarifications should help ergonomists and workstation designers to gain a better understanding of these concepts, which can then be used to improve design tools. We also question which techniques - kinematics, kinetics or muscular activity \u2013 and descriptors are most appropriate for describing intrinsic movement variability and for integration into design tools. By this way, simulations generated by designers for workstation design should be closer to the real movements performed by workers. This review emphasises the complexity of identifying, describing and processing intrinsic movement variability in occupational activities."]},
{"title": "Ergonomics for the inclusion of older workers in the knowledge workforce and a guidance tool for designers", "highlights": ["Approach of Elder workforce inclusion through ergonomics of work environment User Sensitive Design.", "Review of existing source of assistance tools for inclusive design of knowledge work environment.", "Proposal of a theoretical framework to assist designers in the first stage of the inclusive design process.", "Study of relevant information for designers at the first stage of the design process."], "abstract": ["The ageing of the population and the inverted population pyramid is bringing important changes to society as a whole. These changes are associated with the inclusion of an older workforce in knowledge work and the challenge they represent in adapting the work environment accordingly. In order to approach a more universal design of the work environment, industrial designers need support from user-sensitive inclusive design studies. While there are plenty of guidelines and tools containing relevant information, there is a need to develop more appropriate tools for Industrial Designers that cover the initial phase of the design process. This study provides a review of the available tools and guidelines and proposes a theoretical framework intended for developing a design guidance tool for inclusive workstation design."]},
{"title": "Effects of EVA gloves on grip strength and fatigue under low temperature and low pressure", "highlights": ["Both pressure and low temperature would reduce the strength and fatigue of EVA gloves.", "The maximum grip strength and fatigue were influenced significantly in low temperature and pressure.", "For long hours of work, the effect of low temperature is greater than the effect of pressure."], "abstract": ["To study the effects of wearing extravehicular activity (EVA) gloves on grip strength and fatigue in low temperature, low pressure and mixing of two factors (low temperature and low pressure).", "The maximum grip strength and fatigue tests were performed with 10 healthy male subjects wearing gloves in a variety of simulated environments. The data was analysed using the normalization method.", "The results showed that wearing gloves significantly affected the maximum grip strength and fatigue. Pressure (29.6, 39.2\u00a0kPa) had more influence on the maximum grip compared with control group while low temperatures (\u221250,\u00a0\u221290,\u00a0\u2212110\u00a0\u00b0C) had no influence on grip but affected fatigue dramatically. The results also showed that the maximum grip strength and fatigue were influenced significantly in a compound environment.", "Space environment remarkably reduced strength and endurance of the astronauts. However, the effects brought by the compound environment cannot be understood as the superimposition of low temperature and pressure effects."]},
{"title": "Influence of unstable footwear on lower leg muscle activity, volume change and subjective discomfort during prolonged standing", "highlights": ["Lower leg discomfort rating reduced significantly while standing on unstable shoe.", "Lower leg volume change was lesser for unstable shoe during standing.", "The activity level of medial gastrocnemius (MG) muscles (right and left) during standing was greater for unstable shoe.", "The increased variation of muscle activity was only found for Right MG while standing with unstable shoe."], "abstract": ["The present study was an attempt to investigate the effect of unstable footwear on lower leg muscle activity, volume change and subjective discomfort during prolonged standing.", "Ten healthy subjects were recruited to stand for 2\u00a0h in three footwear conditions: barefoot, flat-bottomed shoe and unstable shoe. During standing, lower leg discomfort and EMG activity of medial gastrocnemius (MG) and tibialis anterior (TA) muscles were continuously monitored. Changes in lower leg volume over standing time also were measured.", "Lower leg discomfort rating reduced significantly while subjects standing on unstable shoe compared to the flat-bottomed shoe and barefoot condition. For lower leg volume, less changes also were observed with unstable shoe. The activity level and variation of right MG muscle was greater with unstable shoe compared to the other footwear conditions; however regarding the left MG muscle, significant difference was found between unstable shoe and flat-bottomed shoe only for activity level. Furthermore no significant differences were observed for the activity level and variation of TA muscles (right/left) among all footwear conditions.", "The findings suggested that prolonged standing with unstable footwear produces changes in lower leg muscles activity and leads to less volume changes. Perceived discomfort also was lower for this type of footwear and this might mean that unstable footwear can be used as ergonomic solution for employees whose work requires prolonged standing."]},
{"title": "Supporting productive thinking: The semiotic context for Cognitive Systems Engineering (CSE)", "highlights": ["Pierce's Triadic Model of Semiotics provides context for understanding Rasmussen's work.", "The Triadic Semiotic Model provides common ground for design and evaluation of cognitive systems.", "Rasmussen emphasized human capabilities as a resource for improving system performance.", "Components of Rasmussen's approach are evaluated through the lens of the Triadic Semiotic Model."], "abstract": ["The central thesis of this paper is that Rasmussen framed his approach to Cognitive Systems Engineering from the perspective of a Triadic Semiotic Model. This frame became the context for integrating multiple intellectual threads including Control Theory, Information Theory, Ecological Psychology, and Gestalt Psychology into a coherent theoretical framework. The case is made that the triadic semiotic framework is essential for a complete appreciation of the constructs that were central to Rasmussen's approach: Abstraction Hierarchy, Skill-Rules-Knowledge Model, Ecological Interface Design, and Proactive Risk Management."]},
{"title": "The importance of work organization on workload and musculoskeletal health \u2013 Grocery store work as a model", "highlights": ["Physical workload and musculoskeletal symptoms in grocery store workers are clarified.", "Single work tasks had higher risk for musculoskeletal complaints than mixed one.", "Cashiers had lowest physical loads but highest musculoskeletal complaints.", "The low variation for cashiers might explain high risk for neck/shoulder complaints.", "Combining work tasks with different workloads will improve the work situation."], "abstract": ["We have evaluated the consequences of work organization on musculoskeletal health. Using a postal questionnaire, answered by 1600 female grocery store workers, their main work tasks were identified and four work groups were defined (cashier, picking, and delicatessen work, and a mixed group, who performed a mix of these tasks). The crude odds ratios (ORs) for neck/shoulder complaints were 1.5 (95% CI 1.0\u20132.2), 1.1 (0.7\u20131.5) and 1.6 (1.1\u20132.3), respectively, compared to mixed work. Adjusting for individual and psychosocial factors had no effect on these ORs. For elbows/hands, no significant differences were found. Technical measurements of the workload showed large differences between the work groups. Picking work was the most strenuous, while cashier work showed low loads. Quantitative measures of variation revealed for mixed work high between minutes variation and the highest between/within minutes variation. Combining work tasks with different physical exposure levels increases the variation and may reduce the risk of musculoskeletal complaints."]},
{"title": "Wrist rotations about one or two axes affect maximum wrist strength", "highlights": ["Maximum wrist torques in flexion/extension and radial/ulnar deviation were measured.", "Wrists were neutral or deviated about 1 axis or 2 axes (e.g. flexion and pronation).", "Maximum torque was significantly greater in male subjects compared to females.", "Maximum torque was significantly higher when the wrist was deviated away from neutral.", "These maximum torques and trends should be incorporated into ergonomic software."], "abstract": ["Most wrist strength studies evaluate strength about one axis, and postural deviations about that same axis. The purpose of this study was to determine if wrist posture deviations about one axis (e.g. flexion/extension), or two axes (e.g. flexion/extension and pronation/supination), affect the strength about another axis (e.g. ulnar deviation). A custom-built instrumented handle was used to measure maximum static isometric torque exertions at 18 wrist postures (combinations of flexion/extension, radial/ulnar deviation, and pronation/supination). Ulnar deviation torques were highest when the wrist was in neutral. This pattern was not maintained for the other torque directions; the generated torque tended to be highest when the wrist posture was not neutral. The effects were similar for male and female subjects, although male subjects exerted significantly larger torques in all directions. This study illustrates that there is a complex relationship between wrist posture and maximal wrist torques."]},
{"title": "Thoracic and lumbar posture behaviour in sitting tasks and standing: Progressing the biomechanics from observations to measurements", "highlights": ["Fifty individuals na\u00efve to posture assessment performed three postural conditions:", "10-min computer task, sitting in their \u2018correct\u2019 posture, and standing.", "In the 10-min computer task thoracolumbar and lumbar postures were commonly kyphotic.", "In \u2018correct\u2019 sitting the thoracolumbar angle was similar to that in standing.", "Gender differences in sitting posture were localised to the lumbar region."], "abstract": ["Few studies quantify spinal posture behaviour at both the thoracolumbar and lumbar spinal regions. This study compared spontaneous spinal posture in 50 asymptomatic participants (21 males) during three conditions: 10-min computer task in sitting (participants na\u00efve to the measure), during their perceived \u2018correct\u2019 sitting posture, and standing. Three-dimensional optical tracking quantified surface spinal angles at the thoracolumbar and lumbar regions, and spinal orientation with respect to the vertical. Despite popular belief that lordotic lumbar angles are \u2018correct\u2019 for sitting, this was rarely adopted for 10-min sitting. In 10-min sitting, spinal angles flexed 24(7\u20139)deg at lumbar and 12(6\u20138)deg at thoracolumbar regions relative to standing (", "\u00a0<\u00a00.001). When participants \u2018corrected\u2019 their sitting posture, their thoracolumbar angle\u00a0\u22122(7)deg was similar to the angle in standing\u00a0\u22121(6)deg (", "\u00a0=\u00a01.00). Males were flexed at the lumbar angle relative to females for 10-min sitting, \u2018correct\u2019 sitting and standing, but showed no difference at the thoracolumbar region."]},
{"title": "Self-driving carsickness", "highlights": ["All envisaged scenarios for self-driving cars will lead to an increased risk of motion sickness.", "Motion sickness symptoms will negatively affect safety and user acceptance.", "Self-driving cars cannot simply be thought of as living rooms on wheels.", "Basic perceptual mechanisms need consideration to make self-driving cars a success."], "abstract": ["This paper discusses the predicted increase in the occurrence and severity of motion sickness in self-driving cars. Self-driving cars have the potential to lead to significant benefits. From the driver's perspective, the direct benefits of this technology are considered increased comfort and productivity. However, we here show that the envisaged scenarios all lead to an increased risk of motion sickness. As such, the benefits this technology is assumed to bring may not be capitalised on, in particular by those already susceptible to motion sickness. This can negatively affect user acceptance and uptake and, in turn, limit the potential socioeconomic benefits that this emerging technology may provide. Following a discussion on the causes of motion sickness in the context of self-driving cars, we present guidelines to steer the design and development of automated vehicle technologies. The aim is to limit or avoid the impact of motion sickness and ultimately promote the uptake of self-driving cars. Attention is also given to less well known consequences of motion sickness, in particular negative aftereffects such as postural instability, and detrimental effects on task performance and how this may impact the use and design of self-driving cars. We conclude that basic perceptual mechanisms need to be considered in the design process whereby self-driving cars cannot simply be thought of as living rooms, offices, or entertainment venues on wheels."]},
{"title": "A case study of the implementation of an ergonomics improvement committee in a Brazilian hospital \u2013 Challenges and benefits", "highlights": ["Improvement committees (ICs) can elevate the discussion of ergonomics in the organizational structure.", "ICs allow workers to engage effectively in these processes, in an emancipatory perspective.", "ICs do not have to provide answers for everyone, but should be of a more political nature.", "ICs should be given strategic prominence in order to facilitate ergonomic changes."], "abstract": ["This article discusses the creation of an improvement committee (IC) to implement policies aimed at improving working conditions in a public health institution in the city of S\u00e3o Paulo. Suggestions were proposed for future implementations of this organizational mechanism, pursuant to the presentation of the process of its formation and the main results achieved. The findings led to the conclusion that good outcomes require autonomy and support from management, and the adoption of effective measures to improve and legitimize the improvement committee's existence. Another important issue is facilitating worker involvement and creating a locus for dialog\u00a0among people with different visions within the organization. Thus, two approaches converge: a top-down approach in which policies are defined and improvement actions are actually implemented based on a general outlook of the production and work system, and a bottom-up approach specific to employees who are also engaged in improvement policies and in putting them into practice. It is also possible to point out problems and opportunities arising from actual work situations to the higher levels of management. This kind of approach fits with macroergonomics, because it integrates strategy, organization and work issues. It is possible to discuss the benefits of this approach for companies and provide conditions for workers to engage effectively in these processes. In conclusion, these proposals can be considered from an emancipatory perspective, given that different actors should be able to codetermine working conditions and work content, thus directly influencing their individual and collective experiences. The support and commitment of upper management are essential elements of success in maximizing the effectiveness of this organizational approach."]},
{"title": "Participatory action research in corrections: The HITEC 2 program", "highlights": ["A participatory action research project implemented with correctional officers is described.", "We examine 2 different participatory approaches that use the IDEAS tool.", "Outcomes are varied based on intervention theme and implementation."], "abstract": ["HITEC 2 (Health Improvement through Employee Control 2) is the follow-up to HITEC, a participatory action research (PAR) program that integrates health and work conditions interventions designed by the workforce. HITEC 2 compares intervention programs between two correctional sites, one using a pure workforce level design team and the other using a more structured and time delineated labor-management kaizen effectiveness team.", "HITEC 2 utilizes a seven step participatory Intervention Design and Analysis Scorecard (IDEAS) for planning interventions. Consistent with PAR, process and intervention efficacy measures are developed and administered through workforce representation.", "Participation levels, robustness of participatory structures and sophistication of interventions have increased at each measured interval. Health comparisons between 2008 and 2013 showed increased hypertension, static weight maintenance, and increased \u2018readiness to change\u2019.", "The PAR approaches are robust and sustained. Their long-term effectiveness in this population is not yet clear."]},
{"title": "A literature review on the levels of automation during the years. What are the different taxonomies that have been proposed?", "highlights": ["We present a literature review of the evolution of the levels of autonomy.", "We gather and compare the literature on taxonomies on levels of automation.", "We present the differences between the proposed taxonomies.", "We survey the term adaptive automation, a new trend in the literature on autonomy."], "abstract": ["In this paper we present a literature review of the evolution of the levels of autonomy from the end of the 1950s up until now. The motivation of this study was primarily to gather and to compare the literature that exists, on taxonomies on levels of automation. Technical developments within both computer hardware and software have made it possible to introduce autonomy into virtually all aspects of human-machine systems. The current study, is focusing on how different authors treat the problem of different levels of automation. The outcome of this study is to present the differences between the proposed levels of automation and the various taxonomies, giving the potential users a number of choices in order to decide which taxonomy satisfies their needs better. In addition, this paper surveys deals with the term adaptive automation, which seems to be a new trend in the literature on autonomy."]},
{"title": "Beyond safety outcomes: An investigation of the impact of safety climate on job satisfaction, employee engagement and turnover using social exchange theory as the theoretical framework", "highlights": ["Safety climate perceptions linked with employee outcomes beyond accidents, injuries", "Employee safety climate perceptions were linked to job satisfaction and engagement", "Employee safety climate perceptions were linked to objective turnover rate", "Job satisfaction mediated between safety climate, employee engagement, turnover rate", "Social exchange theory was used to explain the relationships"], "abstract": ["Safety climate, a measure of the degree to which safety is perceived by employees to be a priority in their company, is often implicated as a key factor in the promotion of injury-reducing behavior and safe work environments. Using social exchange theory as a theoretical basis, this study hypothesized that safety climate would be related to employees' job satisfaction, engagement, and turnover rate, highlighting the beneficial effects of safety climate beyond typical safety outcomes. Survey data were collected from 6207 truck drivers from two U.S. trucking companies. The objective turnover rate was collected one year after the survey data collection. Results showed that employees' safety climate perceptions were linked to employees' level of job satisfaction, engagement, and objective turnover rate, thus supporting the application of social exchange theory. Job satisfaction was also a significant mediator between safety climate and the two human resource outcomes (i.e., employee engagement and turnover rate). This study is among the first to assess the impact of safety climate beyond safety outcomes among lone workers (using truck drivers as an exemplar)."]},
{"title": "Work for sustainability: Case studies of Brazilian companies", "highlights": ["Companies considered sustainable includes work-related aspects in the TBL framework.", "Deployment of the strategy does not lead to the full dissemination of sustainability.", "In the work design phase tasks are defined independently of the sustainability discussion.", "Changes in work are not explicitly considered during the introduction of sustainability policies."], "abstract": ["The introduction of strategic corporate sustainability policies is expected to result in the improvement of several issues in companies. One of these issues is work, which should involve greater well-being for workers. Within the context of production engineering, this research connects sustainability and work-related issues, the latter seen in light of the discipline of ergonomics. Based on case studies conducted at four companies considered sustainability benchmarks, we examined how the introduction of the theme of sustainability has influenced work-related issues. The elements analyzed here were the corporate sustainability strategy, organizational practices for deploying the strategy, and the work design phase. The last element is the moment in which work is prescribed in the organization. The results show that, despite the announcement of the inclusion of changes in work, there is not any explicit evidence confirming that such changes are considered as a requirement for corporate sustainability projects."]},
{"title": "Big data and ergonomics methods: A new paradigm for tackling strategic transport safety risks", "highlights": ["Data from on-train data recorders is underused.", "Can be used as an input to existing ergonomics methods.", "300 methods were reviewed and nine leading indicators of human factors risks extracted.", "The proofs-of-concept can all be automated in a full application."], "abstract": ["Big data collected from On-Train Data Recorders (OTDR) has the potential to address the most important strategic risks currently faced by rail operators and authorities worldwide. These risk issues are increasingly orientated around human performance and have proven resistant to existing approaches. This paper presents a number of proof of concept demonstrations to show that long standing ergonomics methods can be driven from big data, and succeed in providing insight into human performance in a novel way. Over 300 ergonomics methods were reviewed and a smaller sub-set selected for proof-of-concept development using real on-train recorder data. From this are derived nine candidate Human Factors Leading Indicators which map on to all of the psychological precursors of the identified risks. This approach has the potential to make use of a significantly underused source of data, and enable rail industry stakeholders to intervene sooner to address human performance issues that, via the methods presented in this paper, are clearly manifest in on-train data recordings. The intersection of psychological knowledge, ergonomics methods and big data creates an important new framework for driving new insights."]},
{"title": "Evaluation of four sensor locations for physical activity assessment", "highlights": ["Physical activity measurements vary substantially based on sensor location.", "Dominant arm measurements displayed the greatest amount of moderate activity.", "Trunk measurements displayed the least amount of moderate activity."], "abstract": ["Direct measurements of physical activity (PA) obtained with inertial measurement units (IMUs) secured to the upper arms and trunk of 36 registered nurses working a full shift were compared to measurements obtained with a commercially-available PA monitor (ActiGraph wGT3X-BT) worn at the waist. Raw accelerations from each device were summarized into PA counts/min and metabolic equivalent (METs) categories using standard definitions. Differences between measurements were examined using repeated measures one-way analyses of variance (ANOVA) and agreement was assessed using Bland-Altman plots. Statistically significant differences were observed between all sensor locations for all PA summary metrics except for between the left and right arm for percentages of work time in the light and moderate counts/min categories. Bland-Altman plots suggested limited agreement between measurements obtained with the IMUs and measurements obtained with the wGT3X-BT waist-worn PA monitor. Results indicate that PA measurements vary substantially based on sensor location."]},
{"title": "Individual differences in BEV drivers\u2019 range stress during first encounter of a critical range situation", "highlights": ["We examined drivers' range stress (RS) during first encounter of critical range.", "Route familiarity and trust in the range estimation system were related to lower RS.", "System competence and personality variables were also related to RS.", "This research has implications for designing strategies aimed at reducing RS.", "This research advances understanding of user experience in low-resource systems."], "abstract": ["It is commonly held that range anxiety, in the form of experienced range stress, constitutes a usage barrier, particularly during the early period of battery electric vehicle (BEV) usage. To better understand factors that play a role in range stress during this critical period of adaptation to limited-range mobility, we examined individual differences in experienced range stress in the context of a critical range situation. In a field experiment, 74 participants drove a BEV on a 94-km round trip, which was tailored to lead to a critical range situation (i.e., small available range safety buffer). Higher route familiarity, trust in the range estimation system, system knowledge, subjective range competence, and internal control beliefs in dealing with technology were clearly related to lower experienced range stress; emotional stability (i.e., low neuroticism) was partly related to lower range stress. These results can inform strategies aimed at reducing range stress during early BEV usage, as well as contribute to a better understanding of factors that drive user experience in low-resource systems, which is a key topic in the field of green ergonomics."]},
{"title": "Walking the line: Understanding pedestrian behaviour and risk at rail level crossings with cognitive work analysis", "highlights": ["Cognitive work analysis (CWA) is applied to pedestrian use of rail level crossings.", "A conceptual model of pedestrian behaviour at RLXs is proposed.", "The use of CWA findings for understanding pedestrian risk at RLXs is demonstrated.", "The CWA findings are used to recommend design improvements for improving safety."], "abstract": ["Pedestrian fatalities at rail level crossings (RLXs) are a public safety concern for governments worldwide. There is little literature examining pedestrian behaviour at RLXs and no previous studies have adopted a formative approach to understanding behaviour in this context. In this article, cognitive work analysis is applied to understand the constraints that shape pedestrian behaviour at RLXs in Melbourne, Australia. The five phases of cognitive work analysis were developed using data gathered via document analysis, behavioural observation, walk-throughs and critical decision method interviews. The analysis demonstrates the complex nature of pedestrian decision making at RLXs and the findings are synthesised to provide a model illustrating the influences on pedestrian decision making in this context (i.e. time, effort and social pressures). Further, the CWA outputs are used to inform an analysis of the risks to safety associated with pedestrian behaviour at RLXs and the identification of potential interventions to reduce risk."]},
{"title": "Ingress and egress motion strategies of elderly and young passengers for the rear seat of minivans with sliding doors", "highlights": ["A quick method was proposed to identify ingress and egress motion strategies.", "Elderly and young passengers differed in their preference of ingress strategies.", "Elderly and young passengers had similar preference of egress strategies.", "The motion strategies seem to be not car dimension dominant but passenger dominant.", "The motion strategies requiring less exertion tend to be adopted more frequently."], "abstract": ["This study investigates the motion strategies performed by elderly and young passengers while entering and exiting the rear seat of minivans with sliding doors. A minivan mock-up was constructed with four adjustable parameters to represent nine different conditions of vehicle geometry. Ten elderly male participants (66.8\u00a0\u00b1\u00a03.8 years old) and ten young male participants (31.5\u00a0\u00b1\u00a06.6 years old) were recruited. Each of them entered and exited the minivan mock-up for five times under each condition, and the motion data were acquired by the optical motion capture system. Based on the criteria derived from previous studies, all motions were automatically categorized into seven ingress motion strategies and seven egress motion strategies. Further, the differences among motion strategies are discussed in terms of vehicle factors and passenger factors, which provide clues for future studies."]},
{"title": "Prevalence and predictors for musculoskeletal discomfort in Malaysian office workers: Investigating explanatory factors for a developing country", "highlights": ["High prevalence of MSD discomfort was reported in a population of Malaysian office workers.", "Predictive models for MSD discomfort in Malaysia were similar to those in developed countries.", "Cultural influences may explain differences in relative importance of MSD discomfort predictors.", "MSD risk management should address cultural differences to be maximally effective."], "abstract": ["Musculoskeletal disorders (MSDs) are a major occupational health issue for workers in developed and developing countries, including Malaysia. Most research related to MSDs has been undertaken in developed countries; given the different regulatory and cultural practices it is plausible that contributions of hazard and risk factors may be different. A population of Malaysian public service office workers were surveyed (N\u00a0=\u00a0417, 65.5% response rate) to determine prevalence and associated predictors of MSD discomfort. The 6-month period prevalence of MSD discomfort was 92.8% (95%CI\u00a0=\u00a090.2\u201395.2%). Akaike's Information Criterion (AIC) analyses was used to compare a range of models and determine a model of best fit. Contributions associated with MSD discomfort in the final model consisted of physical demands (61%), workload (14%), gender (13%), work-home balance (9%) and psychosocial factors (3%). Factors associated with MSD discomfort were similar in developed and developing countries but the relative contribution of factors was different, providing insight into future development of risk management strategies."]},
{"title": "Effects of extended lay-off periods on performance and operator trust under adaptable automation", "highlights": ["We examined the effects of lay-off period and reliability in adaptable automation.", "We took measures of trust, automation reliance, performance, and workload.", "There was no skill decay after the extended lay-off period.", "Operator trust was influenced by system reliability but not by lay-off period.", "Automation reliance was unaffected by lay-off period and reliability."], "abstract": ["Little is known about the long-term effects of system reliability when operators do not use a system during an extended lay-off period. To examine threats to skill maintenance, 28 participants operated twice a simulation of a complex process control system for 2.5\u00a0h, with an 8-month retention interval between sessions. Operators were provided with an adaptable support system, which operated at one of the following reliability levels: 60%, 80% or 100%. Results showed that performance, workload, and trust remained stable at the second testing session, but operators lost self-confidence in their system management abilities. Finally, the effects of system reliability observed at the first testing session were largely found again at the second session. The findings overall suggest that adaptable automation may be a promising means to support operators in maintaining their performance at the second testing session."]},
{"title": "Work system barriers to patient, provider, and caregiver use of personal health records: A systematic review", "highlights": ["A literature search using terms synonymous with \u201cpersonal health record\u201d and \u201cbarrier\u201d was conducted.", "A human factors/ergonomics paradigm was used to code the 60 eligible articles.", "Most articles reported patient-related barriers.", "Barriers most frequently related to \u201cneeds, biases, beliefs, and mood\u201d and \u201ctechnology functions and features\u201d.", "Future research should focus on the interactions between work systems and the role of higher level work system factors."], "abstract": ["This review applied a human factors/ergonomics (HF/E) paradigm to assess individual, work system/unit, organization, and external environment factors generating barriers to patient, provider, and informal caregiver personal health record (PHR) use.", "The literature search was conducted using five electronic databases for the timeframe January 2000 to October 2013, resulting in 4865 citations. Two authors independently coded included articles (n\u00a0=\u00a060).", "Fifty-five, ten and five articles reported barriers to patient, provider and caregiver PHR use, respectively. Barriers centered around 20 subfactors. The most frequently noted were ", " (n\u00a0=\u00a035) and ", " (n\u00a0=\u00a032).", "The HF/E paradigm was effective in framing the assessment of factors creating barriers to PHR use. Design efforts should address literacy, interoperability, access to health information, and secure messaging. A deeper understanding of the interactions between work systems and the role of organization and external environment factors is required."]},
{"title": "Determining the reliability of a custom built seated stadiometry set-up for measuring spinal height in participants with chronic low\u00a0back pain", "highlights": ["Stadiometers can yield useful information regarding the spine.", "Most specialised stadiometers for this purpose are custom built and expensive.", "This study examined the reliability of a set-up using a wall mounted stadiometer.", "This inexpensive set-up was found to be reliable and thus suitable."], "abstract": ["Indirect measurement of disc hydration can be obtained through measures of spinal height using stadiometry. However, specialised stadiometers for this are often custom-built and expensive. Generic wall-mounted stadiometers alternatively are common in clinics and laboratories. This study examined the reliability of a custom set-up utilising a wall-mounted stadiometer for measurement of spinal height using custom built wall mounted postural rods. Twelve participants with non-specific chronic low back pain (CLBP; females n\u00a0=\u00a05, males n\u00a0=\u00a07) underwent measurement of spinal height on three separate consecutive days at the same time of day where 10 measurements were taken at 20\u00a0s intervals. Comparisons were made using repeated measures analysis of variance for \u2018trial\u2019 and \u2018gender\u2019. There were no significant effects by trial or interaction effects of trial x gender. Intra-individual absolute standard error of measurement (SEM) was calculated for spinal height using the first of the 10 measures, the average of 10 measures, the total shrinkage, and the rate of shrinkage across the 10 measures examined as the slope of the curve when a linear regression was fitted. SEMs were 3.1\u00a0mm, 2.8\u00a0mm, 2.6\u00a0mm and 0.212, respectively. Absence of significant differences between trials and the reported SEMs suggests this custom set-up for measuring spinal height changes is suitable use as an outcome measure in either research or clinical practice in participants with CLBP."]},
{"title": "To delegate or not to delegate: A review of control frameworks for autonomous cars", "highlights": ["We present the need for addressing human factors to autonomous cars.", "We outline several different frameworks for delegating control authority between human and the system.", "We focus on the partnership between the driver and the car and the decision making dynamic that needs to take place."], "abstract": ["There have been significant advances in technology and automated systems that will eventually see the use of autonomous cars as commonplace on our roads. Various systems are already available that provide the driver with different levels of decision support. This paper highlights the key human factors issues associated with the interaction between the user and an autonomous system, including assistive decision support and the delegation of authority to the automobile. The level of support offered to the driver can range from traditional automated assistance, to system generated guidance that offers advice for the driver to act upon, and even more direct action that is initiated by the system itself without driver intervention. In many of these instances the role of the driver is slowly moving towards a supervisory role within a complex system rather than one of direct control of the vehicle. Different paradigms of interaction are considered and focus is placed on the partnership that takes place between the driver and the vehicle. Drawing on the wealth of knowledge that exists within the aviation domain and research literature that examines technology partnerships within the cockpit, this paper considers important factors that will assist the automotive community to understand the underlying issues of the human and their interaction within complex systems."]},
{"title": "Effects of mild hypoxia in aviation on mood and complex cognition", "highlights": ["Mild hypoxia equivalent to 8000 ft does not appear to impair complex cognition.", "Aircrew and passengers may feel more fatigued and less vigorous at a cabin altitude of 12,000 ft.", "This may be restored by breathing supplementary oxygen."], "abstract": ["Thirty six volunteer air force personnel were sequentially exposed in a randomized balanced order in a hypobaric chamber to 30\u00a0min of baseline (sea level) and mild hypoxia induced by a specified altitude (sea level, 8000\u00a0ft and 12,000\u00a0ft), followed immediately by breathing 100% oxygen from an oro-nasal mask. Mood and complex cognition were assessed. Analysis of variance indicated that mood (fatigue and vigour) remained the same at 8000\u00a0ft but fatigue was increased (p\u00a0=\u00a00.001) and vigour reduced (p\u00a0=\u00a00.035) at 12,000\u00a0ft and was restored by supplementary oxygen. Complex cognition was not significantly altered by the test conditions. The results of this study do not support prior evidence that mild hypoxia equivalent to either 8000 or 12,000\u00a0ft, impairs complex cognition, but suggests that some aspects of mood may be affected at 12,000\u00a0ft and can be restored by breathing 100% oxygen."]},
{"title": "The use of volumetric projections in Digital Human Modelling software for the identification of Large Goods Vehicle blind spots", "highlights": ["A novel DHM volumetric projection technique allows blind spots to be visualised.", "The project highlighted a key blind spot that exists for Large Goods Vehicles.", "The blind spot was shown to have potential to be a causal factor in accidents.", "The results were used to define a revision to the standard UNECE Regulation 46."], "abstract": ["The aim of the study is to understand the nature of blind spots in the vision of drivers of Large Goods Vehicles caused by vehicle design variables such as the driver eye height, and mirror designs. The study was informed by the processing of UK national accident data using cluster analysis to establish if vehicle blind spots contribute to accidents. In order to establish the cause and nature of blind spots six top selling trucks in the UK, with a range of sizes were digitized and imported into the SAMMIE Digital Human Modelling (DHM) system. A novel CAD based vision projection technique, which has been validated in a laboratory study, allowed multiple mirror and window aperture projections to be created, resulting in the identification and quantification of a key blind spot. The identified blind spot was demonstrated to have the potential to be associated with the scenarios that were identified in the accident data. The project led to the revision of UNECE Regulation 46 that defines mirror coverage in the European Union, with new vehicle registrations in Europe being required to meet the amended standard after June of 2015."]},
{"title": "Building healthy construction workers: Their views on health, wellbeing and better workplace design", "highlights": ["Construction workers care about maintaining good health and wellbeing at work.", "Construction workers have many good ideas to improve their job and workplace.", "It is important that good practice is shared across the industry to facilitate healthy ageing.", "Using elements of participatory ergonomics could enable sharing of good practice."], "abstract": ["Construction is a heavy manual industry where working into later life can be a challenge. An interview study was conducted to explore workers' understanding of their health at work and ways of making their jobs easier, safer or more comfortable. Using purposive sampling, 80 trades\u2019 workers were selected from construction sites in the UK. The Nordic Musculoskeletal Questionnaire and Work Ability Index were used to explore aches and pains and reducing strain on the body. A high prevalence of symptoms was reported and ratings of work ability were high. Workers were aware of the physical demands of their work and had over 250 ideas around health and wellbeing e.g. rucksacks for tools, bespoke benches, adapting PPE, and higher cost solutions e.g. mechanical lifting aids. Engagement of the workforce should be encouraged and feed into change processes in the industry to enable all workers stay fit for work for longer."]},
{"title": "Effects of monetary reward and punishment on information checking behaviour", "highlights": ["The effects of monetary reward and punishment were examined in two experiments.", "Reward and punishment were found to better reduce errors than control group.", "Reward and punishment resulted in more rigorous checking behaviour than control.", "Whether punishment is more effective than reward, or vice versa, remains inconclusive."], "abstract": ["Two experiments were conducted to examine whether checking one's own work can be motivated by monetary reward and punishment. Participants were randomly assigned to one of three conditions: a flat-rate payment for completing the task (Control); payment increased for error-free performance (Reward); payment decreased for error performance (Punishment). Experiment 1 (", "\u00a0=\u00a090) was conducted with liberal arts students, using a general data-entry task. Experiment 2 (", "\u00a0=\u00a090) replicated Experiment 1 with clinical students and a safety-critical \u2018cover story\u2019 for the task. In both studies, Reward and Punishment resulted in significantly fewer errors, more frequent and longer checking, than Control. No such differences were obtained between the Reward and Punishment conditions. It is concluded that error consequences in terms of monetary reward and punishment can result in more accurate task performance and more rigorous checking behaviour than errors without consequences. However, whether punishment is more effective than reward, or vice versa, remains inconclusive."]},
{"title": "Action slips during whole-body vibration", "highlights": ["We examine whether whole-body vibration (WBV) increases frequency of action slips.", "There were more errors of commission (i.e. action slips) during 17\u00a0Hz vertical WBV.", "Responses to stimuli were faster during 17\u00a0Hz vertical WBV.", "17\u00a0Hz vertical WBV can induce impulsive responding.", "The number of action slips may increase because responses are faster during WBV."], "abstract": ["Helicopter aircrew members engage in highly demanding cognitive tasks in an environment subject to whole-body vibration (WBV). Sometimes their actions may not be according to plan (e.g. action slips and lapses). This study used a Sustained Attention to Response Task (SART) to examine whether action slips were more frequent during exposure to WBV. Nineteen participants performed the SART in two blocks. In the WBV block participants were exposed to 17\u00a0Hz vertical WBV, which is typical of larger helicopter working environments. In the No-WBV block there was no WBV. There were more responses to the rare no-go digit 3 (i.e. action slips) in the WBV block, and participants responded faster in the WBV block. These results suggest that WBV influences response inhibition, and can induce impulsive responding. WBV may increase the likelihood of action slips, mainly due to failure of response inhibition."]},
{"title": "The impact of user- and system-initiated personalization on the user experience at large sports events", "highlights": ["Personalized mobile content improves spectator UX at large sports events.", "User and system-initiated personalization approaches each provide distinct benefits.", "User initiated personalization provides accuracy and a feeling of control.", "System-initiated approaches minimise interaction effort from participants.", "An integrated hybrid approach can capitalise on the merits of both approaches."], "abstract": ["This article describes an experimental study investigating the impact on user experience of two approaches of personalization of content provided on a mobile device, for spectators at large sports events. A lab-based experiment showed that a system-driven approach to personalization was generally preferable, but that there were advantages to retaining some user control over the process. Usability implications for a hybrid approach, and design implications are discussed, with general support for countermeasures designed to overcome recognised limitations of adaptive systems."]},
{"title": "Evaluation of thermal formation and air ventilation inside footwear during gait: The role of gait and fitting", "highlights": ["The effect of air behavior inside footwear on foot temperature formation was studied.", "The effect of the energy balance of the foot on skin temperature was examined.", "The cooling effect of ventilation on the arch temperature was observed.", "The ventilation rate was determined based on the footwear air space and gait.", "Ventilation was found to be a key to the design of thermally comfortable footwear."], "abstract": ["Comfort is an important concept in footwear design. The microclimate inside footwear contributes to the perception of thermal comfort. To investigate the effect of ventilation on microclimate formation inside footwear, experiments with subjects were conducted at four gait speeds with three different footwear sizes. Skin temperature, metabolism, and body mass were measured at approximately 25\u00a0\u00b0C and 50% relative humidity, with no solar radiation and a calm wind. The footwear occupancy and ventilation rate were also estimated, with the latter determined using the tracer gas method. The experimental results revealed that foot movement, metabolism, evaporation, radiation, convection, and ventilation were the main factors influencing the energy balance for temperature formation on the surface of the foot. The cooling effect of ventilation on the arch temperature was observed during gait. The significance of the amount of air space and ventilation on the improvement in the thermal comfort of footwear was clarified."]},
{"title": "A comprehensive approach to environmental and human factors into product/service design and development. A review from an ergoecological perspective", "highlights": ["The need of approaching and intervening simultaneously human and environmental problems has been identify. This are the results of a systematic review of design concepts and methods associated with human and environmental factors.", "It shows conceptual and methodological segregation between human factors and environmental factors in published documents.", "Hints are giving towards a comprehensive approach based on ergoecology to strengthening decision making processes. The paper contributes to recognize methods for product/service design and development with perspective human-environmental."], "abstract": ["This article presents the results of a documentary-exploratory review of design methods and concepts associated with human and environmental factors, based on a qualitative-quantitative analysis of coincidences with the fundamentals of ergoecology and in line with sustainable dynamics, with a view to putting the principles of ergoecology into practice in product/service design and development. 61.6% of 696 documents found represent work on conceptual developments, while the remaining 38.4% refer to design methods. Searches were refined using Nvivo-10 software, and 101 documents were obtained about theoretical aspects while 17 focused on the application of methods, and these formed the analysis universe. The results show how little concern there is for working comprehensively on human and environmental aspects, and a trend toward segmentation of human and environmental aspects in the field of product/service design and development can be seen, at both concept and application/methodology levels. It was concluded from the above that comprehensive, simultaneous work is needed on human and environmental aspects, clarity and conceptual unity, in order to achieve sustainability in practical matters and ensure that ergoecology-compatible design methods are applied."]},
{"title": "Effect of alternative video displays on postures, perceived effort, and performance during microsurgery skill tasks", "highlights": ["Compared with microscopes, neck posture was more neutral and neck movements were more frequent on the video displays.", "Task completion times didn\u2019t differ between 2D and 3D, but times were slower on the video displays than the microscope.", "Video displays reduce posture constraints and may reduce musculoskeletal symptoms and fatigue in microsurgery."], "abstract": ["Physical work demands and posture constraint from operating microscopes may adversely affect microsurgeon health and performance. Alternative video displays were developed to reduce posture constraints. Their effects on postures, perceived efforts, and performance were compared with the microscope. Sixteen participants performed microsurgery skill tasks using both stereo and non-stereoscopic microscopes and video displays. Results showed that neck angles were 9\u201313\u00b0 more neutral and shoulder flexion were 9\u201310\u00b0 more elevated on the video display than the microscope. Time observed in neck extension was higher (30% vs. 17%) and neck movements were 3x more frequent on the video display than microscopes. Ratings of perceived efforts did not differ among displays, but usability ratings were better on the microscope than the video display. Performance times on the video displays were 66\u2013110% slower than microscopes. Although postures improved, further research is needed to improve task performance on video displays."]},
{"title": "A macroergonomic perspective on fatigue and coping in the hospital nurse work system", "highlights": ["We explore hospital registered nurses' perceptions of fatigue and coping at work.", "We apply the SEIPS model to find barriers and facilitators to fatigue and coping.", "Factors that contribute to or prevent fatigue span work system components.", "Barriers and facilitators to coping also span all work system components.", "Fatigue risk management in hospital nursing should apply a macroergonomic approach."], "abstract": ["Occupational fatigue in hospital nurses is associated with increased nurse turnover, and decreased nurse health and patient safety. The goal of this study was to explore the factors contributing to or preventing fatigue, and barriers and facilitators to individual nurse coping in hospital work systems. Interviews were conducted and analyzed using a directed qualitative content analysis approach guided by the Systems Engineering Initiative for Patient Safety (SEIPS) model. Themes related to sources of fatigue within each of the five primary components of the SEIPS work system were identified, along with barriers and facilitators to nurses' experiences and strategies for coping with fatigue. Findings from this study provide guidance on what nurses perceive as contributing to fatigue and factors that are helpful and harmful to coping with fatigue within their work system. Implications for fatigue risk management systems (FRMS) are also discussed, in particular the importance of maintaining nurse autonomy in decision-making when implementing fatigue interventions or countermeasures."]},
{"title": "Effect of cardboard under a sleeping bag on sleep stages during daytime nap", "highlights": ["We compared usage of sleeping bags on corrugated cardboard or flooring during naps.", "Using corrugated cardboard decreased the number of awakenings.", "Skin temperature and subjective warmth increased in the corrugated cardboard group.", "Increasing the thermal insulation under the body reduces cold stress.", "Results can be used for practical application in future disaster circumstances."], "abstract": ["Fourteen healthy male subjects slept from 13:30 to 15:30 under ambient temperature and relative humidity maintained at 15\u00a0\u00b0C and 60%, respectively. They slept under two conditions: in a sleeping bag on wooden flooring (Wood) and in a sleeping bag with corrugated cardboard between the bag and the flooring (CC). Polysomnography, skin temperature (Tsk), microclimate, bed climate, and subjective sensations were obtained. The number of awakenings in the CC had significantly decreased compared to that in the Wood. The mean, back, and thigh Tsk, and bed climate temperature were significantly higher in the CC than that in the Wood. Subjective thermal sensations were warmer in the CC than in the Wood. These results suggest that using corrugated cardboard under a sleeping bag may reduce cold stress, thereby decreasing the number of awakenings and increasing subjective warmth; the mean, back, and thigh Tsk; and bed climate temperature."]},
{"title": "Age and work environment characteristics in relation to sleep: Additive, interactive and curvilinear effects", "highlights": ["The age profile of the workforce, and the demands on workers, are both increasing.", "Little is known about how age and work characteristics combine to predict sleep.", "Among offshore personnel, age and work environment jointly predicted sleep measures.", "Age showed linear and curvilinear relationships with sleep duration.", "For high job demand, minimum sleep duration occurred in the age range 44\u201349\u00a0y."], "abstract": ["Although additive combinations of age and work environment characteristics have been found to predict sleep impairment, possible ", " interactions have been largely disregarded. The present study examined linear and curvilinear interactions of age with work environment measures in relation to sleep quality and duration. Survey data were collected from offshore day-shift personnel (N\u00a0=\u00a0901). Main effects and interactions of the age terms with work environment measures (job demand, control, and social support, physical environment and strenuous work) were evaluated. Sleep duration was predicted by a curvilinear interaction, ", " ", " (p\u00a0<\u00a0.005), and by the ", " interaction (p\u00a0<\u00a0.002); sleep quality was predicted by ", " (p\u00a0<\u00a0.002). Job control and physical environment showed significant additive effects. At a time when older employees are encouraged to remain in the workforce, the findings serve to increase understanding of how ageing and work demands jointly contribute to sleep impairment."]},
{"title": "The ability of UK offshore workers of different body size and shape to egress through a restricted window space", "highlights": ["Up to 75% of successful small window egress outcome is size related.", "Bideltoid breadth, chest depth and weight combined explain \u223c70% of egress outcome.", "25% or more of egress outcome relates to non-anatomical factors.", "Predictive testing will inevitably lead to false\u00a0+ve and \u2013ve results.", "Flexibility and tissue compressibility may also explain successful egress."], "abstract": ["404 male offshore workers aged 41.4\u00a0\u00b1\u00a010.7\u00a0y underwent 3D body scanning and an egress task simulating the smallest helicopter window emergency exit size. The 198 who failed were older (P\u00a0<\u00a00.01), taller (P\u00a0<\u00a00.05) and heavier (P\u00a0<\u00a00.0001) than the 206 who passed. Using all extracted dimensions from the scans, binary logistic regression identified a model (refined using backward elimination) which predicted egress outcome with 75.2% accuracy. Using only weight, bideltoid breadth and maximum chest depth, the model achieved \u223c70% accuracy. When anatomical dimensions categorise individuals for small window egress, 25% or more will be misclassified, with false positives (those predicted to fail, but pass) slightly outnumbering false negatives (those predicted to pass, but fail), highlighting the limitations of a predictive approach which treats the body as a rigid object. Differences in flexibility and technique may explain these observations, which may be important considerations for future research."]},
{"title": "Driving while using a smartphone-based mobility application: Evaluating the impact of three multi-choice user interfaces on visual-manual distraction", "highlights": ["The performance metric is tested against two baseline methods (between- and within-trial).", "Multi-step filtering approach performed poorly due to the high precision required.", "Even on a short and ordered list, kinetic scrolling leads to high visual demand.", "Multi-step filtering approach is worth pursuing with better graphic components."], "abstract": ["Innovative in-car applications provided on smartphones can deliver real-time alternative mobility choices and subsequently generate visual-manual demand. Prior studies have found that multi-touch gestures such as kinetic scrolling are problematic in this respect. In this study we evaluate three prototype tasks which can be found in common mobile interaction use-cases. In a repeated-measures design, 29 participants interacted with the prototypes in a car-following task within a driving simulator environment. Task completion, driving performance and eye gaze have been analysed. We found that the slider widget used in the filtering task was too demanding and led to poor performance, while kinetic scrolling generated a comparable amount of visual distraction despite it requiring a lower degree of finger pointing accuracy. We discuss how to improve continuous list browsing in a dual-task context."]},
{"title": "Discomfort of seated persons exposed to low frequency lateral and roll oscillation: Effect of backrest height", "highlights": ["Backrests affect the discomfort caused by frequencies of vibration less than 1.0\u00a0Hz.", "Any benefit depends on backrest height and the frequency and direction of vibration.", "A high backrest reduces discomfort caused by lower frequencies of lateral and roll.", "A high backrest increases discomfort caused by higher frequencies of roll."], "abstract": ["Backrests influence the comfort of seated people. With 21 subjects sitting with three backrest heights (no backrest, short backrest, high backrest) discomfort caused by lateral, roll, and fully roll-compensated lateral oscillation was investigated at frequencies between 0.25 and 1.0\u00a0Hz. With lateral oscillation, the short backrest reduced discomfort at frequencies less than 0.63\u00a0Hz and the high backrest reduced discomfort at frequencies less than 1.0\u00a0Hz. With roll oscillation, the high backrest reduced discomfort at frequencies less than 0.63\u00a0Hz, but increased discomfort at 1.0\u00a0Hz. With fully roll-compensated lateral oscillation, the short backrest reduced discomfort at 0.4\u00a0Hz and the high backrest reduced discomfort at 0.5 and 0.63\u00a0Hz. As predicted by current standards, a backrest can increase discomfort caused by high frequencies of vibration. However, a backrest can reduce discomfort caused by low frequencies, with the benefit depending on the frequency and direction of oscillation and backrest height."]},
{"title": "Personal and other factors affecting acceptance of smartphone technology by older Chinese adults", "highlights": ["Those are younger, with higher education, non-widowed, with better economic condition are more likely to use smartphone.", "Cost was found to be a critical factor influencing behavior intention of older adults.", "Self-satisfaction and facilitating conditions significantly affect perceived usefulness and perceived ease of use."], "abstract": ["It has been well documented that in the 21st century, there will be relatively more older people around the world than in the past. Also, it seems that technology will expand in this era at an unprecedented rate. Therefore, it is of critical importance to understand the factors that influence the acceptance of technology by older people. The positive impact that the use of mobile applications can have for older people was confirmed by a previous study (Plaza et\u00a0al., 2011). The study reported here aimed to explore and confirm, for older adults in China, the key influential factors of smartphone acceptance, and to describe the personal circumstances of Chinese older adults who use smartphone. A structured questionnaire and face to face individual interviews were used with 120 Chinese older adults (over 55). Structural Equation Modeling was used to confirm a proposed smartphone acceptance model based on Technology Acceptance Model (TAM), and the Unified Theory of Acceptance and Use of Technology (UTAUT). The results showed that those who were younger, with higher education, non-widowed, with better economic condition related to salary or family support were more likely to use smartphone. Also, cost was found to be a critical factor influencing behavior intention. Self-satisfaction and facilitating conditions were proved to be important factors influencing perceived usefulness and perceived ease of use."]},
{"title": "Establishing usability heuristics for heuristics evaluation in a specific domain: Is there a consensus?", "highlights": ["Analytical review of 70 studies of domain specific heuristics for usability evaluation.", "There is a deficiency of validation effort following heuristics proposition.", "It is inconclusive whether domain specific heuristics is better than general ones.", "Less than 10% of the studies showed acceptable robustness and rigorousness.", "More than 80% of the studies used similar heuristics as Nielsen's."], "abstract": ["Heuristics evaluation is frequently employed to evaluate usability. While general heuristics are suitable to evaluate most user interfaces, there is still a need to establish heuristics for specific domains to ensure that their specific usability issues are identified. This paper presents a comprehensive review of 70 studies related to usability heuristics for specific domains. The aim of this paper is to review the processes that were applied to establish heuristics in specific domains and identify gaps in order to provide recommendations for future research and area of improvements. The most urgent issue found is the deficiency of validation effort following heuristics proposition and the lack of robustness and rigour of validation method adopted. Whether domain specific heuristics perform better or worse than general ones is inconclusive due to lack of validation quality and clarity on how to assess the effectiveness of heuristics for specific domains. The lack of validation quality also affects effort in improving existing heuristics for specific domain as their weaknesses are not addressed."]},
{"title": "Active workstation allows office workers to work efficiently while sitting and exercising moderately", "highlights": ["Pedalling workstation enables moderate exercise without seriously affecting work.", "Half an hour daily usage achieves minimum physical activity recommendations.", "Most participants would rather use this workstation than exercise outside work.", "Can also be used during free time (sport events, TV, Cinema, PC, commuting, etc.)."], "abstract": ["To determine the effects of a moderate-intensity active workstation on time and error during simulated office work.", "The aim of the study was to analyse simultaneous work and exercise for non-sedentary office workers. We monitored oxygen uptake, heart rate, sweating stains area, self-perceived effort, typing test time with typing error count and cognitive performance during 30\u00a0min of exercise with no cycling or cycling at 40 and 80\u00a0W.", "Compared baseline, we found increased physiological responses at 40 and 80\u00a0W, which corresponds to moderate physical activity (PA). Typing time significantly increased by 7.3% (p\u00a0=\u00a00.002) in C40W and also by 8.9% (p\u00a0=\u00a00.011) in C80W. Typing error count and cognitive performance were unchanged.", "Although moderate intensity exercise performed on cycling workstation during simulated office tasks increases working task execution time with, it has moderate effect size; however, it does not increase the error rate. Participants confirmed that such a working design is suitable for achieving the minimum standards for daily PA during work hours."]},
{"title": "Vibration and impulsivity analysis of hand held olive beaters", "highlights": ["Analysis of the vibrational behaviour of three hand held olive beaters.", "Analysis of frequency weighted and unweighted acceleration signals at the handles.", "The obtained impulsiveness and crest factors were similar to industrial tools.", "Different weighting curves are useful for the beaters vibration analysis."], "abstract": ["To provide more effective evaluations of hand arm vibration syndromes caused by hand held olive beaters, this study focused on two aspects: the acceleration measured at the tool pole and the analysis of the impulsivity, using the crest factor. The signals were frequency weighted using the weighting curve ", " as described in the ISO 5349-1 standard. The same source signals were also filtered by the ", " filter (ISO/TS 15694), because the weighting filter ", " (unlike the ", " filter) could underestimate the effect of high frequency vibration on vibration-induced finger disorders. Ten (experienced) male operators used three beater models (battery powered) in the real olive harvesting condition. High vibration total values were obtained with values never lower than 20\u00a0m", ". Concerning the crest factor, the values ranged from 5 to more than 22. This work demonstrated that the hand held olive beaters produced high impulsive loads comparable to the industry hand held tools."]},
{"title": "Assessing attentional prioritization of front-of-pack nutrition labels using change detection", "highlights": ["We used a change detection method to track the deployment of attention.", "Participants were not given a nutrition-relevant goal.", "Front of pack labels were attended much more rapidly than the nutrition facts panel.", "Color coding nutrient values in front of pack labels increased attentional prioritization.", "Results support the use of color coded front of pack nutrition labels."], "abstract": ["We used a change detection method to evaluate attentional prioritization of nutrition information that appears in the traditional \u201cNutrition Facts Panel\u201d and in front-of-pack nutrition labels. Results provide compelling evidence that front-of-pack labels attract attention more readily than the Nutrition Facts Panel, even when participants are not specifically tasked with searching for nutrition information. Further, color-coding the relative nutritional value of key nutrients within the front-of-pack label resulted in increased attentional prioritization of nutrition information, but coding using facial icons did not significantly increase attention to the label. Finally, the general pattern of attentional prioritization across front-of-pack designs was consistent across a diverse sample of participants. Our results indicate that color-coded, front-of-pack nutrition labels increase attention to the nutrition information of packaged food, a finding that has implications for current policy discussions regarding labeling change."]},
{"title": "Time pressure and regulations on hospital-in-the-home (HITH) nurses: An on-the-road study", "highlights": ["We examined time pressure in hospital-in-the-home nurses while they were driving.", "Time pressure has a negative impact on both the nurses' driving and their emotions.", "Time constraints are neither necessary nor sufficient to elicit time pressure.", "The challenges and uncertainty associated with healthcare and driving are relevant.", "Anticipated and in situ regulations may reduce uncertainties and negative emotions."], "abstract": ["This study investigated both causal factors and consequences of time pressure in hospital-in-the-home (HITH) nurses. These nurses may experience additional stress from the time pressure they encounter while driving to patients' homes, which may result in greater risk taking while driving. From observation in natural settings, data related to the nurses' driving behaviours and emotions were collected and analysed statistically; semi-directed interviews with the nurses were analysed qualitatively. The results suggest that objective time constraints alone do not necessarily elicit subjective time pressure. The challenges and uncertainty associated with healthcare and the driving period contribute to the emergence of this time pressure, which has a negative impact on both the nurses' driving and their emotions. Finally, the study focuses on anticipated and in situ regulations. These findings provide guidelines for organizational and technical solutions allowing the reduction of time pressure among HITH nurses."]},
{"title": "Latent human error analysis and efficient improvement strategies by fuzzy TOPSIS in aviation maintenance tasks", "highlights": ["The systematic method of HEI analysis and MCDM was provided in this research.", "HFACS and RCA were applied to analyze latent human errors.", "Fuzzy TOPSIS was utilized to assess the improvement strategy of latent human errors.", "Sensitivity analysis was used to inspect the robustness of the results of Fuzzy TOPSIS.", "This is the first study to assess human errors by HFACS and Fuzzy TOPSIS."], "abstract": ["The purposes of this study were to develop a latent human error analysis process, to explore the factors of latent human error in aviation maintenance tasks, and to provide an efficient improvement strategy for addressing those errors. First, we used HFACS and RCA to define the error factors related to aviation maintenance tasks. Fuzzy TOPSIS with four criteria was applied to evaluate the error factors. Results show that 1) adverse physiological states, 2) physical/mental limitations, and 3) coordination, communication, and planning are the factors related to airline maintenance tasks that could be addressed easily and efficiently. This research establishes a new analytic process for investigating latent human error and provides a strategy for analyzing human error using fuzzy TOPSIS. Our analysis process complements shortages in existing methodologies by incorporating improvement efficiency, and it enhances the depth and broadness of human error analysis methodology."]},
{"title": "Musculoskeletal disorders in construction: A review and a novel system for activity tracking with body area network", "highlights": ["We present a review of assessment frameworks used for the evaluation of body motion.", "Different methods for measuring working postures and motions are compared.", "We introduce a new system to characterise unsafe postures of construction workers."], "abstract": ["Human body motions have been analysed for decades with a view on enhancing occupational well-being and performance of workers. On-going progresses in miniaturised wearable sensors are set to revolutionise biomechanical analysis by providing accurate and real-time quantitative motion data. The construction industry has a poor record of occupational health, in particular with regard to work-related musculoskeletal disorders (WMSDs). In this article, we therefore focus on the study of human body motions that could cause WMSDs in construction-related activities. We first present an in-depth review of existing assessment frameworks used in practice for the evaluation of human body motion. Subsequently different methods for measuring working postures and motions are reviewed and compared, pointing out the technological developments, limitations and gaps; Inertial Measurement Units (IMUs) are particularly investigated. Finally, we introduce a new system to detect and characterise unsafe postures of construction workers based on the measurement of motion data from wearable wireless IMUs integrated in a body area network. The potential of this system is demonstrated through experiments conducts in a laboratory as well as in a college with actual construction trade trainees."]},
{"title": "A methodology using in-chair movements as an objective measure of discomfort for the purpose of statistically distinguishing between similar seat surfaces", "highlights": ["ICM can compare similar seats.", "Threshold selection and sitting duration is important.", "ICM and discomfort appear related."], "abstract": ["This study presents a method for objectively measuring in-chair movement (ICM) that shows correlation with subjective ratings of comfort and discomfort. Employing a cross-over controlled, single blind design, healthy young subjects (n\u00a0=\u00a021) sat for 18\u00a0min on each of the following surfaces: contoured foam, straight foam and wood. Force sensitive resistors attached to the sitting interface measured the relative movements of the subjects during sitting. The purpose of this study was to determine whether ICM could statistically distinguish between each seat material, including two with subtle design differences. In addition, this study investigated methodological considerations, in particular appropriate threshold selection and sitting duration, when analysing objective movement data. ICM appears to be able to statistically distinguish between similar foam surfaces, as long as appropriate ICM thresholds and sufficient sitting durations are present. A relationship between greater ICM and increased discomfort, and lesser ICM and increased comfort was also found."]},
{"title": "Predicting tool operator capacity to react against torque within acceptable handle deflection limits in automotive assembly", "highlights": ["Tool, workstation, and task parameters were measured for 35 tools, representing 69 installations in an automotive assembly plant.", "The dynamic forces acting against the human tool operator and resulting handle reaction deflections were predicted using a biodynamic tool operator model.", "The proportion of threaded fastener tool operators capable of maintaining published psychophysically derived deflection limits were ascertained.", "While handle deflection did not exceed the limit for most installations, changes in tool speed and operator position improved the six jobs that did."], "abstract": ["The proportion of tool operators capable of maintaining published psychophysically derived threaded fastener tool handle deflection limits were predicted using a biodynamic tool operator model, interacting with the tool, task and workstation. Tool parameters, including geometry, speed and torque were obtained from the specifications for 35 tools used in an auto assembly plant. Tool mass moments of inertia were measured for these tools using a novel device that engages the tool in a rotating system of known inertia. Task parameters, including fastener target torque and joint properties (soft, medium or hard), were ascertained from the vehicle design specifications. Workstation parameters, including vertical and horizontal distances from the operator were measured using a laser rangefinder for 69 tool installations in the plant. These parameters were entered into the model and tool handle deflection was predicted for each job. While handle deflection for most jobs did not exceed the capacity of 75% females and 99% males, six jobs exceeded the deflection criterion. Those tool installations were examined and modifications in tool speed and operator position improved those jobs within the deflection limits, as predicted by the model. We conclude that biodynamic tool operator models may be useful for identifying stressful tool installations and interventions that bring them within the capacity of most operators."]},
{"title": "Sex-based differences in lifting technique under increasing load conditions: A principal component analysis", "highlights": ["Differences in lifting conditions between sexes were analyzed under increasing load conditions.", "Principal component analysis was used to analyze lifting waveforms.", "Single component reconstruction was used to interpret principal components.", "Lifting style was similar between sexes when load standardized to the individual.", "Lifting technique changed with increasing load."], "abstract": ["The objective of the present study was to determine if there is a sex-based difference in lifting technique across increasing-load conditions. Eleven male and 14 female participants (n\u00a0=\u00a025) with no previous history of low back disorder participated in the study. Participants completed freestyle, symmetric lifts of a box with handles from the floor to a table positioned at 50% of their height for five trials under three load conditions (10%, 20%, and 30% of their individual maximum isometric back strength). Joint kinematic data for the ankle, knee, hip, and lumbar and thoracic spine were collected using a two-camera Optotrak motion capture system. Joint angles were calculated using a three-dimensional Euler rotation sequence. Principal component analysis (PCA) and single component reconstruction were applied to assess differences in lifting technique across the entire waveforms. Thirty-two PCs were retained from the five joints and three axes in accordance with the 90% trace criterion. Repeated-measures ANOVA with a mixed design revealed no significant effect of sex for any of the PCs. This is contrary to previous research that used discrete points on the lifting curve to analyze sex-based differences, but agrees with more recent research using more complex analysis techniques. There was a significant effect of load on lifting technique for five PCs of the lower limb (PC1 of ankle flexion, knee flexion, and knee adduction, as well as PC2 and PC3 of hip flexion) (p\u00a0<\u00a00.005). However, there was no significant effect of load on the thoracic and lumbar spine. It was concluded that when load is standardized to individual back strength characteristics, males and females adopted a similar lifting technique. In addition, as load increased male and female participants changed their lifting technique in a similar manner."]},
{"title": "The effects of a passive exoskeleton on muscle activity, discomfort and endurance time in forward bending work", "highlights": ["A passive exoskeleton lowers back muscle activity by 35\u201338% during assembly work.", "The endurance time was found to be almost three times longer when using this system.", "The system reduced discomfort in the back but resulted in more discomfort in the chest."], "abstract": ["Exoskeletons may form a new strategy to reduce the risk of developing low back pain in stressful jobs. In the present study we examined the potential of a so-called passive exoskeleton on muscle activity, discomfort and endurance time in prolonged forward-bended working postures.", "Eighteen subjects performed two tasks: a simulated assembly task with the trunk in a forward-bended position and static holding of the same trunk position without further activity. We measured the electromyography for muscles in the back, abdomen and legs. We also measured the perceived local discomfort. In the static holding task we determined the endurance, defined as the time that people could continue without passing a specified discomfort threshold.", "In the assembly task we found lower muscle activity (by 35\u201338%) and lower discomfort in the low back when wearing the exoskeleton. Additionally, the hip extensor activity was reduced. The exoskeleton led to more discomfort in the chest region. In the task of static holding, we observed that exoskeleton use led to an increase in endurance time from 3.2 to 9.7\u00a0min, on average.", "The results illustrate the good potential of this passive exoskeleton to reduce the internal muscle forces and (reactive) spinal forces in the lumbar region. However, the adoption of an over-extended knee position might be, among others, one of the concerns when using the exoskeleton."]},
{"title": "Evaluating the ability of novices to identify and quantify physical demand elements following an introductory education session: A pilot study", "highlights": ["Students attended a 3-h introductory workshop on physical demands description.", "Participants were asked to identify and quantify physical demands in three case examples.", "Participants identified 80% of the physically demanding elements.", "Participants quantified key measures with more the 10% error from the criterion.", "Novices' have some limitations in their ability to accurately conduct PDDs."], "abstract": ["A Physical Demands Description (PDD) is a resource that describes the physical demands of a job in a systematic way. PDD data are commonly used to make legal, medical, and monetary decisions related to work. Despite the fundamental importance of a PDD, data are often gathered by novice or early career ergonomists, where we have limited knowledge regarding their proficiency in performing PDDs. The purpose of this pilot study was to evaluate novices' proficiency in identifying and quantifying physical demands elements embedded within three job simulations, following a formal PDD education session. The education session was based on the revised Occupational Health Clinics for Ontario Workers (OHCOW, 2014) PDD Handbook. Participants were able to identify physical demands elements with an average success rate of 80%, but were often unable to accurately quantify measures related to each element within a prescribed error threshold of 10%. These data suggest that practitioners should exercise caution when sending novice ergonomists out on their own to complete PDDs."]},
{"title": "Anthropometric characteristics of female smallholder farmers of Uganda \u2013 Toward design of labor-saving tools", "highlights": ["Development of hand-tools to reduce labor burden requires rich anthropometric data.", "Western anthropometric databases do not adequately represent sub-Saharan women.", "Results from measuring 89 women show differences among different groups.", "More measurements are needed for more conclusive results and predictive models."], "abstract": ["Sub-Saharan African women on small-acreage farms carry a disproportionately higher labor burden, which is one of the main reasons they are unable to produce for both home and the market and realize higher incomes. Labor-saving interventions such as hand-tools are needed to save time and/or increase productivity in, for example, land preparation for crop and animal agriculture, post-harvest processing, and meeting daily energy and water needs. Development of such tools requires comprehensive and content-specific anthropometric data or body dimensions and existing databases based on Western women may be less relevant. We conducted measurements on 89 women to provide preliminary results toward answering two questions. First, how well existing databases are applicable in the design of hand-tools for sub-Saharan African women. Second, how universal body dimension predictive models are among ethnic groups. Our results show that, body dimensions between ", " and ", " ethnolinguistic groups are different and both are different from American women. These results strongly support the need for establishing anthropometric databases for sub-Saharan African women, toward hand-tool design."]},
{"title": "The effect of split sleep schedules (6h-on/6h-off) on neurobehavioural performance, sleep and sleepiness", "highlights": ["Split sleep (6h-on/6h-off) schedules were tested under laboratory conditions and compared to a daytime baseline.", "Reaction times and lapses of attention did not differ between baseline and shift days, however, sleepiness increased.", "While sleep was truncated, slow wave sleep was preserved."], "abstract": ["Shorter, more frequent rosters, such as 6h-on/6h-off split shifts, may offer promise to sleep, subjective sleepiness and performance by limiting shift length and by offering opportunities for all workers to obtain some sleep across the biological night. However, there exists a paucity of studies that have examined these shifts using objective measures of sleep and performance. The present study examined neurobehavioural performance, sleepiness and sleep during 6h-on/6h-off split sleep schedules. Sixteen healthy adults (6 males, 26.13y\u00a0\u00b1\u00a04.46) participated in a 9-day laboratory study that included two baseline nights (BL, 10h time in bed (TIB), 2200h-0800h), 4 days on one of two types of 6h-on/6h-off split sleep schedules with 5h TIB during each \u2018off\u2019 period (6h early: TIB 0300h-0800h and 1500h-20000h, or 6-h late: TIB 0900h-1400h and 2100h-0200h), and two recovery nights (10h TIB per night, 2200h-0800h). Participants received 10h TIB per 24h in total across both shift schedules. A neurobehavioural test bout was completed every 2\u00a0h during wake, which included the Psychomotor Vigilance Task (PVT) and the Karolinska Sleepiness Scale (KSS). Linear mixed effects models were used to assess the effect of day (BL, shift days 1\u20134), schedule (6h early, 6h late) and trial (numbers 1\u20136) on PVT lapses (operationalised as the number of reaction times >500\u00a0ms), PVT total lapse time, PVT fastest 10% of reaction times and KSS. Analyses were also conducted examining the effect of day and schedule on sleep variables. Overall, PVT lapses and total lapse time did not differ significantly between baseline and shift days, however, peak response speeds were significantly slower on the first shift day when compared to baseline, but only for those in the 6h-late condition. Circadian variations were apparent in performance outcomes, with individuals in the 6h-late condition demonstrated significantly more and longer lapses and slower peak reaction times at the end of their night shift (0730h) than at any other time during their shifts. In the 6h-early condition, only response speed significantly differed across trials, with slower response speeds occurring at trial 1 (0930h) than in trials 3 (1330h) or 4 (2130h). While subjective sleepiness was higher on shift days than at baseline, sleepiness did not accumulate across days. Total sleep was reduced across split sleep schedules compared to baseline. Overall, these results show that while there was not a cumulative cost to performance across days of splitting sleep, participants obtained less sleep and reported lowered alertness on shift days. Tests near the circadian nadir showed higher sleepiness and increased performance deficits. While this schedule did not produce cumulative impairment, the performance deficits witnessed during the biological night are still of operational concern for industry and workers alike."]},
{"title": "Classifying work rate from heart rate measurements using an adaptive neuro-fuzzy inference system", "highlights": ["We present a new fuzzy-based approach to classify work rate based on HR measurement.", "The proposed classifier accounts for participants' age, HR at rest and body weight.", "The proposed classifier has superior performance over Percent HR Reserve method."], "abstract": ["In a new approach based on adaptive neuro-fuzzy inference systems (ANFIS), field heart rate (HR) measurements were used to classify work rate into four categories: very light, light, moderate, and heavy. Inter-participant variability (physiological and physical differences) was considered. Twenty-eight participants performed Meyer and Flenghi's step-test and a maximal treadmill test, during which heart rate and oxygen consumption (", ") were measured. Results indicated that heart rate monitoring (HR, HR", ", and HR", ") and body weight are significant variables for classifying work rate. The ANFIS classifier showed superior sensitivity, specificity, and accuracy compared to current practice using established work rate categories based on percent heart rate reserve (%HRR). The ANFIS classifier showed an overall 29.6% difference in classification accuracy and a good balance between sensitivity (90.7%) and specificity (95.2%) on average. With its ease of implementation and variable measurement, the ANFIS classifier shows potential for widespread use by practitioners for work rate assessment."]},
{"title": "Soldier-relevant body borne load impacts minimum foot clearance during obstacle negotiation", "highlights": ["Participants create a safety margin to minimize tripping over an obstacle with load.", "Trail foot placement may be important for safely navigating an obstacle with load.", "Obstacle height did not have significant effect on foot clearance.", "Spatiotemporal changes of gait may reduce risk of tripping over taller obstacles."], "abstract": ["Soldiers often trip and fall on duty, resulting in injury. This study examined ten male soldiers' ability to negotiate an obstacle. Participants had lead and trail foot minimum foot clearance (MFC) parameters quantified while crossing a low (305\u00a0mm) and high (457\u00a0mm) obstacle with (19.4\u00a0kg) and without (6\u00a0kg) body borne load. To minimize tripping risk, participants increased lead foot MFC (p\u00a0=\u00a00.028) and reduced lead (p\u00a0=\u00a00.044) and trail (p\u00a0=\u00a00.035) foot variability when negotiating an obstacle with body borne load. While obstacle height had no effect on MFC (p\u00a0=\u00a00.273 and p\u00a0=\u00a00.126), placing the trail foot closer to the high obstacle when crossing with body borne load, resulted in greater lead (R\u00a0=\u00a00.640, ", "\u00a0=\u00a00.241, p\u00a0=\u00a00.046) and trail (R\u00a0=\u00a00.636, ", "\u00a0=\u00a00.287, p\u00a0=\u00a00.048) MFC. Soldiers, when carrying typical military loads, may be able to minimize their risk of tripping over an obstacle by creating a safety margin via greater foot clearance with reduced variability."]},
{"title": "Physical exercise and burnout facets predict injuries in a population-based sample of French career firefighters", "highlights": ["Occupational burnout was an important role in well-being of firefighters.", "Problem-focused coping was a protective factor for firefighter injuries.", "Physical exercise was identified as risk factors of injuries.", "Cognitive weariness and social support seeking was positively related to injuries."], "abstract": ["Although firefighting is known to engender a high rate of injury, few studies have examined the contribution of physical exercise, burnout and coping strategies to firefighting-related injuries. Data were collected from a population-based sample of 220 male firefighters. In a descriptive study, the nature and site of the injuries and the relationships among firefighter injuries, physical exercise, burnout and coping strategies were examined. Sprains were the most prevalent type of injury (98%), followed by tendinitis (40%) and muscle tears (30%). More than two thirds of these injuries were located at the ankle. Weekly hours of physical exercise, cognitive weariness at work, social support seeking, problem-focused coping and emotional exhaustion were significantly related to these injuries. The findings suggest that physical exercise and cognitive weariness can be considered as risk factors for French firefighter injuries, whereas problem-focused coping can be seen as a protective factor. More research is needed to explain the relationship between social support seeking and injury."]},
{"title": "Removing the thermal component from heart rate provides an accurate ", "highlights": ["A practical application of Vogt et\u00a0al.'s method to assess the HR thermal component during forest work is shown.", "The ", " overestimation error is strongly correlated to the thermal strain level observed.", "Vogt et\u00a0al.'s method enabled an accurate estimation of the HR motor and thermal components.", "Removing the thermal HR component from the measured HR eliminated the overestimation error."], "abstract": ["Heart rate (HR) was monitored continuously in 41 forest workers performing brushcutting or tree planting work. 10-min seated rest periods were imposed during the workday to estimate the HR thermal component (\u0394", ") per Vogt et\u00a0al. (1970, 1973). ", " was measured using a portable gas analyzer during a morning submaximal step-test conducted at the work site, during a work bout over the course of the day (range: 9\u201374\u00a0min), and during an ensuing 10-min rest pause taken at the worksite. The ", " estimated, from measured HR and from corrected HR (thermal component removed), were compared to ", " measured during work and rest. Varied levels of HR thermal component (\u0394", " range: 0\u201338\u00a0bpm) originating from a wide range of ambient thermal conditions, thermal clothing insulation worn, and physical load exerted during work were observed. Using raw HR significantly overestimated measured work ", " by 30% on average (range: 1%\u201364%). 74% of ", " prediction error variance was explained by the HR thermal component. ", " estimated from corrected HR, was not statistically different from measured ", ". Work ", " can be estimated accurately in the presence of thermal stress using Vogt et\u00a0al.'s method, which can be implemented easily by the practitioner with inexpensive instruments."]},
{"title": "Thermal sensations and comfort investigations in transient conditions in tropical office", "highlights": ["Evaluating thermal subjective responses in reference to abrupt exposure to warm temperature.", "Up-step temperature transition (neutral-warm condition) resulted in thermal discomfort among people in the tropics.", "Subjective thermal responses were more correlated with thermal skin point located at the forehead than the dorsal left hand."], "abstract": ["The study was done to identify affective and sensory responses observed as a result of hysteresis effects in transient thermal conditions consisting of warm-neutral and neutral - warm performed in a quasi-experiment setting. Air-conditioned building interiors in hot-humid areas have resulted in thermal discomfort and health risks for people moving into and out of buildings. Reports have shown that the instantaneous change in air temperature can cause abrupt thermoregulation responses. Thermal sensation vote (TSV) and thermal comfort vote (TCV) assessments as a consequence of moving through spaces with distinct thermal conditions were conducted in an existing single-story office in a hot-humid microclimate, maintained at an air temperature 24\u00a0\u00b0C (\u00b10.5), relative humidity 51% (\u00b17), air velocity 0.5\u00a0m/s (\u00b10.5), and mean radiant temperature (MRT) 26.6\u00a0\u00b0C (\u00b11.2). The measured office is connected to a veranda that showed the following semi-outdoor temperatures: air temperature 35\u00a0\u00b0C (\u00b12.1), relative humidity 43% (\u00b17), air velocity 0.4\u00a0m/s (\u00b10.4), and MRT 36.4\u00a0\u00b0C (\u00b12.9). Subjective assessments from 36 college-aged participants consisting of thermal sensations, preferences and comfort votes were correlated against a steady state predicted mean vote (PMV) model. Local skin temperatures on the forehead and dorsal left hand were included to observe physiological responses due to thermal transition. TSV for veranda\u2013office transition showed that no significant means difference with TSV office-veranda transition were found. However, TCV collected from warm-neutral (\u22120.24, \u00b11.2) and neutral-warm (\u22120.72, \u00b11.3) conditions revealed statistically significant mean differences (", "\u00a0<\u00a00.05). Sensory and affective responses as a consequence of thermal transition after travel from warm-neutral-warm conditions did not replicate the hysteresis effects of brief, slightly cool, thermal sensations found in previous laboratory experiments. These findings also indicate that PMV is an acceptable alternative to predict thermal sensation immediately after a down-step thermal transition (\u22641\u00a0min exposure duration) for people living in a hot-humid climate country."]},
{"title": "Categorisation of visualisation methods to support the design of Human-Computer Interaction Systems", "highlights": ["Established an inventory of 23 Human-Computer Interaction (HCI) visualisation methods.", "Identified 5 selection approaches.", "Presented and provided evidence of how the visualisation methods are categorised in each selection approach.", "Discussed the limitations and benefits of each selection approach."], "abstract": ["During the design of Human-Computer Interaction (HCI) systems, the creation of visual artefacts forms an important part of design. On one hand producing a visual artefact has a number of advantages: it helps designers to externalise their thought and acts as a common language between different stakeholders. On the other hand, if an inappropriate visualisation method is employed it could hinder the design process. To support the design of HCI systems, this paper reviews the categorisation of visualisation methods used in HCI. A keyword search is conducted to identify a) current HCI design methods, b) approaches of selecting these methods. The resulting design methods are filtered to create a list of just visualisation methods. These are then categorised using the approaches identified in (b). As a result 23 HCI visualisation methods are identified and categorised in 5 selection approaches (", ", ", ", ", ", ", ", and ", ")."]},
{"title": "Combining situated Cognitive Engineering with a novel testing method in a case study comparing two infusion pump interfaces", "highlights": ["Infusion pump interface designed with situated Cognitive Engineering was validated.", "Usability validation took place by comparison with a reference interface.", "A novel process tracing technique was used for analysis.", "Task performance increased with novel interface.", "Novel method was feasible for validating safety of medical devices."], "abstract": ["We validated the usability of a new infusion pump interface designed with a situated Cognitive Engineering approach by comparing it to a reference interface using a novel testing method employing repeated measurements and process measures, in addition to traditional outcome measures. The sample consisted of 25 nurses who performed eight critical tasks three times. Performance measures consisted of number and type of errors, deviations from a pre-established normative path solution, task completion times, number of keystrokes, mental effort and preferences in use. Results showed that interaction with the new interface resulted in 18% fewer errors, 90% fewer normative path deviations, 42% lower task completion times, 40% fewer keystrokes, 39% lower mental effort and 76% more subjective preferences in use. These outcomes suggest that within the scope of this case study, combining the situated Cognitive Engineering approach with a novel testing method addresses various shortcomings of earlier testing methods."]},
{"title": "Exploring sub-optimal use of an electronic risk assessment tool for venous thromboembolism", "highlights": ["We examined use and views of an electronic risk assessment tool for VTE.", "We found the tool was used as intended only 36% of the time.", "Poor tool design was a key factor contributing to sub-optimal use.", "Sub-optimal use was also due to poor understanding and awareness of the tool."], "abstract": ["International guidelines and consensus groups recommend using a risk assessment tool (RAT) to assess Venous Thromboembolism (VTE) risk prior to the prescription of prophylaxis. We set out to examine how an electronic RAT was being used (i.e. if by the right clinician, at the right time, for the right purpose) and to identify factors influencing utilization of the RAT. A sample of 112 risk assessments was audited and 12 prescribers were interviewed. The RAT was used as intended in only 40 (35.7%) cases (i.e. completed by a doctor within 24\u00a0h of admission, prior to the prescription of prophylaxis). We identified several reasons for sub-optimal use of the RAT, including beliefs about the need for a RAT, poor awareness of the tool, and poor RAT design. If a user-centred approach had been adopted, it is likely that a RAT would not have been implemented or that problematic design issues would have been identified."]},
{"title": "Perspectives on recycling centres and future developments", "highlights": ["Improvement of safety, economic and environmental performance is needed.", "Sorting quality influences working conditions, environment and system performance.", "Future needs include industrialization, design and organization of recycling centres.", "System performance is obtained through a multidisciplinary approach.", "A checklist for planning/rebuilding recycling centres is included."], "abstract": ["The overall aim of this paper is to draw combined, all-embracing conclusions based on a long-term multidisciplinary research programme on recycling centres in Sweden, focussing on working conditions, environment and system performance. A second aim is to give recommendations for their development of new and existing recycling centres and to discuss implications for the future design and organisation. Several opportunities for improvement of recycling centres were identified, such as design, layout, ease with which users could sort their waste, the work environment, conflicting needs and goals within the industry, and industrialisation. Combining all results from the research, which consisted of different disciplinary aspects, made it possible to analyse and elucidate their interrelations. Waste sorting quality was recognized as the most prominent improvement field in the recycling centre system. The research identified the importance of involving stakeholders with different perspectives when planning a recycling centre in order to get functionality and high performance. Practical proposals of how to plan and build recycling centres are given in a detailed checklist."]},
{"title": "Optimization of healthcare supply chain in context of macro-ergonomics factors by a unique mathematical programming approach", "highlights": ["Macro-ergonomics optimization of healthcare supply chain.", "A unique data envelopment analysis approach is used.", "This study identifies weights of each factor and preferred alternatives.", "Results showed that the influential shaping factor is teamwork.", "First macro-ergonomics study of healthcare supply chain by DEA."], "abstract": ["This study presents an integrated approach for analyzing the impact of macro-ergonomics factors in healthcare supply chain (HCSC) by data envelopment analysis (DEA). The case of this study is the supply chain (SC) of a real hospital. Thus, healthcare standards and macro-ergonomics factors are considered to be modeled by the mathematical programming approach. Over 28 subsidiary SC divisions with parallel missions and objectives are evaluated by analyzing inputs and outputs through DEA. Each division in this HCSC is considered as decision making unit (DMU). This approach can analyze the impact of macro-ergonomics factors on supply chain management (SCM) in healthcare sector. Also, this method ranks the relevant performance efficiencies of each HCSC. In this study by using proposed method, the most effective macro-ergonomics factor on HCSC is identified as \u201cteamwork\u201d issue. Also, this study would help managers to identify the areas of weaknesses in their SCM system and set improvement target plan for the related SCM system in healthcare industry. To the best of our knowledge, this is the first study for macro-ergonomics optimization of HCSC."]},
{"title": "Soccer players' fitting perception of different upper boot materials", "highlights": ["Soccer players' fitting perception of different upper boot materials were assessed.", "The type of leather of the upper boot influences the fitting perception.", "The kangaroo leather was perceived as the closest condition to the preferred fitting.", "The synthetic leather was perceived as the furthest condition to the preferred fitting.", "These results are relevant to the comfort and fit of players and purchase decision-making."], "abstract": ["The present study assessed the influence of upper boot materials on fitting perception. Twenty players tested three soccer boots only differing in the upper boot material (natural calf leather, natural kangaroo leather and synthetic leather). Players reported fitting perception and preference on specific foot areas using a perceived fitting scale. Ratings were averaged for every foot area. Repeated measures ANOVA was used to analyze the differences between boots. The kangaroo leather boots were perceived tighter and closer to the preferred fitting in general fitting, metatarsals area and instep area. The synthetic leather boots were perceived as the loosest and as the most distant boot from the preferred fitting in medial front area and instep area. In conclusion, the type of upper boot material influences the fitting perception of soccer players. The kangaroo leather was the material whose fitting was perceived closest to the players fitting preference."]},
{"title": "Ecodriving in hybrid electric vehicles \u2013 Exploring challenges for user-energy interaction", "highlights": ["We examined ecodriving in HEVs and challenges for optimal user-energy interaction.", "Impact of motivation and knowledge on fuel efficiency varied with environmental complexity.", "There was considerable inter-individual variance in ecodriving strategy selection.", "Drivers had diverse conceptualisations of HEV energy efficiency and named several false beliefs.", "Design guidelines for ecodriving support systems are derived to facilitate ecodriving."], "abstract": ["Hybrid electric vehicles (HEVs) can help to reduce transport emissions; however, user behaviour has a significant effect on the energy savings actually achieved in everyday usage. The present research aimed to advance understanding of HEV drivers' ecodriving strategies, and the challenges for optimal user-energy interaction. We conducted interviews with 39 HEV drivers who achieved above-average fuel efficiencies. Regression analyses showed that technical system knowledge and ecodriving motivation were both important predictors for ecodriving efficiency. Qualitative data analyses showed that drivers used a plethora of ecodriving strategies and had diverse conceptualisations of HEV energy efficiency regarding aspects such as the efficiency of actively utilizing electric energy or the efficiency of different acceleration strategies. Drivers also reported several false beliefs regarding HEV energy efficiency that could impair ecodriving efforts. Results indicate that ecodriving support systems should facilitate anticipatory driving and help users locate and maintain drivetrain states of maximum efficiency."]},
{"title": "A model for developing job rotation schedules that eliminate sequential high workloads and minimize between-worker variability in cumulative daily workloads: Application to automotive assembly lines", "highlights": ["Job rotation schedule was proposed to reduce cumulative workload from the successive use of the same body region.", "Workloads on automotive assembly lines were assessed with REBA.", "Workstations on assembly line were classified into high and low workload workstations with the median score of body parts.", "Mathematical job rotation model was developed and solved with CPLEX.", "Job rotation schedule was designed to prevent workers from working at high workload stations sequentially."], "abstract": ["The aim of study is to suggest a job rotation schedule by developing a mathematical model in order to reduce cumulative workload from the successive use of the same body region. Workload assessment using rapid entire body assessment (REBA) was performed for the model in three automotive assembly lines of chassis, trim, and finishing to identify which body part exposed to relatively high workloads at workstations. The workloads were incorporated to the model to develop a job rotation schedule. The proposed schedules prevent the exposure to high workloads successively on the same body region and minimized between-worker variance in cumulative daily workload. Whereas some of workers were successively assigned to high workload workstation under no job rotation and serial job rotation. This model would help to reduce the potential for work-related musculoskeletal disorders (WMSDs) without additional cost for engineering work, although it may need more computational time and relative complex job rotation sequences."]},
{"title": "Working postures and physical activity among registered nurses", "highlights": ["Nurses spent a small proportion of their work time in extreme postures.", "Nurses had few opportunities for rest and recovery.", "Nurses in the high activity group spent more time in extreme postures."], "abstract": ["Nurses report a high prevalence of musculoskeletal discomfort, particularly of the low back and neck/shoulder. This study characterized the full-shift upper arm and trunk postures and movement velocities of registered nurses using inertial measurement units (IMUs). Intensity of occupational physical activity (PA) was also ascertained using a waist-worn PA monitor and using the raw acceleration data from each IMU. Results indicated that nurses spent a relatively small proportion of their work time with the arms or trunk in extreme postures, but had few opportunities for rest and recovery in comparison to several other occupational groups. Comparisons between nurses in different PA groups suggested that using a combination of accelerometers secured to several body locations may provide more representative estimates of physical demands than a single, waist-worn PA monitor. The findings indicate a need for continued field-based research with larger sample sizes to facilitate the development of maximally effective intervention strategies."]},
{"title": "A static organization in a dynamic context \u2013 A qualitative study of changes in working conditions for Swedish engine officers", "highlights": ["Maritime engine room officers commonly agree that the rapid technological development and reduced staffing onboard have contributed to higher workload and altered work tasks.", "Engine room officers have not been given enough or proper resources to handle the new requirements of the altered working conditions in the shipping industry.", "Collegial support and comradeship are seen as the main resources of the onboard organization."], "abstract": ["During the last decades the shipping industry has undergone rapid technical developments and experienced hard economic conditions and increased striving for profitability. This has led to reduced staffing and changes in task performance, which has been reported to increase workload for the remaining seafarers. The working conditions on board have a number of distinct and in many ways unique characteristics, which makes the job demands and resources for seafarers unique in several ways. The purpose of this study was to assess how engine room staff perceives how these major technical and organizational changes in the shipping industry have affected job demands as well as resources. The study compiled individual interviews and focus groups interviews with engine crew members where they were asked to elaborate on the psychosocial work environment and the major changes in the working conditions on board. Engine crew describes a work situation where they feel a lack of resources. The content of the work has changed, staffing has been reduced, new tasks are being added but the organization of the crew and the design of the work place remains unaltered."]},
{"title": "Exposure\u2013response relationships for work-related neck and shoulder musculoskeletal disorders \u2013 Analyses of pooled uniform data sets", "highlights": ["This study explored relationships between work factors and neck/shoulder disorders.", "We recorded complaints/diagnoses in 33 groups (3141 workers), within which workers performed similar tasks.", "We assessed postures and muscular activity in sub-groups by direct measurements with technical instruments.", "Neck disorders were associated with head and arm posture, trapezius and forearm muscle activity, and wrist velocity.", "Shoulder disorders were associated with head, arm and wrist velocity, and trapezius and forearm muscle activity."], "abstract": ["There is a lack of quantitative data regarding exposure\u2013response relationships between occupational risk factors and musculoskeletal disorders in the neck and shoulders. We explored such relationships in pooled data from a series of our cross-sectional studies.", "We recorded the prevalence of complaints/discomfort (Nordic Questionnaire) and diagnoses (physical examination) in 33 groups (24 female and 9 male) within which the workers had similar work tasks (3141 workers, of which 817 were males). In representative sub-groups, we recorded postures and velocities of the head (N\u00a0=\u00a0299) and right upper arm (inclinometry; N\u00a0=\u00a0306), right wrist postures and velocities (electrogoniometry; N\u00a0=\u00a0499), and muscular activity (electromyography) in the right trapezius muscle (N\u00a0=\u00a0431) and forearm extensors (N\u00a0=\u00a0206). We also assessed the psychosocial work environment (Job Content Questionnaire).", "Uni- and multivariate linear meta-regression analysis revealed several statistically significant group-wise associations. Neck disorders were associated with head inclination, upper arm elevation, muscle activity of the trapezius and forearm extensors and wrist posture and angular velocity. Right-side shoulder disorders were associated with head and upper arm velocity, activity in the trapezius and forearm extensor muscles and wrist posture and angular velocity.", "The psychosocial work environment (low job control, job strain and isostrain) was also associated with disorders. Women exhibited a higher prevalence of neck and shoulder complaints and tension neck syndrome than men, when adjusting for postures, velocities, muscular activity or psychosocial exposure.", "In conclusion, the analyses established quantitative exposure\u2013response relationships between neck and shoulder disorders and objective measures of the physical workload on the arm. Such information can be used for risk assessment in different occupations/work tasks, to establish quantitative exposure limits, and for the evaluation of preventive measures."]},
{"title": "Observer performance in estimating upper arm elevation angles under ideal viewing conditions when assisted by posture matching software", "highlights": ["On average, observers were virtually unbiased in estimating upper arm elevation to the nearest 1\u00b0.", "Observers were most proficient at estimating 0\u00b0 and 90\u00b0, and least proficient at 60\u00b0 postures.", "Within observer variance was considerable, indicating the risk of relying on single observations.", "If high precision is required & repeated observations are not feasible, inclinometry is suggested."], "abstract": ["Selecting a suitable body posture measurement method requires performance indices of candidate tools. Such data are lacking for observational assessments made at a high degree of resolution. The aim of this study was to determine the performance (bias and between- and within-observer variance) of novice observers estimating upper arm elevation postures assisted by posture matching software to the nearest degree from still images taken under ideal conditions. Estimates were minimally biased from true angles: the mean error across observers was less than 2\u00b0. Variance between observers was minimal. Considerable variance within observers, however, underlined the risk of relying on single observations. Observers were more proficient at estimating 0\u00b0 and 90\u00b0 postures, and less proficient at 60\u00b0. Thus, under ideal visual conditions observers, on average, proved proficient at high resolution posture estimates; further investigation is required to determine how non-optimal image conditions, as would be expected from occupational data, impact proficiency."]},
{"title": "Rasmussen's legacy: A paradigm change in engineering for safety", "highlights": ["System theory provides a formal foundation for much improved hazard analysis that includes humans as part of the system.", "STAMP, a new model of causation built on Rasmussen's ideas, provides a way to understand accidents in sociotechnical systems.", "Rasmussen's abstraction hierarchy underlies Intent Specifications, which ground specifications on psychological principles.", "Rasmussen's model of human-task mismatch can be used to extend engineering hazard analysis to include human error."], "abstract": ["This paper describes three applications of Rasmussen's idea to systems engineering practice. The first is the application of the abstraction hierarchy to engineering specifications, particularly requirements specification. The second is the use of Rasmussen's ideas in safety modeling and analysis to create a new, more powerful type of accident causation model that extends traditional models to better handle human-operated, software-intensive, sociotechnical systems. Because this new model has a formal, mathematical foundation built on systems theory (as was Rasmussen's original model), new modeling and analysis tools become possible. The third application is to engineering hazard analysis. Engineers have traditionally either omitted human from consideration in system hazard analysis or have treated them rather superficially, for example, that they behave randomly. Applying Rasmussen's model of human error to a powerful new hazard analysis technique allows human behavior to be included in engineering hazard analysis."]},
{"title": "Differences in vision performance in different scenarios and implications for design", "highlights": ["We present data from a survey of visual acuity with 362 participants.", "We use a range of vision measures of particular relevance to product design.", "We give recommendations for text sizes to use in different situations.", "Text needs to be about 18% larger for comfortable rather than perceived threshold viewing.", "It needs to be a further 20% larger if users are not going to use reading glasses."], "abstract": ["To design accessibly, designers need good, relevant population data on visual abilities. However, currently available data often focuses on clinical vision measures that are not entirely relevant to everyday product use. This paper presents data from a pilot survey of 362 participants in the UK, covering a range of vision measures of particular relevance to product design. The results from the different measures are compared, and recommendations are given for relative text sizes to use in different situations. The results indicate that text needs to be 17\u201318% larger for comfortable rather than perceived threshold viewing, and a further 20% larger when users are expected to wear their everyday vision setup rather than specific reading aids."]},
{"title": "Supporting the human life-raft in confronting the juggernaut of technology: Jens Rasmussen, 1961\u20131986", "highlights": ["Thematic survey of Jens Rasmussen's papers (and Ris\u00f8 work reports) from 1961 to 1986.", "Theme 1: Rasmussen's engineering epistemology.", "Theme 2: Rasmussen's approach of conceptualizing the problem of technical reliability as a systems problem.", "Theme 3: Rasmussen's conceptualization of the operator in technical contexts.", "Theme 4: Supporting the operator's everyday knowing and acting for correct functioning of the system.", "Theme 5: Emphasis on a generalized qualitative and categorical approach for systems design.", "Theme 6: Relation between operator and designer in systems design."], "abstract": ["Jens Rasmussen's contribution to the field of human factors and ergonomics has had a lasting impact. Six prominent interrelated themes can be extracted from his research between 1961 and 1986. These themes form the basis of an engineering epistemology which is best manifested by his abstraction hierarchy. Further, Rasmussen reformulated technical reliability using systems language to enable a proper human-machine fit. To understand the concept of human-machine fit, he included the operator as a central component in the system to enhance system safety. This change resulted in the application of a qualitative and categorical approach for human-machine interaction design. Finally, Rasmussen's insistence on a working philosophy of systems design as being a joint responsibility of operators and designers provided the basis for averting errors and ensuring safe and correct system functioning."]},
{"title": "Validity of a small low-cost triaxial accelerometer with integrated logger for uncomplicated measurements of postures and movements of head, upper back and upper arms", "highlights": ["A new triaxial accelerometer was evaluated for measuring physical workload.", "The accelerometer is well suited for measuring postures and movements during work.", "The accelerometer is appropriate as objective method for ergonomic risk assessments."], "abstract": ["Repetitive work and work in constrained postures are risk factors for developing musculoskeletal disorders. Low-cost, user-friendly technical methods to quantify these risks are needed. The aims were to validate inclination angles and velocities of one model of the new generation of accelerometers with integrated data loggers against a previously validated one, and to compare meaurements when using a plain reference posture with that of a standardized one. All mean (n\u00a0=\u00a012 subjects) angular RMS-differences in 4 work tasks and 4 body parts were <2.5\u00b0 and all mean median angular velocity differences <5.0 \u00b0/s. The mean correlation between the inclination signal-pairs was 0.996. This model of the new generation of triaxial accelerometers proved to be comparable to the validated accelerometer using a data logger. This makes it well-suited, for both researchers and practitioners, to measure postures and movements during work. Further work is needed for validation of the plain reference posture for upper arms."]},
{"title": "Holistic sustainable development: Floor-layers and micro-enterprises", "highlights": ["Needs to give more attention to microbusinesses as a specific entity.", "How OHS is inextricably linked with the survival and development of the trade.", "The active solutions sought with floor layers broaden current views on SD.", "The import of approaching MiEs as a complex system rather than individual elements.", "Need for developing strategies combining both material and structural solutions."], "abstract": ["Attracting and retaining workers is important to ensuring the sustainability of floor laying businesses, which are for the most part micro-enterprises (MiE). The aim of this paper is to shed light on the challenges MiE face in OHS implementation in the context of sustainable development. Participative ergonomics and user-centred design approaches were used. The material collected was reviewed to better understand the floor layers' viewpoints on sustainability. The solutions that were retained and the challenges encountered to make material handling and physical work easier and to develop training and a website are presented. The importance of OHS as a sustainability factor, its structuring effect, what distinguishes MiE from small businesses and possible strategies for workings with them are also discussed."]},
{"title": "Estimation of inertial parameters of the lower trunk in pregnant Japanese women: A longitudinal comparative study and application to motion analysis", "highlights": ["We quantified inertial parameters of the lower trunk in pregnant Japanese women.", "Kinetic analyses were performed with both estimated and standard parameters.", "Inertia parameters of the lower trunk changed as pregnancy progressed.", "There were more significant differences in kinetic data at the late gestation.", "The use of appropriate parameters would provide the validity of kinetic analyses."], "abstract": ["We aimed to quantify the inertial parameters of the lower trunk segment in pregnant Japanese women and compare kinetic data during tasks calculated with parameters estimated in this study to data calculated with standard parameters. Eight pregnant women and seven nulliparous women participated. Twenty-four infrared reflective markers were attached to the lower trunk, and the standing position was captured by eight infrared cameras. The lower trunk was divided into parts, and inertial parameters were calculated. Pregnant women performed a movement task that involved standing from a chair, picking up plates, and walking forward after turning to the right. Kinetic analysis was performed using standard inertial parameters and the newly calculated parameters. There were more significant differences between methods in the kinetic data at the latter stages of pregnancy. The inertial parameters calculated in this study should be used to ensure the validity of biomechanical studies of pregnant Japanese women."]},
{"title": "Using event related potentials to identify a user's behavioural intention aroused by product form design", "highlights": ["Product form designs can evoke diverse user experience.", "No neural research on behavioural intention aroused by product form design has been done.", "Product form designs with an ability to arouse subjects' behavioural intention can evoke enhanced N300 and LPP.", "The proposed method leads to the development of user experience measurement and product evaluation."], "abstract": ["The capacity of product form to arouse user's behavioural intention plays a decisive role in further user experience, even in purchase decision, while traditional methods rarely give a fully understanding of user experience evoked by product form, especially the feeling of anticipated use of product. Behavioural intention aroused by product form designs has not yet been investigated electrophysiologically. Hence event related potentials (ERPs) were applied to explore the process of behavioural intention when users browsed different smart phone form designs with brand and price not taken into account for mainly studying the brain activity evoked by variety of product forms. Smart phone pictures with different anticipated user experience were displayed with equiprobability randomly. Participants were asked to click the left mouse button when certain picture gave them a feeling of behavioural intention to interact with. The brain signal of each participant was recorded by Curry 7.0. The results show that pictures with an ability to arouse participants' behavioural intention for further experience can evoke enhanced N300 and LPPs (late positive potentials) in central-parietal, parietal and occipital regions. The scalp topography shows that central-parietal, parietal and occipital regions are more activated. The results indicate that the discrepancy of ERPs can reflect the neural activities of behavioural intention formed or not. Moreover, amplitude of ERPs occurred in corresponding brain areas can be used to measure user experience. The exploring of neural correlated with behavioural intention provide an accurate measurement method of user's perception and help marketers to know which product can arouse users' behavioural intention, maybe taken as an evaluating indicator of product design."]},
{"title": "Predicting stretcher carriage: Investigating variations in bilateral carry tests", "highlights": ["All bilateral carry tests had strong performance correlations to stretcher carriage.", "Sensitivity and specificity analyses revealed good predictive ability of all tests.", "Manipulating speed and object had minimal impact on predicting stretcher carriage.", "Predictive equations are presented to set cut-scores for each test option."], "abstract": ["Carrying a casualty on a stretcher is a critical task within military and emergency service occupations. This study evaluated the impact of manipulating carry speed and the object type in bilateral carries on the ability to predict performance and reflect the physical and physiological requirements of a unilateral stretcher carry. We demonstrated that three task-related predictive tests; a jerry can carry performed at 4.5\u00a0km\u00a0h", "or 5.0\u00a0km\u00a0h", " and a kettle-bell carry performed at 5.0\u00a0km\u00a0h", " were strongly predictive of the physical and physiological demands of an individual participating as part of a four-person stretcher carry team. Therefore, bilateral predictive assessments have the utility for predicting the suitability of employees to effectively and safely conduct a four-person unilateral stretcher carry."]},
{"title": "Does drywall installers' innovative idea reduce the ergonomic exposures of ceiling installation: A field case study", "highlights": ["Focus groups with drywall installers in participatory manner.", "Workers' innovative idea for ceiling installation.", "Quantitative exposure data was collected at baseline and intervention.", "Exposure was reduced at the intervention."], "abstract": ["The study was conducted to assess an intervention suggested by the workers to reduce the physical or ergonomic exposures of the drywall installation task.", "The drywall installers were asked to brainstorm on innovative ideas that could reduce their ergonomic exposures during the drywall installation work. The workers proposed the idea of using a \u2018deadman\u2019 (narrow panel piece) to hold the panels to the ceiling while installing them. The researcher collected quantitative exposure data (PATH, 3DSSPP) at the baseline and intervention phases and compared the phases to find out any change in the exposure while using the \u2018deadman\u2019.", "Results showed that ergonomic exposures (such as overhead arm and awkward trunk postures and heavy load handling) were reduced at the intervention phase while using the \u2018deadman\u2019 with an electrically operated lift.", "The concept of the \u2018deadman\u2019, which was shown to help reduce musculoskeletal exposures during ceiling installation, can be used to fabricate a permanent ergonomic tool to support the ceiling drywall panel."]},
{"title": "What makes icons appealing? The role of processing fluency in predicting icon appeal in different task contexts", "highlights": ["Ease of processing is a heuristic determining preferences in a wide range of tasks.", "Ease of processing affected appeal evaluations for icons.", "Icons which were easier to find and identify were evaluated more positively.", "Appeal evaluations mirrored patterns of performance; both depend on processing ease.", "Ease of processing heuristics may explain relationships observed between usability and appeal."], "abstract": ["Although icons appear on almost all interfaces, there is a paucity of research examining the determinants of icon appeal. The experiments reported here examined the icon characteristics determining appeal and the extent to which processing fluency \u2013 the subjective ease with which individuals process information \u2013 was used as a heuristic to guide appeal evaluations. Participants searched for, and identified, icons in displays. The initial appeal of icons was held constant while ease of processing was manipulated by systematically varying the complexity and familiarity of the icons presented and the type of task participants were asked to carry out. Processing fluency reliably influenced users' appeal ratings and appeared to be based on users' unconscious awareness of the ease with which they carried out experimental tasks."]},
{"title": "Effect of an on-hip load-carrying belt on physiological and perceptual responses during bimanual anterior load carriage", "highlights": ["Physiological and perceptual effects of an on-hip load-carrying belt (HLCB) during bimanual anterior load carriage were evaluated.", "The HLCB could lower heart rate, oxygen uptake, minute ventilation and peripheral rating of perceived exertion when carrying a 15\u00a0kg load.", "Using HLCB or similar device is therefore recommended for bimanual anterior load carriage of 15\u00a0kg or probably larger."], "abstract": ["Manual load carriage continues to be a major contributor of musculoskeletal injury. This study investigates the physiological and subjective effects of an on-hip load-carrying belt (HLCB) during bimanual anterior load carriage. Fifteen healthy male participants walked on a level ground treadmill at 4.5\u00a0km/h for 5\u00a0min carrying 5, 10 and 15\u00a0kg loads with hands and arms in front of the body, with and without using the HLCB (WD and ND). Heart rate, normalized oxygen uptake, minute ventilation and, central and peripheral ratings of perceived exertion were the dependent variables. The mean heart rate, normalized oxygen uptake, minute ventilation and peripheral rating of perceived exertion increased significantly with load under both WD and ND conditions. At a load of 15\u00a0kg, the mean heart rate, normalized oxygen uptake, minute ventilation and peripheral rating of perceived exertion were significantly lower by 6.6%, 8.0%, 11.8% and 13.9% respectively in WD condition when compared to the ND condition. There was no significant difference between WD and ND conditions with 5 or 10\u00a0kg load. It can be concluded that the HLCB could reduce a person's physiological and peripheral perceptual responses when walking on a level ground treadmill at 4.5\u00a0km/h with a load of 15\u00a0kg. Using a HLCB or similar device is therefore recommended for bimanual anterior load carriage for loads of 15\u00a0kg or probably larger."]},
{"title": "Driving without wings: The effect of different digital mirror locations on the visual behaviour, performance and opinions of drivers", "highlights": ["Digital mirrors can overcome many of the limitations of traditional reflective mirrors in cars.", "A driving simulator study evaluated 5 different configurations of digital mirrors.", "Digital mirrors allowed drivers to more rapidly pick up salient information from the road scene.", "Drivers preferred configurations that preserved elements of real-world mapping.", "Further work is required to validate results."], "abstract": ["Drivers' awareness of the rearward road scene is critical when contemplating or executing lane-change manoeuvres, such as overtaking. Preliminary investigations have speculated on the use of rear-facing cameras to relay images to displays mounted inside the car to create \u2018digital mirrors'. These may overcome many of the limitations associated with traditional \u2018wing\u2019 and rear-view mirrors, yet will inevitably effect drivers' normal visual scanning behaviour, and may force them to consider the rearward road scene from an unfamiliar perspective that is incongruent with their mental model of the outside world. We describe a study conducted within a medium-fidelity simulator aiming to explore the visual behaviour, driving performance and opinions of drivers while using internally located digital mirrors during different overtaking manoeuvres. Using a generic UK motorway scenario, thirty-eight experienced drivers conducted overtaking manoeuvres using each of five different layouts of digital mirrors with varying degrees of \u2018real-world\u2019 mapping. The results showed reductions in decision time for lane changes and eyes-off road time while using the digital mirrors, when compared with baseline traditional reflective mirrors, suggesting that digital displays may enable drivers to more rapidly pick up the salient information from the rearward road scene. Subjectively, drivers preferred configurations that most closely matched existing mirror locations, where aspects of real-world mapping were largely preserved. The research highlights important human factors issues that require further investigation prior to further development/implementation of digital mirrors within vehicles. Future work should also aim to validate findings within real-world on-road environments whilst considering the effects of digital mirrors on other important visual behaviour characteristics, such as depth perception."]},
{"title": "The Helmet Fit Index \u2013 An intelligent tool for fit assessment and design customisation", "highlights": ["A novel method is proposed to objectively compare fitting accuracy of helmets.", "The Helmet Fit Index (HFI) provides a fit score on a scale from 0 to 100.", "The index is a suitable indicator of fit when compared to subjective assessments.", "Females and Asian people experienced lower fit scores than males and Caucasians.", "The index can be used for helmet design optimization and customisation."], "abstract": ["Helmet safety benefits are reduced if the headgear is poorly fitted on the wearer's head. At present, there are no industry standards available to assess objectively how a specific protective helmet fits a particular person. A proper fit is typically defined as a small and uniform distance between the helmet liner and the wearer's head shape, with a broad coverage of the head area. This paper presents a novel method to investigate and compare fitting accuracy of helmets based on 3D anthropometry, reverse engineering techniques and computational analysis. The Helmet Fit Index (HFI) that provides a fit score on a scale from 0 (excessively poor fit) to 100 (perfect fit) was compared with subjective fit assessments of surveyed cyclists. Results in this study showed that quantitative (HFI) and qualitative (participants' feelings) data were related when comparing three commercially available bicycle helmets. Findings also demonstrated that females and Asian people have lower fit scores than males and Caucasians, respectively. The HFI could provide detailed understanding of helmet efficiency regarding fit and could be used during helmet design and development phases."]},
{"title": "River and fish pollution in Malaysia: A green ergonomics perspective", "highlights": ["Human factors increase heavy metal concentrations in river water in Malaysia.", "The bioaccumulation of toxic metals in the food chain impacts social wellness.", "There is limited awareness of the dangers of high heavy metal concentrations.", "Policy interventions can address human factors (e.g., enhancing awareness).", "More collaboration between stakeholders can enhance policy effectiveness."], "abstract": ["Human activities, such as industrial, agricultural, and domestic pursuits, discharge effluents into riverine ecological systems that contains aquatic resources, such as fish, which are also used by humans. We conducted case studies in Malaysia to investigate the impacts of these human activities on water and fish resources, as well as on human well-being from an ergonomics perspective. This research shows that a green ergonomics approach can provide us with useful insights into sustainable relationships between humans and ecology in facilitating human well-being in consideration of the overall performance of the social-ecological system. Heavy metal concentrations contained in the effluents pollute river water and contaminate fish, eventually creating significant health risks and economic costs for residents, including the polluters. The study suggests a number of policy interventions to change human behavior and achieve greater collaboration between various levels of government, academia, civil society, and businesses to help establish sustainable relationships between humans and ecology in Malaysia."]},
{"title": "Quantification of long cane usage characteristics with the constant contact technique", "highlights": ["Cane usage characteristics show deviation from mobility skill training.", "Grasp type shows large inter-subject variability but low intra-subject variability.", "Roll angle range of a cane small to integrate a sensor on a long cane."], "abstract": ["While a number of Electronic Travel Aids (ETAs) have been developed over the past decades, the conventional long cane remains the most widely utilized navigation tool for people with visual impairments. Understanding the characteristics of long cane usage is crucial for the development and acceptance of ETAs. Using optical tracking, cameras and inertial measurement units, we investigated grasp type, cane orientation and sweeping characteristics of the long cane with the constant contact technique. The mean cane tilt angle, sweeping angle, and grip rotation deviation were measured. Grasp type varied among subjects, but was maintained throughout the experiments, with thumb and index finger in contact with the cane handle over 90% of the time. We found large inter-subject differences in sweeping range and frequency, while the sweeping frequency showed low intra-subject variability. These findings give insights into long cane usage characteristics and provide critical information for the development of effective ETAs."]},
{"title": "Using the decision ladder to understand road user decision making at actively controlled rail level crossings", "highlights": ["The decision ladder was used to examine decision making at rail level crossings.", "Critical decision questionnaire data was used to populate the decision ladder.", "Decision making at rail level crossings varies within and between road user groups.", "Rail level crossings permit flexibility for circumvention especially in pedestrians.", "Provision of more information timed appropriately may prompt safer decisions."], "abstract": ["Rail level crossings (RLXs) represent a key strategic risk for railways worldwide. Despite enforcement and engineering countermeasures, user behaviour at RLXs can often confound expectations and erode safety. Research in this area is limited by a relative absence of insights into actual decision making processes and a focus on only a subset of road user types. One-hundred and sixty-six road users (drivers, motorcyclists, cyclists and pedestrians) completed a diary entry for each of 457 naturalistic encounters with RLXs when a train was approaching. The final eligible sample comprised 94 participants and 248 encounters at actively controlled crossings where a violation of the active warnings was possible. The diary incorporated Critical Decision Method probe questions, which enabled user responses to be mapped onto Rasmussen's decision ladder. Twelve percent of crossing events were non-compliant. The underlying decision making was compared to compliant events and a reference decision model to reveal important differences in the structure and type of decision making within and between road user groups. The findings show that engineering countermeasures intended to improve decision making (e.g. flashing lights), may have the opposite effect for some users because the system permits a high level of flexibility for circumvention. Non-motorised users were more likely to access information outside of the warning signals because of their ability to achieve greater proximity to the train tracks and the train itself. The major conundrum in resolving these issues is whether to restrict the amount of time and information available to users so that it cannot be used for circumventing the system or provide more information to help users make safe decisions."]},
{"title": "Resilience skills as emergent phenomena: A study of emergency departments in Brazil and the United States", "highlights": ["Resilience in two EDs is investigated.", "Origins of resilience skills (RSs).", "Model of RSs as emergent phenomena.", "Instantiation of the model.", "Factors that support or degrade RSs."], "abstract": ["Although the use of resilience skills (RSs) by emergency department (ED) front-line staff is ubiquitous, the nature and origin of these skills tend to be taken for granted. This study investigates the research question \u201cwhere do RSs come from\u201d? Case studies in two EDs were undertaken in order to answer the research question: one in Brazil and the other in the United States. The case studies adopted the same data collection and analysis procedures, involving interviews, questionnaires, observations, and analysis of documents. A model for describing RSs as emergent phenomena is proposed. The model indicates that RSs arise from interactions between: work constraints, hidden curriculum, gaps in standardized operating procedures, organizational support for resilience, and RSs themselves. An instantiation of the model is illustrated by a critical event identified from the American ED. The model allows the identification of leverage points for influencing the development of RSs, instead of leaving their evolution purely to chance."]},
{"title": "Suitability of virtual prototypes to support human factors/ergonomics evaluation during the design", "highlights": ["Both AR and VE prototypes support the assessment of human factors/ergonomics during the design.", "The VE system was more suitable to support the assessment of visibility, reach, and the use of tools than the AR system.", "System model characteristics can impact the suitability of the virtual prototype for the HFE evaluation.", "To assess HFE issues related to environment, force and time, more sensory modalities are required."], "abstract": ["In recent years, the use of virtual prototyping has increased in product development processes, especially in the assessment of complex systems targeted at end-users. The purpose of this study was to evaluate the suitability of virtual prototyping to support human factors/ergonomics evaluation (HFE) during the design phase. Two different virtual prototypes were used: augmented reality (AR) and virtual environment (VE) prototypes of a maintenance platform of a rock crushing machine. Nineteen designers and other stakeholders were asked to assess the suitability of the prototype for HFE evaluation. Results indicate that the system model characteristics and user interface affect the experienced suitability. The VE system was valued as being more suitable to support the assessment of visibility, reach, and the use of tools than the AR system. The findings of this study can be used as a guidance for the implementing virtual prototypes in the product development process."]},
{"title": "Quantifying warfighter performance in a target acquisition and aiming task using wireless inertial sensors", "highlights": ["IMU sensor array proposed as a means to measure warfighter aiming performance.", "IMU array detected decreases in warfighter aiming performance as a result of fatigue.", "IMUs are effective at pre/post comparisons of fatigue, load and equipment variations.", "IMUs have wide applicability, including: athletics, rehabilitation, and performance."], "abstract": ["An array of inertial measurement units (IMUS) was experimentally employed to analyze warfighter performance on a target acquisition task pre/post fatigue. Eleven participants (5M/6F) repeated an exercise circuit carrying 20\u00a0kg of equipment until fatigued. IMUs secured to the sacrum, sternum, and a rifle quantified peak angular velocity magnitude (PAVM) and turn time (TT) on a target acquisition task (three aiming events with two 180\u00b0 turns) within the exercise circuit. Turning performance of two turns was evaluated pre/post fatigue. Turning performance decreased with fatigue. PAVMs decreased during both turns for the sternum (", "\u00a0<\u00a00.001), sacrum (", "\u00a0=\u00a00.007) and rifle (", "\u00a0=\u00a00.002). TT increased for the sternum (", "\u00a0=\u00a00.001), sacrum (", "\u00a0=\u00a00.003), and rifle (", "\u00a0=\u00a00.02) during turn 1, and for the rifle (", "\u00a0=\u00a00.04) during turn 2. IMUs detected and quantified changes in warfighter aiming performance after fatigue. Similar methodologies can be applied to many movement tasks, including quantifying movement performance for load, fatigue, and equipment conditions."]},
{"title": "Using kinematic reduction for studying grasping postures. An application to power and precision grasp of cylinders", "highlights": ["Reducing hand kinematics with PCA is proposed to study grasping in ergonomics.", "Kinematics of power and precision grasps of cylinders is reduced to 5 factors.", "Factors: ", ", ", ", ", ", ", " and ", ".", "Diameter affects power and precision grasps, weight only affects precision grasp.", "Hand size is not enough to explain the effect of the subject on the grasping posture."], "abstract": ["The kinematic analysis of human grasping is challenging because of the high number of degrees of freedom involved. The use of principal component and factorial analyses is proposed in the present study to reduce the hand kinematics dimensionality in the analysis of posture for ergonomic purposes, allowing for a comprehensive study without losing accuracy while also enabling velocity and acceleration analyses to be performed. A laboratory study was designed to analyse the effect of weight and diameter in the grasping posture for cylinders. This study measured the hand posture from six subjects when transporting cylinders of different weights and diameters with precision and power grasps. The hand posture was measured using a Vicon", " motion-tracking system, and the principal component analysis was applied to reduce the kinematics dimensionality. Different ANOVAs were performed on the reduced kinematic variables to check the effect of weight and diameter of the cylinders, as well as that of the subject. The results show that the original twenty-three degrees of freedom of the hand were reduced to five, which were identified as ", ", ", ", ", ", ", " and ", ". Both cylinder diameter and weight significantly affected the precision grasping posture: diameter affects ", ", ", " and ", ", while weight affects ", ", ", " and ", ". The power-grasping posture was mainly affected by the cylinder diameter, through ", ", ", " and ", ". The grasping posture was largely affected by the subject factor and this effect couldn't be attributed only to hand size. In conclusion, this kinematic reduction allowed identifying the effect of the diameter and weight of the cylinders in a comprehensive way, being diameter more important than weight."]},
{"title": "Mind the gap \u2013 Deriving a compatible user mental model of the home heating system to encourage sustainable behaviour", "highlights": ["A novel approach to specify the \u2018scaffolding\u2019 underlying Norman's 7 stages of activity.", "Development of a compatible user mental model of the domestic heating system that supports sustainable behaviour.", "Demonstrating how different mental model elements are applied at the 7 stages of activity to target climate change strategies."], "abstract": ["Householders' behaviour with their home heating systems is a considerable contributor to domestic energy consumption. To create a design specification for the \u2018scaffolding\u2019 needed for sustainable behaviour with home heating controls, Norman's (1986) Gulf of Execution and Evaluation was applied to the home heating system. A Home Heating Design Model (DM) was produced with a home heating expert. Norman's (1986) 7 Stages of Activity were considered to derive a Compatible User Mental Model (CUMM) of a typical Heating System. Considerable variation in the concepts needed at each stage was found. Elements that could be derived from the DM supported stages relating to action specification, execution, perception and interpretation, but many are not communicated in the design of typical heating controls. Stages relating to goals, intentions and evaluation required concepts beyond the DM. A systems view that tackles design for sustainable behaviour from a variety of levels is needed."]},
{"title": "Influence of different stool types on muscle activity and lumbar posture among dentists during a simulated dental screening task", "highlights": ["The influence of different stool types in dentists is presented.", "Classical dental stools facilitate a 90\u00b0 hip angle during sitting.", "Saddle and Ghopec stools allow for a greater hip angle.", "Sitting with a 125\u00b0 angle activated abdominal muscles more and back muscles less.", "The Ghopec is considered the most suitable stool to maintain neutral posture."], "abstract": ["Whereas in the past dental stools typically facilitated a 90\u00b0 hip angle, a number of currently available alternative designs allow for a more extended hip posture. The present study investigated the influence of different stool types on muscle activity and lumbar posture. Twenty five participants completed a simulated dental procedure on a standard stool, a saddle and the Ghopec. The latter stool comprises a seat pan consisting of a horizontal rear part for the pelvis and an inclinable sloping down front part for the upper legs, with a vertically and horizontally adjustable back rest. Lumbar posture was most close to neutral on the Ghopec, whereas sitting on a standard/saddle stool resulted in more flexed/extended postures respectively. Sitting with a 90\u00b0 angle (standard stool) resulted in higher activation of back muscles while sitting with a 125\u00b0 angle (saddle and Ghopec) activated abdominal muscles more, although less in the presence of a backrest (Ghopec). To maintain neutral posture during dental screening, the Ghopec is considered the most suitable design for the tasks undertaken."]},
{"title": "Obesity-related changes in prolonged repetitive lifting performance", "highlights": ["Obesity and obesity-by-time interaction effect on lifting performance were studied.", "The obese had 10\u00b0 lower trunk flexion angle and 17% lower RMS jerk.", "Over time, the obese group increased their acceleration.", "This was observed during the lifting portion with the box in hand.", "Current psychophysical limits are usable with caution, particularly when fatigued."], "abstract": ["Despite the rising prevalence of obesity, little is known about its moderating effects on injury risk factors, such as fatigue, in occupational settings. This study investigated the effect of obesity, prolonged repetitive lifting and their interaction on lifting performance of 14 participants, 7 obese (mean body mass index (BMI): 33.2\u00a0kg\u00a0m", ") and 7 non-obese (mean BMI: 22.2\u00a0kg\u00a0m", ") subjects. To present a physically challenging task, subjects performed repetitive lifting for 1\u00a0h at 120% of their maximum acceptable weight of lift. Generalized linear mixed models were fit to posture and acceleration data. The obese group bent to a \u223c10\u00b0 lower peak trunk sagittal flexion angle, had 17% lower root mean square (RMS) jerk and took 0.8\u00a0s longer per lift. Over time, the obese group increased their trunk transverse and sagittal posterior accelerations while the non-obese maintained theirs. Although the majority of lifting variables were unaffected by BMI or its interaction with prolonged lifting duration, the observed differences, combined with a greater upper body mass, necessitate a more cautious use of existing psychophysical lifting limits for individuals who are obese, particularly when fatigued."]},
{"title": "Occupational sitting behaviour and its relationship with back pain \u2013 A pilot study", "highlights": ["A new Sitting Categorisation Technology (", ") was successfully introduced.", " allows classification of sitting posture with an accuracy >80%.", "Occupational sitting behaviour was quantified in 20 subjects.", "Individuals with back pain exhibited a more static sitting behaviour.", " shows potential for monitoring sitting behaviour in the office environment."], "abstract": ["Nowadays, working in an office environment is ubiquitous. At the same time, progressively more people suffer from occupational musculoskeletal disorders. Therefore, the aim of this pilot study was to analyse the influence of back pain on sitting behaviour in the office environment.", "A textile pressure mat (64-sensor-matrix) placed on the seat pan was used to identify the adopted sitting positions of 20 office workers by means of random forest classification. Additionally, two standardised questionnaires (Korff, BPI) were used to assess short and long-term back pain in order to divide the subjects into two groups (with and without back pain). Independent t-test indicated that subjects who registered back pain within the last 24\u00a0h showed a clear trend towards a more static sitting behaviour. Therefore, the developed sensor system has successfully been introduced to characterise and compare sitting behaviour of subjects with and without back pain."]},
{"title": "Using archetypes to create user panels for usability studies: Streamlining focus groups and user studies", "highlights": ["9 Clusters and archetype persons to represent the clusters' ear characteristics are generated out of 200 participants.", "The cluster people are validated through a usability study.", "A methodological framework to develop a representative user panel is presented."], "abstract": ["Designers at the conceptual phase of products such as headphones, stress the importance of comfort, e.g. executing comfort studies and the need for a reliable user panel. This paper proposes a methodology to issue a reliable user panel to represent large populations and validates the proposed framework to predict comfort factors, such as physical fit. Data of 200 heads was analyzed by forming clusters, 9 archetypal people were identified out of a 200 people's ear database. The archetypes were validated by comparing the archetypes' responses on physical fit against those of 20 participants interacting with 6 headsets. This paper suggests a new method of selecting representative user samples for prototype testing compared to costly and time consuming methods which relied on the analysis of human geometry of large populations."]},
{"title": "Towards a new paradigm: Activity level balanced sustainability reporting", "highlights": ["Novel analytical framework.", "Real-life based case studies.", "Critical discussion of current practice.", "Approaches for advancing sustainability reporting.", "Roles for ergonomics professionals to combat global warming by compelling attention to human factors."], "abstract": ["Technoeconomic paradigms based economic growth theories suggest that waves of technological innovations drove the economic growth of advanced economies. Widespread economic degradation and pollution is an unintended consequence of such growth. Tackling environmental and social issues at firm levels would help us to overcome such issues at macro-levels. Consequently, the Triple Bottom Line (TBL) reporting approach promotes firm level economic, environmental and social performances. Incorporating Zink's (2014) 3-pillar presentation model, this paper indicates that economic, social and environmental performances tend to be reported at firm level. All three pillars are not covered evenly at the activity levels. Thus, a loophole is identified whereby excellent environmental performance at activity levels could potentially leave poor social performance undisclosed. A refinement of the TBL paradigm, whereby all three pillars are covered at the activity level, is suggested, to enhance sustainability reporting."]},
{"title": "Route complexity and simulated physical ageing negatively influence wayfinding", "highlights": ["Route complexity negatively influences wayfinding performance in terms of route efficiency and walking speed.", "Simulated elderly participants have higher heart rates and respiratory rates during a wayfinding task.", "Simulated physical ageing and route complexity do not interact on wayfinding performance and physiological outcomes.", "Physical ageing was simulated in an age-simulation field experiment by using gerontologic suits.", "A portable heart rate monitor was used to assess physiological outcomes like heart rate and respiratory rate."], "abstract": ["The aim of this age-simulation field experiment was to assess the influence of route complexity and physical ageing on wayfinding. Seventy-five people (aged 18\u201328) performed a total of 108 wayfinding tasks (i.e., 42 participants performed two wayfinding tasks and 33 performed one wayfinding task), of which 59 tasks were performed wearing gerontologic ageing suits. Outcome variables were wayfinding performance (i.e., efficiency and walking speed) and physiological outcomes (i.e., heart and respiratory rates). Analysis of covariance showed that persons on more complex routes (i.e., more floor and building changes) walked less efficiently than persons on less complex routes. In addition, simulated elderly participants perform worse in wayfinding than young participants in terms of speed (p\u00a0<\u00a00.001). Moreover, a linear mixed model showed that simulated elderly persons had higher heart rates and respiratory rates compared to young people during a wayfinding task, suggesting that simulated elderly consumed more energy during this task."]},
{"title": "Why we love or hate our cars: A qualitative approach to the development of a quantitative user experience survey", "highlights": ["We present a novel way of developing questionnaires for measuring user experience.", "Items were generated using users' natural and domain-specific language.", "The survey is sensitive to real-life experiences.", "Results were highly reliable to measure drivers' appraisals of their cars."], "abstract": ["This paper presents a more ecologically valid way of developing theory-based item questionnaires for measuring user experience. In this novel approach, items were generated using natural and domain-specific language of the research population, what seems to have made the survey much more sensitive to real experiences than theory-based ones. The approach was applied in a survey that measured car experience. Ten in-depth interviews were conducted with drivers inside their cars. The resulting transcripts were analysed with the aim of capturing their natural utterances for expressing their car experience. This analysis resulted in 71 categories of answers. For each category, one sentence was selected to serve as a survey-item. In an online platform, 538 respondents answered the survey. Data reliability, tested with Cronbach alpha index, was 0.94, suggesting a survey with highly reliable results to measure drivers' appraisals of their cars."]},
{"title": "Maintaining knife sharpness in industrial meat cutting: A matter of knife or meat cutter ability", "highlights": ["The ability to maintain the cutting edge sharp is notably individual.", "The knife quality influence on sharpness is inferior to meat cutters' influence.", "Knife usage time is a relevant predictor for discomfort and ultimately MSD.", "Beef cutting may require up to at least six freshly sharpened knives per day."], "abstract": ["Knife sharpness is imperative in meat cutting. The aim of this study was to compare the impact of knife blade steel quality with meat cutters\u2019 individual ability to maintain the cutting edge sharp in an industrial production setting. Twelve meat cutters in two different companies using three different knives during normal production were studied in this quasi-experimental study. Methods included were measuring knife cutting force before and after knife use, time knives were used, ratings of sharpness and discomfort and interviews. Results showed that the meat cutters' skill of maintaining sharpness during work had a much larger effect on knife sharpness during work than the knife steel differences. The ability was also related to feelings of discomfort and to physical exertion. It was found that meat cutters using more knives were more likely to suffer from discomfort in the upper limbs, which is a risk for developing MSD."]},
{"title": "Interactive visualizations for decision support: Application of Rasmussen's abstraction-aggregation hierarchy", "highlights": ["Rasmussen's abstraction-aggregation hierarchy was employed to develop two decision support systems maintenance and diagnostics.", "Experimental evaluations of both systems indicted that higher abstraction displays enhanced the performance of experienced personnel.", "Less experienced personnel found the higher abstraction displays less useful, in some cases even confusing.", "Training and/or aiding should focus on enhancing abilities of less experienced personnel to use higher abstraction displays when appropriate."], "abstract": ["Data visualization has of late received an enormous amount of attention from both researchers and practitioners. Even the popular press often includes impressive visualizations of various data sets. Interactive visualizations frequently include data visualizations, but they differ in that users employ the visualizations to make inferences, reach conclusions, and make decisions that result in changed and/or new visualizations. Data visualizations emphasize \u201cwhat is,\u201d but interactive visualizations address \u201cwhat if.\u201d In this way, interactive visualizations are often intended for decision support. This article addresses the design of interactive visualizations for decision support. An overall methodology is presented; central to this methodology is Jens Rasmussen's abstraction-aggregation hierarchy. The results of two applications and evaluations of the outcomes of using this methodology are discussed. The first application focused on interactive visualizations for helicopter maintenance. The second application addressed \u201centerprise diagnostics\u201d in the automobile industry where subjects were asked to diagnose the cause of failed automobile brands. The results of these two applications are used to assess the efficacy of the proposed methodology."]},
{"title": "Improving the detectability of oxygen saturation level targets for preterm neonates: A laboratory test of tremolo and beacon sonifications", "highlights": ["Too much supplemental oxygen can be a hazard for some premature neonates.", "Pulse oximetry sonifications do not reveal when oxygen saturation is too high.", "We added tremolo or a reference beacon to the current pulse oximetry sonification.", "The Tremolo sonification supported better monitoring than the current sonification.", "The Beacon sonification supported better monitoring than the current sonification."], "abstract": ["Recent guidelines recommend oxygen saturation (SpO", ") levels of 90%\u201395% for preterm neonates on supplemental oxygen but it is difficult to discern such levels with current pulse oximetry sonifications. We tested (1) whether adding levels of tremolo to a conventional log-linear pulse oximetry sonification would improve identification of SpO", " ranges, and (2) whether adding a beacon reference tone to conventional pulse oximetry confuses listeners about the direction of change. Participants using the Tremolo (94%) or Beacon (81%) sonifications identified SpO", " range significantly more accurately than participants using the LogLinear sonification (52%). The Beacon sonification did not confuse participants about direction of change. The Tremolo sonification may have advantages over the Beacon sonification for monitoring SpO", " of preterm neonates, but both must be further tested with clinicians in clinically representative scenarios, and with different levels of ambient noise and distractions."]},
{"title": "Standing on a declining surface reduces transient prolonged standing induced low back pain development", "highlights": ["Assessed alternative standing posture to alleviate prolonged standing low back pain.", "Participants stood on a declining surface and level ground for 75\u00a0min.", "Pain reports of those with low back pain on level ground decreased 58% on decline.", "Greater hip flexion, trunk center of gravity translated posteriorly on decline surface.", "Declining surface could help stabilize pelvis, alter the lower lumbar arc, prevent pain."], "abstract": ["While alternating standing position on a sloped surface has proven successful at reducing low back pain during standing, the purpose of this study was to evaluate standing solely on a declining surface to isolate the influence of the postural change. Seventeen participants performed two 75-min\u00a0prolonged standing occupational simulations\u2013 level ground and declining surface. Fifty-three percent of participants (9/17) were categorized as pain developers during the level ground standing condition. For these same pain developers, their average maximum pain scores were 58% lower during sloped standing. All participants showed greater hip flexion, trunk-to-thigh angle flexion, and posterior translation of the trunk center of gravity when standing on the sloped surface. These postural changes could cause the muscles crossing the hip posteriorly to increase passive stiffness and assist with stabilizing the pelvis. This study stresses the importance of hip kinematics, not just lumbar spine posture, in reducing prolonged standing induced low back pain."]},
{"title": "Iterative user centered design for development of a patient-centered fall prevention toolkit", "highlights": ["A patient-centered fall prevention paper toolkit was redesigned with decision support.", "Based on the first intervention to demonstrate a significant reduction in patient falls in an RCT.", "Nurses, patients, and family members were included in the iterative design cycle.", "Redesigned paper Fall T.I.P.S toolkit shown to be easier to use and preferred by nursing staff."], "abstract": ["Due to the large number of falls that occur in hospital settings, inpatient fall prevention is a topic of great interest to patients and health care providers. The use of electronic decision support that tailors fall prevention strategy to patient-specific risk factors, known as Fall T.I.P.S (Tailoring Interventions for Patient Safety), has proven to be an effective approach for decreasing hospital falls. A paper version of the Fall T.I.P.S toolkit was developed primarily for hospitals that do not have the resources to implement the electronic solution; however, more work is needed to optimize the effectiveness of the paper version of this tool. We examined the use of human factors techniques in the redesign of the existing paper fall prevention tool with the goal of increasing ease of use and decreasing inpatient falls. The inclusion of patients and clinical staff in the redesign of the existing tool was done to increase adoption of the tool and fall prevention best practices. The redesigned paper Fall T.I.P.S toolkit showcased a built in clinical decision support system and increased ease of use over the existing version."]},
{"title": "Constitutive kinematic modes and shapes during vehicle ingress/egress", "highlights": ["The motion during ingress/egress is decomposed into multi-joint constituent modes.", "The modes combine to form shapes associated with ingress/egress.", "Shapes maximise correlation to ease of ingress/egress.", "Knowledge of comfort shapes are instructive for interior vehicle design."], "abstract": ["A study was undertaken to investigate the kinematics of older users of passenger vehicles during ingress/egress and to seek correlations between their movement and comfort rating assigned by the subjects to the ease of vehicle ingress and egress. A principal component analysis was performed on the subjects\u2019 kinematics to identify the underlying modes of movement employed by the subjects. It was found that a small number of modes could describe the movements of all the subjects across all of the vehicles. Within the subspace defined by the modal vectors, shapes were found which correlated to the comfort rating for ease of ingress and egress which the subjects had assigned to each of the cars. Knowledge of these shapes which correspond to good and poor ingress and egress will be useful to the designers of interiors and exteriors of passenger vehicles for the older person. It is recommended that vehicle designs for the older person should attempt to avoid body positions which require excessive ankle articulation and lumbar flexion/extension during ingress and egress."]},
{"title": "Evaluation of overhead guide sign sheeting materials to increase visibility and safety for drivers", "highlights": ["We compared three retroreflective sheeting materials used for overhead guide signs.", "Diamond Grade sheeting provides the highest sign visibility for drivers at night.", "We didn't find statistical difference between Diamond Grade and High Intensity.", "High Intensity is considered an alternative solution for DOTs with limited budgets."], "abstract": ["Overhead guide sign visibility must increase to improve driver safety on roadways. Two methods increase overhead guide sign visibility: sign illumination and use of retroreflective sheeting materials. This paper compares three types of retroreflective sheeting: Engineering Grade (type I), Diamond Grade (type XI), and High Intensity (type IV). A field experiment was conducted at night using licensed drivers to determine the optimum retroreflective sheeting material that increases sign visibility and legibility. Results showed that, of the three types of retroreflective sheeting, Diamond Grade (type XI) sheeting requires minimum illuminance to be visible, followed by High Intensity (type IV) sheeting. Cost analysis, including labor, maintenance, and material cost components of the three retroreflective sheeting materials, showed that High Intensity (type IV) could increase sign visibility and legibility at night for Departments of Transportation with limited budgets, consequently increasing driver safety on roadways."]},
{"title": "Effects of elevation change on mental stress in high-voltage transmission tower construction workers", "highlights": ["The task workload was similar between working surface heights.", "Workers perceived an increasing mental stress as working surface height increased.", "One possible reason for the stress is the sense of insecurity and uncertainty.", "Another possible contributor to stress is the working environment.", "The third possible explanation for the stress is visual issues."], "abstract": ["High-voltage transmission tower construction is a high-risk operation due to the construction site locations, extreme climatic factors, elevated working surfaces, and narrow working space. To comprehensively enhance our understanding of the psychophysiological phenomena of workers in extremely high tower constructions, we carried out a series of field experiments to test and compare three working surface heights in terms of frequency-domain heart rate variability (HRV) measurements. Twelve experienced male workers participated in this experiment. The dependent variables, namely, heart rate (HR), normalized low-frequency power (nLF), normalized high-frequency power (nHF), and LF-to-HF power ratio (LF/HF), were measured with the Polar RS800CX heart rate monitor. The experimental results indicated that the task workload was similar between working surface heights. Tower construction workers perceived an increased level of mental stress as working surface height increased."]},
{"title": "Effects of indoor slippers on plantar pressure and lower limb EMG activity in older women", "highlights": ["Walking with slippers can increase foot contact area at foot landing.", "Wearing slippers can redistribute pressure over the plantar of the foot.", "The EMG differences between the slippers and barefoot were not significant.", "Soft slippers with a more compliant footbed result in a higher perception of comfort."], "abstract": ["Open-toe mule slippers are popular footwear worn at home especially by older women. However, their biomechanical effects are still poorly understood. The objective of this study is to therefore evaluate the physical properties of two typical types of open-toe mule slippers and the changes in plantar pressure and lower limb muscle activity of older women when wearing these slippers. Five walking trials have been carried out by ten healthy women. The results indicate that compared to barefoot, wearing slippers results in significant increases in the contact area of the mid-foot regions which lead to plantar pressure redistribution from metatarsal heads 2\u20133 and the lateral heel to the midfoot regions. However, there is no significant difference in the selected muscle activity across all conditions. The findings enhance our understanding of slipper features associated with changes in biomechanical measures thereby providing the basis of slipper designs for better foot protection and comfort."]},
{"title": "Eliciting good teaching from humans for machine learners", "highlights": ["We aim to improve Interactive Machine Learning by influencing the human teacher.", "We propose Teaching Guidance: instructions for teachers, to improve their input.", "Teaching Guidance is derived from optimal or heuristic teaching algorithms.", "We performed experiments to compare human teaching with and without teaching guidance.", "We found that Teaching Guidance substantially improves the data provided by teachers."], "abstract": ["We propose using computational teaching algorithms to improve human teaching for machine learners. We investigate example sequences produced naturally by human teachers and find that humans often do not spontaneously generate optimal teaching sequences for arbitrary machine learners. To elicit better teaching, we propose giving humans ", ", which are instructions on how to teach, derived from computational teaching algorithms or heuristics. We present experimental results demonstrating that teaching guidance substantially improves human teaching in three different problem domains. This provides promising evidence that human intelligence and flexibility can be leveraged to achieve better sample efficiency when input data to a learning system comes from a human teacher."]},
{"title": "Playing with knowledge: A virtual player for \u201cWho Wants to Be a Millionaire?\u201d that leverages question answering techniques", "highlights": ["We model a virtual player for \u201cWho Wants to be a Millionaire\u201d game.", "The virtual player uses Question Answering over Wikipedia and DBpedia knowledge.", "We performed experiments on the Italian and the English version of the game.", "The virtual player outperforms human players to correctly answer to questions of the game.", "The virtual player outperforms human players to play real games both in terms of level of the game reached and average income."], "abstract": ["This paper describes the techniques used to build a virtual player for the popular TV game \u201cWho Wants to Be a Millionaire?\u201d. The player must answer a series of multiple-choice questions posed in natural language by selecting the correct answer among four different choices. The architecture of the virtual player consists of 1) a ", " (QA) module, which leverages Wikipedia and DBpedia datasources to retrieve the most relevant passages of text useful to identify the correct answer to a question, 2) an ", " (AS) module, which assigns a score to each candidate answer according to different criteria based on the passages of text retrieved by the Question Answering module, and 3) a ", " (DM) module, which chooses the strategy for playing the game according to specific rules as well as to the scores assigned to the candidate answers.", "We have evaluated both the accuracy of the virtual player to correctly answer to questions of the game, and its ability to play real games in order to earn money. The experiments have been carried out on questions coming from the official Italian and English boardgames. The average accuracy of the virtual player for Italian is ", ", which is significantly better than the performance of human players, which is equal to ", ". The average accuracy of the virtual player for English is ", ". The comparison with human players is not carried out for English since, playing successfully the game heavily depends on the players' knowledge about popular culture, and in this experiment we have only involved a sample of Italian players. As regards the ability to play real games, which involves the definition of a proper strategy for the usage of lifelines in order to decide whether to answer to a question even in a condition of uncertainty or to retire from the game by taking the earned money, the virtual player earns \u20ac 114,531 on average for Italian, and \u20ac 88,878 for English, which exceeds the average amount earned by the human players to a greater extent (\u20ac 5926 for Italian)."]},
{"title": "Fighter pilots' heart rate, heart rate variation and performance during an instrument flight rules proficiency test", "highlights": ["During an instrument flight rules proficiency test, heart rate (HR)/heart rate variation (HRV) are sensitive measures of varying levels of pilot mental workload.", "During an instrument flight rules proficiency test, HR/HRV can distinguish fighter pilots' mental workload differences in situations where variations in performance are either insignificant or significant.", "HR/HRV measuring adds value to instrument flight rules proficiency testing."], "abstract": ["Increased task demand will increase the pilot mental workload (PMWL). When PMWL is increased, mental overload may occur resulting in degraded performance. During pilots' instrument flight rules (IFR) proficiency test, PMWL is typically not measured. Therefore, little is known about workload during the proficiency test and pilots' potential to cope with higher task demands than those experienced during the test. In this study, fighter pilots' performance and PMWL was measured during a real IFR proficiency test in an F/A-18 simulator. PMWL was measured using heart rate (HR) and heart rate variation (HRV). Performance was rated using Finnish Air Force's official rating scales. Results indicated that HR and HRV differentiate varying task demands in situations where variations in performance are insignificant. It was concluded that during a proficiency test, PMWL should be measured together with the task performance measurement."]},
{"title": "Efficient algorithms for game-theoretic betweenness centrality", "highlights": ["We propose a betweenness centrality based on the Shapley value and Semivalue.", "We develop polynomial algorithms for our game-theoretic metrics.", "We evaluate our measures in scenarios where simultaneous node failures occur.", "Our measures obtain better results than standard betweenness centrality.", "We provide an empirical evaluation of algorithms on real-life and random graphs."], "abstract": ["Betweenness centrality measures the ability of different nodes to control the flow of information in a network. In this article, we extend the standard definition of betweenness centrality using ", "\u2014a family of solution concepts from cooperative game theory that includes, among others, the Shapley value and the Banzhaf power index. Any Semivalue-based betweenness centrality measure (such as, for example, the Shapley value-based betweenness centrality measure) has the advantage of evaluating the importance of individual nodes by considering the roles they each play in different groups of nodes. Our key result is the development of a general polynomial-time algorithm to compute the Semivalue-based betweenness centrality measure, and an even faster algorithm to compute the Shapley value-based betweenness centrality measure, both for weighted and unweighted networks. Interestingly, for the unweighted case, our algorithm for computing the Shapley value-based centrality has the same complexity as the best known algorithm for computing the standard betweenness centrality due to Brandes ", ". We empirically evaluate our measures in a simulated scenario where nodes fail simultaneously. We show that, compared to the standard measure, the ranking obtained by our measures reflects more accurately the influence that different nodes have on the functionality of the network."]},
{"title": "Measuring inconsistency in probabilistic logic: rationality postulates and Dutch book interpretation", "highlights": ["Consistency, independence and continuity are incompatible postulates.", "Minimal inconsistent sets are not suitable to analyze probabilistic incon-sistencies.", "Independence can be weakened considering the underlying consolidation process.", "Inconsistency and incoherence measures based on distances and Dutch books coincide."], "abstract": ["Inconsistency measures have been proposed as a way to manage inconsistent knowledge bases in the AI community. To deal with inconsistencies in the context of conditional probabilistic logics, rationality postulates and computational efficiency have driven the formulation of inconsistency measures. Independently, investigations in formal epistemol-ogy have used the betting concept of Dutch book to measure an agent's degree of incoherence. In this paper, we show the impossibility of joint satisfiability of the proposed postulates, proposing to replace them by more suitable ones. Thus we reconcile the rationality postulates for inconsistency measures in probabilistic bases and show that several inconsistency measures suggested in the literature and computable with linear programs satisfy the reconciled postulates. Additionally, we give an interpretation for these feasible measures based on the formal epistemology concept of Dutch book, bridging the views of two so far separate communities in AI and Philosophy. In particular, we show that incoherence degrees in formal epistemology may lead to novel approaches to inconsistency measures in the AI view."]},
{"title": "Patient acuity as a determinant of paramedics' frequency of being exposed to physically demanding work activities", "highlights": ["Patient acuity (call urgency) is a determinant of paramedics' physical demands.", "Urgent calls required more physically demanding actions.", "Moderately urgent calls required the longest time to transfer care at the hospital.", "Urgent calls are perceived as more emotionally, clinically and physically demanding."], "abstract": ["The purpose of this investigation was to examine if paramedics' frequency of being exposed to highly physically demanding activities, or their perception of physical, clinical, and emotional demands were altered by patients' acuity level, operationalized using the Canadian Triage and Acuity Scale (CTAS).", "Physical demands descriptions (PDD) were compiled from thirteen services across Canada. The observation sessions took place during a minimum of two full-shift (12-h) ride-outs at each service. Data were obtained from 53 ride-outs, which included a total of 190 calls.", "Higher urgency calls (CTAS level I or II) required significantly more stretcher handling, equipment handling, and intravenous (IV) work, also prompting higher ratings of perceived clinical, physical, and emotional demand. Independent of CTAS level, stretcher loading with patient (15.0%), horizontal patient transfer (13.7%), and pushing/pulling the stretcher with patient (13.1%) were identified as the most physically demanding tasks.", "Patient acuity is an important determinant affecting the frequency for which paramedics are exposed to work tasks with inherent ergonomic hazards (e.g., handling a stretcher with a patient). Patient acuity also affects paramedics' perceived clinical, physical, and emotional demands of a call."]},
{"title": "Evaluating the low back biomechanics of three different office workstations: Seated, standing, and perching", "highlights": ["Perching was associated with lower spinal loads compared to standing.", "Supported movement resulted in lower discomfort relative to prolonged standing.", "Heart rate variability may provide an objective understanding of discomfort."], "abstract": ["The objective of this study was to evaluate how different workstations may influence physical behavior in office work through motion and how that may affect spinal loads and discomfort. Twenty subjects performed a typing task in three different workstations (seated, standing, and perching) for one hour each. Measures of postural transitions, spinal loads, discomfort, and task performance were assessed in order to understand the effects of workstation interaction over time. Results indicated that standing had the most amount of motion (6\u20138 shifts/min), followed by perching (3\u20137 shifts/min), and then seating (<1 shift/min). Standing had the highest reports of discomfort and seating the least. However, spinal loads were highest in A/P shear during standing (190N posterior shear, 407N anterior shear) compared to perching (65N posterior shear, 288N anterior shear) and seating (106N posterior shear, 287 anterior shear). These loads are below the risk threshold for shear, but may still elicit a cumulative response. Perching may induce motion through supported mobility in the perching stool, whereas standing motion may be due to postural discomfort. Office workstation designs incorporating supported movement may represent a reasonable trade-off in the costs-benefits between seating and standing."]},
{"title": "Algorithms for computing strategies in two-player simultaneous move games", "highlights": ["We present algorithms for computing strategies in zero-sum simultaneous move games.", "The algorithms include exact algorithms and Monte Carlo sampling algorithms.", "We compare the algorithms in the offline computation and the online game-playing.", "Novel exact algorithm dominates in the offline equilibrium strategy computation.", "Novel sampling algorithms can guarantee convergence to optimal strategies."], "abstract": ["Simultaneous move games model discrete, multistage interactions where at each stage players simultaneously choose their actions. At each stage, a player does not know what action the other player will take, but otherwise knows the full state of the game. This formalism has been used to express games in general game playing and can also model many discrete approximations of real-world scenarios. In this paper, we describe both novel and existing algorithms that compute strategies for the class of two-player zero-sum simultaneous move games. The algorithms include exact backward induction methods with efficient pruning, as well as Monte Carlo sampling algorithms. We evaluate the algorithms in two different settings: the offline case, where computational resources are abundant and closely approximating the optimal strategy is a priority, and the online search case, where computational resources are limited and acting quickly is necessary. We perform a thorough experimental evaluation on six substantially different games for both settings. For the exact algorithms, the results show that our pruning techniques for backward induction dramatically improve the computation time required by the previous exact algorithms. For the sampling algorithms, the results provide unique insights into their performance and identify favorable settings and domains for different sampling algorithms."]},
{"title": "Thermo-physiological comfort of soft-shell back protectors under controlled environmental conditions", "highlights": ["A method for thermo-physiological comfort assessment of back-protectors is proposed.", "Microclimate temperature and humidity under back-protector shells grow very fast.", "The presence of a vest in back-protector design affects thermal comfort.", "Ventilation through the soft foam shell plays a primary role in thermal comfort."], "abstract": ["The aim of the study was to investigate thermo-physiological comfort of three back protectors identifying design features affecting heat loss and moisture management. Five volunteers tested the back protectors in a climatic chamber during an intermittent physical activity. Heart rate, average skin temperature, sweat production, microclimate temperature and humidity have been monitored during the test. The sources of heat losses have been identified using infrared thermography and the participants answered a questionnaire to express their subjective sensations associated with their thermo-physiological condition. The results have shown that locally torso skin temperature and microclimate depended on the type of back protector, whose design allowed different extent of perspiration and thermal insulation. Coupling physiological measurements with the questionnaire, it was found that overall comfort was dependent more on skin wetness than skin temperature: the participants preferred the back protector with the highest level of ventilation through the shell and the lowest level of microclimate humidity."]},
{"title": "Intervention development to reduce musculoskeletal disorders: Is the process on target?", "highlights": ["Effective MSD interventions require a systematic approach to identify and control all causal factors.", "Evidence suggests interventions are not appropriately targeted limiting their effectiveness.", "A stage of change model is proposed as a model to develop more targeted interventions to reduce MSDs."], "abstract": ["Work related musculoskeletal disorders remain an intractable OHS problem. In 2002, Haslam proposed applying the stage of change model to target ergonomics interventions and other health and safety prevention activities. The stage of change model proposes that taking into account an individual's readiness for change in developing intervention strategies is likely to improve uptake and success. This paper revisits Haslam's proposal in the context of interventions to reduce musculoskeletal disorders. Effective MSD interventions require a systematic approach and need to take into account a combination of measures. Research evidence suggests that in practice, those charged with the management of MSDs are not consistently adopting such an approach. Consequently, intervention development may not represent contemporary best practice. We propose a potential method of addressing this gap is the stage of change model, and use a case study to illustrate this argument in tailoring intervention development for managing MSDs."]},
{"title": "The relationship between air layers and evaporative resistance of male Chinese ethnic clothing", "highlights": ["The evaporative resistance and the air gap of male Chinese ethnic costumes were determined.", "The relationship between evaporative resistance and air gap was analysed.", "Clothing total evaporative resistance showed a downward-opening parabolic correlation with the air gap.", "Each body part also showed similar downward-opening parabolic curves but with different leading coefficients."], "abstract": ["In this study, the air layer distribution and evaporative resistances of 39 sets of male Chinese ethnic clothing were investigated using a sweating thermal manikin and the three-dimensional (3D) body scanning technique. Relationships between the evaporative resistance and air layers (i.e., air gap thickness and air volume) were explored. The results demonstrated that the clothing total evaporative resistance increases with the increasing air gap size/air volume, but the rate of increase gradually decreases as the mean air gap size or the total air volume becomes larger. The clothing total evaporative resistance reaches its maximum when the average air gap size and the total air volume are 41.6\u00a0mm and 69.9\u00a0dm", ", respectively. Similar general trends were also found between local mean air gap size and clothing local evaporative resistance at different body parts. However, different body parts show varied rates of increase and decrease in the local evaporative resistance. The research findings provide a comprehensive database for predicting overall and local human thermal comfort while wearing male Chinese ethnic clothing."]},
{"title": "Efficient feature-preserving local projection operator for geometry reconstruction", "highlights": ["We present a feature-preserving locally optimal projection for static model.", "We present a spatio-temporal locally optimal projection for time-varying surfaces.", "We accelerate the projection operator using the random sampling technique."], "abstract": ["This paper proposes an efficient and Feature-preserving Locally Optimal Projection operator (FLOP) for geometry reconstruction. Our operator is bilateral weighted, taking both spatial and geometric feature information into consideration for feature-preserving approximation. We then present an accelerated FLOP operator based on the random sampling of the Kernel Density Estimate (KDE), which produces reconstruction results close to those generated using the complete point set data, to within a given accuracy. Additionally, we extend our approach to time-varying data reconstruction, called the Spatial\u2013Temporal Locally Optimal Projection operator (STLOP), which efficiently generates temporally coherent and stable feature-preserving results. The experimental results show that the proposed algorithms are efficient and robust for feature-preserving geometry reconstruction on both static models and time-varying data sets."]},
{"title": "Handling missing values and unmatched features in a CBR system for hydro-generator design", "highlights": ["Case base is constructed based on domain ontology to improve retrieval efficiency.", "Case representation is proposed and cases are represented by a unified tree model.", "The cost function is proposed to measure the semantic difference between two cases.", "The similarity function is defined based on the cost function.", "Experiments are executed to evaluate the performance of the proposed CBR system."], "abstract": ["Hydro-generator design is a complex problem and case based reasoning (CBR) can improve its efficiency, but there are missing values and unmatched features which decrease the accuracy of CBR. In order to solve the problems brought by missing values and unmatched features, a similarity measurement is proposed by improving the edit distance which is widely used as a similarity measurement. In the proposed CBR system, the case base is constructed based on domain ontology to improve the retrieval efficiency. Then a case representation is proposed and cases are represented by a unified tree model. Next, by combining the edit distance with feature weights and the semantic meanings of case nodes, the cost function is proposed to measure the semantic difference and the conditions which make it a metric are discussed. Lastly, the similarity function is defined based on the cost function. A case study is presented to illustrate the use of the proposed CBR system, and then the experiments are executed to evaluate its performance in dealing with missing values and unmatched features respectively. The results validate that the proposed CBR system can handle missing values and unmatched features effectively."]},
{"title": "Deformable part inspection using a spring\u2013mass system", "highlights": ["A spring\u2013mass system is proposed for modeling deformation of shell parts.", "An expression to calculate the stiffness of the bending springs is presented.", "Radial basis functions are used to set the initial value for optimization.", "Radial basis functions are applied to speed up the calculation of deformations."], "abstract": ["In order to inspect deformable parts, recent works use virtual deformation on a digitized version of a real-part to bring the part model back to its nominal shape. This simulation mimics the real process called fixturing, which is normally used by the manufacturer to bring back the part into its nominal shape once installed. To perform such virtual deformation Finite Element Methods (FEMs) are used in order to meet the precision requirements of the inspection process. This paper presents a method based on a spring\u2013mass system, whose formulation is much simpler than the FEM, which allows the calculation of deformations of shell type parts with accuracy comparable to FEM. Furthermore, due to the simplicity in its formulation the algorithm can be implemented more easily than the FEM. The system is composed of two types of springs: one type models membrane behavior of the part\u2019s mesh model and the second type models the flexion behavior between each mesh elements. We show that by applying the proposed mass-spring model, it is possible to reduce the calculation time by 80% over standard FEM calculation opening the door to real-time inspection."]},
{"title": "Study of the effective cutter radius for end milling of free-form surfaces using a torus milling cutter", "highlights": ["Importance of effective radius in studying milling of free-form surfaces with a torus cutter.", "Establishing an analytical expression of the effective radius of a torus milling cutter working in translation.", "Same expression used to calculate the effective radius near instantaneously.", "Presentation of an industrial example showing potential use of the expression."], "abstract": ["When end milling free-form surfaces using a torus milling cutter, the notion of cutter effective radius is often used to address the procedure for removal of material from a purely geometrical perspective. Using an original analytical approach, the present study establishes a relation enabling the value of this effective radius to be easily computed. The limits of validity of this relation are then discussed and precisely defined.", "By way of an illustration, an example of how this relation can be used to generate a numerical tool for analysis of the possibilities for machining free-form surfaces on multi-axis machine-tools is also presented."]},
{"title": "Relationship matrix based automatic assembly sequence generation from a CAD model", "highlights": ["Automatic collection of constraint data from CAD models for assembly planning.", "Algorithm for generating assembly sequences based on the CAD constraints.", "Generated assembly sequences are validated via interference and stability analysis.", "System able to be used with any modern CAD system, with demonstration in CREO."], "abstract": ["Currently in industry, design and communication of a product assembly is through the use of computer-aided design (CAD) systems. However, there are no commercial systems that can automatically generate feasible assembly sequence plans. There is past and current academic research in methods to provide automatic assembly sequence planning. Assembly sequence planning using a commercial system often relies on an expert assembly sequence planner, and it is predominantly done manually. This requires a great amount of time and expert knowledge; assembly sequence plans generated may not even be the most efficient. The ability to automatically generate assembly sequence plans will lead to the reduction of planning time, less reliance on the amount of knowledge required, and better plans at earlier stages of the design process. CAD models are based on feature constraints to create and define an assembly. The challenges to automatically generate assembly sequences using CAD models lie in intelligent reasoning and analysis of the modelled assembly data. Based on past research findings, there is a reason to believe that assembly constraints used in CAD assembly models can provide essential information related to the assembly process. This paper presents a system that can analyse and utilize assembly data available from a CAD model to generate assembly sequences. The system also considers a user input as a type of assembly constraint. The system is capable of producing a set of ranked feasible assembly sequence plans for an operator to evaluate. A matrix approach has been adopted to process the information retained from a CAD model. Interference and stability studies are carried out during the creation of assembly sequence plans. The outputs are ranked based on the ease of assembly and the stability of the generated assembly sequence plans. Case studies are used to evaluate the system and the feasibility of the output. A case study using a two stroke engine is presented, which demonstrates how the system generates assembly sequence plans."]},
{"title": "Watermarking 3D CAPD models for topology verification", "highlights": ["We propose a novel watermarking scheme for topology verification of CAPD models.", "Watermark bits are embedded in a subset of the model\u2019s connection points.", "It is a semi-fragile and blind watermarking scheme.", "It provides good tamper localization accuracy.", "It is robust against similarity transformations and simplification."], "abstract": ["The Computer-Aided Plant Design (CAPD) model characterizes its peculiar complex topology among a tremendous number of plant components under complex constraints rather than just geometrical shapes. Lots of watermarking schemes for CAD models have been proposed for geometrical information protection or authentication. However, in the literature, none of them has mentioned the problem of topology authentication for CAPD models yet. In this paper, a semi-fragile watermarking algorithm for topology authentication of CAPD models is presented. We first discuss the problem of topology authentication of CAPD models. Then a subset of the model\u2019s connection points are selected as mark points, also called watermark carriers, according to the mark point selecting principle. We encode the topological relation among components into watermarks. Afterwards, the topology based watermarks are embedded in mark points to keep them in a predefined relationship with neighboring connection points so that any changes will ruin the relationship between marked connection points and neighboring connection points. To the best of our knowledge, our algorithm is the first semi-fragile and blind scheme that can authenticate and verify the topology of CAPD models. Experimental results show that our approach not only can detect and locate malicious topology attacks such as components modification and joint ends modification, but also is robust against various non-malicious attacks such as similarity transformations and simplification."]},
{"title": "Generic face adjacency graph for automatic common design structure discovery in assembly models", "highlights": ["Transform abstract relationships in assembly models into measurable entities.", "A more quantitative descriptor for assembly models has been developed.", "Transform the 3D assembly model into a 2D coordinate system.", "Common design structures in the assembly model can be discovered for reuse."], "abstract": ["Common Design Structure Discovery (CDSD) is to identify local structures shared by multiple models. Nowadays it is mainly restricted to part models. Extending it to assembly models can produce a significant value for assembly design reuse. However, current descriptions of assembly models usually capture topological information qualitatively, considering little geometric information, and thus are not suitable for CDSD in assembly models (CDSDA). To counter this problem, this paper proposes a generic face adjacency graph (GFAG) which is extended from the face adjacency graph for B-Rep part model description. GFAG can transform abstract relationships in assembly models into measurable entities by introducing a concept of mating face pair (MFP), thus facilitating a more quantitative and consistent description of parts and relationships in assembly models. Corresponding to geometric faces and edges in a part model, GFAG treats parts and relationships in an assembly model as generic faces and generic edges respectively. To make GFAG have a higher discrimination capability, a node in GFAG captures the geometric information of a part together with its mating parts by shape parameters and also quantitatively incorporates a relationship between parts by shape parameters of an MFP. By doing so, GFAG can take more geometric information, together with topological information, into account quantitatively, and thus can be mapped into a 2D coordinate system for easy validation. We also extend a discovery algorithm to validate the feasibility of GFAG for CDSDA, and the results demonstrate the expected effectiveness."]},
{"title": "A feature-based method of rapidly detecting global exact symmetries in CAD models", "highlights": ["A feature-based method is proposed to rapidly detect the symmetries in CAD models.", "A study on the relationship between feature information and symmetries is conducted.", "How to determine the symmetries of the Boolean combinations of features is framed.", "Symmetries of numerous CAD models have been successfully detected by our approach."], "abstract": ["Detecting global exact symmetries in CAD models is of great importance in the research of CAD/CAE integration. Therefore, a method is proposed in this paper to rapidly detect the global exact rotational and reflectional symmetries in feature-based CAD models. The theories of determining the symmetries of the Boolean combinations of the features are framed. Based on these theories, our approach is processed as follows. First, the features of the CAD models are classified into congruent feature sets. Next, through the study on the relationship between feature information and the symmetries of features, by using only feature information, as many symmetries of the feature sets as possible are detected. Then these feature sets are sorted into an ordered sequence. Finally, symmetries of the entire model can be derived by successively merging and verifying the symmetries of feature sets in the ordered sequence. Experimental results show that the global exact symmetries can be robustly and rapidly detected."]},
{"title": "Structure and spatial consistency of network-based space layouts for building and product design", "highlights": ["A schema is defined for space layouts that represent spatial relations as networks.", "Spatial constraints for spatial consistency checking extend the schema.", "An inconsistency resolution operation resolves inconsistencies in a layout.", "A layout modeling system prototype has been implemented and validated with examples."], "abstract": ["Network-based space layouts are schematic models of whole spaces, subspaces, and related physical elements. They address diverse space modeling needs in building and product design. A schema (data model) for network-based space layouts is defined that is influenced by existing space schemas. Layout elements and selected spatial relations form a geometric network. The network is embedded in 3-space and facilitates analysis with graph and network algorithms. Spatial constraints on layout elements and spatial relations extend the schema to support spatial consistency checking. Spatially consistent layouts are required for reliable network analysis and desirable for layout modification operations. An operation is introduced that evaluates spatial constraints to detect and semi- or fully-automatically resolve spatial inconsistencies in a layout. A layout modeling system prototype that includes proof-of-concept implementations of the layout schema extended by spatial constraints and the inconsistency resolution operation is described. Layouts of a floor of an office building and a rack server cabinet have been modeled with the system prototype."]},
{"title": "Collision free region determination by modified polygonal Boolean operations", "highlights": ["An algorithm to determine the collision free region is proposed.", "The collision free region is a useful tool for cutting and packing problems with irregular items.", "Degenerated elements (edges and vertexes) represent local compaction situations.", "The collision free regions determines the presence of local compaction for the current item."], "abstract": ["Cutting and packing problems are found in numerous industries such as garment, wood and shipbuilding. The collision free region concept is presented, as it represents all the translations possible for an item to be inserted into a container with already placed items. The often adopted nofit polygon concept and its analogous concept inner fit polygon are used to determine the collision free region. Boolean operations involving nofit polygons and inner fit polygons are used to determine the collision free region. New robust non-regularized Boolean operations algorithm is proposed to determine the collision free region. The algorithm is capable of dealing with degenerated boundaries. This capability is important because degenerated boundaries often represent local optimal placements. A parallelized version of the algorithm is also proposed and tests are performed in order to determine the execution times of both the serial and parallel versions of the algorithm."]},
{"title": "Curvature-guided adaptive ", "highlights": ["We present an adaptive ", "-spline fitting algorithm, with several new components.", "We propose a curvature-guided fitting strategy to effectively capture model features.", "We introduce a process for faithful re-parameterization.", "We present a process for re-placing the initial structure of the spline surface."], "abstract": ["The problem of fitting spline surfaces to triangular mesh models is of importance in computer-aided design. Many fitting algorithms have been developed. This paper proposes several novel plug-and-play components or strategies: ", " \u00a0 ", "-", " and ", ", which can be used to enhance fitting algorithms. We also present an adaptive ", "-spline fitting algorithm integrating these components and strategies. Extensive experiments have been conducted to demonstrate these components. Our fitting algorithm can generate spline surfaces that well respect the geometrical features of input mesh models and have a more compact representation."]},
{"title": "A novel geometric flow approach for quality improvement of multi-component tetrahedral meshes", "highlights": ["A mesh quality improvement method for multi-component tetrahedral meshes is proposed.", "A multi-component tetrahedral mesh with non-manifold boundary is more difficult to improve.", "Both location and connection of mesh vertices are optimized.", "All tetrahedra are positive and well-shaped after quality improvement.", "Boundary meshes are smoothed and regularized with shape-preservation."], "abstract": ["This paper presents an efficient and novel geometric flow-driven method for mesh optimization of multi-component tetrahedral meshes with non-manifold boundaries. The presented method is composed of geometric optimization and topological transformation techniques, so that both location and topology of mesh vertices are optimized. Due to the complexity of non-manifold boundaries, we categorize the boundary vertices into three groups: surface vertices, curve vertices, and fixed vertices. Each group of boundary vertices is modified by different shape-preserving geometric flows in order to smooth and regularize boundary meshes. Meanwhile, all vertices are relocated by minimizing an energy functional which is relevant to the quality measure of tetrahedra. In addition, face-swapping and edge-removal operations are employed to eliminate poorly-shaped elements. Finally, the performance of our method is compared with a state of the art technique, named Stellar, for a dozen single-component meshes. We obtain similar or even better results with much less running time. Moreover, we validate the presented method on several multi-component tetrahedral meshes, and the results demonstrate that the mesh quality is improved significantly."]},
{"title": "Robust topology optimisation of bi-modulus structures", "highlights": ["Robust topology optimisation of bi-modulus material is developed.", "Multiple loading conditions are considered in simulation.", "Material replacement method is adopted for simplifying structural analysis.", "Sensitivity of robust compliance is derived.", "Optimal topology is found to be force-direction dependent when materials display bi-modulus behaviour."], "abstract": ["This study proposes a robust topology optimisation method for the design of bi-modulus structures under uncertain multiple loading conditions (MLC). The objective of the design optimisation is to minimise the standard deviation of the weighted structural compliance. The gradient-based method is applied to perform a sensitivity analysis for the identification of optimal design variables. A material replacement method is used to overcome difficulty in the sensitivity analysis due to the stress-dependent behaviour of the original bi-modulus material. In the material replacement operation, two new isotropic materials are identified to replace the original bi-modulus material according to its two moduli. To reduce the side effects of the material replacement operation on the final design, the local stiffness is modified in terms of the stress state. Typical numerical examples are used to demonstrate the effectiveness of the proposed method to the final design, including the load uncertainty on the optimal bi-modulus layout, as well as other factors, such as loading direction and the ratio between the two moduli of the bi-modulus material. The comparison between layouts of isotropic and bi-modulus materials also shows that the final bi-modulus material distribution is sensitive to loading directions in practical designs."]},
{"title": "Design and finite element-based fatigue prediction of a new self-expandable percutaneous mitral valve stent", "highlights": ["A novel nitinol design for percutaneous mitral valve replacement is presented.", "The design can address issues of valve migration and paravalvular leakage.", "Crimpability of the self-expandable stent was studied by the finite element method.", "Fatigue analysis has been simulated for the design under various cyclic loadings."], "abstract": ["Percutaneous heart valve replacement is currently limited to the replacement of pulmonary and aortic valves in a targeted group of patients. Designing a heart valve for mitral valve replacement is further limited by its distinctive anatomical feature, which places a constraint on its range of design options. To overcome such limitations, the objectives of this study were to use computational modeling and simulation to design a new nitinol-based mitral valve stent and evaluate its crimpability and fatigue behavior. A self-expandable stent with new features that could address the issues of valve migration and paravalvular leaks was generated using the CAD-based conceptual modeling. Its expansion, crimpability, deployment patterns, and fatigue behavior were simulated and analyzed. Our simulations incorporated cyclic cardiac muscle loading, cyclic blood pressure loading, as well as cyclic valve-leaflet forces in the fatigue life assessment for mitral valves. Our results showed that the stent model passed the fatigue test under the aforementioned loading conditions. Our model provides a simple, fast and cost-effective tool to quantitatively determine the fatigue resistance of stent components. This is of great value to the design of new prosthetic heart valve models, as well as to surgeons involved in valve replacement."]},
{"title": "Functionally heterogeneous porous scaffold design for tissue engineering", "highlights": ["Functionally gradient porous scaffolds are designed for tissue engineering.", "An optimum deposition angle has been determined for multi-functional regions.", "The proposed methodology designs scaffolds with gradient porosity.", "Provides more control over the desired porosity level.", "The designed heterogeneous scaffolds are fabricated with a bio-additive process."], "abstract": ["Porous scaffolds with interconnected and continuous pores have recently been considered as one of the most successful tissue engineering strategies. In the literature, it has been concluded that properly interconnected and continuous pores with their spatial distribution could contribute to perform diverse mechanical, biological and chemical functions of a scaffold. Thus, there has been a need for reproducible and fabricatable scaffold design with controllable and functional gradient porosity. Improvements in Additive Manufacturing (AM) processes for tissue engineering and their design methodologies have enabled the development of controlled and interconnected scaffold structures. However homogeneous scaffolds with uniform porosity do not capture the intricate spatial internal micro architecture of the replaced tissue and thus are not capable of capturing the design. In this work, a novel heterogeneous scaffold modeling is proposed for layered-based additive manufacturing processes. First, layers are generated along the optimum build direction considering the heterogeneous micro structure of tissue. Each layer is divided into functional regions based on the spatial homogeneity factor. An area weight based method is developed to generate the spatial porosity function that determines the deposition pattern for the desired gradient porosity. To design a multi-functional scaffold, an optimum deposition angle is determined at each layer by minimizing the heterogeneity along the deposition path. The proposed methodology is implemented and illustrative examples are also provided. The effective porosity is compared between the proposed design and the conventional uniform porous scaffold design. Sample designed structures have also been fabricated with a novel micro-nozzle biomaterial deposition system. The result has shown that the proposed methodology generates scaffolds with functionally gradient porosity."]},
{"title": "Integrated construction and simulation of tool paths for milling dental crowns and bridges", "highlights": ["Integrated approach of tool path generation and simulation for milling in the field of dental technology.", "Implicit representations of surfaces and solids by means of level-sets.", "Isoplanar/isoparametric and isogeodesic sampling for finishing milling.", "GPU-based implementation of predictive feed rate optimization and tool path evaluation."], "abstract": ["The paper presents an integrated and therefore novel approach of tool path generation and simulation for milling in the field of dental technology. Both the geometry of the dental implants and the characteristics of the cutter, machine and raw material have an immediate influence on the stability of the milling process and the quality of the result. Regarding this issue, the paper focuses on the construction of tool path segments on a triangulated surface using an isoparametric and isogeodesic approach. The segments are subsequently chained and simulated in order to provide an optimized tool path in terms of path length and feed rate. The simulation particularly evaluates a tool load to guide feed rate selection and the distance error of the machined surface. Implicit representations of surfaces and solids by means of level sets are applied where appropriate in order to increase robustness and universality of tool path construction."]},
{"title": "A roadmap for parametric CAD efficiency in the automotive industry", "highlights": ["We describe the particular constraints of design in the automotive industry.", "An efficient use of parametric CAD requires a clear strategy.", "Roadmap for CAD methodologies and KBE approach.", "It can be used by CAD managers in the automotive industry."], "abstract": ["3D CAD systems are used in product design for simultaneous engineering and to improve productivity. CAD tools can substantially enhance design performance. Although 3D CAD is a widely used and highly effective tool in mechanical design, mastery of CAD skills is complex and time-consuming. The concepts of parametric\u2013associative models and systems are powerful tools whose efficiency is proportional to the complexity of their implementation. The availability of a framework for actions that can be taken to improve CAD efficiency can therefore be highly beneficial. Today, a clear and structured approach does not exist in this way for CAD methodology deployment. The novelty of this work is therefore to propose a general strategy for utilizing the advantages of parametric CAD in the automotive industry in the form of a roadmap. The main stages of the roadmap are illustrated by means of industrial use cases. The first results of his research are discussed and suggestions for future work are given."]},
{"title": "Global obstacle avoidance and minimum workpiece setups in five-axis machining", "highlights": ["Classification of different types of obstacles encountered in five-axis NC machining.", "Efficient numerical algorithms on how to represent dynamic obstacles.", "Efficient computing of feasible regions considering all types of obstacles.", "A heuristic optimization algorithm for finding the best workpiece setup.", "A thorough analysis of both static and dynamic obstacles."], "abstract": ["The state-of-the-art tool path computation algorithms for five-axis machining consider only the workpiece and clamping device for collision avoidance. However, in a real five-axis machining process, there are many other types of obstacles beyond workpiece and clamps that the tool assembly must avoid, e.g., sensors and other intrusive devices. In such cases, the only solution at present is by means of computer simulation of the machining process after the tool path has been computed. If collision is found, it requires re-computing the tool path and/or changing the setup of the workpiece. This process is then re-iterated until all the collisions are resolved. As a result, the process is time consuming and requires excessive human intervention. In this paper, we present rigorous analyses of the obstacles in five-axis machining and propose efficient numerical algorithms for calculating and representing them. Using our results, the obstacle-free tool orientations can be determined completely at the tool path planning stage, rather than relying on the simulation afterward. In addition, as a direct application of our mathematical modeling, we present a heuristic-based solution to the optimal workpiece setups problem: finding a minimum number of workpiece setups for an arbitrary sculpture part surface so that it can be machined completely on a given five-axis machine without colliding with the obstacles. We use orthogonal table\u2013table five-axis machines as an example and work out a numerical experiment using the proposed solution."]},
{"title": "An efficient human model customization method based on orthogonal-view monocular photos", "highlights": ["We report a new precise human model customization method using customer\u2019s photos.", "We propose a comprehensive human body geometrical shape representation.", "We learn a novel relationship model between human body\u2019s 2D size and 3D shape.", "We develop a new deformation algorithm that minimizes distortion in human modeling."], "abstract": ["Human body modeling is a central task in computer graphics. In this paper, we propose an intelligent model customization method, in which customer\u2019s detailed geometric characteristics can be reconstructed using limited size features extracted from the customer\u2019s orthogonal-view photos. To realize model customization, we first propose a comprehensive shape representation to describe the geometrical shape characteristics of a human body. The shape representation has a layered structure and corresponds to important feature curves that define clothing size. Next, we identify and model a novel relationship model between 2D size features and 3D shape features for each cross-section using real subject scanned data. We predict a customer\u2019s cross-sectional 3D shape based on size features extracted from the customer\u2019s photos, and then we reconstruct the customer\u2019s shape representation using predicted cross-sections. We develop a new deformation algorithm that deforms a template model into a customized shape using the reconstructed 3D shape representation. A total of 30 subjects, male and female, with varied body shapes have been recruited to verify the model customization method. The customized models show high degree of resemblance of the subjects, with accurate body sizes; the accuracy of the models is comparable to scan. It shows that the method is a feasible and efficient solution for human model customization that fulfills the specific needs of the clothing industry."]},
{"title": "Generalizing the advancing front method to composite surfaces in the context of meshing constraints topology", "highlights": ["A new automatic mesh generation algorithm over composite geometry.", "An extension of the advancing front method.", "Applied in the context of meshing constraints topology (virtual topology).", "Allows the automatic simplification (defeaturing) of CAD models."], "abstract": ["Being able to automatically mesh composite geometry is an important issue in the context of CAD\u2013FEA integration. In some specific contexts of this integration, such as using virtual topology or meshing constraints topology (MCT), it is even a key requirement. In this paper, we present a new approach to automatic mesh generation over composite geometry. The proposed mesh generation approach is based on a generalization of the advancing front method (AFM) over curved surfaces. The adaptation of the AFM to composite faces (composed of multiple boundary representation (B-Rep) faces) involves the computation of complex paths along these B-Rep faces, on which progression of the advancing front is based. Each mesh segment or mesh triangle generated through this progression on composite geometry is likely to lie on multiple B-Rep faces and consequently, it is likely to be associated with a composite definition across multiple parametric spaces. Collision tests between new front segments and existing mesh elements also require specific and significant adaptations of the AFM, since a given front segment is also likely to lie on multiple B-Rep faces. This new mesh generation approach is presented in the context of MCT, which requires being able to handle composite geometry along with non-manifold boundary configurations, such as edges and vertices lying in the interior domain of B-Rep faces."]},
{"title": "Dimensional and geometrical errors of three-axis CNC milling machines in a virtual machining system", "highlights": ["A virtual machining system is presented in order to create machined parts.", "Geometrical errors of three-axis milling machine tools were modeled.", "The real errors are enforced to the nominal machining G-codes by developed software.", "To validate the system, surfaces of virtual and real machined parts are compared."], "abstract": ["Virtual machining systems are applying computers and different types of software in manufacturing and production in order to simulate and model errors of real environment in virtual reality systems. Many errors of CNC machine tools have an effect on the accuracy and repeatability of part manufacturing. Some of these errors can be reduced by controlling the machining process and environmental parameters. However geometrical errors which have a big portion of total error need more attention. In this paper a virtual machining system which simulates the dimensional and geometrical errors of real three-axis milling machining operations is described. The system can read the machining codes of parts and enforce 21 errors associated with linear and rotational motion axes in order to generate new codes to represent the actual machining operation. In order to validate the system free form profiles and surfaces of virtual and real machined parts are compared in order to present the reliability and accuracy of the software."]},
{"title": "An adaptive normal estimation method for scanned point clouds with sharp features", "highlights": ["Reliable estimation of normals for scanned point clouds containing sharp features.", "A robust method for noisy point clouds with outliers.", "Automatic evaluation of the local adaptive parameters employed in the method."], "abstract": ["Normal estimation is an essential task for scanned point clouds in various CAD/CAM applications. Many existing methods are unable to reliably estimate normals for points around sharp features since the neighborhood employed for the normal estimation would enclose points belonging to different surface patches across the sharp feature. To address this challenging issue, a robust normal estimation method is developed in order to effectively establish a proper neighborhood for each point in the scanned point cloud. In particular, for a point near sharp features, an anisotropic neighborhood is formed to only enclose neighboring points located on the same surface patch as the point. Neighboring points on the other surface patches are discarded. The developed method has been demonstrated to be robust towards noise and outliers in the scanned point cloud and capable of dealing with sparse point clouds. Some parameters are involved in the developed method. An automatic procedure is devised to adaptively evaluate the values of these parameters according to the varying local geometry. Numerous case studies using both synthetic and measured point cloud data have been carried out to compare the reliability and robustness of the proposed method against various existing methods."]},
{"title": "Partial retrieval of CAD models based on local surface region decomposition", "highlights": ["A B-rep model is decomposed into surface regions with different convexity.", "An optimal procedure is utilized to guide the surface partition.", "A kind of region codes is adopted to describe the surface regions.", "Partial model retrievals are realized by matching the region codes between models."], "abstract": ["Most existing methods for 3D model retrieval focus on the global shape description and matching. However, partial shape description and retrieval may be used more frequently in the fields of product design and manufacture. In order to resolve the problem that the retrieval efficiency for complex model descriptors is low, a CAD model retrieval method based on local surface region decomposition is presented in this paper. First, according to the salient geometric features of the mechanical part, the surface boundary of a solid model is divided into local convex, concave and planar regions. Then, we give a kind of region codes that describe the surface regions and their links in the CAD model. Finally, the model retrievals are realized based on the similarity measurement between two models\u2019 region codes. Experimental results have shown that this approach is able to support partial retrieval of CAD models."]},
{"title": "Improving assembly precedence constraint generation by utilizing motion planning and part interaction clusters", "highlights": ["Spatial clustering to automatically detect and manage assembly-level part interactions.", "Use of motion planning to assess motion feasibility.", "An algorithm to generate assembly precedence constraints."], "abstract": ["In this paper, we present a technique that combines motion planning and part interaction clusters to improve generation of assembly precedence constraints. In particular, this technique automatically finds, and clusters, parts that can mutually affect each other\u2019s accessibility, and hence may impose assembly constraints. This enables the generation of accurate precedence constraints without needing to examine all possible assembly sequences. Given an assembly model, our technique generates potential disassembly layers: spatial clustering is used to generate part sets. Next, motion planning based on rapidly-exploring random trees (RRT) with multiple trees is used to evaluate the interaction between these part sets. Specifically, motion planning is used to determine which part sets can be removed from the assembly. These sets are added to the first disassembly layer and removed from the assembly. Part sets that can be removed from the simplified assembly are then added to the second layer. If the process gets stuck, parts in the parent set are regrouped, and the process continues until all disassembly layers are found. The resulting structure reveals precedence relationships among part sets, which can be used to generate feasible assembly sequences for each part set and the whole assembly. We present theoretical results related to the algorithms developed in the paper. Computational results from tests on a variety of assemblies are presented to illustrate our approach."]},
{"title": "Classification, representation, and automatic extraction of deformation features in sheet metal parts", "highlights": ["Classification, representation and extraction of deformation features addressed.", "Basic deformation features Bend and Wall defined as a specific arrangement of faces.", "Compound deformation features handled as a specific combination of Bends and Walls.", "Compound deformation features represented as a Basic Deformation Features Graph.", "Algorithms to extract basic features and compound features presented."], "abstract": ["This paper presents classification, representation and extraction of deformation features in sheet-metal parts. The thickness is constant for these shape features and hence these are also referred to as constant thickness features. The deformation feature is represented as a set of faces with a characteristic arrangement among the faces. Deformation of the base-sheet or forming of material creates Bends and Walls with respect to a base-sheet or a reference plane. These are referred to as Basic Deformation Features (BDFs). Compound deformation features having two or more BDFs are defined as characteristic combinations of Bends and Walls and represented as a graph called Basic Deformation Features Graph (BDFG). The graph, therefore, represents a compound deformation feature uniquely. The characteristic arrangement of the faces and type of bends belonging to the feature decide the type and nature of the deformation feature. Algorithms have been developed to extract and identify deformation features from a CAD model of sheet-metal parts. The proposed algorithm does not require folding and unfolding of the part as intermediate steps to recognize deformation features. Representations of typical features are illustrated and results of extracting these deformation features from typical sheet metal parts are presented and discussed."]},
{"title": "G", "highlights": ["We model a quasi-developable surface interpolating two arbitrary space curves.", "The surface is represented by an aggregate of four-sided Bezier patches.", "These patches can be optimally assembled in terms of developability degree.", "The resultant surface can obtain ", " continuity."], "abstract": ["Surface development is used in many manufacturing planning operations, e.g., for garments, ships and automobiles. However, most freeform surfaces used in design are not developable, and therefore the developed patterns are not isometric to the original design surface. In some domains, the CAD model is created by interpolating two given space curves. In this paper, we propose a method to obtain a G", " quasi-developable Bezier surface interpolating two arbitrary space curves. The given curves are first split into a number of piecewise Bezier curves and elemental Bezier patches each of which passes through four splitting points are constructed. All neighboring elemental patches are G", " connected and they are assembled optimally in terms of the degree of developability (the integral Gaussian curvature). Experiments show that the final composite Bezier surface is superior to a lofted one which is defined regardless of the final surface developability."]},
{"title": "A comprehensive process of reverse engineering from 3D meshes to CAD models", "highlights": ["Primitive extraction: detect primitive which corresponds locally to the 3D mesh.", "Adjacency relation determination: define the relationship between primitives.", "Wire construction: based on the intersection curves between neighboring primitives.", "B-Rep creation: that works even in the case of an outline on a periodic surface."], "abstract": ["In an industrial context, most manufactured objects are designed using CAD (Computer-Aided Design) software. For visualization, data exchange or manufacturing applications, the geometric model has to be discretized into a 3D mesh composed of a finite number of vertices and edges. However, the initial model may sometimes be lost or unavailable. In other cases, the 3D discrete representation may be modified, e.g. after numerical simulation, and no longer corresponds to the initial model. A retro-engineering method is then required to reconstruct a 3D continuous representation from the discrete one.", "In this paper, we present an automatic and comprehensive retro-engineering process dedicated mainly to 3D meshes obtained initially by mechanical object discretization. First, several improvements in automatic detection of geometric primitives from a 3D mesh are presented. Then a new formalism is introduced to define the topology of the object and compute the intersections between primitives. The proposed method is validated on 3D industrial meshes."]},
{"title": "Statistical tolerance analysis of over-constrained mechanisms with gaps using system reliability methods", "highlights": ["Gaps cannot be considered as random variables.", "The tolerance analysis issue is formulated thanks to the quantifier notion.", "Two defect probabilities are defined: functionality defect probability and assembly defect probability.", "Defect probabilities are computed using a system reliability method: FORM system."], "abstract": ["One of the aims of statistical tolerance analysis is to evaluate a predicted quality level at the design stage. One method consists of computing the defect probability ", " expressed in parts per million (ppm). It represents the probability that a functional requirement will not be satisfied in mass production. This paper focuses on the statistical tolerance analysis of over-constrained mechanisms containing gaps. In this case, the values of the functional characteristics depend on the gap situations and are not explicitly formulated with respect to part deviations. To compute ", ", an innovative methodology using system reliability methods is presented. This new approach is compared with an existing one based on an optimization algorithm and Monte Carlo simulations. The whole approach is illustrated using two industrial mechanisms: one inspired by a producer of coaxial connectors and one prismatic pair. Its major advantage is to considerably reduce computation time."]},
{"title": "Automatically generating assembly tolerance types with an ontology-based approach", "highlights": ["Automatic generation of recommended assembly tolerance types is implemented.", "The proposed approach can further reduce the number of recommended assembly tolerance types.", "The approach is relatively complete and highly efficient and supports the semantic interoperability.", "The approach can enhance the clarity, uniformity and consistency in the design of assembly tolerance types.", "Ontology-based technique is a useful technique to model and reason structure and constraint knowledge."], "abstract": ["In most cases, designers have to manually specify both assembly tolerance types and values when they design a mechanical product. Different designers will possibly specify different assembly tolerance types and values for the same nominal geometry. Furthermore, assembly tolerance specification design of a complex product is a highly collaborative process, in which semantic interoperability issues significantly arise. These situations will cause the uncertainty in assembly tolerance specification design and finally affect the quality of the product. In order to reduce the uncertainty and to support the semantic interoperability in assembly tolerance specification design, an ontology-based approach for automatically generating assembly tolerance types is proposed. First of all, an extended assembly tolerance representation model is constructed by introducing a spatial relation layer. The constructed model is hierarchically organized and consists of part layer, assembly feature surface layer, and spatial relation layer. All these layers are defined with Web Ontology Language (OWL) assertions. Next, a meta-ontology for assembly tolerance representations is constructed. With this meta-ontology, the domain-specific assembly tolerance representation knowledge can be derived by reusing or inheriting the classes or properties. Based on this, assembly tolerance representation knowledge is formalized using OWL. As a result, assembly tolerance representation knowledge has well-defined semantics due to the logic-based semantics of OWL, making it possible to automatically detect inconsistencies of assembly tolerance representation knowledge bases. The mapping relations between spatial relations and assembly tolerance types are represented in Semantic Web Rule Language (SWRL). Furthermore, actual generation processes of assembly tolerance types are carried out using Java Expert System Shell (JESS) by mapping OWL-based structure knowledge and SWRL-based constraint knowledge into JESS facts and JESS rules, respectively. Based on this, an approach for automatically generating assembly tolerance types is proposed. Finally, the effectiveness of the proposed approach is demonstrated by a practical example."]},
{"title": "A study of surface reconstruction for 3D mannequins based on feature curves", "highlights": ["A feature curve based method for 3D mannequin surface reconstruction is proposed.", "The 3D mannequin is shaped by ", "-spline surfaces.", "A minimum energy method is used to improve the constructed mannequins\u2019 quality.", "3D mannequin forms for different human factors can be morphologically generated.", "The feasibility of computer-aided fashion design and manufacturing is improved."], "abstract": ["Fashion Design is an industry closely connected to our daily life. Computer-aided fashion design can enhance the efficiency of product development, in which a 3D mannequin links the development and application of the entire design system, thus rendering the 3D mannequin data required for fashion design very important. This study proposed a systematic method for surface reconstruction of 3D mannequins based on feature curves. First of all, the study applied reverse engineering methods to scan a mannequin model commonly used in the fashion design profession by extracting grid points that represent the shape features from the scanned data after the segmentation of the mannequin model through feature surfaces. Then, the shape of the entire 3D mannequin is reconstructed using ", "-spline surfaces. Simultaneously, the continuity among the connected ", "-spline surfaces is adjusted with tangent vector adjustment methods based on the minimum energy required for improving the quality of the shaped surfaces. Finally, the 3D mannequin is applied to the computer-aided fashion design system being developed to achieve product development using synchronous design methods."]},
{"title": "Subdivision surfaces integrated in a CAD system", "highlights": ["Perfect integration of subdivision surfaces in a CAD system.", "Interoperability of subdivision surfaces with other representations.", "B-rep representation of a subdivision solid and of hybrid NURBS/subdivision objects.", "Local correction of critical curvature and parameterization behavior."], "abstract": ["The main roadblock that has limited the usage of subdivision surfaces in computer-aided design (CAD) systems is the lack of quality and precision that a model must achieve for being suitable in the engineering and manufacturing phases of design. The second roadblock concerns the integration into the modeling workflows, that, for engineering purposes, means providing a precise and controlled way of defining and editing models possibly composed of different geometric representations. This paper documents the experience in the context of a European project whose goal was the integration of subdivision surfaces in a CAD system. To this aim, a new CAD system paradigm with an extensible geometric kernel is introduced, where any new shape description can be integrated through the two successive steps of parameterization and evaluation, and a hybrid boundary representation is used to easily model different kinds of shapes. In this way, the newly introduced geometric description automatically inherits any pre-existing CAD tools, and it can interact in a natural way with the other geometric representations supported by the CAD system.", "To overcome the irregular behavior of subdivision surfaces in the neighborhood of extraordinary points, we locally modify the limit surface of the subdivision scheme so as to tune the analytic properties without affecting its geometric shape. Such a correction is inspired by the polynomial blending approach in Levin (2006) ", " and Zorin (2006)\u00a0 ", ", which we extend in some aspects and generalize to multipatch surfaces evaluable at arbitrary parameter values. Some modeling examples will demonstrate the benefits of the proposed integration, and some tests will confirm the effectiveness of the proposed local correction patching method."]},
{"title": "Efficient feature tracking of time-varying surfaces using multi-scale motion flow propagation", "highlights": ["Present a multi-scale geometry motion flow for feature tracking of time-varying surfaces.", "We can track features on the time-varying surfaces in large deformation.", "We can process both mesh-based and point based time-varying surfaces."], "abstract": ["This paper presents a framework for efficient feature tracking of time-varying surfaces. The framework can not only capture the dynamic geometry features on time-varying surfaces, but can also compute the accurate boundaries of the geometry features. The basic idea of the proposed approach is using the multi-scale motion flow and surface matching information to propagate the feature frame on time-varying surfaces. We first define an effective multi-scale geometry motion flow for the time-varying surfaces, which efficiently propagates the geometry features along the time direction of the time-varying surfaces. By combining both the approximately invariant signature vectors and geometry motion flow vectors, we also incorporate the shape matching into the system to process feature tracking for time-varying surfaces in large deformation while with low frame sampling rate. Our approach does not depend on the topological connection of the underlying surfaces. Thus, it can process both mesh-based and point-based time-varying surfaces without vertex-to-vertex correspondence across the frames. Feature tracking results on different kinds of time-varying surfaces illustrate the efficiency and effectiveness of the proposed method."]},
{"title": "A hybrid differential evolution augmented Lagrangian method for constrained numerical and engineering optimization", "highlights": ["A method hybridizing augmented Lagrangian multiplier and differential evolution algorithm is proposed.", "We formulate a bound constrained optimization problem by a modified augmented Lagrangian function.", "The proposed algorithm is successfully tested on several benchmark test functions and four engineering design problems."], "abstract": ["We present a new hybrid method for solving constrained numerical and engineering optimization problems in this paper. The proposed hybrid method takes advantage of the differential evolution (DE) ability to find global optimum in problems with complex design spaces while directly enforcing feasibility of constraints using a modified augmented Lagrangian multiplier method. The basic steps of the proposed method are comprised of an outer iteration, in which the Lagrangian multipliers and various penalty parameters are updated using a first-order update scheme, and an inner iteration, in which a nonlinear optimization of the modified augmented Lagrangian function with simple bound constraints is implemented by a modified differential evolution algorithm. Experimental results based on several well-known constrained numerical and engineering optimization problems demonstrate that the proposed method shows better performance in comparison to the state-of-the-art algorithms."]},
{"title": "Engineering feature design for level set based structural optimization", "highlights": ["A method to design engineering features in structural optimization is proposed.", "It combines CSG modeling and level set based shape and topology optimization.", "Feature design and structural optimization are unified under the level set framework.", "A truly optimal structure with features can be designed conveniently."], "abstract": ["Engineering features are regular and simple shape units containing specific engineering significance. It is useful to combine feature design with structural optimization. This paper presents a generic method to design engineering features for level set based structural optimization. A Constructive Solid Geometry based Level Sets (CSGLS) description is proposed to represent a structure based on two types of basic entities: a level set model containing either a feature shape or a freeform boundary. By treating both entities implicitly and homogeneously, the optimal design of engineering features and freeform boundary are unified under the level set framework. For feature models, constrained affine transformations coupled with an accurate particle level set updating scheme are utilized to preserve feature characteristics, where the design velocity approximates continuous shape variation via a least squares fitting. Meanwhile, freeform models undergo a standard shape and topology optimization using a semi-Lagrangian level set scheme. With this method, various feature requirements can be translated into a CSGLS model, and the constrained motion provides flexible mechanisms to design features at different stages of the model tree. As a result, a truly optimal structure with engineering features can be created in a convenient way. Several numerical examples are provided to demonstrate the applicability and potential of this method."]},
{"title": "Iso-parametric tool-path planning for point clouds", "highlights": ["A point-based iso-parametric tool-path planning method was proposed.", "The formulas for computing forward and side step were simplified significantly.", "Boundary conformed tool-path was generated for point clouds."], "abstract": ["Due to the compute-intensiveness and the lack of robustness of the algorithms for reconstruction of meshes and spline surfaces from point clouds, there is a need for further research in the topic of direct tool-path planning based on point clouds. In this paper, a novel approach for planning iso-parametric tool-path from a point cloud is presented. Since such planning falls into the iso-parametric category, it intrinsically depends on the parameterization of point clouds. Accordingly, a point-based conformal map is employed to build the parameterization. Based on it, formulas of computing path parameters are derived, which are much simpler than the conventional ones. By regularizing parameter domain and on the basis of the previous formulas, boundary conformed tool-path can be generated with forward and side step calculated against specified chord deviation and scallop height, respectively. Experimental results are given to illustrate the effectiveness of the proposed methods."]},
{"title": "An IGA-based design support system for realistic and practical fashion designs", "highlights": ["A sketch design system is proposed for customers to create/customise fashion designs.", "A representation model classifies various designs as three-level design elements.", "A knowledge model governs the generation of designs and avoids impractical designs.", "Interactive genetic algorithms govern design creation based on users\u2019 preferences."], "abstract": ["In this paper, a customised fashion design system is proposed for non-professional users (general customers) to create their preferred fashion designs in a user-friendly way. The proposed sketch design system consists of a sketch representation and composing method, an interactive genetic algorithm (IGA)-based design model, and a user-friendly interface. The sketch representation and composing method generates feasible design sketches, based on the design parameters defined by the IGA-based design model, and the sketches are presented to customers via the user-friendly interface. Experimental results have demonstrated that the proposed system is effective in generating fashion design sketches reflecting users\u2019 preference."]},
{"title": "Efficient time-optimal feedrate planning under dynamic constraints for a high-order CNC servo system", "highlights": ["A relationship between the tracking error and the input signal is established.", "The tracking error constraint for a CNC system is reduced to a kinematic constraint.", "The nonlinear constraints on jerk are reduced to linear constraints.", "Feedrate planning under confined tracking error is reduced to convex optimization."], "abstract": ["In this paper, the time-optimal feedrate planning problem under confined feedrate, axis velocity, axis acceleration, axis jerk, and axis tracking error for a high-order CNC servo system is studied. The problem is useful in that the full ability of the CNC machine is used to enhance the machining productivity while keeping the machining precision under a given level. However, the problem is computationally challenging. The main contribution of this paper is to approximate the problem nicely by a finite-state convex optimization problem which can be solved efficiently. The method consists of two key ingredients. First, a relationship between the tracking error and the input signal in a high-order CNC servo system is established. As a consequence, the tracking error constraint is reduced to a constraint on the kinematic quantities. Second, a novel method is introduced to relax the nonlinear constraints on kinematic quantities to linear ones. Experimental results are used to validate the proposed method."]},
{"title": "Automatic mesh generation and transformation for topology optimization methods", "highlights": ["Towards the integration of topology optimization methods with CAD.", "Automatic transformation of topology optimization results into 3D shapes.", "New mesh generation and transformation algorithms for heterogeneous geometry."], "abstract": ["This paper presents automatic tools aimed at the generation and adaptation of unstructured tetrahedral meshes in the context of ", " or ", " geometry. These tools are primarily intended for applications in the domain of topology optimization methods but the approach introduced presents great potential in a wider context. Indeed, various fields of application can be foreseen for which meshing heterogeneous geometry is required, such as finite element simulations (in the case of heterogeneous materials and assemblies, for example), animation and visualization (medical imaging, for example). Using B-Rep concepts as well as specific adaptations of advancing front mesh generation algorithms, the mesh generation approach presented guarantees, in a simple and natural way, mesh continuity and conformity across interior boundaries when trying to mesh a composite domain. When applied in the context of topology optimization methods, this approach guarantees that ", " and ", " are meshed so that finite elements are tagged as ", " and ", " and so that continuity and conformity are guaranteed at the interface between ", " and ", ". The paper also presents how mesh transformation and mesh smoothing tools can be successfully used when trying to derive a functional shape from raw topology optimization results."]},
{"title": "Geometric modelling of drawbeads using vertical section sweeping", "highlights": ["Full three-dimensional geometric model of drawbeads in automotive draw dies.", "A vertical section sweeping process for the geometric model of drawbeads.", "Fully automated and robust procedure for the modelling of drawbeads."], "abstract": ["This paper presents a swept surface for the geometric model of drawbeads mounted on curved surfaces. Vertical section sweeping is used for the swept surface. The section curves are the cross sections of the drawbeads and the shapes of the section curves are variable. Also, the section planes are normal to the projection of the drawbead curve along the die opening direction. This paper presents a scheme for computing the variable vertical section curves and the smooth endings for the swept surface of a drawbead. The geometric model presented has a no-undercut shape in the die opening direction and easily represents the smooth ending at ends of drawbeads."]},
{"title": "Designing heterogeneous porous tissue scaffolds for additive manufacturing processes", "highlights": ["Heterogeneous porous architecture of tissue scaffolds is designed.", "To improve cell survivability, radial channels are optimally generated.", "Iso-porous curves are optimally determined to generate the spatial porosity.", "A continuous deposition path planning is developed for additive processes."], "abstract": ["A novel tissue scaffold design technique has been proposed with controllable heterogeneous architecture design suitable for additive manufacturing processes. The proposed layer-based design uses a bi-layer pattern of radial and spiral layers consecutively to generate functionally gradient porosity, which follows the geometry of the scaffold. The proposed approach constructs the medial region from the medial axis of each corresponding layer, which represents the geometric internal feature or the spine. The radial layers of the scaffold are then generated by connecting the boundaries of the medial region and the layer\u2019s outer contour. To avoid the twisting of the internal channels, reorientation and relaxation techniques are introduced to establish the point matching of ruling lines. An optimization algorithm is developed to construct sub-regions from these ruling lines. Gradient porosity is changed between the medial region and the layer\u2019s outer contour. Iso-porosity regions are determined by dividing the sub-regions peripherally into pore cells and consecutive iso-porosity curves are generated using the iso-points from those pore cells. The combination of consecutive layers generates the pore cells with desired pore sizes. To ensure the fabrication of the designed scaffolds, the generated contours are optimized for a continuous, interconnected, and smooth deposition path-planning. A continuous zig\u2013zag pattern deposition path crossing through the medial region is used for the initial layer and a biarc fitted iso-porosity curve is generated for the consecutive layer with ", " continuity. The proposed methodologies can generate the structure with gradient (linear or non-linear), variational or constant porosity that can provide localized control of variational porosity along the scaffold architecture. The designed porous structures can be fabricated using additive manufacturing processes."]},
{"title": "A triangulation-based hole patching method using differential evolution", "highlights": ["A new hole patching method is proposed to repair the defective model.", "The information on both sides of the boundary around the considered hole is used.", "The points in the hole region are predicted by differential evolution.", "The operations of mesh optimization are used to improve the quality of the mesh."], "abstract": ["In this work, a new hole patching method (namely as, HPDE) is proposed to repair the damaged or ill-scanned three dimensional objects in real engineering applications. Our method differentiates from other related algorithms mainly on the following three aspects. Firstly, our algorithm sufficiently utilizes the point information around the considered hole for each prediction by constructing point correspondences on both sides of the boundary curve of the hole; secondly, the missing points in the hole region are predicted by the algorithm of differential evolution (DE), which is used to obtain the topological and geometrical structures of the mesh in the hole region; thirdly, operations of mesh optimization are adopted for improving the quality of the obtained triangulation mesh. Numerical results on kinds of holes with complex shape and large curvature, and a comparison with two recently proposed algorithms verify the effectiveness of the algorithm, further experiments on the noisy data points illustrate the robustness of the algorithm against noise."]},
{"title": "Assembly unit partitioning for hull structure in shipbuilding", "highlights": ["A hull assembly model based on the attributed connection oriented graph is presented.", "Using the fuzzy assessment rule to establish the assembly relation matrix of hull.", "The fuzzy cluster method to analyze partition of assembly units is given out.", "To establish evaluation model on assembly ability of hull.", "Using the fuzzy synthetic evaluation to decide the optimal partitioning scheme."], "abstract": ["In order to solve the problem that low automatic degree to the block assembly process design will negatively influence construction cycle time and construction quality in shipbuilding, this paper presents a newly developed determination system of assembly units for the hull structure. Firstly, the assembly information model of a hull block which includes the part information and the linkage information between parts is proposed following the structural features and assembly process of the hull structure. Secondly, the assembly relation matrix is established on the basis of the fuzzy assessment rule and then the fuzzy clustering method to analyze partition of assembly units is given out. Thirdly, through analyzing the assembly ability of hull structures, the set of evaluating indexes is founded whose weights are decided by an analytic hierarchy process (AHP). Meanwhile, the model for the assessment is provided by fuzzy synthetic evaluation (FSE) and the comprehensive assessment values of assembly partition schemes can be computed to judge the final optimization scheme. A block assembly is taken as an example to verify the proposed method, and the results show that it is an effective method for solving the partition problem of hull structures."]},
{"title": "Product modeling framework based on interaction feature pair", "highlights": ["The framework can make parts pre-interact with each other at part modeling stage.", "It can integrate more design activities and support different modeling approaches.", "It relies on an interaction feature pair (IFP) that can be constructed mathematically.", "An IFP incorporates more information, especially the behavioral information."], "abstract": ["Designing products effectively and efficiently is of great significance. However, currently part models are usually created without knowing how they will interact with other parts, leading to gaps between part modeling and assembly modeling and between other applications, and to tedious and redundant labor. This paper proposes a novel product modeling framework to address the problems. The framework is different from current product modeling systems from two aspects. On the architecture level, a new module based on a concept of interaction feature pair (IFP) is developed. An IFP incorporates information of interaction type, related feature pairs and behavioral information that fulfill the interactions. The new module can model the structure of IFPs mathematically through operators and functions defined in a space spanned from six basic IFPs. It can also utilize the constituent elements of an IFP as state variables to form behavior models for the IFP. On the process level, the IFP-based framework can support both bottom-up and top-down approaches, and integrate part modeling and assembly modeling together by changing the workflows. Concretely, IFPs will be embedded into part models at part modeling stage to make them pre-interact with each other, and at assembly modeling stage, parts will be assembled by instantiating the embedded IFPs instead of specifying mating constraints, thus reducing the tedious and redundant labor. Incorporating knowledge of different domains, IFPs can also be developed to integrate more applications together. The implementation of the framework is demonstrated through a prototype system."]},
{"title": "Development of a simulated living-environment platform: Design of BCI assistive software and modeling of a virtual dwelling place", "highlights": ["SLEP=BCI\u00a0system+assistive software tailored to motor-disabled people\u00a0+\u00a0virtual dwelling place.", "The platform was configured to provide different levels of workload.", "Human\u2013computer interaction is strengthened by user guidance.", "This is important in BCI research, as the prototypes are created for isolated people.", "This approach could avoid the assumption of locked-in BCI users\u2019 requirements."], "abstract": ["In brain\u2013computer interfaces (BCIs), the user mental constrains and the cognitive workload involved are frequently overlooked. These factors are aggravated by neuromuscular dysfunction, collateral complications, and side effects of medication in motor-impaired people. We therefore proposed to develop a simulated living-environment platform (SLEP) that was tailored to severely paralyzed people and also allowed the progressive user\u2013system adaptation through increasingly demanding scenarios. This platform consisted of a synchronous motor imagery based BCI system, an everyday assistive computer program, and a virtual dwelling place. The SLEP was tested in 11 healthy users, where the user\u2013system adaptation was evaluated according to the BCI accuracy for classifying the user control tasks. The user heart rate was also incorporated in the evaluation in order to verify the progressiveness of such adaptation. The results of this study showed that user performance tended to increase from the least to the most challenging scenario in learning situations. The results also showed that nine of the eleven users controlled the BCI system in cue-driven mode, completing over half of the tasks. Two of the eleven users controlled the BCI system in target-driven mode, completing two tasks. Taken together, these results suggest that the progressive adaptation in BCI systems can enhance the performance, the persistence and the confidence of the users, even when they are immersed in simulated daily-living situations."]},
{"title": "Footwear bio-modelling: An industrial approach", "highlights": ["We present two interrelated models for shoe customisation.", "We present a bio-deformable model of the foot.", "We present a deformable model of the shoe last.", "Both models are used to successfully make customised shoes."], "abstract": ["There is a growing need within the footwear sector to customise the design of the last from which a specific footwear style is to be produced. This customisation is necessary for user comfort and health reasons, as the user needs to wear a suitable shoe. For this purpose, a relationship must be established between the user foot and the last with which the style will be made; up until now, no model has existed that integrates both elements. On the one hand, traditional customised footwear manufacturing techniques are based on purely artisanal procedures which make the process arduous and complex; on the other hand, geometric models proposed by different authors present the impossibility of implementing them in an industrial environment with limited resources for the acquisition of morphometric and structural data for the foot, apart from the fact that they do not prove to be sufficiently accurate given the non-similarity of the foot and last. In this paper, two interrelated geometric models are defined, the first, a bio-deformable foot model and the second, a deformable last model. The experiments completed show the goodness of the model, with it obtaining satisfactory results in terms of comfort, efficiency and precision, which make it viable for use in the sector."]},
{"title": "A method for computer-aided specification of geometric tolerances", "highlights": ["Automated generation of GD&T specifications for mechanical assemblies.", "Data-driven identification of most precision requirements from assembly process data.", "Rule-based selection of datum reference frames and tolerance types on parts.", "Text-based input and output, discussion of CAD implementation issues."], "abstract": ["The paper describes a method for the generation of tolerance specifications from product data. The problem is nontrivial due to the increasing adoption of geometric dimensioning criteria, which call for the use of many types of geometric tolerances to completely and unambiguously represent the design intent and the many constraints deriving from manufacturing, assembly and inspection processes. All these issues have to be modeled and explicitly provided to a generative specification procedure, which may thus need a large amount of input data. The proposed approach tries to avoid this difficulty by considering that most precision requirements to be defined relate to the assembly process, and can be automatically derived by analyzing the contact relations between parts and the assembly operations planned for the product. Along with possible user-defined additional requirements relating to function, assembly requirements are used in a rule-based geometric reasoning procedure to select datum reference frames for each part and to assign tolerance types to part features. A demonstrative software tool based on the developed procedure has allowed to verify its correctness and application scope on some product examples."]},
{"title": "Curvature tensor computation by piecewise surface interpolation", "highlights": ["Closed form Taubin integral for smooth or piecewise smooth surfaces.", "An improved local surface interpolation scheme for triangular meshes.", "Novel method for accurate curvature tensor estimation of triangle meshes."], "abstract": ["Estimating principal curvatures and principal directions of a smooth surface represented by a triangular mesh is an important step in many CAD or graphics related tasks. This paper presents a new method for curvature tensor estimation on a triangular mesh by replacing flat triangles with triangular parametric patches. An improved local interpolation scheme of cubic triangular B\u00e9zier patches to vertices and vertex normals of triangle meshes is developed. Piecewise parametric surfaces that have ", " continuity across boundary curves of adjacent patches and ", " continuity at the joint vertices are obtained by the interpolation scheme. A closed form expression of Taubin integral\u2013a ", " symmetric matrix in integral formulation\u2013is derived based on the piecewise parametric surfaces. Principal curvatures and principal directions are then computed from the Taubin integral. The proposed method does not need to parameterize data points or solve a linear system which is usually required by other surface fitting methods. Compared to several state-of-the-art curvature estimation methods, the proposed method can generate more accurate results for general surface meshes. The experiments have demonstrated its accuracy, robustness and effectiveness."]},
{"title": "Abstraction of mid-surfaces from solid models of thin-walled parts: A divide-and-conquer approach", "highlights": ["Identification of and discussion on general issues on the existing methods for mid-surface abstraction.", "Adoption of the divide-and-conquer paradigm to address the issues on the mid-surface abstraction.", "Providing a general algorithm to abstract more complete mid-surfaces for complex solid models."], "abstract": ["Abstraction of mid-surfaces from solid models of thin-walled parts is becoming a useful function for idealizing the solid models for engineering analysis. Various abstraction techniques have been developed and adapted to commercial CAD systems. However, they are generally lacking in completeness and robustness. That is, for some complex solid models, valid mid-surfaces are not created and mid-surfaces which are created may not be valid or of little use in practice. Existing techniques utilize rules and heuristics in detecting appropriate face-pairs and generating mid-surface patches, but these rules and heuristics often do not apply to complex solid models. To address this problem, a divide-and-conquer approach to mid-surface abstraction is proposed in this paper. A solid model is decomposed into simple volumes, and mid-surfaces of the simple volumes are abstracted. The mid-surfaces of the simple volumes are then composed into the mid-surfaces of the original solid model. The proposed method has been implemented and tested with practical examples. The results of case studies have been presented to attest to the usefulness of the proposed method. Some general issues on mid-surface abstraction are also discussed."]},
{"title": "Progressive and iterative approximation for least squares B-spline curve and surface fitting", "highlights": ["A new progressive and iterative approximation method for least square fitting (LSPIA) is presented.", "LSPIA can handle a point set of large size.", "LSPIA is so flexible that it allows the adjustment of the number of control points, and a knot vector in the iterations.", "LSPIA is easy to make the fitting curve hold the shape preserving property.", "LSPIA can be performed in parallel efficiently."], "abstract": ["The progressive and iterative approximation (PIA) method is an efficient and intuitive method for data fitting. However, in the classical PIA method, the number of the control points is equal to that of the data points. It is not feasible when the number of data points is very large. In this paper, we develop a new progressive and iterative approximation for least square fitting (LSPIA). LSPIA constructs a series of fitting curves (surfaces) by adjusting the control points iteratively, and the limit curve (surface) is the least square fitting result to the given data points. In each iteration, the difference vector for each control point is a weighted sum of some difference vectors between the data points and their corresponding points on the fitting curve (surface). Moreover, we present a simple method to compute the practical weight whose corresponding convergence rate is comparable to that of the theoretical best weight. The advantages of LSPIA are two-fold. First, with LSPIA, a very large data set can be fitted efficiently and robustly. Second, in the incremental data fitting procedure with LSPIA, a new round of iterations can be started from the fitting result of the last round of iterations, thus saving great amount of computation. Lots of empirical examples illustrated in this paper show the efficiency and effectiveness of LSPIA."]},
{"title": "Efficient energy evaluations for active B-Spline/NURBS surfaces", "highlights": ["Efficient Active B-Spline/NURBS energy evaluations based on an Analytic Solution.", "Efficient algorithm for computing the stiffness of an Active B-Spline/NURBS Surface.", "Accurate and stable algorithms for computing integrals of B-Spline Basis Functions.", "Detailed analysis of computational efficiency, accuracy and stability.", "Up to four times faster than standard Gaussian Quadrature for practical cases."], "abstract": ["The construction of the stiffness matrix associated with an Active B-Spline/NURBS surface is one of the most important but time consuming operations performed in CAD/CAM/CAE. This paper aims to address this problem and presents a novel, computationally efficient, generalised mathematical framework and accompanying algorithms based on an analytic solution to the problem. The approach is shown to extend seamlessly to the problem of computing mass, damping and forcing matrices, and importantly, can handle variable mass, damping, and stiffness coefficients. The capabilities of the algorithms are illustrated and their respective performances verified through detailed analysis of the computational efficiency, accuracy and stability in several practical case studies. The main benefit of the proposed approach is a reduction in computation times required for the evaluation of the stiffness matrix by up to a factor of 4, over the standard Gaussian Quadrature approach, for the practical cases considered, while preserving a high degree of accuracy and stability. Additionally, no assumptions regarding the problem complexity, degree, or regularity of the knot vector are imposed upon the solution in order to achieve the computational saving."]},
{"title": "Enhanced Product Lifecycle Information Management using \u201ccommunicating materials\u201d", "highlights": ["A system for an enhanced Product LifeCycle Information Management (PLIM) is proposed.", "A dual challenge is addressed: providing a new intelligent material & a high degree of data synchronization.", "These challenges are addressed with the combination of ", " and \u201ccommunicating materials\u201d.", "A scenario using \u201cmedical garments\u201d shows how the desired degree of data synchronization is reached.", "Research perspectives on data synchronization regarding existing standards are introduced."], "abstract": ["With traditional PLM (Product Lifecycle Management), people think toward the future: first comes product development, then manufacturing, then support and finally disposal \u2014 data flows only in the forward direction. With the ", " (Closed-Loop Lifecycle Management) concept, there are also flows going backward, enabling better visibility and control of the product throughout its PLC (Product Life Cycle). ", " uses \u201cintelligent products\u201d to gather information at any instant of the PLC and to make it available on downstream or upstream PLC phases. However, the information is often deported on a database and is accessed remotely via a network pointer carried by the product. To bring the ", " concept a step further, this paper addresses a dual challenge: (i) providing a new kind of intelligent material capable of undergoing physical transformations without losing its communication ability and the data that is stored on it, (ii) providing a framework to achieve a high degree of data synchronization (i.e. enabling data updates on the product, regardless of the network availability). An applicative scenario is presented, showing how this kind of material is put into practice in the context of \u201ccommunicating medical garments\u201d and how the requirements of the desired degree of data synchronization can be reached."]},
{"title": "Continuous penetration depth", "highlights": ["The resulting configuration causes the two objects to just touch.", "The penetration depth magnitude and direction continuously change for a continuous relative motion in configuration space.", "The derivatives of the continuous penetration depth are well defined if the contact space is smooth and the motion is continuous.", "Applicability to convex and non-convex contact spaces that are homeomorphic to a sphere."], "abstract": ["We present a new measure for computing the ", " between two intersecting rigid objects. We generate a set of samples in the configuration space, precompute an approximation of the ", " for two intersecting objects using binary classification techniques, and construct a bijective mapping between the spherical space and the precomputed contact space. For a given in-collision configuration, we search the spherical space for the nearest neighbor, and find the corresponding image in the contact space based on the predefined spherical parameterization. The resulting image is a witness equivalent to the nearest configuration, and it is used to formulate the penetration depth direction based on our measure. Unlike prior algorithms, our algorithm guarantees that both the penetration depth magnitude and direction are continuous with respect to the motion parameters. Our algorithm is approximate in the sense that we approximate the exact contact space. We have applied our algorithm to complex rigid models composed of tens or hundreds of thousands of triangles, and the runtime query takes only around 0.01\u00a0ms."]},
{"title": "A parallel algorithm for improving the maximal property of Poisson disk sampling", "highlights": ["A simple algorithm for improving the maximal property of Poisson disk sampling.", "The algorithm is fully parallel and can be implemented on the GPU.", "It works for 2D and 3D, and can also be extended to surface in an intrinsic manner."], "abstract": ["This paper presents a simple yet effective algorithm to improve an arbitrary Poisson disk sampling to reach the maximal property, i.e., no more Poisson disk can be inserted. Taking a non-maximal Poisson disk sampling as input, our algorithm efficiently detects the regions allowing additional samples and then generates Poisson disks in these regions. The key idea is to convert the complicated plane or space searching problem into a simple searching on circles or spheres, which is one dimensional lower than the original sampling domain. Our algorithm is memory efficient and flexible, which generates maximal Poisson disk sampling in an arbitrary 2D polygon or 3D polyhedron. Moreover, our parallel algorithm can be extended from the Euclidean space to curved surfaces in an intrinsic manner. Thanks to its parallel structure, our method can be implemented easily on modern graphics hardware. We have observed significance performance improvement compared to the existing techniques."]},
{"title": "Optimizing polycube domain construction for hexahedral remeshing", "highlights": ["An effective volumetric polycube parameterization algorithm.", "An automatic hexahedral mesh generation pipeline for general 3D models.", "A polycube domain optimization algorithm based on homotopic morphological operations."], "abstract": ["Polycube mapping can provide regular and global parametric representations for general solid models. Automatically constructing effective polycube domains, however, is challenging. We present an algorithm for polycube construction and volumetric parameterization. The algorithm has three steps: pre-deformation, polycube construction and optimization, and mapping computation. Compared with existing polycube mapping methods, our algorithm can robustly generate desirable polycube domain shape and low-distortion volumetric parameterization. It can be used for automatic high-quality hexahedral mesh generation."]},
{"title": "Improving spatial coverage while preserving the blue noise of point sets", "highlights": ["Local smoothing to optimize Voronoi cell aspect ratios.", "Simultaneously achieve random and well-spaced points.", "Image filtering applications.", "Meshing applications."], "abstract": ["We explore the notion of a Well-spaced Blue-noise Distribution (WBD) of points, which combines two desirable properties. First, the point distribution is random, as measured by its spectrum having blue noise. Second, it is well-spaced in the sense that the minimum separation distance between samples is large compared to the maximum coverage distance between a domain point and a sample, i.e.\u00a0its Voronoi cell aspect ratios ", " are small. It is well known that maximizing one of these properties destroys the other: uniform random points have no aspect ratio bound, and the vertices of an equilateral triangular tiling have no randomness. However, we show that there is a lot of room in the middle to get good values for both. Maximal Poisson-disk sampling provides ", " and blue noise. We show that a standard optimization technique can improve the well-spacedness while preserving randomness.", "Given a random point set, our Opt-", " \u00a0algorithm iterates over the points, and for each point locally optimizes its Voronoi cell aspect ratio ", ". It can improve ", " to a large fraction of the theoretical bound given by a structured tiling: improving from 1.0 to around 0.8, about half-way to 0.58, while preserving most of the randomness of the original set. In terms of both ", " and randomness, the output of Opt-", " \u00a0compares favorably to alternative point improvement techniques, such as centroidal Voronoi tessellation with a constant density function, which do not target ", " directly. We demonstrate the usefulness of our output through meshing and filtering applications. An open problem is constructing from scratch a WBD distribution with a guarantee of ", "."]},
{"title": "Isogeometric analysis on triangulations", "highlights": ["Isogeometric analysis on triangulation of a domain bounded by NURBS curves.", "Geometry and solution represented by bivariate splines in Bernstein\u2013B\u00e9zier form.", "Approach to construct parametric domain and construct ", "-smooth basis functions.", "Applicable to complex topologies and allow highly localized refinement.", "Isogeometric analysis of linear elasticity and advection\u2013diffusion demonstrated."], "abstract": ["We present a method for isogeometric analysis on the triangulation of a domain bounded by NURBS curves. In this method, both the geometry and the physical field are represented by bivariate splines in Bernstein\u2013B\u00e9zier form over the triangulation. We describe a set of procedures to construct a parametric domain and its triangulation from a given physical domain, construct ", "-smooth basis functions over the domain, and establish a rational Triangular B\u00e9zier Spline (rTBS) based geometric mapping that ", "-smoothly maps the parametric domain to the physical domain and exactly recovers the NURBS boundaries at the domain boundary. As a result, this approach can achieve automated meshing of objects with complex topologies and allow highly localized refinement. Isogeometric analysis of problems from linear elasticity and advection\u2013diffusion analysis is demonstrated."]},
{"title": "Extraction of generative processes from B-Rep shapes and application to idealization transformations", "highlights": ["Generative processes are derived from extrusion primitives identified in the object.", "Maximal primitives are identified and removed from the object to simplify its shape.", "The object is simplified up to single extrusions to produce a construction graph.", "The extracted graph is a non trivial representation of the object with additive process.", "Extrusion primitives and interfaces are used to generate an idealized model for FEA."], "abstract": ["A construction tree is a set of shape generation processes commonly produced with CAD modelers during a design process of B-Rep objects. However, a construction tree does not bring all the desired properties in many configurations: dimension modifications, idealization processes, etc. Generating a non trivial set of generative processes, possibly forming a construction graph, can significantly improve the adequacy of some of these generative processes to meet user\u2019s application needs. This paper proposes to extract generative processes from a given B-rep shape as a high-level shape description. To evaluate the usefulness of this description, finite element analyses (FEA) and particularly idealizations are the applications selected to evaluate the adequacy of additive generative processes. Non trivial construction trees containing generic extrusion and revolution primitives behave like well established CSG trees. Advantageously, the proposed approach is primitive-based, which ensures that any generative process of the construction graph does preserve the realizability of the corresponding volume. In the context of FEA, connections between idealized primitives of a construction graph can be efficiently performed using their interfaces. Consequently, generative processes of a construction graph become a high-level object structure that can be tailored to idealizations of primitives and robust connections between them."]},
{"title": "A sweep and translate algorithm for computing voxelized 3D Minkowski sums on the GPU", "highlights": ["Algorithm for computing voxelized Minkowski sum of two input polyhedra.", "New decomposition formula for computing Minkowski sum, with proof.", "Efficient GPU implementation using stencil shadow volumes."], "abstract": ["Computing the Minkowski sum of two arbitrary polyhedra in ", " is difficult because of high combinatorial complexity. We present an algorithm for directly computing a voxelization of the Minkowski sum of two closed watertight input polyhedra for applications such as path planning that do not require a boundary representation as output. We introduce a new decomposition formula for computing the Minkowski sum and prove its correctness. We describe an efficient Graphics Processing Unit (GPU) implementation of the algorithm using stencil shadow volumes to create a solid voxelization of the Minkowski sum."]},
{"title": "High-quality vertex clustering for surface mesh segmentation using Student-", "highlights": ["We propose a method for high-quality vertex clustering using Student-", " mixture model.", "Less redundant clusters, which have no small isolated fragments, have been obtained.", "High-quality clusters have been obtained which correspond to the underlying surfaces."], "abstract": ["In order to robustly perform segmentation for industrial design objects measured by a 3-D scanning device, we propose a new method for high-quality vertex clustering on a noisy mesh. Using ", " ", " with the ", " approximation, we develop a vertex clustering algorithm in the 9-D space composed of three kinds of principal curvature measures along with vertex position and normal component. The normal component is added, because it well describes the surface-features and is less influenced by noise, and the positional component suppresses redundant clusters due to the normal one. Furthermore, in order to enhance the robustness for noisy data, considering mesh topology as a spatial constraint and letting the vertices in its surroundings belong to the same cluster by diffusion process, we protect generating many small fragments due to noise. We demonstrate effectiveness of our method by applying it to the real-world scanned data."]},
{"title": "Implicit matrix representations of rational B\u00e9zier curves and surfaces", "highlights": ["Introduction of matrix representations of B\u00e9zier curves and surfaces.", "Algebraic properties of matrix representations are presented.", "Solving of the inversion problem by means of matrix representations.", "Numerical behavior of matrix representations through singular value decomposition.", "Ray-tracing of B\u00e9zier surfaces by means of matrix representations."], "abstract": ["We introduce and study a new implicit representation of rational B\u00e9zier curves and surfaces in the 3-dimensional space. Given such a curve or surface, this representation consists of a matrix whose entries depend on the space variables and whose rank drops exactly on this curve or surface. Our approach can be seen as an extension of the moving lines implicitization method introduced by Sederberg, from non-singular matrices to the more general context of singular matrices. In the first part of this paper, we describe the construction of these new implicit matrix representations and their main geometric properties, in particular their ability to solve efficiently the inversion problem. The second part of this paper aims to show that these implicitization matrices adapt geometric problems, such as intersection problems, to the powerful tools of numerical linear algebra, in particular to one of the most important: the singular value decomposition. So, from the singular values of a given implicit matrix representation, we introduce a real evaluation function. We show that the variation of this function is qualitatively comparable to the Euclidean distance function. As an interesting consequence, we obtain a new determinantal formula for implicitizing a rational space curve or surface over the field of real numbers. Then, we show that implicit matrix representations can be used with numerical computations, in particular there is no need for symbolic computations to use them. We give some rigorous results explaining the numerical stability that we have observed in our experiments. We end the paper with a short illustration on ray tracing of parameterized surfaces."]},
{"title": "A unified method for hybrid subdivision surface design using geometric partial differential equations", "highlights": ["We composite surface subdivision and the GPDE to form a unified method for freeform surface design.", "We present a novel technique to evaluate the finite element basis functions.", "This is a first attempt for constructing the GPDE subdivision surface with hybrid control meshes."], "abstract": ["Surface subdivision gains popularity in surface design owing to its flexible applications for geometry with complicated topology and simple computational scheme, and the geometric partial differential equation (GPDE) method is an advanced technology for constructing high-quality smooth surfaces. In this paper, we composite these two ingredients to form a unified method for freeform surface design. We choose the mean curvature flow and Willmore flow as our driven GPDEs, and the finite element method coupled with a hybrid Loop and Catmull\u2013Clark subdivision algorithm as the numerical simulation method. This research presents a novel technique to evaluate the finite element basis functions and the first attempt for constructing the GPDE subdivision surface with hybrid control meshes consisting of triangles and quadrilaterals. Numerical experiments show that the construction method is efficient and robust, yielding high-quality hybrid subdivision surfaces."]},
{"title": "Solving multivariate polynomial systems using hyperplane arithmetic and linear programming", "highlights": ["A new scalable algorithm for solving systems of multivariate polynomials.", "The concept of hyperplane arithmetic, which is used in our algorithm.", "A benchmark of example systems that are scalable in the number of variables.", "Implementation and comparison with previous algorithms."], "abstract": ["Solving polynomial systems of equations is an important problem in many fields such as computer-aided design, manufacturing and robotics. In recent years, subdivision-based solvers, which typically make use of the properties of the B\u00e9zier/", "-spline representation, have proven successful in solving such systems of polynomial constraints. A major drawback in using subdivision solvers is their lack of scalability. When the given constraint is represented as a tensor product of its variables, it grows exponentially in size as a function of the number of variables. In this paper, we present a new method for solving systems of polynomial constraints, which scales nicely for systems with a large number of variables and relatively low degree. Such systems appear in many application domains. The method is based on the concept of ", ", which can be viewed as a generalization of interval arithmetic. We construct bounding hyperplanes, which are then passed to a linear programming solver in order to reduce the root domain. We have implemented our method and present experimental results. The method is compared to previous methods and its advantages are discussed."]},
{"title": "Geometric computation and optimization on tolerance dimensioning", "highlights": ["LPGUM model is extended to propose a new complete representation of tolerances.", "A new analysis method for the tolerance estimation via geometric computations.", "A new optimization method for improving a dimension scheme to reduce tolerances."], "abstract": ["This paper presents an efficient geometric method of tolerance analysis for optimizing dimensioning and providing an optimal processing plan for a discrete part. Geometric primitives are used to represent part features, and dependencies in the dimensions between parts are represented by a topological graph. The ordering of these dependencies can have a significant effect on the tolerance zones in the part. To obtain tolerance zones from the dependencies, the conventional parametric method of tolerance analysis is decomposed into a set of geometric computations, which are combined and cascaded to obtain the tolerance zones in the geometric representations. Geometric optimization is applied to the topological graph in order to find a solution that provides not only an optimal dimensioning scheme but also an optimal plan for manufacturing the physical part. The applications of our method include tolerance analysis, dimension scheme optimization, and process planning."]},
{"title": "Geometric interoperability via queries", "highlights": ["Queries solve interoperability problems that are unsolved by a data-centric approach.", "Interoperability, interchangeability, and integration use a semantic reference model.", "A hierarchy solves interoperation in design and manufacturing of incidence structures."], "abstract": ["The problem of geometric (model and system) interoperability is conceptualized as a non-trivial generalization of the problem of part interchangeability in mechanical assemblies. Interoperability subsumes the problems of geometric model quality, exchange, and interchangeability, as well as system integration. Until now, most of the interoperability proposals have been data-centric. Instead, we advocate a query-centric approach that can deliver interoperable solutions to many common geometric tasks in computer aided design and manufacturing, including model acquisition and exchange, metrology, and computer aided design/analysis integration."]},
{"title": "Automated fixture configuration for rapid manufacturing planning", "highlights": ["Automatic rapid modular fixture configuration.", "Fast hashing algorithms to generate form/force closure grasps.", "Automatic selection & assembly of modular elements from user specified library.", "Includes accessibility & collision constraints for machining/inspection.", "Results for complex parts in realistic process plans."], "abstract": ["The wide adoption of agile manufacturing systems has necessitated the design and use of fixtures or work holding devices that have in-built flexibility to rapidly respond to part design changes. Despite the availability of reconfigurable fixtures, practical fixture configuration largely remains an experience driven manual activity to enable customization for varying workpiece geometry, and most automated solutions do not scale well to accommodate such variation. In this paper, we address the problem of rapidly synthesizing a realistic fixture that will guarantee stability and immobility of a specified polyhedral work-part. We propose that the problem of automated fixture layout may be approached in two distinct stages. First, we determine the spatial locations of clamping points on the work piece boundary using the principles of force and form closure, to ensure immobility of the fixtured part under external perturbation. In particular, we show that the candidate restraints mapped to the six dimensional vector space of wrenches (force\u2013moment pairs) may be hashed in a straightforward manner to efficiently generate force closure configurations that restrain part movement against large external wrenches. When clamps are allowed to exert arbitrarily high reaction forces on the part, the spatial arrangement of the clamping locations ensures the part is in form closure. On generating force/form closure configurations, the chosen locations are matched against a user-specified library of reconfigurable clamps to synthesize a valid fixture layout comprising clamps that are accessible and collision free with each other and the part. Additionally, in the case of determining machining setups the clamps are chosen to avoid collisions with the moving cutting tool. We demonstrate fast algorithms to perform both location selection and fixture matching, and show several results that underscore the practical application of our solution in automated manufacturing process planning."]},
{"title": "Geometry seam carving", "highlights": ["Mesh deformation with preservation of salient features.", "Transfer of the concept of seam carving from images to 3D meshes.", "Combination of elastic mesh editing with a novel discrete mesh deformation approach.", "Adaptation of the surface tessellation to the degree of the deformation distortion."], "abstract": ["We present a novel approach to feature-aware mesh deformation. Previous mesh editing methods are based on an elastic deformation model and thus tend to uniformly distribute the distortion in a least-squares sense over the entire deformation region. Recent results from image resizing, however, show that discrete local modifications such as deleting or adding connected seams of image pixels in regions with low saliency lead to far superior preservation of local features compared to uniform scaling \u2014 the image retargeting analog to least-squares mesh deformation. Hence, we propose a discrete mesh editing scheme that combines elastic as well as plastic deformation (in regions with little geometric detail) by transferring the concept of seam carving from image retargeting to the mesh deformation scenario. A geometry seam consists of a connected strip of triangles within the mesh\u2019s deformation region. By collapsing or splitting the interior edges of this strip, we perform a deletion or insertion operation that is equivalent to image seam carving and can be interpreted as a local plastic deformation. We use a feature measure to rate the geometric saliency of each triangle in the mesh and a well-adjusted distortion measure to determine where the current mesh distortion asks for plastic deformations, i.e., for deletion or insertion of geometry seams. Precomputing a fixed set of low-saliency seams in the deformation region allows us to perform fast seam deletion and insertion operations in a predetermined order such that the local mesh modifications are properly restored when a mesh editing operation is (partially) undone. Geometry seam carving hence enables the deformation of a given mesh in a way that causes stronger distortion in homogeneous mesh regions while salient features are preserved much better."]},
{"title": "Computation of components\u2019 interfaces in highly complex assemblies", "highlights": ["Computation of interfaces between components in complex CAD assemblies.", "GPU ray-casting technique for efficient processing of large assemblies.", "Accurate NURBS geometry of the interfaces, stored in standard STEP file."], "abstract": ["The preparation of CAD models from complex assemblies for simulation purposes is a very time-consuming and tedious process, since many tasks such as meshing and idealization are still completed manually. Herein, the detection and extraction of geometric interfaces between components of the assembly is of central importance not only for the simulation objectives but also for all necessary shape transformations such as idealizations or detail removals. It is a repetitive task in particular when complex assemblies have to be dealt with. This paper proposes a method to rapidly and fully automatically generate a precise geometric description of interfaces in generic B-Rep CAD models. The approach combines an efficient GPU ray-casting technique commonly used in computer graphics with a graph-based curve extraction algorithm. Not only is it able to detect a large number of interfaces efficiently, it also provides an accurate Nurbs geometry of the interfaces, that can be stored in a plain STEP file (Iso 10303-1:1994 (1994))\u00a0 ", " for further downstream treatment. We demonstrate our approach on examples from aeronautics and automotive industry."]},
{"title": "An optimization approach for constructing trivariate ", "highlights": ["Automatically construct a trivariate tensor-product ", "-spline solid.", "The minimal Jacobian of the resulting solid is positive.", "A volumetric functional is minimized to improve parametrization quality.", "Deformation, constraint aggregation, and divide-and-conquer techniques combined."], "abstract": ["In this paper, we present an approach that automatically constructs a trivariate tensor-product ", "-spline solid via a gradient-based optimization approach. Given six boundary ", "-spline surfaces for a solid, this approach finds the internal control points so that the resulting trivariate ", "-spline solid is valid in the sense the minimal Jacobian of the solid is positive. It further minimizes a volumetric functional to improve resulting parametrization quality.", "For a trivariate ", "-spline solid even with moderate shape complexity, direct optimization of the Jacobian of the ", "-spline solid is computationally prohibitive since it would involve thousands of design variables and hundreds of thousands of constraints. We developed several techniques to address this challenge. First, we develop initialization methods that can rapidly generate initial parametrization that are valid or near-valid. We then use a divide-and-conquer approach to partition the large optimization problem into a set of separable sub-problems. For each sub-problem, we group the ", "-spline coefficients of the Jacobian determinant into different blocks and make one constraint for each block of coefficients. This is achieved by taking an aggregate function, the Kreisselmeier\u2013Steinhauser function value of the elements in each block. With block aggregation, it reduces the dimension of the problem dramatically. In order to further reduce the computing time at each iteration, a hierarchical optimization approach is used where the input boundary surfaces are coarsened to difference levels. We optimize the distribution of internal control points for the coarse representation first, then use the result as initial parametrization for optimization at the next level. The resulting parametrization can then be further optimized to improve the mesh quality.", "Optimized trivariate parametrization from various boundary surfaces and the corresponding parametrization metric are given to illustrate the effectiveness of the approach."]},
{"title": "Robust cascading of operations on polyhedra", "highlights": ["Geometric rounding algorithm for cascaded operations on polyhedra.", "Algorithm rounds, perturbs, and restores validity.", "Output provably valid and close to input.", "Algorithm validated on packing three polyhedra into minimal box."], "abstract": ["We present a ", " algorithm for robustly implementing ", " on polyhedra where the output of each operation is an input to the next operation. The rounding algorithm reduces the bit-precision of the input and eliminates degeneracy. To do so, it rounds coordinates to floating point and randomly perturbs them, then prunes away invalid portions of the resulting polyhedron. We demonstrate the rounding algorithm on a packing algorithm with ten cascaded Minkowski sums and set operations."]},
{"title": "Electromagnetic control of charged particulate spray systems\u2014Models for planning the spray-gun operations", "highlights": ["Derived criteria to control charged particulate sprays using electromagnetic fields.", "Such criteria can be used in physically based computer simulations for such sprays.", "Coupled multi-physics aspects of such modifications are presented.", "These aspects are critical for identifying core issues to guide future research."], "abstract": ["Charged particulate spray systems are common in many industrial and manufacturing processes. Using externally applied electromagnetic fields\u2013the dynamics of these particulate sprays can be altered to achieve improved functionality and access to spray-sites that are hard to reach. With such an alteration the spray-particulate dynamics can become non-intuitive\u2013thereby motivating a physically based modeling strategy to plan the spray-gun operations and translate this into the actual spray deposition on the target surface. In this paper we use the dynamics of charged particles to construct a set of simple geometric arguments for the identification of the mapping between the spray-gun trajectory (on its plane of traversal) and the spray-deposit location (on the plane of the target-surface). The parametric dependence of the mapping on spray-gun operation parameters (comprising nozzle velocity and trajectory) and external magnetic fields (comprising field strength and the region of applied field) is discussed. The role of such arguments in constructing appropriate computer simulation frameworks is then illustrated through an example of a discrete element simulation. Sensitivity to process parameters like particulate size and spray-gas velocity are also characterized for a given applied field."]},
{"title": "Kinematic skeleton extraction from 3D articulated models", "highlights": ["A kinematic skeleton extraction method for articulated bodies is proposed.", "It is a hybrid approach combining topology-based and geometry-based methods.", "It does not require manually-chosen feature points or markers.", "It is independent of both postures and the number of branches of the model.", "The results can be directly applied to the character rigging."], "abstract": ["We propose a method to extract a kinematic skeleton from an articulated 3D model. Unlike the curve skeleton which concentrates on the abstract structure of given model, we aim to obtain the skeleton with accurate joint positions, which can be directly used for character rigging and animation. Restricting our focus on the articulated body, we construct its skeleton in a bottom-up manner starting from a set of automatically obtained feature points. Our method is a hybrid approach combining the advantages of topology-based and geometry-based methods using the Morse theory and the shape descriptor. It does not require manually-chosen feature points or markers, and is independent of both postures and the number of branches of the given model. Experiments show that our method can efficiently extract kinematic skeletons of various kinds of articulated bodies, and also that the results can be directly applied to the character rigging."]},
{"title": "Puzzhull: Cavity and protrusion hierarchy to fit conformal polygons", "highlights": ["We define cavities and protrusions of a 2D polygon which are key geometric features of a polygon.", "We propose an algorithm to compute cavities and protrusions.", "We propose a method for 2D polygonal piece fitting using a hierarchy of geometric features defined by cavities and protrusions."], "abstract": ["In this paper, we present a simple definition for, and a method to find, cavities and protrusions of a 2D polygon. Using these, we fit conformal polygons with each other, which is similar to a jigsaw puzzle and in a general case is NP-hard to solve. We first build a hierarchy of cavities and protrusions for each polygon and use this hierarchy to check for matches between these geometric features of two polygons. This data structure allows for early rejection of mismatches and thus speeds up the fitting process. We show using many examples, that most of the common configurations in exact polygon fitting can be handled by this algorithm in polynomial time. In case of exact, yet non-unique matches, this algorithm will solve the problem in exponential time."]},
{"title": "GaFinC: Gaze and Finger Control interface for 3D model manipulation in CAD application", "highlights": ["A multi-modal control method using finger and gaze for 3D manipulation is proposed.", "Independent gaze pointing interface increases the intuitiveness of the zooming task.", "The performance of GaFinC is applicable to actual CAD tools.", "Interviews of user experience report higher intuitiveness than a mouse."], "abstract": ["Natural and intuitive interfaces for CAD modeling such as hand gesture controls have received a lot of attention recently. However, in spite of its high intuitiveness and familiarity, their use for actual applications has been found to be less comfortable than a conventional mouse interface because of user physical fatigue over long periods of operation. In this paper, we propose an improved gesture control interface for 3D modeling manipulation tasks that possesses conventional interface level usability with low user fatigue while maintaining a high level of intuitiveness. By analyzing problems associated with previous hand gesture controls in translation, rotation and zooming, we developed a multi-modal control interface GaFinC: Gaze and Finger Control interface. GaFinC can track precise hand positions, recognizes several finger gestures, and utilizes an independent gaze pointing interface for setting the point of interest. To verify the performance of GaFinC, tests of manipulation accuracy and time are conducted and their results are compared with those of a conventional mouse. The comfort and intuitiveness level are also scored by means of user interviews. As a result, although the GaFinC interface posted insufficient performance in accuracy and times compared with a mouse, it shows applicable level performance. Also users found it to be more intuitive than a mouse interface while maintaining a usable level of comfort."]},
{"title": "How the Beast really moves: Cayley analysis of mechanism realization spaces using CayMos", "highlights": ["Bijective representation of connected components as curves in a minimal ambient dimension.", "Defining and finding the \u201cdistance\u201d between two different connected components.", "Analysis and visualization of realization spaces for well-studied mechanisms."], "abstract": ["For a common class of 2D mechanisms called 1-", ", the following fundamental problems have remained open: (a) How to canonically represent (and visualize) the connected components in the Euclidean realization space. (b) How to efficiently find two realizations representing the shortest \u201cdistance\u201d between two connected components. (c) How to classify and efficiently find all the connected components, and the path(s) of continuous motion between two realizations in the same connected component, with or without restricting the ", " (sometimes called orientation type).", "For a subclass of 1-dof tree-decomposable linkages that includes many commonly studied 1-dof linkages, we solve these problems by representing a connected component of the Euclidean realization space as a curve in a carefully chosen Cayley (non-edge distance) parameter space; and proving that the representation is bijective. We also show that the above set of Cayley parameters is ", " for all generic linkages with the same underlying graph, and can be found efficiently. We add an implementation of these theoretical and algorithmic results into the new software CayMos, and give (to the best of our knowledge) the first complete analysis, visualization and new observations about the realization spaces of many commonly studied 1-dof linkages such as the amusing and well-known Strandbeest, Cardioid, Limacon and other linkages."]},
{"title": "Precise convex hull computation for freeform models using a hierarchical Gauss map and a Coons bounding volume hierarchy", "highlights": ["Present an interactive-speed algorithm for computing the precise convex hull of freeform geometric models.", "Employing two pre-built data structures, a hierarchical Gauss map and a Coons bounding volume hierarchy, we develop an efficient culling technique that can eliminate the majority of redundant surface patches.", "Construct the precise convex hull boundary using numerical methods."], "abstract": ["We present an interactive-speed algorithm for computing the precise convex hull of freeform geometric models. The algorithm is based on two pre-built data structures: (i) a Gauss map organized in a hierarchy of normal pyramids and (ii) a Coons bounding volume hierarchy (CBVH) which effectively approximates freeform surfaces with a hierarchy of bilinear surfaces. For the axis direction of each normal pyramid, we sample a point on the convex hull boundary using the CBVH. The sampled points together with the hierarchy of normal pyramids serve as a hierarchical approximation of the convex hull, with which we can eliminate the majority of redundant surface patches. We compute the precise trimmed surface patches on the convex hull boundary using a numerical tracing technique and then stitch them together in a correct topology while filling the gaps with tritangent planes and bitangent developable scrolls. We demonstrate the effectiveness of our algorithm using experimental results."]},
{"title": "Modeling flow features with user-guided streamline parameterization", "highlights": ["We describe a streamline-based aesthetic feature creation method for product styling.", "Stroke-based editing handles mimic fluid streamlines.", "A fast linear method for field-guided parameterization is described.", "Aesthetic flow-like surface features can be generated with ease.", "Application results are shown in industrial product design examples."], "abstract": ["Streamline-like, free-form features that \u201cflow\u201d on a base shape are often utilized in the design of products ranging from automobiles to everyday consumer products. Providing computational support for the design of such features is challenging, because of the open-endedness of the design explorations involved, and the necessity to rapidly and precisely capture the design intents expressed in very simple forms, such as free-form sketches. We present a novel approach for designing streamline-based, free-form surface features in the context of product design. Using our approach, the user first designs a network of streamlines on the base shape, by performing a stroke-constrained mesh parameterization. Then, the user utilizes these streamlines as a curvilinear scaffold for creating 3D free-form features that are bounded and parameterized by these streamlines. The user is able to apply fine-grained control of the outline, profile and extent of the resulting 3D features by manipulating the streamlines. We demonstrate the capability of this approach on several product models."]},
{"title": "Longest-edge algorithms for size-optimal refinement of triangulations", "highlights": ["We improve geometrical results on longest-edge refinement algorithms.", "We provide new results on the refinement propagation of the Lepp-bisection algorithm.", "The iterative application of the algorithm improves the quality of the triangulation.", "We perform an empirical study of the algorithm and the behavior of the propagation.", "We also review mathematical properties of the iterative longest-edge bisection."], "abstract": ["Longest-edge refinement algorithms were designed to iteratively refine the mesh for finite-element applications by maintaining mesh quality (assuring a bound on the smallest angle). In this paper we improve geometrical results on longest-edge refinement algorithms and provide precise bounds on the refinement propagation. We prove that the iterative application of the algorithm gradually reduces the average extent of the propagation per target triangle, tending to affect only two triangles. We also include empirical results which are in complete agreement with the theory."]},
{"title": "Modeling piecewise helix curves from 2D sketches", "highlights": ["Our method reconstructs a piecewise helix curve from a 2D sketch.", "Helices are computed such that their projection matches the 2D curve.", "An optimization is performed to minimize the tangent discontinuity of the helices."], "abstract": ["We describe a method for a reconstructing piecewise helix curve from its 2D sketch. The system takes as input a hand-drawn polygonal curve and generates a piecewise helix curve such that its orthogonal projection matches the input curve. The first step is an algorithm to generate a set of helices such that their orthogonal projection approximates the input curve. This step is followed by a global optimization to minimize the tangent discontinuity of the junctions of the helices while keeping the fitting error small."]},
{"title": "A solution process for simulation-based multiobjective design optimization with an application in the paper industry", "highlights": ["The challenges of simulation-based multiobjective design optimization are analyzed.", "A three-stage solution process is proposed, featuring interactive decision making.", "Demonstrated by a case study of multiobjective design optimization of a paper mill.", "Applicable to computationally intensive black-box formulations of real-life problems."], "abstract": ["In this paper, we address some computational challenges arising in complex simulation-based design optimization problems. High computational cost, black-box formulation and stochasticity are some of the challenges related to optimization of design problems involving the simulation of complex mathematical models. Solving becomes even more challenging in case of multiple conflicting objectives that must be optimized simultaneously. In such cases, application of multiobjective optimization methods is necessary in order to gain an understanding of which design offers the best possible trade-off. We apply a three-stage solution process to meet the challenges mentioned above. As our case study, we consider the integrated design and control problem in paper mill design where the aim is to decrease the investment cost and enhance the quality of paper on the design level and, at the same time, guarantee the smooth performance of the production system on the operational level. In the first stage of the three-stage solution process, a set of solutions involving different trade-offs is generated with a method suited for computationally expensive multiobjective optimization problems using parallel computing. Then, based on the generated solutions an approximation method is applied to create a computationally inexpensive surrogate problem for the design problem and the surrogate problem is solved in the second stage with an interactive multiobjective optimization method. This stage involves a decision maker and her/his preferences to find the most preferred solution to the surrogate problem. In the third stage, the solution best corresponding that of stage two is found for the original problem."]},
{"title": "Linear algebraic representation for topological structures", "highlights": ["A proper mathematical model for all topological structures is a (co)chain complex.", "We propose a linear algebraic representation (LAR) scheme for representing such complexes.", "The LAR scheme is fully implemented using sparse matrices.", "The LAR scheme provides efficient support for topological queries and constructions."], "abstract": ["With increased complexity of geometric data, topological models play an increasingly important role beyond boundary representations, assemblies, finite elements, image processing, and other traditional modeling applications. While many graph- and index-based data structures have been proposed, no standard representation has emerged as of now. Furthermore, such representations typically do not deal with representations of mappings and functions and do not scale to support parallel processing, open source, and client-based architectures. We advocate that a proper mathematical model for all topological structures is a (co)chain complex: a sequence of (co)chain spaces and (co)boundary mappings. This in turn implies all topological structures may be represented by a collection of sparse matrices. We propose a Linear Algebraic Representation (LAR) scheme for mod 2 (co)chain complexes using CSR matrices and show that it supports a variety of topological computations using standard matrix algebra, without any overhead in space or running time. A full open source implementation of LAR is available and is being used for a variety of applications."]},
{"title": "Mechanical assembly planning using ant colony optimization", "highlights": ["We model an ant colony optimization based method for mechanical assembly planning.", "The computation framework couples both a solution generation and an optimization search.", "The proposed search strategy improves the performance of the assembly planning method."], "abstract": ["In mechanical assembly planning research, many intelligent methods have already been reported over the past two decades. However, those methods mainly focus on the optimal assembly solution search while another important problem, the generation of solution space, has received little attention. This paper proposes a new methodology for the assembly planning problem. On the basis of a disassembly information model which has been developed to represent all theoretical assembly/disassembly sequences, two decoupled problems, generating the solution space and searching for the best result, are integrated into one computation framework. In this framework, using an ant colony optimization algorithm, the solution space of disassembly plans can be generated synchronously during the search process for best solutions. Finally, the new method\u2019s validity is verified by a case study."]},
{"title": "Interpreting the semantics of GD&T specifications of a product for tolerance analysis", "highlights": ["STEP models are not capable for interpreting the product data semantics automatically.", "We discussed mapping of STEP models to OWL-based ontology.", "Product\u2019s GD&T information mapped from STEP to OWL-based models.", "Implementations have been carried out in Prot\u00e9g\u00e9 using NIST\u2019s OntoSTEP plug-in.", "Ontology based model is used to interpret GD&T semantics for tolerance analysis."], "abstract": ["The representation and management of product information in its life cycle require standardized data exchange protocols. ISO 10303, informally known as the Standard for Exchange of Product model data (STEP), is such a standard that has been used widely by the industries. The information language used for STEP is EXPRESS. Even though the EXPRESS language is capable of developing well defined and syntactically correct product models, the semantics of product data are represented implicitly. Hence, it is difficult to interpret the semantics of data for different product life cycle phases for different application domains. OntoSTEP, developed at NIST, provides semantically enriched product models in OWL. In this paper, we would like to present how to interpret the Geometric Dimensioning and Tolerancing (GD&T) specifications in STEP for tolerance analysis by utilizing OntoSTEP. This process requires (1) developing the tolerance-analysis-oriented information model in EXPRESS, (2) combining this model with the ISO 10303 product model, (3) translating the combined model into OWL and (4) defining semantic web rule language to map the GD&T specifications to the specifications needed for the tolerance analysis. This study will help users interpret the GD&T specifications of a product differently as required in different phases of the product\u2019s life cycle."]},
{"title": "Feature-aware partitions from the motorcycle graph", "highlights": ["A given quadrilateral mesh is segmented into quadrilateral partitions.", "As many as possible highly-curved regions are located on the partition boundaries.", "The generated partitions are suitable for parametric surface fitting.", "Partition boundaries are improved via local path flipping operations.", "Feature curves in the model are integrated into the proposed framework."], "abstract": ["Today\u2019s quad-meshing techniques generate high-quality quadrilateral meshes whose extraordinary vertices (i.e., not four-valence vertices except on the boundary) are generally located in highly curved regions. The motorcycle graph (MCG) algorithm of Eppstein et al. can be used to generate structured partitions of such quadrilateral meshes. However, it is not always possible for it to capture feature curves in the highly-curved parts of the model on the partition boundaries because model geometry is not taken into account.", "This study investigated feature-aware algorithms representing extensions of the MCG algorithm. Initial partitioning is first performed using a speed control algorithm identical to the MCG algorithm except that it assigns variable rather than constant speed to particles. Partition boundaries are then improved via local path flipping operations. The MCG algorithm and the speed control algorithm are intended to trace as many feature curves as possible, but do not necessarily trace all of them. For this reason, feature curves are extracted and integrated into the proposed framework by adding seeds located at ordinary vertices in addition to extraordinary seeds. The proposed algorithm generates partitions that are still structured, and has been tested with quadrilateral mesh models generated using the mixed integer quadrangulation technique of Bommes et al."]},
{"title": "The application of ubiquitous multimodal synchronous data capture in CAD", "highlights": ["A generic ubiquitous data capture framework is demonstrated via two case studies.", "Variety of inputs, interactions, biophysical data and design solutions are captured.", "Tight temporal synchronisation with commodity data logging tools is achieved.", "Demonstrates engineering knowledge capture linking CAD and PLM via generated metadata.", "The framework\u2019s use in future CAD and PLM systems is extrapolated."], "abstract": ["Design is an interactive and iterative process where the designer\u2019s skills and knowledge are fused with emotive rationales aided by design tools. A design solution is thus influenced by the designer\u2019s creativity, experience and emotional perception. Consequently, there is a need within computer aided design (CAD) research for ubiquitous tools to capture the affective states of engineers during design activities to further understand the product design process.", "This paper proposes a generic framework for ubiquitous multimodal synchronous data capture, based around the capture of CAD system activities, to monitor and log a variety of inputs, interactions, biophysical data and design solutions with a view to providing meta and chronological performance data for post design task analysis. The framework has been employed in two use cases namely, a CAD station activity and a collaborative design review. The results of these trials validated the architecture and use of the ubiquitous data capture approach demonstrating the practical application of time-phased data capture, analysis and the subsequent output of metadata in CAD environments providing a new perspective on, and a new way of investigating CAD-based design activities.", "This research also extrapolates the framework\u2019s usefulness into future CAD and PLM systems by arguing why and how they need to adopt such ubiquitous platforms. It also subjectively points to potential opportunities and issues that might arise when implementing the ubiquitous multimodal metadata architecture in a real-life environment."]},
{"title": "A fuzzy psycho-physiological approach to enable the understanding of an engineer\u2019s affect status during CAD activities", "highlights": ["To monitor the emotional changes of the participants as they carry out an engineering design task.", "To investigate and develop a repeatable methodology to capture these individual motions during CAD system operation.", "To validate the captured emotions using other methods.", "To determine if there is a correlation between an engineer\u2019s emotions and associated CAD tasks."], "abstract": ["Affective computing involves human\u2013computer interaction (HCI) where an interface can detect and respond in context to a user\u2019s emotions. Emotions play an essential role in the daily activities of a human during work and, as a consequence, can critically affect their decision-making. Recent research on affective computing opens the door to exploring the emotional aspects of computer aided tasks and HCI; however, there is little research in this area related to computer aided design (CAD). There is an established connection between emotional and cognitive processes and without these emotional markers decision-making in humans is almost non-existent.", "The holy grail of engineering design is for a concept to be \u2018right-first-time\u2019 and only by delving into the affect-cognitive domain will insights in terms of the associability of critical decision-making and the underlying intent of engineering decisions and tasks be better understood. Work in this field aims to recognize, interpret and process human emotions in order to support and adapt computer-based systems and their associated interfaces to user behaviors and to elicit appropriate system responses. This is a very contemporary area in engineering design research. One key aspect of provisioning CAD systems with the capability to detect the emotional state of the engineer is to lessen the chance of distorted reasoning as a result of irrational judgements. Work in this area can potentially identify emotional markers that influence the coherence/incoherence of reasoning against knowledge as well as the potential quality and suitability of a specific engineering solution or approach.", "This paper proposes and investigates a methodology to determine the emotional aspects attributed to a set of CAD design tasks by analyzing the CAD operators\u2019 psycho-physiological signals. Two sets of experiments were conducted. A pilot case study focused on modeling tasks carried out in Solid Edge\u2122\u00a0was run to prove the validity of the methodology, the tools applied and subsequent analysis. The main case study extended this to more wide-ranging user logging, incorporating a series of configuration and optimization tasks in Siemens NX\u2122\u00a0for a real design task. Psycho-physiological signals of electroencephalography (EEG) and galvanic skin resistance (GSR)/electrocardiography (ECG) were recorded along with a log of CAD system user interactions. A fuzzy logic model was established to map the psycho-physiological signals to a set of key emotions, namely frustration, satisfaction, engagement and challenge and the results analyzed.", "The methodology and subsequent analysis applied was found to be repeatable. Over the various task action-chains the elicitation and interpretation of each participant\u2019s emotions were successfully carried out with significant correlations demonstrated between the associated engineers\u2019 CAD activities and their reported emotional states."]},
{"title": "A physiological study of relationship between designer\u2019s mental effort and mental stress during conceptual design", "highlights": ["We propose a new approach to process EEG and HRV signals for design activities.", "We assume that an inverse U curve relationship exists between mental effort and mental stress.", "We found that designers\u2019 mental effort is the lowest at the highest mental stress.", "There is no significant difference in mental effort at low and medium stress levels."], "abstract": ["In the development of the next generation computer-aided design (CAD) systems, it is important to consider systematically the interactions between designers and design tools. As one of the first steps in quantifying such relations, we propose a method to investigate the relation between a designer\u2019s mental stress and mental effort. We hypothesize that mental effort is low at low and high stress levels but high at a medium stress level. To test the hypothesis, we conducted experiments on seven subjects. Design activities, body movements, brain signals and heart rate were recorded during a design process. Mental stress was quantified by LF/HF ratio and mental effort was quantified by EEG energy. The statistical analysis shows that mental effort is the lowest at high stress level and there is no significant difference in mental effort between medium stress level and low stress level."]},
{"title": "Iterative refinement of hierarchical ", "highlights": ["An iterative refinement strategy for generating hierarchical ", "-meshes is proposed.", "The required ordering of line-segments which form a ", "-mesh is specified.", "The construction of spline basis functions is obtained.", "The proposed algorithm includes standard hierarchical ", "-splines."], "abstract": ["In this paper we propose a strategy for generating consistent hierarchical ", "-meshes which allow local refinement and offer a way to obtain spline basis functions with highest order smoothness incrementally. We describe the required ordering of line-segments during refinement and the construction of spline basis functions. We give our strategy for generating consistent hierarchical ", "-meshes over any shape of a two-dimensional domain."]},
{"title": "Automation and optimisation of Family Mould Cavity and Runner Layout Design (FMCRLD) using genetic algorithms and mould layout design grammars", "highlights": ["The nature of family mould layout design is highly combinatorial and generative.", "Family mould layout design using Genetic Algorithms and Mould Layout Design Grammars is proposed.", "New fitness functions for family mould layout design optimisation are proposed.", "Group-oriented Mould Layout Design Grammar-based genetic operators and chromosome are proposed.", "We successfully automate and optimise family mould layout design by computer."], "abstract": ["Family Mould Cavity Runner Layout Design (FMCRLD) is the most demanding and critical task in the early Conceptual Mould Layout Design (CMLD) phase. Traditional experience-dependent manual FCMRLD workflow causes long design lead time, non-optimum designs and human errors. However, no previous research can support FMCRLD automation and optimisation. The nature of FMCRLD is non-repetitive and generative. The complexity of FMCRLD optimisation involves solving a complex two-level combinatorial layout design optimisation problem. Inspired by the theory of evolutionary design in nature \u201cSurvival of the Fittest\u201d and the biological genotype\u2013phenotype mapping process of the generation of form in living systems, this research first proposes an innovative evolutionary FMCRLD approach using Genetic Algorithms (GA) and Mould Layout Design Grammars (MLDG) that can automate and optimise such generative and complex FMCRLD with its explorative and generative design process embodied in a stochastic evolutionary search. Based on this approach, an Intelligent Conceptual Mould Layout Design System (ICMLDS) prototype has been developed. The ICMLDS is a powerful intelligent design system as well as an interactive design-training system that can encourage and accelerate mould designers\u2019 design alternative exploration, exploitation and optimisation for better design in less time. This research innovates the traditional manual FMCRLD workflow to eliminate costly human errors and boost the less-experienced mould designer\u2019s ability and productivity in performing FCMRLD during the CMLD phase."]},
{"title": "A high capacity reversible data hiding method for 2D vector maps based on ", "highlights": ["We develop a high capacity reversible data hiding method for 2D vector maps.", " coordinates are exploited to expand the capacity.", "The data capacity achieves nearly ", " bits/vertex.", "The reversibility, invisibility, and computational complexity are good.", "For polylines and polygons, content preserving operations can be resisted."], "abstract": ["In this paper, we propose a high capacity reversible data hiding method for 2D vector maps based on ", " coordinates. In the scheme, we calculate two ", " coordinates for each ", " coordinate, and embed ", " (", ") secret bits by modifying the state value of the interval which is created by the two corresponding ", " coordinates. Since nearly every coordinate can carry ", " bits, the data capacity in bits achieves nearly ", " times the number of vertices in the vector map. In addition, to resist content preserving operations, the vertex traversing order is defined. Experimental results and analysis show that the proposed method provides good reversibility, invisibility and computational complexity and is robust against the feature rearrangement and vertex reversing attacks for polylines and polygons."]},
{"title": "Defining tools to address over-constrained geometric problems in Computer Aided Design", "highlights": ["Support for analyzing over-constraint sketches in CAD.", "Use of non-cartesian geometric modeling.", "Tool for decision making in constraint choice.", "Methodology to solve over-constraint problems.", "Possible application in collaborative environment."], "abstract": ["This paper proposes a new tool for decision support to address geometric over-constrained problems in Computer Aided Design (CAD). It concerns the declarative modeling of geometrical problems. The core of the coordinate free solver used to solve the Geometric Constraint Satisfaction Problem (GCSP) was developed previously by the authors. This research proposes a methodology based on Michelucci\u2019s witness method to determine whether the structure of the problem is over-constrained. In this case, the authors propose a tool for assisting the designer in solving the over-constrained problem by ensuring the consistency of the specifications. An application of the methodology and tool is presented in an academic example."]},
{"title": "A multi-threaded algorithm for computing the largest non-colliding moving geometry", "highlights": ["Introduce a novel algorithm to compute largest volume to pass along a path.", "Introduce also a multi-threaded version of the algorithm.", "Novel use of the existing data structures that scale linearly with the problem size.", "Abstracts away the geometry so it can be used with any combination of geometries."], "abstract": ["In this article we present an algorithm to compute the maximum size of an object, in three dimensions, that can move collision-free along a fixed trajectory through a virtual environment. This can be seen as a restricted version of the general problem of computing the maximum size of an object to move collision-free from a start position to a goal position. We compute the maximum size by dividing the object into numerous small boxes and computing which ones collide with the virtual environment during the movement along the given trajectory. The algorithm presented is optimized for multi-threaded computer architectures and also uses data structures that leave a small memory footprint making it suitable for use with large virtual environments (defined by, e.g., millions or billions of points or triangles)."]},
{"title": "Constructing a meta-model for assembly tolerance types with a description logic based approach", "highlights": ["Constraint relations are formalized by assertional axioms.", "Assembly tolerance types are defined with terminological axioms.", "Assembly tolerance types are generated automatically in the meta-model.", "The meta-model has rigorous logic-based semantics.", "The meta-model lays a good foundation for realizing semantic interoperability."], "abstract": ["There is a critical requirement for semantic interoperability among heterogeneous computer-aided tolerancing (CAT) systems with the sustainable growing demand of collaborative product design. But current data exchange standard for exchanging tolerance information among these systems can only exchange syntaxes and cannot exchange semantics. Semantic interoperability among heterogeneous CAT systems is difficult to be implemented only with this standard. To address this problem, some meta-models of tolerance information supporting semantic interoperability and an interoperability platform based on these meta-models should be constructed and developed, respectively. This paper mainly focuses on the construction of a meta-model for assembly tolerance types with a description logic ", " based approach. Description logics, a family of knowledge representation languages for authoring ontologies, are well-known for having rigorous logic-based semantics which supports semantic interoperability. ", " can provide a formal method to describe the research objects and the relations among them. In this formal method, constraint relations among parts, assembly feature surfaces and geometrical features are defined with some ", " assertional axioms, and the meta-model of assembly tolerance types is constructed through describing the spatial relations between geometrical features with some ", " terminological axioms. Besides, ", " can also provide a highly efficient reasoning algorithm to automatically detect the inconsistency of the knowledge base, a finite set of assertional and terminological axioms. With this reasoning algorithm, assembly tolerance types for each pair of geometrical features are generated automatically through detecting the inconsistencies of the knowledge base. An application example is provided to illustrate the process of generating assembly tolerance types."]},
{"title": "A variational-difference numerical method for designing progressive-addition lenses", "highlights": ["We propose a variational-difference numerical method for designing progressive-addition lenses.", "The method can be very easily understood and implemented by optical engineers.", "The method can provide satisfactory designs for optical engineers in several seconds.", "The method can be a powerful candidate tool for designing various free-form lenses."], "abstract": ["We propose a variational-difference method for designing the optical free form surface of progressive-addition lenses (PALs). The PAL, which has a front surface with three important zones including the far-view, near-view and intermediate zones, is often used to remedy presbyopia by distributing optical powers of the three zones progressively and smoothly. The problem for designing PALs could be viewed as a functional minimization problem. Compared with the existing literature which solved the problem by the B-spline finite element method, the essence of the proposed variational-difference numerical method lies in minimizing the functional directly by finite difference method and/or numerical quadratures rather than in approximating the solution of the corresponding Euler\u2013Lagrange equation to the functional. It is very easily understood and implemented by optical engineers, and the numerical results indicate that it can produce satisfactory designs for optical engineers in several seconds. We believe that our method can be a powerful candidate tool for designing various specifications of PALs."]},
{"title": "A generalized surface subdivision scheme of arbitrary order with a tension parameter", "highlights": ["Presents a generalized ", "-spline surface subdivision with arbitrary order ", ".", "The scheme is conveniently implemented using local operations.", "It produces ", " limit surfaces with ", " at extraordinary vertices.", "It can define exact analytic, sweeping and revolution surfaces.", "New rules for sharp and semi-sharp features are embedded in local operations."], "abstract": ["This article presents a generalized ", "-spline surface subdivision scheme of arbitrary order with a tension parameter. We first propose a tensor-product subdivision scheme that produces ", " order generalized ", "-spline limit surfaces. Generalized ", "-spline surface is the unified and extended form of ", "-splines, trigonometric ", "-splines and hyperbolic ", "-splines (Fang et\u00a0al. 2010). The tensor product subdivision scheme can be used to generate various surfaces of revolution, including those generated by classical analytic curves that can be exactly represented by generalized ", "-spline curves. By extending a bi-order (say ", ") tensor-product scheme to meshes of arbitrary topology, we further propose a generalized surface subdivision scheme with a tension parameter. Several well-known subdivision schemes, including Doo\u2013Sabin subdivision, Catmull\u2013Clark subdivision, and two other subdivision schemes proposed by Morin et\u00a0al. (2001) and Stam (2001), become special cases of the generalized subdivision scheme. The tension parameter can be used to adjust the shape of subdivision surfaces. The scheme produces higher order ", " continuous limit surfaces except at extraordinary points where the continuity is ", ". Convenient and hierarchical methods are also presented for embedding sharp features and semi-sharp features on the resulting limit surfaces."]},
{"title": "Compensation of geometrical errors of CAM/CNC machined parts by means of 3D workpiece model adaptation", "highlights": ["Geometrical inaccuracies are reduced by adaption of the 3D workpiece model.", "3D model adaption is analogous to a truss structure with compliant constraints.", "There is reduced criticality of the finishing pass.", "There is automatic regeneration of tool trajectories using commercial CAM.", "The approach is compatible with proprietary and access-restricted numeric controls."], "abstract": ["In modern industry conditions, it is very important to develop methodologies for reducing costs and achieving the maximum quality of machined parts, especially considering the dimensional accuracy of workpieces. Geometrical and dimensional inaccuracies are due to several factors, such as workpiece and tool deformation during machining, thermal distortions, tool wear, and machine tool inaccuracy. There are two main approaches used to improve the accuracy of workpieces: mapping the tool\u2013workpiece displacement and altering the finishing tool path or the interpolated tool position to compensate the dimensional errors. The aim of this paper is to propose a new compensation approach, based on adaptation of the geometrical 3D CAD model used to generate trajectories by CAM software. The concept is to produce a first workpiece using a CAM-generated tool path. Then, the workpiece is measured using optical methods and the displacements between the ideal workpiece model and the measured point-cloud are calculated. Eventually, the displacement vectors are applied to calculate a compensated workpiece model. Such model is then used as a reference by CAM software to calculate the compensated tool path, which is applied for production of subsequent workpieces. The mathematical background and implementation details are given together with an example of application to a benchmark workpiece purposely machined with inaccurate tools. As the results show, the new approach was able to compensate the geometrical inaccuracies of the benchmark workpiece."]},
{"title": "Modeling 3D garments by examples", "highlights": ["Designing 3D garments by directly compositing 3D parts from existing examples.", "Decoupling the high coupling between garment design and pattern making.", "The first attempt for examples-based 3D garment modeling."], "abstract": ["3D garments are created by assembling 2D patterns on human models in current mainstream garment modeling methods, which usually calls for professional design skills. The high coupling between the 2D pattern making and 3D garment design blocks the designers\u2019 creations. In this paper, a novel approach that decouples the high coupling between garment design and pattern making is proposed. Our approach is examples based. 3D new garment models are created on individual human models by compositing 3D parts from garment examples rather than 2D patterns. The main benefit of our approach is that it is easy to operate and capable to create elegant individualized garments with highly detailed geometries. Garment prototypes represented in Coons surface automatically generated from individual human models are used to guide the garment parts composition and locally parameterize the garment parts. With the local parameterization, the garment parts and the newly created garment models can be interactively edited by altering the parametric curves on the garment prototypes. Our approach makes it possible for both the designers and the common customers to express their creations and imaginations conveniently in a 3D graphical way."]},
{"title": "Material driven design for a chocolate pavilion", "highlights": ["Determination of structural properties of compound chocolate.", "Physical exploration of material appropriate structural systems for chocolate.", "Parametrically integrated design-to-construction process for free-form chocolate shell structures.", "Integration of material specific structural, manufacturing, and construction constraints into the design process."], "abstract": ["This paper presents a study of chocolate\u2019s structurally unusual material properties and a parametric design-to-construction approach for an architectural chocolate pavilion. Chocolate\u2019s rheological properties suggested exploration of four structural typologies: a pneumatic form, an inverted branching form, a saddle form, and an inverted hanging cloth form. Material tests revealed a compressive strength/weight ratio 24 times smaller than standard concrete. To use unreinforced chocolate, this restriction dictated a form with minimal bending: an inverted hanging shell with voids. An integrated form-finding, void-optimization and mold layout process was employed to minimize self-weight. Pre-casting planar pieces allowed for best control of material quality but added further design constraints. Prototypes demonstrated how the parametric workflow allows design exploration driven by adjustable material constraints, further integrating design and construction into an interdependent process."]},
{"title": "Reversible watermarking for 2D CAD engineering graphics based on improved histogram shifting", "highlights": ["Difference histogram shifting is used for reversible watermarking of 2D CAD graphics.", "Difference histograms are constructed by coordinate values and phase values.", "Two reversible watermarking schemes are put forward for 2D CAD graphics.", "HS_C can obtain a good capacity, while HS_P can achieve a good robustness."], "abstract": ["According to the characteristics of the coordinates and phases in 2D CAD engineering graphics, difference histograms are constructed. Consequently, two reversible watermarking schemes based on difference histogram shifting are put forward. The first scheme HS_C implements the integer part of coordinate values to construct the adjacent difference histogram and hides messages in the peak and zero point pairs. The scheme can achieve high capacity and good invisibility in the graphics with high correlated coordinates. Instead of coordinates, the second scheme HS_P adopts the integer part of the phase value as the cover data, and the watermark is embedded by shifting and modifying the difference histogram of the phase. It is robust against translation, rotation and even scaling. Experimental results show that both schemes are strictly reversible. The proposed schemes have great potential to be applied for content authentication or secret communication of 2D CAD engineering graphics."]},
{"title": "Skin Model Shapes: A new paradigm shift for geometric variations modelling in mechanical engineering", "highlights": ["We explain the core concept and fundamentals of Skin Model Shapes.", "Skin Model Shapes are Skin Model representatives from a simulation perspective.", "Approaches for the Skin Model Shape generation in discrete geometry are highlighted.", "Overview over applications for Skin Model Shapes in mechanical engineering is given."], "abstract": ["Geometric deviations are inevitably observable on manufactured workpieces and have huge influences on the quality and function of mechanical products. Therefore, many activities in geometric variations management have to be performed to ensure the product function despite the presence of these deviations. Dimensional and Geometrical Product Specification and Verification (GPS) are standards for the description of workpieces. Their lately revision grounds on GeoSpelling, which is a univocal language for geometric product specification and verification and aims at providing a common understanding of geometric specifications in design, manufacturing, and inspection. The Skin Model concept is a basic concept within GeoSpelling and is an abstract model of the physical interface between a workpiece and its environment. In contrast to this understanding, established models for computer-aided modelling and engineering simulations make severe assumptions about the workpiece surface. Therefore, this paper deals with operationalizing the Skin Model concept in discrete geometry for the use in geometric variations management. For this purpose, Skin Model Shapes, which are particular Skin Model representatives from a simulation perspective, are generated. In this regard, a Skin Model Shape is a specific outcome of the conceptual Skin Model and comprises deviations from manufacturing and assembly. The process for generating Skin Model Shapes is split into a prediction and an observation stage with respect to the available information and knowledge about expected geometric deviations. Moreover, applications for these Skin Model Shapes in the context of mechanical engineering are given."]},
{"title": "A mathematical model for simulating and manufacturing ball end mill", "highlights": ["A new mathematical model for grinding the ball end mill is proposed.", "The rake face of ball end mill is modeled as a developable surface.", "The conditions of engagement between wheel and the rake face are established.", "The configuration of the flute surface was directly computed."], "abstract": ["The performance of ball end mill cutters in cutting operations is influenced by the configuration of the rake and clearance faces in the ball component. From the mathematical design of a cutting edge curve, the rake face can be defined by the rake angle and the width of the rake face at each cross section along the cutting edge. We propose the fundamental conditions that must govern the engagement between the grinding wheel and the designed rake face in order to avoid interference while machining a ball end mill. As a result, a new mathematical model for determining the wheel location and a software program for simulating the generation of the rake face of a ball end mill are proposed. In addition, methods for grinding the clearance face in both concave and flat-shapes are introduced. The flute surface generated by a disk wheel during the grinding process is determined on the basis of a tangency condition. The results of the experiment and the simulation are compared to validate the proposed model."]},
{"title": "Robust reconstruction of 2D curves from scattered noisy point data", "highlights": ["We propose a robust 2D reconstruction method from unorganized noisy point data.", "The outliers and noise of data can be effectively detected and smoothed.", "Sharp corners are preserved properly in the output curves with our method."], "abstract": ["In this paper, a robust algorithm is proposed for reconstructing 2D curve from unorganized point data with a high level of noise and outliers. By constructing the quadtree of the input point data, we extract the \u201cgrid-like\u201d boundaries of the quadtree, and smooth the boundaries using a modified Laplacian method. The skeleton of the smoothed boundaries is computed and thereby the initial curve is generated by circular neighboring projection. Subsequently, a normal-based processing method is applied to the initial curve to smooth jagged features at low curvatures areas, and recover sharp features at high curvature areas. As a result, the curve is reconstructed accurately with small details and sharp features well preserved. A variety of experimental results demonstrate the effectiveness and robustness of our method."]},
{"title": "Optimal multi-degree reduction of B\u00e9zier curves with geometric constraints", "highlights": ["Optimal degree reduction of B\u00e9zier curves with various geometric constraints is presented.", "The degree-reduced curves are explicitly derived with some geometric constraints.", "With ", "-continuity at two endpoints, our method is optimal and efficient."], "abstract": ["In this paper we present a novel algorithm for the multi-degree reduction of B\u00e9zier curves with geometric constraints. Based on the given constraints, we construct an objective function which is abstracted from the approximation error in ", "-norm. Two types of geometric constraints are tackled. With the constraints of ", "-continuity at one endpoint and ", "-continuity (or ", "-continuity) at the other endpoint, we derive the optimal degree-reduced curves in explicit form. With the constraints of ", "-continuity at two endpoints, the problem of degree reduction is equivalent to minimizing a bivariate polynomial function of degree 4. Compared with the traditional methods, we derive the optimal degree-reduced curves more effectively. Finally, evaluation results demonstrate the effectiveness of our method."]},
{"title": "Interactive design exploration for constrained meshes", "highlights": ["A general optimization framework for deforming meshes under constraints.", "Soft constraints and hard constraints are handled in a unified way.", "An efficient parallel solver suitable for interactive applications.", "A system for exploring the feasible shapes of constrained meshes in real time."], "abstract": ["In architectural design, surface shapes are commonly subject to geometric constraints imposed by material, fabrication or assembly. Rationalization algorithms can convert a freeform design into a form feasible for production, but often require design modifications that might not comply with the design intent. In addition, they only offer limited support for exploring alternative feasible shapes, due to the high complexity of the optimization algorithm.", "We address these shortcomings and present a computational framework for interactive shape exploration of discrete geometric structures in the context of freeform architectural design. Our method is formulated as a mesh optimization subject to shape constraints. Our formulation can enforce soft constraints and hard constraints at the same time, and handles equality constraints and inequality constraints in a unified way. We propose a novel numerical solver that splits the optimization into a sequence of simple subproblems that can be solved efficiently and accurately.", "Based on this algorithm, we develop a system that allows the user to explore designs satisfying geometric constraints. Our system offers full control over the exploration process, by providing direct access to the specification of the design space. At the same time, the complexity of the underlying optimization is hidden from the user, who communicates with the system through intuitive interfaces."]},
{"title": "A methodology for transferring principles of plant movements to elastic systems in architecture", "highlights": ["Plant movements.", "Kinetic structures.", "Biomimetics.", "Facade shading.", "Compliant mechanisms."], "abstract": ["In architecture, kinetic structures enable buildings to react specifically to internal and external stimuli through spatial adjustments. These mechanical devices come in all shapes and sizes and are traditionally conceptualized as uniform and compatible modules. Typically, these systems gain their adjustability by connecting rigid elements with highly strained hinges. Though this construction principle may be generally beneficial, for architectural applications that increasingly demand custom-made solutions, it has some major drawbacks. Adaptation to irregular geometries, for example, can only be achieved with additional mechanical complexity, which makes these devices often very expensive, prone to failure, and maintenance-intensive.", "Searching for a promising alternative to the still persisting paradigm of rigid-body mechanics, the authors found inspiration in flexible and elastic plant movements. In this paper, they will showcase how today\u2019s computational modeling and simulation techniques can help to reveal motion principles in plants and to integrate the underlying mechanisms in flexible kinetic structures. By using three case studies, the authors will present key motion principles and discuss their scaling, distortion, and optimization. Finally, the acquired knowledge on bio-inspired kinetic structures will be applied to a representative application in architecture, in this case as flexible shading devices for double curved facades."]},
{"title": "Human factors study on the usage of BCI headset for 3D CAD modeling", "highlights": ["We present a novel BCI based user interface for conceptual 3D modeling.", "The BCI performs CAD functions such as 3D shape creation and manipulation.", "Video abstract: ", ".", "A human factors study to assess intuitiveness of BCI in 3D modeling is presented."], "abstract": ["Since its inception, computer aided 3D modeling has primarily relied on the Windows, Icons, Menus, Pointer (WIMP) interface in which user input is in the form of keystrokes and pointer movements. The brain\u2013computer interface (BCI) is a novel modality that uses the brain signals of a user to enable natural and intuitive interaction with an external device. In this paper we present a human factors study on the use of an Emotiv EEG BCI headset for 3D CAD modeling. The study focuses on substituting the conventional computer mouse- and keyboard-based inputs with inputs from the Emotiv EEG headset. The main steps include (1) training the headset to recognize user-specific EEG/EMG signals and (2) assigning the classified signals to emulate keystrokes which are used to activate/control different commands of a CAD package. To assess the performance of the new system, we compared the time taken by the users to create the 3D CAD models using both the conventional and BCI-based interfaces. In addition, to exhibit the adaptability of the new system, we carried out the study for a set of CAD models of varying complexity."]},
{"title": "Obtaining a spiral path for machining STL surfaces using non-deterministic techniques and spherical tool", "highlights": ["Machining STL surfaces, without a volume inside, is useful and it can be optimized.", "To optimize this process we use the piece contour, defining 2D spiral paths.", "With non-deterministic techniques, the third coordinate of the tool-path is obtained.", "This method was simulated and applied to a workpiece, with positive results.", "Its precision was tested using confocal microscopy and coordinate-measuring machine."], "abstract": ["For milling an arbitrary surface, several models exist in which the tool must be moved close and away from the workpiece with its consequential problems caused by the contact between the tool and the mechanized material. One of these problems is the undesirable mechanizing marks on the surface, which can be avoided with the use of tangential tool-paths, causing another disadvantage: the additional time-consumption. Spiral tool-paths eliminate these problems and allow for continuous machining of the part without approach or withdrawal.", "Deterministic models for obtaining the tool positions referred to the part need to apply complex mathematical operations. For this reason, the idea of using non-deterministic techniques, with a controlled tolerance margin, allows to avoid those mathematical and geometrical operations. It also makes possible to reduce the processing times with no loss of dimensional quality and a good superficial finishing, which was confirmed with a coordinate measuring machine and with confocal microscopy, respectively.", "Finally, the effectiveness of the described method was assessed comparing its results with the performance of three of the most relevant CAM software commercially available. The obtained results are shown at the end of this paper."]},
{"title": "Ubiquitous conceptual design of a ubiquitous application: A textile SME case study for real time manufacturing monitoring", "highlights": ["An index to evaluate the ubiquitous level of product-service systems was proposed.", "A framework integrating methods and tools supported the conceptual design stage.", "Hardware\u2013Software were integrated through a Ubiquitous Design Support Environment.", "An industrial implementation allowed the evaluation of a Ubiquitous service."], "abstract": ["Advances in Information and Communication Technologies (ICT), computing, networking, mechanics and electronics are changing the people\u2019s way of life. Several research efforts are leading the design and development of Artifact and Service Combination (ASC) with the implementation of Ubiquitous Technologies (UTs) in multidisciplinary sectors. However, the design process of such systems often ends in the implementation of conventional approaches and tools. A Ubiquitous Design Support Environment (UDSE) comprising an application intended to guide the different activities, tools and resources applied at the conceptual design stage is presented. After needs analysis, multidisciplinary collaborations are also required in order to generate innovative conceptual solutions, focusing this approach in the conceptual design stage of traditional design methods. Some activities from the conceptual design stage are enhanced through the use of the UDSE as well as through the use of a novel ubiquity assessment tool for concept selection and validation of Ubiquitous Products and Services. Finally, a case study on a Small and Medium Enterprise (SME) from textile sector, in a developing country, is presented to analyze and validate the presented concepts."]},
{"title": "The use of a particle method for the modelling of isotropic membrane stress for the form finding of shell structures", "highlights": ["Derivation of equilibrium equations for loaded shell structures with a varying isotropic stress.", "The application of the finite element method for the numerical implementation.", "The introduction of a new particle method for the numerical implementation."], "abstract": ["The best known isotropic membrane stress state is a soap film. However, if we allow the value of the isotropic stress to vary from point to point then the surface can carry gravity loads, either as a hanging form in tension, or as a masonry shell in compression. The paper describes the theory of isotropic membrane stress under gravity load and introduces a particle method for its numerical simulation for the form finding of shell structures."]},
{"title": "Coordinate-free geometry and decomposition in geometrical constraint solving", "highlights": ["We solve 2D/3D geometric constraints by using coordinate-free formulation.", "Statements are translated into formalism that reduces the number of equations.", "A decomposition algorithm is proposed to produce small subsystems.", "Subsystems are solved by homotopy."], "abstract": ["In CAD, a designer usually specifies mechanisms or objects by the means of sketches supporting dimension requirements like distances between points, angles between lines, and so on. This kind of geometric constraint satisfaction problems presents two aspects which solvers have to deal with: first, the sketches can contain hundreds of constraints, and, second, the problems are invariant by rigid body motions. Concerning the first issue, several decomposition methods have been designed taking invariance into account by fixing/relaxing coordinate systems. On the other hand, some researchers have proposed to use distance geometry in order to exploit invariance by rigid body motions. This paper describes a method that allows us to use distance geometry and decomposition in the same framework."]},
{"title": "Efficient decomposition of line drawings of connected manifolds without face identification", "highlights": ["Decomposes a complex drawing into its constituting simple manifolds.", "Does not require face information to achieve the task.", "Basic two stage strategy: basic decomposition and subsequent repair.", "Reduces the time for subsequent face finding and 3D reconstruction greatly."], "abstract": ["This paper presents an algorithm for decomposing complex line drawings which depict connected 3D manifolds into multiple simpler drawings of individual manifolds. The decomposition process has three stages: decomposition at non-manifold vertices, along non-manifold edges and across internal faces. Once non-manifold vertices and/or edges are found, the decomposition can be performed straightforwardly. Thus the major task in this paper is decomposition across internal faces. This has two steps: basic decomposition and repair of incomplete parts. The decomposition process is performed before face identification which is computationally expensive. After decomposition, the time for face finding is much reduced and this, in turn, greatly improves the process for 3D reconstruction, which is the ultimate goal."]},
{"title": "Development of a digital framework for the computation of complex material and morphological behavior of biological and technological systems", "highlights": ["A physical/digital framework is developed for form- and bending-active architectures.", "Banana leaf stalks (petioles) are researched for their robust structural capacities.", "Spring-based software, called ", ", is utilized to simulate a variety of material behaviors.", "The extensible and calibrated framework fosters enhanced biomimetic insights."], "abstract": ["Research in material behavior involves the study of relationships between material composition and capacities to negotiate internal and external pressures. Tuning material composition for performance allows for the integration of multifaceted functionality and embedded responsiveness within minimal material means. The relationships of material composition and system performance can be dissected into properties of topology (in count, type and association), forces (as the simulation of contextual ", "), and materiality (material properties and constraints of fabrication). When resourcing information about these aspects of material behavior from biological or technological systems, the physical precedents, as specimens and/or models, serve as the primary, and often sole, exemplar. While this is necessary to initiate the study of material make-up as it relates to specific morphological performance, there is an inherent limit when asking how and to what degree the knowledge resourced from that instance applies when alterations from the norm are generated. This research proposes the possibility for testing variants of a morphological system using physical models as the precedent while incorporating multiple means of computational analysis for extensive exploration. The framework begins with the initial stage of deducing principles, regarding material organization and behavior, through comparative physical and computational study. Subsequently, through methods of abduction, new vocabularies of form and potentials in performance are generated primarily through computational exploration.", "The framework is shaped by research into the design and materialization of complex pre-stressed form- and bending-active architectures. A novel aspect of this framework is the development of a software environment called ", ". In this environment, material behavior is simulated using basic spring-based (particle system) methods. The novel contribution of this software is in providing means for both manual and algorithmic manipulations of mesh topologies and material properties ", " the form-finding process. A series of architectural prototypes, which range in scale, define rules for the relationship between topological-material complexity and the sequencing of particular exploratory methods. The studies define the value of the physical precedent as it engenders further material prototypes, spring-based explorations and simulations with finite element analysis. These rules and methods are further elaborated upon through studying the particularly fascinating structural capacity of banana leaf stalks, a material system which is stiff in bending yet highly flexible in torsion. Of interest is a functional robustness which allows for the negotiation of both self-weight and wind loading for a large and fully integrated leaf structure. Methods of simulation and meta-heuristics are developed to address the continual material and topological differentiation of the banana leaf stalk. Case studies are based upon examination of specimens from the species ", " and ", " Mechanical properties and geometric descriptions of isolated moments within the stalk provide the basis for computational comparison. Fundamental properties and behaviors are extracted from the plant specimens, yet a full description is not possible because of the plant\u2019s intricate spatial structure. In this case, the computational means serve to elucidate upon the behavior of the complete system as well as provide avenues for exploring its variants. This paper describes an extensible and calibrated framework which can foster enhanced biomimetic insights by explorations which are based upon but extend well beyond initial biological and/or technological precedents."]},
{"title": "Interactive real-time physics", "highlights": ["Unified physics engine for both static\u2013dynamic load analysis, structural optimization and form finding.", "The implementation combines dynamic relaxation and co-rotational formulation.", "Interactive structural analysis as a teaching and design aid.", "Interactive structural form-finding."], "abstract": ["Real-time physics simulation has been extensively used in computer games, but its potential has yet to be fully realized in design and education. We present an interactive 3D physics engine with a wide variety of applications.", "In common with traditional FEM, the use of a local element stiffness matrix is retained. However, unlike typical non-linear FEM routines elements forces, moments and inertia are appropriately lumped at nodes following the dynamic relaxation method. A semi-implicit time integration scheme updates linear and angular momentum, and subsequently the local coordinate frames of the nodes. A co-rotational approach is used to compute the resultant field of displacements in global coordinates including the effect of large deformations. The results obtained compare well against established commercial software.", "We demonstrate that the method presented allows the making of interactive structural models that can be used in teaching to develop an intuitive understanding of structural behaviour. We also show that the same interactive physics framework allows real-time optimization that can be used for geometric and structural design applications."]},
{"title": "Function-based morphing methodology for parameterizing patient-specific models of human proximal femurs", "highlights": ["Function-based morphing (FBM) method for patient-specific proximal femur was proposed.", "FBM morphs the patient-specific model based on the biomechanical functions.", "Detailed algorithms for a robust morphing are presented in this paper.", "FBM provides a systematical way to analyze biomechanical responses of a specific bone."], "abstract": ["This paper presents a novel morphing method for parameterizing patient-specific femur models based on femoral biomechanical functions. The proposed function-based morphing (FBM) method aims to provide a robust way to independently morph each partial functional region of the target femur structure by simply assigning the given functional parameters such as the femoral head diameter, neck length and diameter, and neck inclination angle. FBM includes three steps: (1) feature recognition to segment a femoral model into functional regions, (2) simplification to estimate the original parameters of the model and to define the morphing criteria as geometrical constraints, and (3) morphing to obtain the required shape by applying FBM fields that convert the terms of parametric changes into morphing vector terms for each segmented region. The proposed method was validated on a total of 48 patient-specific femur models. These models were parameterized and morphed without unexpected parametric changes, and the averaged error between the required parameters and the re-estimated parameters after morphing was 3.47%. Our observations indicate that the variation models developed in this study can be used as fundamentals for various functional sensitivity analyses for predicting changes in biomechanical responses due to the morphological changes of a subject-specific femur structure."]},
{"title": "Tool-adaptive offset paths on triangular mesh workpiece surfaces", "highlights": ["We present a method for automatic tool-adaptive path planning for freeform surfaces.", "The method is based on an implicit (level set) path representation.", "The method is worked out for freeform workpieces represented by triangular meshes.", "We demonstrate the usefulness of this approach for milling and spray coating."], "abstract": ["Path-oriented, computer-controlled manufacturing systems work by moving a tool along a path in order to affect a workpiece. A common approach to the construction of a surface-covering path is to take a finite family of offset curves of a given seed curve with increasing offsets. This results in a set of quasi-parallel curves. The offset is chosen so that a tool moving along the curves has the desired impact at every surface point. In cases where the region of influence of a tool is different across the surface, an offset value necessary in one region may lead to a curve offset lower than required in other regions. The paper presents a general method of offset curve construction with tool-adaptive offsets. The offset path is obtained as a family of iso-curves of an anisotropic distance function of a seed curve on the workpiece surface. Anisotropy is defined by a metric tensor field on the surface. An application-independent algorithmic framework of the method for workpiece surfaces represented by a triangular mesh is presented. Its usefulness is demonstrated on the problem of varying cusp heights for milling and for spray coating of surfaces with a spray gun moved by an industrial robot."]},
{"title": "Fibrous structures: An integrative approach to design computation, simulation and fabrication for lightweight, glass and carbon fibre composite structures in architecture based on biomimetic design principles", "highlights": ["We designed and fabricated a full-scale architectural pavilion as a fibre-reinforced polymer monocoque.", "We developed three digital models to design, simulate, analyse and optimize geometric solutions.", "We transferred biomimetic principles into the fibre-reinforced polymer laminate design."], "abstract": ["In this paper the authors present research into an integrative computational design methodology for the design and robotic implementation of fibre-composite systems. The proposed approach is based on the concurrent and reciprocal integration of biological analysis, material design, structural analysis, and the constraints of robotic filament winding within a coherent computational design process. A particular focus is set on the development of specific tools and solvers for the generation, simulation and optimization of the fibre layout and their feedback into the global morphology of the system. The methodology demonstrates how fibre reinforced composites can be arranged and processed in order to meet the specific requirements of architectural design and building construction. This was further tested through the design and fabrication of a full-scale architectural prototype."]},
{"title": "Decomposition of geometric constraint graphs based on computing fundamental circuits. Correctness and complexity", "highlights": ["A new algorithm to solve the 2D geometric constraint problem is described.", "The graph is decomposed according to a set of fundamental circuits.", "Fundamental circuits are induced by a spanning tree.", "We prove the algorithm soundness.", "The worst running time is quadratic with the number of geometric elements."], "abstract": ["In geometric constraint solving, Decomposition\u2013Recombination solvers (DR-solvers) refer to a general solving approach where the problem is divided into a set of sub-problems, each sub-problem is recursively divided until reaching basic problems which are solved by a dedicated equational solver. Then the solution to the starting problem is computed by merging the solutions to the sub-problems.", "Triangle- or tree-decomposition is one of the most widely used approaches in the decomposition step in DR-solvers. It may be seen as decomposing a graph into three subgraphs such that subgraphs pairwise share one graph vertex. Shared vertices are called ", ". Then a merging step places the geometry in each sub-problem with respect to the other two.", "In this work we report on a new algorithm to decompose biconnected geometric constraint graphs by searching for hinges in fundamental circuits of a specific planar embedding of the constraint graph. We prove that the algorithm is correct."]},
{"title": "Multi-dimensional dynamic programming in ruled surface fitting", "highlights": ["A multi-dimensional dynamic programming based ruled surface fitting scheme to a freeform rational surface.", "Providing solutions for the ", " (RSF) problem and the ", " (RSP) problem in the discrete domain.", "Error-bound can be achieved by using the surface\u2013surface composition technique.", "Highly parallel algorithms running on GPUs are employed to evaluate the multi-dimensional dynamic programming."], "abstract": ["Ruled surfaces play an important role in many manufacturing and construction applications. In this work, we explore a multi-dimensional dynamic programming based ruled surface fitting scheme to a given freeform rational surface, ", ". Considering two initial opposite boundaries of ", ", sampled into a discrete piecewise linear polyline representation, the ruled surface fitting problem is reduced to a pairing-search between the polylines and elevations above the polylines, in the normal directions of ", ". ", " solution is sought for the four dimensions prescribed by the two polylines and the two elevation levels along the surface normals. This multi-dimensional dynamic programming is evaluated using highly parallel algorithms running on GPUs that ensures the best fit to the sampled data. In order to evaluate the fitting error with respect to ", ", we derive a scheme to compute a bound from above on the maximal error between a bilinear surface patch (formed by two consecutive point-pairs) and its corresponding surface region on ", ". Surface\u2013surface composition is employed to extract the corresponding surface region on ", " to compare against. Finally, the above ruled surface fitting approach is also extended into a discrete algorithm to find the non-isoparametric subdivision curve on ", " when a discrete recursive piecewise-ruled surface fitting is considered. A five- or seven-dimensional dynamic programming solution is employed towards this end and once again, surface\u2013surface composition is employed to extract the two subdivided patches as tensor products."]},
{"title": "A multipoint method for 5-axis machining of triangulated surface models", "highlights": ["We give a method for positioning a radiused end milled cutter on an STL surface.", "Tool is positioned to have two points of contact.", "We give simulation and machining results."], "abstract": ["In this paper, we present a multipoint machining method for the 5-axis machining of triangulated surfaces with radiused end mills. The main idea is to drop the tool onto the surface to find an initial point of contact, and then rotate the tool while maintaining tangency with this initial point of contact until a second point of contact is found. The proposed procedure ensures a gouge-free position with two points of contact, allowing for a larger side step than a single point of contact method. This proof of concept paper presents the mathematical equations that must be solved to position the tool with two points of contacts on an STL surface. The paper further verifies the concept with simulations and presents experimental results to confirm the simulations."]},
{"title": "Cell packing structures", "highlights": ["Recent and ongoing research in architectural geometry.", "Links between cell packing structures and discrete differential geometry.", "Applications, e.g. to shading and indirect lighting.", "Interplay of geometry, optimization, statics, manufacturing.", "Combining form, function and fabrication into novel design tools."], "abstract": ["This paper is an overview of architectural structures which are either composed of polyhedral cells or closely related to them. We introduce the concept of a support structure of such a polyhedral cell packing. It is formed by planar quads and obtained by connecting corresponding vertices in two combinatorially equivalent meshes whose corresponding edges are coplanar and thus determine planar quads. Since corresponding triangle meshes only yield trivial structures, we focus on support structures associated with quad meshes or hex-dominant meshes. For the quadrilateral case, we provide a short survey of recent research which reveals beautiful relations to discrete differential geometry. Those are essential for successfully initializing numerical optimization schemes for the computation of quad-based support structures. Hex-dominant structures may be designed via Voronoi tessellations, power diagrams, sphere packings and various extensions of these concepts. Apart from the obvious application as load-bearing structures, we illustrate here a new application to shading and indirect lighting. On a higher level, our work emphasizes the interplay between geometry, optimization, statics, and manufacturing, with the overall aim of combining form, function and fabrication into novel integrated design tools."]},
{"title": "Five-axis tool path generation in CNC machining of ", "highlights": ["Introducing ", "-spline surface to CAM area.", "Improved space-filling curve (ISFC) tool path for ", "-spline surfaces machining.", "ISFC generates a non-retracted tool path.", "Tool path planning algorithm for irregular boundaries and holes in pre-image.", "Compensation path algorithm to avoid uncut materials on tool path corners."], "abstract": ["Because of its flexible topology and robust data structure, the ", "-spline surface has become the trend of free-form surfaces representation in the realm of CAD design, animation and CAE. Yet its application in manufacturing has not been fully explored. In this work, the possibility of direct tool path generation on the ", "-spline surface has been discussed. An improved space-filling curve (ISFC) tool path planning algorithm has been proposed to exploit the advantage of ", "-spline as a mathematical representation of free-form surfaces in CAM process, as well as to overcome its disadvantages such as irregular boundaries and holes in the pre-image. The turning problem in traditional SFC has been tackled using Hermite compensation curves. Finally, a prototype system has been developed to implement the proposed algorithm and actual machining has been conducted. The result shows the feasibility as well as the efficiency of the proposed method for ", "-spline surfaces tool path generation compared to commercial CAM system."]},
{"title": "Meteorosensitive architecture: Biomimetic building skins based on materially embedded and hygroscopically enabled responsiveness", "highlights": ["Access and instrumentalisation of computational capacities within organic systems.", "Formal complexity through singular parametric differentiation in material behaviour.", "Environment cognisant architectural systems with climate dependent formal behaviour.", "Embedded biomimetic intelligence through material programming."], "abstract": ["In this paper, the authors present research into autonomously responsive architectural systems that adapt to environmental changes using hygroscopic material properties. Instead of using superimposed layers of singular purpose mechanisms\u2013for sensing, actuation, control and power\u2013in the form of high-tech electronic equipment as is emblematic for current approaches to climate responsiveness in architecture, the presented research follows an integrative, no-tech strategy that can be considered to follow biological rather than mechanical principles. In nature plants employ different systems to respond to environmental changes. One particularly promising way is hygroscopic actuation, as it allows for metabolically independent movement and thus provides an interesting model for autonomous, passive and materially embedded responsiveness. The paper presents a comprehensive overview of the parameters, variables and syntactic elements that enable the development of such meteorosensitive architectural systems based on the biomimetic transfer of the hygroscopic actuation of plant cones. It provides a summary of five years of research by the authors on architectural systems which utilize the hygroscopic qualities of wooden veneer as a naturally produced constituent within weather responsive composite systems, which is presented through an extensive analysis of research samples, prototypes at various scales, and two comprehensive case studies of full scale constructions."]},
{"title": "Material ecologies for synthetic biology: Biomineralization and the state space of design", "highlights": ["Synthetic Biology requires understanding physical materials as computational agents.", "Biological materials may not be produced but induced.", "Biological state spaces include cell, and chemical and physical constraints.", "Synthetic Biology represents a new opportunity for material ecologies."], "abstract": ["This paper discusses the role that material ecologies might have in the emerging engineering paradigm of Synthetic Biology (hereafter SB). In this paper we suggest that, as a result of the paradigm of SB, a new way of considering the relationship between computation and material forms is needed, where computation is embedded into the material elements themselves through genetic programming. The paper discusses current trends to conceptualize SB in traditional engineering terms and contrast this from design speculations in terms of bottom-up processes of emergence and self-organization. The paper suggests that, to reconcile these positions, it is necessary to think about the design of new material systems derived from engineering living organisms in terms of a state space of production. The paper analyses this state space using the example of biomineralization, with illustrations from simple experiments on bacteria-induced calcium carbonate. The paper suggests a framework involving three interconnected state spaces defined as: cellular (the control of structures within the cell structures within a cell, and specifically DNA and its expression through the process of transcription and translation); chemical (considered to occur outside the cell, but in direct chemical interaction with the interior of the cell itself); physical (which constitutes the physical forces and energy within the environment). We also illustrate, in broad terms, how such spaces are interconnected. Finally the paper will conclude by suggesting how a material ecologies approach might feature in the future development of SB."]},
{"title": "Shape optimization for human-centric products with standardized components", "highlights": ["A mixed-integer shape optimization framework.", "The ability of design transfer with standardized components.", "An approach using standardized components in the customization of freeform objects.", "Design automation of customized products for fitting human bodies."], "abstract": ["In this paper, we present an optimization framework for automating the shape customization of human-centric products, which can be mounted on or embedded in human body (such as exoskeletal devices and implants). This kind of products needs to be customized to fit the body shapes of users. At present, the design customization for freeform objects is often taken in an interactive manner that is inefficient. We investigate a method to automate the procedure of customization. Major difficulty in solving this problem is caused by the not freely changed shape of components. They should be selected from a series of standardized shapes. Different from the existing approaches that allow fabricating all components by customized production, we develop a new method to generate customized products by using as-many-as-possible standardized components. Our work is based on a mixed-integer shape optimization framework."]},
{"title": "Octree-based, automatic building fa\u00e7ade generation from LiDAR data", "highlights": ["Introducing a new automatic method to detect boundary points of fa\u00e7ade features.", "Boundary points were extracted based on a local score of data points.", "The algorithm automatically detected all openings and filled non-openings.", "This was achieved without any supplemental datasets or user knowledge.", "Building models were generally more accurate and faster than previous works."], "abstract": ["This paper introduces a new, octree-based algorithm to assist in the automated conversion of laser scanning point cloud data into solid models appropriate for computational analysis. The focus of the work is for typical, urban, vernacular structures to assist in better damage prediction prior to tunnelling. The proposed Fa\u00e7adeVoxel algorithm automatically detects boundaries of building fa\u00e7ades and their openings. Next, it checks and automatically fills unintentional occlusions. The proposed method produced robust and efficient reconstructions of building models from various data densities. When compared to measured drawings, the reconstructed building models were in good agreement, with only 1% relative errors in overall dimensions and 3% errors in openings. In addition, the proposed algorithm was significantly faster than other automatic approaches without compromising accuracy."]},
{"title": "A comprehensive study of three dimensional tolerance analysis methods", "highlights": ["Introduce four major 3D tolerance analysis models briefly.", "Make a comprehensive comparison and discussion between them.", "Expound the connotation of 3D tolerance analysis.", "Present a perspective overview of the future research about 3D tolerance analysis."], "abstract": ["Three dimensional (3D) tolerance analysis is an innovative method which represents and transfers tolerance in 3D space. The advantage of 3D method is taking both dimensional and geometric tolerances into consideration, compared with traditional 1/2D tolerance methods considering dimensional tolerances only. This paper reviews four major methods of 3D tolerance analysis and compares them based on the literature published over the last three decades or so. The methods studied are Tolerance-Map (T-Map), matrix model, unified Jacobian\u2013Torsor model and direct linearization method (DLM). Each of them has its advantages and disadvantages. The T-Map method can model all of tolerances and their interaction while the mathematic theory and operation may be challenging for users. The matrix model based on the homogeneous matrix which is classical and concise has been the foundation of some successful computer aided tolerancing software (CATs), but the solution of constraint relations composed of inequalities is complicated. The unified Jacobian\u2013Torsor model combines the advantages of the torsor model which is suitable for tolerance representation and the Jacobian matrix which is suitable for tolerance propagation. It is computationally efficient, but the constraint relations between components of torsor need to be considered to improve its accuracy and validity. The DLM is based on the first order Taylor\u2019s series expansion of vector-loop-based assembly models which use vectors to represent either component dimensions or assembly dimensions. Geometric tolerances are operated as dimensional tolerances in DLM, which is not fully consistent with tolerancing standards. The results of four models with respect to an example are also listed to make a comparison. Finally, a perspective overview of the future research about 3D tolerance analysis is presented."]},
{"title": "Complex concrete structures", "highlights": ["An overview is given that combine existing casting techniques with digital fabrication for the fabrication of complex concrete structures.", "The focus is set on Smart Dynamic Casting a technique that combines digital fabrication with slipforming and building material science.", "An overview of the experimental set up and procedure is given.", "Experimental prototype results are described."], "abstract": ["Over the course of the 20th century, architectural construction has gone through intense innovation in its material, engineering and design, radically transforming the way buildings were and are conceived. Technological and industrial advances enabled and challenged architects, engineers and constructors to build increasingly complex architectural structures from concrete. Computer-aided design and manufacturing (CAD/CAM) techniques have, more recently, rejuvenated and increased the possibilities of realizing ever more complex geometries. Reinforced concrete is often chosen for such structures as almost any shape can be achieved when placed into a formwork. However, most complex forms generated with these digital design tools bear little relation to the default modes of production used in concrete construction today. A large gap has emerged between the possibilities offered by the digital technology in architectural design and the reality of the building industry, where actually no efficient solutions exist for the production of complex concrete structures. This paper presents construction methods that unfold their full potential by linking digital design, additive fabrication and material properties and hence allow accommodating the construction of complex concrete structures. The emphasis is set on the on-going research project ", " (SDC) where advanced material design and robotic fabrication are interconnected in the design and fabrication process of complex concrete structures. The proposed fabrication process is belonging to an emerging architectural phenomenon defined first as Digital Materiality by Gramazio & Kohler (2008) or more recently as Material Ecologies by Neri Oxman\u00a0 ", "."]},
{"title": "Simplification of feature-based 3D CAD assembly data of ship and offshore equipment using quantitative evaluation metrics", "highlights": ["Shipyards need simplified equipment 3D CAD assembly data for system-level design.", "Three simplification operations applicable for 3D CAD assembly data are developed.", "Evaluation metrics considering multiple simplification criteria are proposed.", "The evaluation metrics allow discriminatory priority of specific criteria."], "abstract": ["In the design process for ship outfitting and offshore plants, an equipment catalog database is compiled in order for shipyards to reutilize data effectively. However, the current procedure for building such a catalog causes wastage of time because the modelers in the shipyard must perform manual modeling of the 3D CAD data in order to decrease the size of 3D CAD data and adopt a different level of detail (LOD) depending on the purpose of its use. This problem arises because equipment suppliers are not willing to give all of their 3D CAD data to shipyards, out of fear of the loss of intellectual property. Moreover, the 3D CAD data of equipment suppliers have a high LOD, while a shipyard\u2019s 3D CAD data have a relatively low LOD. Therefore, it is necessary to introduce an automated method to simplify the 3D CAD assembly data for equipment that is received from the equipment supplier. In addressing this problem, this study first proposes criteria for a simplification process and quantitative evaluation metrics for the simplification of 3D CAD assembly data, considering the characteristics of equipment data in the shipbuilding industry. Based on these findings, a simplification system was developed, and, four experimental test cases that were conducted on-site were used to verify the proposed system. The results showed that the data to be stored could be reduced to at least 25% of the original 3D CAD assembly data while ports, outer boundaries, and connectivity between CAD parts could be maintained."]},
{"title": "An intelligent approach for dimensioning completeness inspection in 3D based on transient geometric elements", "highlights": ["A new algorithm to inspect the completeness of 3D dimensioning is described.", "The constrained states of geometric element are simply identified.", "The transformed constraints reduce the complexity of system.", "The time-consuming iterative calculations are avoided.", "8 types of geometric elements and 9 types of geometric constraints are processed."], "abstract": ["Complete dimensioning plays an important role in digital product design. This paper proposes an intelligent reasoning approach to inspect the dimensioning completeness for 3D mechanical parts. Firstly, the problem of inspecting the dimension completeness is transformed into the problem of determining the geometric element (GE) constrained states. Then, 8 types of general geometric elements are decomposed into basic geometric elements (BGE) and basic dimensional elements (BDE), and an intermediate model named transient geometric element (TGE) is proposed to reflect the influences of geometric constraints (GC) on the BGEs. Thereafter, 3 types of complex constraints are decomposed into 6 types of simple constraints, which are then utilized to form the generation rules of the TGEs. Then, an upgrading geometric reasoning method of creating new TGEs is developed to get the BGE\u2019s constrained state and the dimension usage status. Finally, the completeness states of dimensions are acquired according to the dimension usage status and the BGE\u2019s constrained states. The presented approach is tested by a pre-dimensioned mechanical part composed of many typical geometric elements and constraints, and the result demonstrates that the dimensioning completeness states can be successfully inspected."]},
{"title": "Fast global and partial reflective symmetry analyses using boundary surfaces of mechanical components", "highlights": ["An approach to global and local reflective symmetry analyses for B-Rep CAD models.", "Use of a divide-and-conquer principle over the B-Rep faces as atomic entities.", "The symmetry accuracy obtained is of the order of the CAD kernel modeling accuracy.", "The process time is very low, and compatible with user\u2019s interactions."], "abstract": ["Axisymmetry and planar reflective symmetry properties of mechanical components can be used throughout a product development process to restructure the modeling process of a component, simplify the computation of tool path trajectories, assembly trajectories, etc. To this end, the restructured geometric model of such components must be at least as accurate as the manufacturing processes used to produce them, likewise their symmetry properties must be extracted with the same level of accuracy to preserve the accuracy of their geometric model. The proposed symmetry analysis is performed on a B-Rep CAD model through a divide-and-conquer approach over the boundary of a component with faces as atomic entities. As a result, it is possible to identify rapidly all global symmetry planes and axisymmetry as well as local symmetries. Also, the corresponding algorithm is fast enough to be inserted in CAD/CAM operators as part of interactive modeling processes, it performs at the same level of tolerance than geometric modelers and it is independent of the face and edge parameterizations."]},
{"title": "Optimal barrel cutter selection for the CNC machining of blisk", "highlights": ["Geometric properties and location of four types of barrel cutters are introduced.", "Optimal tool flute size selection of barrel cutter to avoid the local gouges.", "Optimization of tool shaft diameter to avoid the global interferences.", "Numerical examples for proving the validity of the proposed method."], "abstract": ["Barrel cutters have been widely used in the flank milling of blisk. In this paper, a new method is proposed to select an optimal barrel cutter in the interest of the high productivity in the CNC machining of blisk. The geometric properties and location of four types of barrel cutters are firstly introduced. Then the optimal size of the tool flute with a greater barrel radius is determined. Meanwhile, the local gouges and kinematic constraints of the CNC machine tool are also taken into account. Furthermore, in order to avoid the global interferences between the tool shaft and the blade in process, the center point of circular arc generatrix is adjusted. Finally, the tool shaft diameter is optimized to improve tool rigidity and avoid the global interferences between the tool shaft and the adjacent blades. The numerical examples prove that a feasible barrel cutter with the higher productivity can be obtained by the proposed method in this paper. Therefore, this method can be directly applied in the CNC machining of blisk or centrifugal impeller."]},
{"title": "Polynomial spline interpolation of incompatible boundary conditions with a single degenerate surface", "highlights": ["Propose a method to interpolate a four-sided region with incompatible boundary.", "Achieve ", " continuity with the boundary except for incompatible corner points.", "Utilize the property of multi-valued normal vectors at degenerate points.", "The proposed method is constructive and straightforward."], "abstract": ["Coons\u2019 construction generates a surface patch that interpolates four groups of specified boundary curves and the corresponding cross-boundary derivative curves. This constructive method is simple and widely used in computer aided design. However, at the corner points, it requires compatibility of the boundary conditions, which is usually difficult to satisfy in practice. In order to handle the incompatible case where the normal directions respectively indicated by two adjacent boundaries do not agree with each other at the common corner point, we utilize the property of degenerate parametric surfaces that the normal directions can converge to multiple values at degenerate points, and therefore the local degenerate geometry can satisfy conflicting conditions simultaneously. Following this idea, we use a single patch of ", "-degree polynomial spline surface with four degenerate corners to interpolate incompatible boundary conditions, which are represented by ", "-degree polynomial spline curves with ", " continuity. This method is based on symbolic operations and polynomial reparameterizations for polynomial splines, and without introducing any theoretical errors, it achieves ", " continuity on the boundary except for the four corner points."]},
{"title": "Robust localization to align measured points on the manufactured surface with design surface for freeform surface inspection", "highlights": ["A method is developed for predicting variances of localized measurement points.", "Robust rough localization is conducted by matching various geometric properties.", "Robust fine localization is conducted by selecting the optimal coordinate system."], "abstract": ["Inspection of a manufactured freeform surface can be conducted by sampling measurement points on the manufactured surface and comparing the measurement points with the ideal design geometry and its tolerance. Since the measurement coordinate system and design coordinate system are usually different, these measured points should be first aligned with the design surface through localization. In this research, robust localization methods are developed for both rough localization and fine localization processes. For rough localization, some target measurement points are selected and their corresponding points on the design surface are obtained based on similarities in curvatures and distances of these points. Compared with curvatures that are often used in localization, the distances are less sensitive to the errors introduced in manufacturing and measurement processes. In fine localization, uncertainties in the measurement and localization processes are considered to predict the uncertainties of the localized measurement points. The optimal design coordinate system is also selected such that the uncertainties of the localized measurement points can be minimized. Two case studies are provided to demonstrate the effectiveness of the developed methods for freeform surface inspection."]},
{"title": "Algebraic graph statics", "highlights": ["General, non-procedural approach to graphical analysis of two-dimensional structures.", "Equilibrium equations derived from reciprocal relation between form and force graphs.", "States of stress of structural systems from analysis of equivalent unloaded networks.", "Construction of planar straight-line drawings of planar form graphs.", "Computational back-end for interactive graphic statics software."], "abstract": ["This paper presents a general, non-procedural, algebraic approach to graphical analysis of structures. Using graph theoretical properties of reciprocal graphs, the geometrical relation between the form and force diagrams used in graphic statics is written algebraically. These formulations have been found to be equivalent to the equilibrium equations used in matrix analysis of planar, self-stressed structural systems. The significance and uses of this general approach are demonstrated through several examples and it is shown that it provides a robust back-end for a real-time, interactive and flexible computational implementation of traditional graphic statics."]},
{"title": "A compact shape descriptor for triangular surface meshes", "highlights": ["A compact Shape-DNA is presented to describe the shape of a triangular surface mesh.", "Compact Shape-DNA is composed of low frequencies of DFT of processed Shape-DNA.", "The method reduces up to 97% space and time consumptions compared to Shape-DNA."], "abstract": ["Three-dimensional shape-based descriptors have been widely used in object recognition and database retrieval. In the current work, we present a novel method called compact Shape-DNA (cShape-DNA) to describe the shape of a triangular surface mesh. While the original Shape-DNA technique provides an effective and isometric-invariant descriptor for surface shapes, the number of eigenvalues used is typically large. To further reduce the space and time consumptions, especially for large-scale database applications, it is of great interest to find a more compact way to describe an arbitrary surface shape. In the present approach, the standard Shape-DNA is first computed from the given mesh and then processed by surface area-based normalization and line subtraction. The proposed cShape-DNA descriptor is composed of some low frequencies of the discrete Fourier transform of the processed Shape-DNA. Several experiments are shown to illustrate the effectiveness and efficiency of the cShape-DNA method on 3D shape analysis, particularly on shape comparison and classification."]},
{"title": "Flat-end cutter orientation on a quadric in five-axis machining", "highlights": ["The conditions to avoid local gouging and rear gouging are formulated.", "The machined strip width is evaluated analytically.", "Two orientation angles are fully exploited to maximise the width without gouging.", "The theory has been successfully applied in 5-axis sculptured surface machining."], "abstract": ["The authors have recently developed methods for cutter orientation and tool path generation in 5-axis sculptured surface machining, where the design surface is approximated locally by a quadric. This paper presents, from a purely geometric perspective, the fundamental theory for optimising the cutter orientation on a quadric, which maximises the machined strip width whilst avoiding local and rear gouging. The analysis focuses on the flat-end cutter which is modelled by a circular cylinder but can be generalised for any fillet-end cutter using an appropriate offset of the design surface and the concept of geometric equivalency. The theory is illustrated by three examples."]},
{"title": "Shape recognition of CAD models via iterative slippage analysis", "highlights": ["We present a new shape recognition method by iterative slippage analysis.", "The exact normal is found to be one of the key points for slippage analysis.", "The appropriate region is found to be the other key points for slippage analysis.", "A knowledge guided region growing method is used to get the appropriate region.", "An iterative normal modification method is used to obtain the exact normal."], "abstract": ["A new slippage analysis method for recognizing basic primitive surfaces of CAD models is presented in this paper. Obtaining the exact normal and searching the appropriate local region of each point are found to be the key steps for determining the local slippage motion type. First, the tensor voting-based boundary point recognition method is integrated to preprocess the original points. Then, the local slippage analysis method is used to initialize the point type. Furthermore, the appropriate region of each point is acquired by the region growing method. Meanwhile, the middle level information (the basic primitive surface types and the representative parameters) is found, guiding the modification of the normal of each point and the iterative detection of the surface types. Finally, the middle level information-based smooth method is introduced to refine the boundary of each basic primitive surface. The empirical results show that the proposed algorithm is efficient and robust for recognizing primitive shapes from CAD models of mechanical parts."]},
{"title": "A fully geometric approach for interactive constraint-based structural equilibrium design", "highlights": ["The initial definition of structural behaviors is of primary importance when seeking material efficiency.", "A CAD approach is introduced allowing the user to build any plane static equilibrium interactively and graphically.", "All the design freedoms of the structural problem are permanently contained within dynamic graphical regions of positions.", "Techniques benefiting from fully geometric abstraction offer some computational simplifications and new capabilities.", "As a result, this approach frees the designer from the usual hierarchical and chronological structural design processes."], "abstract": ["This paper introduces computational techniques to support architects and structural designers in the shaping of strut-and-tie networks in static equilibrium. Taking full advantage of geometry, these techniques build on the reciprocal diagrams of graphic statics and enhance the interactive handling of them with two devices: (1) nodes\u2013considered as the only variables\u2013are constrained within Boolean combinations of graphic regions, and (2) the user modifies the diagrams by means of successive operations whose geometric properties do not at any time jeopardize the static equilibrium. This constructive approach enables useful design-oriented capabilities: a graphical control of multiple solutions, the direct switching of the dependencies hierarchy, the execution of dynamic conditional statements using static constraints, the computation of interdependencies, and coordinate-free methods for ensuring consistency between certain continuums of solutions. The paper describes a computer implementation of these capabilities."]},
{"title": "Iso-level tool path planning for free-form surfaces", "highlights": ["A new and unified framework for optimizing tool path is proposed.", "Tool path is represented as the iso-level curves of a scalar function.", "Properties of tool path are encoded into that of the scalar function.", "Formulas for controlling the scalar function are derived.", "Optimal tool path regarding iso-scallop and smoothness is generated."], "abstract": ["The aim of tool path planning is to maximize the efficiency against some given precision criteria. In practice, scallop height should be kept constant to avoid unnecessary cutting, while the tool path should be smooth enough to maintain a high feed rate. However, iso-scallop and smoothness often conflict with each other. Existing methods smooth iso-scallop paths one-by-one, which make the final tool path far from being globally optimal. This paper proposes a new framework for tool path optimization. It views a family of iso-level curves of a scalar function defined over the surface as tool path so that desired tool path can be generated by finding the function that minimizes certain energy functional and different objectives can be considered simultaneously. We use the framework to plan globally optimal tool path with respect to iso-scallop and smoothness. The energy functionals for planning iso-scallop, smoothness, and optimal tool path are respectively derived, and the path topology is studied too. Experimental results are given to show effectiveness of the proposed methods."]},
{"title": "A fine-interpolation-based parametric interpolation method with a novel real-time look-ahead algorithm", "highlights": ["Optimization in both of the two interpolation stages.", "Full consideration of various kinematical constraints.", "Methodology of parameters adjustments in fine interpolation.", "Application of a novel look-ahead algorithm in rough interpolation.", "Application in open architecture CNC."], "abstract": ["Parametric interpolation is presently supported by majority of CNC systems because of its various advantages over traditional linear/circular interpolation. Two stages (i.e.\u00a0rough interpolation and fine interpolation) involved in parametric interpolation are complementary to each other in terms of affecting machining quality significantly. So far much work has been conducted to improve the machining process with various rough interpolation adjustments, while with little research on fine interpolation. To further alleviate the feedrate jump between two adjacent rough interpolation periods, a fine interpolating strategy implemented within one rough interpolation period can be utilized to make the feedrate alteration comparatively smooth. Meanwhile, an arc is adopted to substitute the linear path to reduce the chord errors caused by rough interpolation. Besides, as one of the major difficulties of parametric interpolation is the feedrate determination concerning a wide variety of technical parameters, a real-time look-ahead feedrate generation method which can determine the decelerating position rapidly and accurately is proposed in this paper. The look-ahead approach can generate the feedrate profile to satisfy the geometrical constraints and kinematical characteristics determined by machine tools. Finally, the proposed parametric interpolation method is performed in an open architecture CNC platform to machine parametric curves. The results are satisfactory and are able to verify the robustness and effectiveness of the proposed algorithm."]},
{"title": "Surface design based on direct curvature editing", "highlights": ["Surface design based on direct curvature editing is introduced.", "A point-based curvature control is extended to a curve-based control.", "A log-aesthetic curve is embedded into existing design surfaces."], "abstract": ["This paper presents a novel method for modifying the shapes of existing uniform bi-cubic B-spline surfaces by interactively editing the curvatures along isoparametric curves. The method allows us to edit the curvatures of the two intersecting isoparametric curves at each knot with specified positions, unit tangents, and unit normals. The user adjusts the radii of circles, representing the radii of curvature in the ", " and ", " isoparametric directions directly via a GUI without having to work with control points and knots. Such shape specifications are converted into iterative repositionings of the control points on the basis of geometrical rules. Using these point-based curvature-editing techniques, we successfully embedded log-aesthetic curves into existing surfaces along their isoparametric curves. Moreover, we were able to distribute the cross curvature with log-aesthetic variation along the isoparametric curves. We applied our technique to the design of automobile hood surfaces to demonstrate the effectiveness of our algorithms."]},
{"title": "An analytical method for obtaining cutter workpiece engagement during a semi-finish in five-axis milling", "highlights": ["An analytical method for Cutter-Workpiece Engagement (CWE) calculation in five-axis milling is proposed.", "Two types of cutters: flat-end and toroidal cutters are covered in this research.", "The method is applicable for semi-finish milling.", "Straight (Flat)\u2014staircase sculptured surfaces as a result of three-axis rough milling are used as a reference surface for CWE.", "Various tool inclination angles and scheduled feedrate are tested to see the influence on the CWE (cut area)."], "abstract": ["In five-axis milling, determining the continuously changing Cutter Workpiece Engagement (CWE) remains a challenge. Solid models and discrete models are the most common methods used to predict the engagement region. However, both methods suffer from long computation times. This paper presents an analytical method to define the CWE for toroidal and flat-end cutters during ", " of sculptured parts. The staircase workpiece model that resulted from rough milling was used to verify the method. The length of each cut at every engagement angle can be determined by finding two points: the lower engagement (LE) point and the upper engagement (UE) point. An extension of the method used to calculate the ", " in swept envelope development was utilized to define the LE-point. The test showed that the existence of an inclination angle significantly affected the location of the LE-point.", "For the UE-point, it was first assumed that the workpiece surface was flat. A recalculation of the CWE was then performed to obtain a more accurate engagement profile with the actual surface. A technique called the ", " method was employed to obtain the UE-point when it was located on the toroidal side of the cutting tool. Alternatively, a method called the ", " method was used to calculate the UE-point for a flat-end cutter on the cylindrical side of the toroidal cutter. The proposed model was successfully used to generate CWE data for two model parts with different surface profiles. The accuracy was verified twice: first, by comparing the coordinates of the UE-points with respect to the workpiece surface and second, by using Siemens-NX. The results proved that the proposed method was accurate. Moreover, because this method is analytical, it is more efficient in terms of computation time compared with discrete models."]},
{"title": "Mendable consistent orientation of point clouds", "highlights": ["An orientation-benefit normal estimation method is proposed.", "We use multi-sources normal propagation to achieve more consistent orientation.", "Propagation sources are extracted automatically to alleviate the manual work.", "A considerable amount of comparisons with state-of-the-art are provided."], "abstract": ["Consistent normal orientation is challenging in the presence of noise, non-uniformities and thin sharp features. None of any existing local or global methods is capable of orienting all point cloud models consistently, and none of them offers a mechanism to rectify the inconsistent normals. In this paper, we present a new normal orientation method based on the multi-source propagation technique with two insights: faithful normals respecting sharp features tend to cause incorrect orientation propagation, and propagation orientation just using one source is problematic. It includes a novel orientation-benefit normal estimation algorithm for reducing wrong normal propagation near sharp features, and a multi-source orientation propagation algorithm for orientation improvement. The results of any orientation methods can be corrected by adding more credible sources, interactively or automatically, then propagating. To alleviate the manual work of interactive orientation, we devise an automatic propagation source extraction method by visibility voting. It can be applied directly to find multiple credible sources, combining with our orientation-benefit normals and multi-source propagation technique, to generate a consistent orientation, or to rectify an inconsistent orientation. The experimental results show that our methods generate consistent orientation more or as faithful as those global methods with far less computational cost. Hence it is more pragmatic and suitable to handle large point cloud models."]},
{"title": "Motorcycle graph enumeration from quadrilateral meshes for reverse engineering", "highlights": ["Motorcycle graphs of a given quadrilateral mesh are enumerated.", "Optimum motorcycle graph is found for reverse engineering.", "Highly curved parts of models are placed on motorcycle edges wherever possible.", "Mesh is cut into several sub-meshes.", "Enumeration is performed in each sub-mesh separately."], "abstract": ["Recently proposed quad-meshing techniques allow the generation of high-quality semi-regular quadrilateral meshes. This paper outlines the generation of quadrilateral segments using such meshes. Quadrilateral segments are advantageous in reverse engineering because they do not require surface trimming or surface parameterization. The motorcycle graph algorithm of Eppstein et al. produces the motorcycle graph of a given quadrilateral mesh consisting of quadrilateral segments. These graphs are preferable to base complexes, because the mesh can be represented with a smaller number of segments, as T-joints (where the intersection of two neighboring segments does not involve the whole edge or the vertex) are allowed in quadrilateral segmentation.", "The proposed approach in this study enumerates all motorcycle graphs of a given quadrilateral mesh and optimum graph for reverse engineering is then selected. Due to the high computational cost of enumerating all these graphs, the mesh is cut into several sub-meshes whose motorcycle graphs are enumerated separately. The optimum graph is then selected based on a cost function that produces low values for graphs whose edges trace a large number of highly curved regions in the model. By applying several successive enumeration steps for each sub-mesh, a motorcycle graph for the given mesh is found. We also outline a method for the extraction of feature curves (sets of highly curved edges) and their integration into the proposed algorithm. Quadrilateral segments generated using the proposed techniques are validated by B-spline surfaces."]},
{"title": "MetaMesh: A hierarchical computational model for design and fabrication of biomimetic armored surfaces", "highlights": [" is a hierarchical computational construct to generate articulated armored surfaces.", "The ancient armored fish ", " provides source of bio-inspiration.", "Local, regional and global levels of organization embed functional differentiation.", "Articulation of scale units is preserved by neighborhood morphing techniques.", "The model is adaptable to a wide array of complex hosting surfaces."], "abstract": ["Many exoskeletons exhibit multifunctional performance by combining protection from rigid ceramic components with flexibility through articulated interfaces. Structure-to-function relationships of these natural bioarmors have been studied extensively, and initial development of structural (load-bearing) bioinspired armor materials, most often nacre-mimetic laminated composites, has been conducted. However, the translation of segmented and articulated armor to bioinspired surfaces and applications requires new computational constructs. We propose a novel hierarchical computational model, ", ", that adapts a segmented fish scale armor system to fit complex \u201chost surfaces\u201d. We define a \u201chost\u201d surface as the overall geometrical form on top of which the scale units are computed. ", " operates in three levels of resolution: (i) ", "\u2014to construct unit geometries based on shape parameters of scales as identified and characterized in the ", " exoskeleton, (ii) ", "\u2014to encode articulated connection guides that adapt units with their neighbors according to directional schema in the mesh, and (iii) ", "\u2014to generatively extend the unit assembly over arbitrarily curved surfaces through global mesh optimization using a functional coefficient gradient. Simulation results provide the basis for further physiological and kinetic development. This study provides a methodology for the generation of biomimetic protective surfaces using segmented, articulated components that maintain mobility alongside full body coverage."]},
{"title": "Isogeometric analysis suitable trivariate NURBS representation of composite panels with a new offset algorithm", "highlights": ["Analysis suitable trivariate NURBS of composite panels are constructed.", "A new curve/surface offset algorithm required by analysis is devised.", "CAD to FEA route for composite panels is streamlined in the proposed framework."], "abstract": ["Trivariate NURBS (non-uniform rational B-splines) representation of composite panels which is suitable for three-dimensional isogeometric analysis (IGA) is constructed with a new curve/surface offset algorithm. The proposed offset algorithm, which is required by IGA, is non-existent in the CAD literature. Using the presented approach, finite element analysis of composite panels can be performed with the only input being the geometry representation of the composite surface. The method proposed provides a bi-directional system in which one can go forward from CAD to analysis and backwards from analysis to CAD. This is believed to facilitate the design of composite structures. Different parts (patches) can be parametrized independently of each other and glued together, in the finite element solver, by a discontinuous Galerkin method. A stress analysis of curved composite panel with stiffeners is provided to demonstrate the proposed framework."]},
{"title": "Voxel-based fabrication through material property mapping: A design method for bitmap printing", "highlights": ["Bitmap printing workflow enables digital fabrication in printer\u2019s native resolution.", "Voxel-based design and representation of objects for multi-material printing.", "Using 3D printed light guides, deformation of materials can be sensed."], "abstract": ["We present a bitmap printing method and digital workflow using multi-material high resolution Additive Manufacturing (AM). Material composition is defined based on voxel resolution and used to fabricate\u00a0a design object\u00a0with locally varying material stiffness, aiming to\u00a0satisfy the design objective. In this workflow voxel resolution is set by the printer\u2019s native resolution, eliminating the need for slicing and path planning. Controlling geometry and material property variation at the resolution of the printer provides significantly greater control over structure\u2013property\u2013function relationships. To demonstrate the utility of the bitmap printing approach we apply it to the design of a\u00a0customized prosthetic socket. Pressure-sensing elements are concurrently fabricated with the socket, providing possibilities for evaluation of the socket\u2019s fit. The level of control demonstrated in this study\u00a0cannot be achieved using traditional CAD tools and volume-based AM workflows, implying that new CAD workflows must be developed in order to enable designers to harvest the capabilities of AM."]},
{"title": "A geometric reasoning approach to hierarchical representation for B-rep model retrieval", "highlights": ["Hierarchy of B-rep model is generated by geometric reasoning automatically.", "This shape descriptor eases global and partial retrieval at level of detail.", "Interactive prototype system gives why and how similar features are matched."], "abstract": ["3D solid model similarity is dependency of many intelligent design applications, such as design reuse, part management, case-based reasoning, and cost estimation. Matching and comparing its intrinsic boundary representation (B-rep) is a critical issue to retrieval. In this paper, we proposed a geometric reasoning approach to generate hierarchy for B-rep model retrieval. We extracted the winged-edge data structure to support series algorithms for underlying geometric reasoning, which is mainly composed of 3 steps to build hierarchy: partitioning, assembling and simplifying. This hierarchical representation is featured with level of detail ", " retaining geometric and topological information which is proved to be efficient in both global and partial retrieval. Our approach is based on the standard for the exchange of product information ", ", which is suitable for data exchange between heterogeneous CAD systems. The result of case studies from prototype implementation demonstrates its effectiveness and efficiency."]},
{"title": "Semantic interoperability of knowledge in feature-based CAD models", "highlights": ["Design intent should be captured and the semantics should be processed by intelligent systems.", "We investigate the use of Semantic Web technologies for the exchange of \u201cintelligent\u201d CAD models.", "We define axioms and mapping rules to achieve semantic integration between CAD ontologies.", "We extend semantic integration with a similarity measurement to detect similar design features."], "abstract": ["A major issue in product development is the exchange and sharing of product knowledge among many actors. This knowledge includes many concepts such as design history, component structure, features, parameters, constraints, and more.", "Regarding CAD models, most of the current CAD systems provide feature-based design for the construction of solid models and to carry, semantically, product information throughout its life cycle. Unfortunately, existing solutions and standards, such as STEP, for exchanging product information, are limited to the process of geometrical data, where semantics assigned to product model are completely lost during the translation process. Moreover, STEP does not provide a sound basis to reason with knowledge.", "The work described in this paper is part of our approach based on the development of OWL ontologies to preserve semantics associated with product data. In this work, we will focus on the semantic integration of these ontologies by defining axioms and rules. The integration process relies basically on reasoning capabilities provided by description logics in order to recognize automatically additional mappings among ontologies entities. Furthermore, the mapping process is enhanced with a semantic similarity measure to detect similar design features. Similarity measure integrates all aspects of OWL DL language. Thus, similarity functions are defined for each type of entity to involve all the features that make its definition. However, this will enable data analysis, as well as manage and discover implicit relationships among product data based on semantic modeling and reasoning."]},
{"title": "Topology optimization with meshless density variable approximations and BESO method", "highlights": ["An improved meshless density variable approximation BESO method is founded.", "The essential boundary condition is enforced by using the CSRBF.", "The Shepard function is used to create a dual-level density approximation. Numerical instabilities can be improved."], "abstract": ["An improved meshless density variable approximation is incorporated into the BESO method for topology optimization of continuum structures in this paper. The essential boundary condition is enforced by using the compactly supported radial basis function (CSRBF). The Shepard function is used to create a physically meaningful dual-level density approximation. Numerical examples show that the proposed method is feasible and fidelity for the topology optimization of continuum structures. The common numerical instabilities of the BESO method do not exist in the final results."]},
{"title": "Multi-morphology transition hybridization CAD design of minimal surface porous structures for use in tissue engineering", "highlights": ["We proposed two CAD methods for heterogeneous scaffold designs.", "The SF method was an improvement on transfinite interpolation.", "The GRBF method could handle the pixel or voxel cases.", "Bio-mimetic structures could be achieved.", "The proposed CAD method was suitable for an additive manufacturing process."], "abstract": ["There has been considerable work on scaffold design based on triply periodic minimal surfaces (TPMS). Current methods used to adjust the parameters in one type of TPMS model are insufficiently flexible for designing architectures comprising multiple porous structures. In this paper, we propose two CAD methods that combine different TPMS-based structures with given transition boundaries. One method is a sigmoid function (SF) method that can be effectively applied to simple transition boundary cases. The second method is a Gaussian radial basis function (GRBF) method that can be applied to more general cases. These methods provide for placing given TPMS-based substructures on given 3D subspaces with perfect transitions to their adjacent substructures within a scaffold domain. We present various examples of functionally graded porous structures with desired internal porous structures and external geometries to demonstrate the effectiveness of these two methods. The resulting models can be exported as STL-files and be fabricated using an additive manufacturing process."]},
{"title": "Motion-based method for estimating time required to attach self-adhesive insulators", "highlights": ["This paper presents motion-based time estimation scheme for attaching insulators.", "The scheme has been developed by analyzing the motions of attaching 350 insulators.", "It estimates times by identifying the features of insulators and MODAPTS rules.", "Estimates vary from the actual value by an average of 9.5%.", "It is realistic compared to MTM (13%\u201318%) and AEM (15%)."], "abstract": ["A self-adhesive insulator is a component of a home appliance that is used to suppress vibration or prevent humidity affecting the internal parts of the appliance. There is a wide range of types and designs available, allowing them to be applied to areas having different shapes. At the design stage, once an insulator design has been developed sufficiently to identify its dimensions and features, the attaching time and baseline cost must be estimated with reasonable accuracy to enable a comparison of vendor quotes. However, the current estimation method is not sufficiently accurate in terms of the baseline cost. This paper presents a motion-based time-estimating scheme with which the time required for the attachment of such insulators can be calculated more accurately. The scheme has been developed by analyzing the motions needed to attach 350 insulators and then designating representative motions and their time values. For this purpose, a modular arrangement of predetermined time standards (MODAPTS) is adopted. Motion-based time-estimation method is useful in terms of simplicity and accuracy. It enables design engineers to estimate the time required for the attachment based only on a drawing of the insulator and a few MODAPTS rules. Estimates made with this method should vary from the actual value by no more than 9.5%."]},
{"title": "Interactive rendering of NURBS surfaces", "highlights": ["RPNS, a new pipeline for the efficient rendering of NURBS surfaces, is presented.", "It is based on a non-recursive evaluation of a NURBS surface\u2019s basis function on GPU.", "Different GPU implementations of RPNS are proposed on DirectX11.", "A comparison with an approach based on NURBS\u2013B\u00e9zier conversion on CPU is carried out.", "RPNS achieves real-time frame rates even when the models are interactively deformed."], "abstract": ["NURBS (", ") surfaces are one of the most useful primitives employed for high quality modeling in CAD/CAM tools and graphics software. Since direct evaluation of NURBS surfaces on the GPU is a highly complex task, the usual approach for rendering NURBS is to perform the conversion into B\u00e9zier surfaces on the CPU, and then evaluate and tessellate them on the GPU. In this paper we present a new proposal for rendering NURBS surfaces directly on the GPU in order to achieve interactive and real-time rendering. Our proposal, Rendering Pipeline for NURBS Surfaces (RPNS), is based on a new primitive KSQuad that uses a regular and flexible processing of NURBS surfaces, while maintaining their main geometric properties to achieve real-time rendering. RPNS performs an efficient adaptive discretization to fine tune the density of primitives needed to avoid cracks and holes in the final image, applying an efficient non-recursive evaluation of the basis function on the GPU. An implementation of RPNS using current GPUs is presented, achieving real-time rendering rates of complex parametric models. Our experimental tests show a performance several orders of magnitude higher than traditional approximations based on NURBS to B\u00e9zier conversion."]},
{"title": "A questioning based method to automatically acquire expert assembly diagnostic knowledge", "highlights": ["Method for automatic acquisition of assembly diagnostic knowledge by asking simple questions to experts.", "Knowledge needs and types identified for diagnosis and resolution of assembly issues.", "Construction of knowledge based system to provide expert advice to assembly planners."], "abstract": ["In the domain of manual mechanical assembly, expert knowledge is an important means of supporting assembly planning that leads to fewer issues during actual assembly. Knowledge based systems can be used to provide assembly planners with expert knowledge as advice. However, acquisition of knowledge remains a difficult task to automate, while manual acquisition is tedious, time-consuming, and requires engagement of knowledge engineers with specialist knowledge to understand and translate expert knowledge. This paper describes the development, implementation and preliminary evaluation of a method that asks a series of questions to an expert, so as to automatically acquire necessary diagnostic and remedial knowledge as rules for use in a knowledge based system for advising assembly planners diagnose and resolve issues. The method, called a questioning procedure, organizes its questions around an assembly situation which it presents to the expert as the context, and adapts its questions based on the answers it receives from the expert."]},
{"title": "Evolutionary topology optimization of continuum structures with a global displacement control", "highlights": ["The proposed method achieves optimal designs with globally controllable deflections.", "Solutions with the proposed method can maintain aerodynamic shape when deformed.", "Economic lightweight designs are achieved with a volume minimization scheme.", "Robust and effective numerical algorithms are proposed for clear 0\u20131 designs."], "abstract": ["The conventional compliance minimization of load-carrying structures does not directly deal with displacements that are of practical importance. In this paper, a global displacement control is realized through topology optimization with a global constraint that sets a displacement limit on the whole structure or certain sub-domains. A volume minimization problem is solved by an extended evolutionary topology optimization approach. The local displacement sensitivities are derived following a power-law penalization material model. The global control of displacement is realized through multiple local displacement constraints on dynamically located critical nodes. Algorithms are proposed to secure the stability and convergence of the optimization process. Through numerical examples and by comparing with conventional stiffness designs, it is demonstrated that the proposed approach is capable of effectively finding optimal solutions which satisfy the global displacement control. Such solutions are of particular importance for structural designs whose deformed shapes must comply with functioning requirements such as aerodynamic performances."]},
{"title": "An algebraic taxonomy for locus computation in dynamic geometry", "highlights": ["A taxonomy for locus computation in dynamic geometry is proposed.", "An algorithm for automatic locus computation using the Gr\u00f6bner Cover is described.", "A prototype of web application implementing the main algorithm is provided."], "abstract": ["The automatic determination of geometric loci is an important issue in Dynamic Geometry. In Dynamic Geometry systems, it is often the case that locus determination is purely graphical, producing an output that is not robust enough and not reusable by the given software. Parts of the true locus may be missing, and extraneous objects can be appended to it as side products of the locus determination process. In this paper, we propose a new method for the computation, in dynamic geometry, of a locus defined by algebraic conditions. It provides an analytic, exact description of the sought locus, making possible a subsequent precise manipulation of this object by the system. Moreover, a complete taxonomy, cataloging the potentially different kinds of geometric objects arising from the locus computation procedure, is introduced, allowing to easily discriminate these objects as either extraneous or as pertaining to the sought locus. Our technique takes profit of the recently developed Gr\u00f6bnerCover algorithm. The taxonomy introduced can be generalized to higher dimensions, but we focus on 2-dimensional loci for classical reasons. The proposed method is illustrated through a web-based application prototype, showing that it has reached enough maturity as to be considered a practical option to be included in the next generation of dynamic geometry environments."]},
{"title": "Design and implementation of an integrated surface texture information system for design, manufacture and measurement", "highlights": ["We developed a surface texture information system.", "Profile and areal surface texture modules each with five components is constructed.", "Category theory based knowledge representation mechanism is devised.", "We developed a platform to integrate the information system with CAx systems."], "abstract": ["The optimized design and reliable measurement of surface texture are essential to guarantee the functional performance of a geometric product. Current support tools are however often limited in functionality, integrity and efficiency. In this paper, an integrated surface texture information system for design, manufacture and measurement, called \u201cCatSurf\u201d, has been designed and developed, which aims to facilitate rapid and flexible manufacturing requirements. A category theory based knowledge acquisition and knowledge representation mechanism has been devised to retrieve and organize knowledge from various Geometrical Product Specifications (GPS) documents in surface texture. Two modules (for profile and areal surface texture) each with five components are developed in the CatSurf. It also focuses on integrating the surface texture information into a Computer-aided Technology (CAx) framework. Two test cases demonstrate design process of specifications for the profile and areal surface texture in AutoCAD and SolidWorks environments respectively."]},
{"title": "A personalized ellipsoid modeling method and matching error analysis for the artificial femoral head design", "highlights": ["We can obtain a more accurate personalized model of the femoral head prosthesis.", "The model can well match with the acetabulum.", "Static and dynamic matching error analyses can be implemented in our system.", "We can obtain a practical means for personalized modeling before manufacture."], "abstract": ["For curing the worldwide disease \u2014 avascular necrosis of femoral head, the matching quality between the femoral head prosthesis and the acetabulum plays an important role in the operative treatment of the artificial femoral head replacement. In order to obtain a more accurate model of the femoral head prosthesis for the specified patient, a new personalized modeling system is presented in this paper. It is different from our previous system based on the sphere fitting method. This new system can reconstruct a more accurate ellipsoid model of the femoral head for the specified patient. It can recover the necrotic femoral heads into the satisfactory models. These models can well match with the acetabulum. Also, the static and dynamic matching error analyses for the reconstructed models can be implemented in this system. This new system can give a theoretical model for the accurate operation locating in the treatment of artificial femoral head replacement. And this system also provides an innovative practical means for the personalized modeling of the artificial femoral head before the prosthesis manufacture procedure."]},
{"title": "A generic uniform scallop tool path generation method for five-axis machining of freeform surface", "highlights": ["Uniform scallop tool path has been generated via cutting simulation.", "Grass/CC rings are calculated both in parametric and 3D Euclidean space.", "Optimized methods are used to fast calculate the grass/CC ring.", "The method is free of local geometry assumptions; thus is more precise.", "The method is generic for any cutter, parametric surface and tool path pattern."], "abstract": ["In this paper, a generic uniform scallop tool path generation method for five-axis machining is presented. Unlike the conventional methods which are based on the local surface geometry assumptions, this method is inspired by cutting simulation. Initially, the designed surface is planted with dense grasses. If a cutter is put onto the surface, the affected grasses will be cut short. All the affected grasses form a grass ring on the surface. When the cutter moves along the previous tool path, the envelope of the grass rings will form a machining band. Based on the machining band, cutter contact points can be found on the surface to ensure that the cutting edge touches exactly on the side of the band. These cutter contact points are fitted to construct the next tool path. In this way, all the tool paths can be generated recursively. An optimization is also developed to improve the computing efficiency of the path generation process. The proposed uniform scallop tool path generation method is generic. It can be popularized to (1) any kind of end mill with various sizes, (2) any kind of parametric surface and (3) directional- or contour-parallel tool path topologies. Another salient feature of this method is that it is free of local surface geometry assumptions, so the obtained tool paths are more precise. The proposed method is implemented and evaluated with several freeform surface examples. The feasibility of the method is also verified by actual cutting experiment."]},
{"title": "Layered shape grammars", "highlights": ["We propose a computer-aided conceptual design system to assist modelling in the early phases of design.", "Our system enhances shape grammars with layers and logic predicates.", "Layers improve time performance and structuring of shape grammars, and predicates control the application of shape grammars.", "We have applied these new techniques to examples taken from the architectural and video games domains."], "abstract": ["In this article we propose a computer-aided conceptual design system to assist modelling at the early stages of design. More precisely, we address the problem of providing the designer with design alternatives that can be used as starting points of the design process. To guide the generation of such alternatives according to a given set of design requirements, the designer can express both visual knowledge in the form of basic geometric transformation rules, and also logic constraints that guide the modelling process. Our approach is based on the formalism of shape grammars, and supplements the basic algorithms with procedures that integrate logic design constraints and goals. Additionally, we introduce a layered scheme for shape grammars that can greatly reduce the computational cost of shape generation. Shape grammars, constraints, goals and layers can be handled through a graphic environment. We illustrate the functionalities of ShaDe through two use cases taken from the architectural design and video games domains, and also evaluate the performance of the system."]},
{"title": "A case-based design with 3D mesh models of architecture", "highlights": ["Manipulate 3D mesh models of architecture.", "Recognition of floors, stairs, walls from the mesh models.", "Representation of the topologies of multi-story buildings.", "Construct new composition satisfying user-defined topology."], "abstract": ["This paper presents a case-based design (CBD) focusing on the segmentation of building models and the synthesis of new compositions. Our TRAMMA (Topology Recognition and Aggregation of Mesh Models of Architecture) program is developed as an application of the method. The segmentation includes the recognition of floors, stairs, and walls out of the mesh models. The aggregation process recombines the building segments according to the user-defined topology. The topology representation of multi-story buildings and the spatial synthesis of 3D building models are integrated in the program."]},
{"title": "Compliant assembly variation analysis of aeronautical panels using unified substructures with consideration of identical parts", "highlights": ["Finite element models of compliant parts are condensed into unified substructures.", "Substructures are reused among identical parts by transformation.", "The propagation of deviations during assembly is modeled based on substructures.", "An aeronautical panel assembly case is studied following the proposed method."], "abstract": ["The assembly process of aeronautical panel usually involves numerous parts and subprocesses. Thus a large number of Finite Element Analysis (FEA) runs are required to construct its compliant Deviation Propagation Model (DPM), when using the traditional compliant assembly variation analysis methods such as the Method of Influence Coefficients and the Linear Contact method. In this paper, an efficient DPM construction method based on substructures is proposed to reduce the modeling complexity. Finite element models of compliant parts are condensed into substructures which have relatively fewer Degrees of Freedom (DOFs). And for the identical parts commonly used in the aeronautical panels, once the substructure of one part is generated, the substructures of the others can be obtained by transformation of the generated one. By properly selecting the retained DOFs, the same substructures are applicative throughout the overall modeling process. Based on these unified substructures, the DPM of aeronautical panel assembly process is constructed, without executing extra FEA runs. A case study on a panel subassembly of the side fuselage is used to illustrate the proposed method. The results show that, this method reduces the model size as well as the model construction work, while maintaining the same accuracy as complete finite element analysis conducted in a commercial software package."]},
{"title": "Cloud-based design and manufacturing: A new paradigm in digital manufacturing and design innovation", "highlights": ["We present a new paradigm in digital manufacturing and design innovation, namely cloud-based design and manufacturing (CBDM).", "We identify the common key characteristics of CBDM.", "We define a requirement checklist that any idealized CBDM system should satisfy.", "We compare CBDM with other relevant but more traditional collaborative design and distributed manufacturing systems.", "We describe an idealized CBDM application example scenario."], "abstract": ["Cloud-based design manufacturing (CBDM) refers to a service-oriented networked product development model in which service consumers are enabled to configure, select, and utilize customized product realization resources and services ranging from computer-aided engineering software to reconfigurable manufacturing systems. An ongoing debate on CBDM in the research community revolves around several aspects such as definitions, key characteristics, computing architectures, communication and collaboration processes, crowdsourcing processes, information and communication infrastructure, programming models, data storage, and new business models pertaining to CBDM. One question, in particular, has often been raised: is cloud-based design and manufacturing actually a new paradigm, or is it just \u201cold wine in new bottles\u201d? To answer this question, we discuss and compare the existing definitions for CBDM, identify the essential characteristics of CBDM, define a systematic requirements checklist that an idealized CBDM system should satisfy, and compare CBDM to other relevant but more traditional collaborative design and distributed manufacturing systems such as web- and agent-based design and manufacturing systems. To justify the conclusion that CBDM can be considered as a new paradigm that is anticipated to drive digital manufacturing and design innovation, we present the development of a smart delivery drone as an idealized CBDM example scenario and propose a corresponding CBDM system architecture that incorporates CBDM-based design processes, integrated manufacturing services, information and supply chain management in a holistic sense."]},
{"title": "An interactive motion planning framework that can learn from experience", "highlights": ["Maximal sphere sequence represents the scenario. The centerline of maximal sphere sequence represents the scenario skeleton.", "The motion of object has close relationship with the change of the skeleton and volume size of scenario.", "Motion learning is based on computing the similarity of scenarios by dynamic time warping (DTW).", "Scenario retrieval is highly efficient due to the hierarchical clustering in motion library.", "Releasing humans from rotation manipulation in complex assembly verification with learned motion experience."], "abstract": ["The accessibility verification of the assembly/disassembly plays an important role in the process of product design. In the last decade, the sampling based motion planners have been successfully applied to solve the accessibility verification. However, the narrow passage which is a common problem in the assembly tasks is still a bottleneck. Meanwhile, the requirement of perception and emotion assessment drives the interaction between users and automatic path planners in the virtual assembly process. In this paper, a curve matching method is used to explore the implicit relationship between the topological information of scenarios and the motion of objects, based on which an interactive motion planning framework that can learn from experience is constructed.", "Our framework consists of two main processes: a learning process and a motion generation process. In the former process, the motion segment (a part of motion path) and its related scenario segment (a part of workspace passed through by the object) are gathered, after an interactive motion planning process finds a collision-free motion path or reaches the conclusion of inaccessibility. According to the similarity between the skeletons of scenario segments, the gathered scenario segments and motion segments are organized by a hierarchical structure in the motion library. The latter process permits users to control only one point in the workspace for the selection of a new scenario, and then the similar scenarios are retrieved from the motion library, to help quickly detect the connectivity of the new scenario and generate a repaired motion path to guide users with feasible manipulations. We highlight the performance of our framework on a challenging problem in 2D, in which a non-convex object passes through a cluttered environment filled with randomly shaped and located non-convex obstacles."]},
{"title": "Spiral tool path generation for diamond turning optical freeform surfaces of quasi-revolution", "highlights": ["Space Archimedean spiral is defined.", "A new spiral tool path generation based on space Archimedean spiral is proposed to machining freeform surfaces with big slope.", "The proposed method can be used for 3-axis and 4-axis ultraprecision diamond turning optical freeform surfaces of quasi-revolution."], "abstract": ["Space Archimedean spiral is defined firstly in this paper. Thereafter, a new spiral tool path generation based on space Archimedean spiral is proposed for diamond turning optical freeform surfaces of quasi-revolution, which is defined as a surface close to some surface of revolution. By projecting the space Archimedean spiral onto the freeform surface along the normal direction of the base surface instead of a fixed direction like traditional method, a quasi-uniform spiral tool path on the freeform surface can be obtained. This method can be used on diamond turning optical freeform surfaces. Finally, two examples are presented to prove its effectiveness and adaptability."]},
{"title": "Automatic balancing of 3D models", "highlights": ["We revisit a number of 3D print technologies and discuss their characteristics.", "We present an automatic, optimization based method for balancing 3D models.", "The balance is improved by creating internal cavities and by rotating the model.", "We pay special attention to make FDM printed models stand."], "abstract": ["3D printing technologies allow for more diverse shapes than are possible with molds and the cost of making just one single object is negligible compared to traditional production methods. However, not all shapes are suitable for 3D print. One of the remaining costs is therefore human time spent on analyzing and editing a shape in order to ensure that it is fit for production. In this paper, we seek to automate one of these analysis and editing tasks, namely improving the balance of a model to ensure that it stands. The presented method is based on solving an optimization problem. This problem is solved by creating cavities of air and distributing dense materials inside the model. Consequently, the surface is not deformed. However, printing materials with significantly different densities is often not possible and adding cavities of air is often not enough to make the model balance. Consequently, in these cases, we will apply a rotation of the object which only deforms the shape a little near the base. No user input is required but it is possible to specify manufacturing constraints related to specific 3D print technologies. Several models have successfully been balanced and printed using both polyjet and fused deposition modeling printers."]},
{"title": "An algorithm for CAD tolerancing integration: Generation of assembly configurations according to dimensional and geometrical tolerances", "highlights": ["Tolerance integration in CAD model.", "The realistic modeling of the parts and assemblies.", "Prediction of the tolerance impacts.", "Updating the mating constraints."], "abstract": ["For several years, Digital Mock-Up (DMU) has been improved by the integration of many tools as Finite Element (FE) Analysis, Computer Aided Manufacturing (CAM), and Computer Aided Tolerancing (CAT) in the Computer Aided Design (CAD) model. In the geometrical model, the tolerances, which specify the requirements for the proper functioning of mechanical systems, are formally represented. The nominal modeling of the parts and assemblies does not allow the prediction of the tolerance impacts on the simulation results as the optimization of mechanical system assemblability. So, improving the CAD model to be closer to the realistic model is a necessity to verify and validate the mechanical system assemblability. This paper proposes a new approach to integrate the tolerances in CAD model by the determination of the configurations with defects of a CAD part, used in a mechanical system. The realistic parts are computed according to the dimensional and geometrical tolerances. This approach provides an assembly result closer to the real assembly of the mechanical system. The Replacement of the nominal parts by the realistic ones requires the redefinition of the initially defined assembly mating constraints. The update of the mating constraints is performed by respecting an Objective Function of the Assembly (OFA). Integrating tolerances in CAD allows the visualization and simulation of the mechanical assemblies\u2019 behavior in their real configuration and the detection of possible interference and collision effects between parts which are undetectable in the nominal state."]},
{"title": "Perception-driven adaptive compression of static triangle meshes", "highlights": ["New adaptive compression method based on shape perception.", "Suitable for both single rate and progressive encoding.", "Works with any kind of perceptual error metric.", "Improvement with respect to most commonly used approaches."], "abstract": ["Mesh compression is an important task in geometry processing. It exploits geometric coherence of the data to reduce the amount of space needed to store a surface mesh. Most techniques perform compression employing a uniform data quantization over the whole surface. Research in shape perception, however, suggests that there are parts of the mesh that are visually more relevant than others. We present a novel technique that performs an adaptive compression of a static mesh, using the largest part of the bit budget on the relevant vertices while saving space on encoding the less significant ones. Our technique can be easily adapted to work with ", " perception-based error metric. The experiments show that our adaptive approach is at least comparable with other state-of-the-art techniques, while in some cases it provides a significant reduction of the bitrate of up to 15%. Additionally, our approach provides much faster decoding times than comparable perception-motivated compression algorithms."]},
{"title": "Efficient direct rendering of deforming surfaces via shared subdivision trees", "highlights": ["We present Shared Subdivision Trees (SST) to rasterize implicit surfaces on GPUs.", "We address the problem of efficiently rendering implicit surfaces which undergo a nonlinear deformation throughout the rendering process.", "We map Shared Subdivision Trees well to parallel computing platforms such as CUDA."], "abstract": ["In this paper, we present a subdivision-based approach to rasterize implicit surfaces embedded in volumetric B\u00e9zier patches undergoing a nonlinear deformation. Subdividing a given patch into simpler patches to perform the surface rasterization task is numerically robust, and allows guaranteeing visual accuracy even in the presence of geometric degeneracies. However, due to its memory requirements and slow convergence rates, subdivision is challenging to be used in an interactive environment. Unlike previous methods employing subdivision, our approach is based on the idea where for a given patch only one subdivision tree is maintained and shared among pixels. Furthermore, as the geometry of the object changes from frame to frame, a flexible data structure is proposed to manage the geometrically varying B\u00e9zier patches. The resulting algorithm is general and maps well to parallel computing platforms such as CUDA. We demonstrate on a variety of representative graphics and visualization examples that our GPU scheme scales well and achieves up to real-time performance on consumer-level graphics cards by guaranteeing visual accuracy."]},
{"title": "Geometric shapes of C-B\u00e9zier curves", "highlights": ["Transformation matrixes between any C-B\u00e9zier curve and its separated form.", "Geometric shape and the explicit expressions of the geometric characters of any C-B\u00e9zier curve.", "Sufficient and necessary conditions for the C-B\u00e9zier basis constructing some classical curves.", "Some classical curves can be constructed intuitively."], "abstract": ["In this paper, we focus on the geometric shapes of the C-B\u00e9zier curves for the space span", ". First, any C-B\u00e9zier curve is divided into a B\u00e9zier curve and a trigonometric part. So any C-B\u00e9zier curve describes the trajectory of a point orbiting around a center in an elliptical orbit while the orbital plane is moving as the ellipse center translating along a B\u00e9zier curve. Second, the geometric characters of the C-B\u00e9zier curve (the control points of the center B\u00e9zier curve, the trajectories of the vertices and the foci of the ellipse, etc.), can all be explicitly presented by the control points of the C-B\u00e9zier curve. Third, considering some special cases, we give the sufficient and necessary conditions of C-B\u00e9zier basis forming B\u00e9zier curve, ellipse, circle, common helix, and so on. Lastly, we show how to build some geometrically intuitive curves through the C-B\u00e9zier basis without rational forms."]},
{"title": "Spherical volume-preserving Demons registration", "highlights": ["We introduce a volume-preserving registration framework for brain shift analysis.", "A volume-preserving mapping is supported by a rigorous continuous theory.", "The registration is performed on spherical tetrahedron mesh with MRI gray value.", "The registration can retain the equality of local volume elements.", "Our method can register the brain efficiently while preserving the volume."], "abstract": ["In order to analyze the brain shift situation accurately, we need to register the medical image and analyze its deformation. In this paper, we introduce a framework with volume-preserving registration for brain shift analysis. First, a volume-preserving mapping is introduced for general manifolds supported by a rigorous continuous theory. The registration is then performed on the spherical tetrahedron mesh with MRI gray values. The registration can retain the equality of local volume elements while registering the manifold to a template at the same time. We use simulated brain shift data to test our method. The results show that our method can efficiently register the brain while preserving the volume of each vertex."]},
{"title": "Isogeometric segmentation: The case of contractible solids without non-convex edges", "highlights": ["Segmentation of a 3D solid without non-convex edges into topological hexahedra.", "Method is based on the edge graph of the solid.", "Decomposition is done by means of simple combinatorial and geometric criteria.", "Number of resulting topological hexahedra is small."], "abstract": ["We present a novel technique for segmenting a three-dimensional solid with a 3-vertex-connected edge graph consisting of only convex edges into a collection of topological hexahedra. Our method is based on the edge graph, which is defined by the sharp edges between the boundary surfaces of the solid. We repeatedly decompose the solid into smaller solids until all of them belong to a certain class of predefined base solids. The splitting step of the algorithm is based on simple combinatorial and geometric criteria. The segmentation technique described in the paper is part of a process pipeline for solving the isogeometric segmentation problem that we outline in the paper."]},
{"title": "-Regression based subdivision schemes for noisy data", "highlights": ["An ", "-regression based subdivision scheme is proposed to handle noisy curve/surface data with outliers.", "A fast numerical optimization method named dynamic iterative reweighted least squares is proposed to solve this problem.", "The most advantage of the proposed method is that it removes noises and outliers without any prior information about the input data."], "abstract": ["Fitting curve and surface by least-regression is quite common in many scientific fields. It, however cannot properly handle noisy data with impulsive noises and outliers. In this article, we study ", "-regression and its associated reweighted least squares for data restoration. Unlike most existing work, we propose the ", "-regression based subdivision schemes to handle this problem. In addition, we propose fast numerical optimization method: dynamic iterative reweighted least squares to solve this problem, which has closed form solution for each iteration. The most advantage of the proposed method is that it removes noises and outliers without any prior information about the input data. It also extends the least square regression based subdivision schemes from the fitting of a curve to the set of observations in 2-dimensional space to a ", "-dimensional hyperplane to a set of point observations in ", "-dimensional space. Wide-ranging experiments have been carried out to check the usability and practicality of this new framework."]},
{"title": "Dynamic meshing for deformable image registration", "highlights": ["We study how the superimposed mesh structure would influence the Finite Element Method (FEM)-based image registration process.", "We propose a mesh generation algorithm based on how the mesh will influence the registration process, using the discrete Centroidal Voronoi Tessellation idea.", "We present a parallel algorithm to compute and update the mesh structure efficiently during image registration."], "abstract": ["Finite element method (FEM) is commonly used for deformable image registration. However, there is no existing literature studying how the superimposed mesh structure would influence the image registration process. We study this problem in this paper, and propose a dynamic meshing strategy to generate mesh structure for image registration. To construct such a dynamic mesh during image registration, three steps are performed. Firstly, a density field that measures the importance of a pixel/voxel\u2019s displacement to the registration process is computed. Secondly, an efficient contraction\u2013optimization scheme is applied to compute a discrete Centroidal Voronoi Tessellation of the density field. Thirdly, the final mesh structure is constructed by its dual triangulation, with some post-processing to preserve the image boundary. In each iteration of the deformable image registration, the mesh structure is efficiently updated with GPU-based parallel implementation. We conduct experiments of the new dynamic mesh-guided registration framework on both synthetic and real medical images, and compare our results with the other state-of-the-art FEM-based image registration methods."]},
{"title": "Retrieval of non-rigid 3D shapes from multiple aspects", "highlights": ["Multiple aspects are considered for non-rigid 3D shape retrieval.", "Two distinctive viewpoints are considered for shape representation.", "A new retrieval optimization approach is proposed.", "A shape filtering algorithm is designed to remove the junk shapes."], "abstract": ["As non-rigid 3D shape plays increasingly important roles in practical applications, this paper addresses its retrieval problem by considering three aspects: shape representation, retrieval optimization, and shape filtering. (1) For shape representation, two kinds of features are considered. We first propose a new integration kernel based local descriptor, and then an efficient voting scheme is designed for shape representation. Besides, we also study the commute times as shape distributions, which grasp the spatial shape information globally. Both of them capture shape information from different viewpoints based on the same embedding basis. (2) We then study the typical problem of retrieval optimization. Prior works show poor stability under different similarity windows. To deal with this deficiency, we propose to model the problem as a distance mapping on a graph in spectral manifold space. (3) Usually, for each retrieval input, a list is returned and there may be lots of irrelevant results. We develop an algorithm to filter them out by combining multiple kernels. Finally, three public datasets are employed for performance evaluation and the results show that the studied techniques have contributed a lot in promoting the recognition rate of non-rigid 3D shapes."]},
{"title": "Progressive 3D shape segmentation using online learning", "highlights": ["A progressive interactive 3D shape segmentation method is proposed.", "Online learning is adopted to train the segmentation model accumulatively.", "The segmentation model can be updated incrementally when new shapes are added.", "Segmentation can be more accurate with the increasing of the segmented shapes."], "abstract": ["In this article, we propose a progressive 3D shape segmentation method, which allows users to guide the segmentation with their interactions, and does segmentation gradually driven by their intents. More precisely, we establish an online framework for interactive 3D shape segmentation, without any boring collection preparation or training stages. That is, users can collect the 3D shapes while segment them, and the segmentation will become more and more precise as the accumulation of the shapes.", "Our framework uses Online Multi-Class LPBoost (OMCLP) to train/update a segmentation model progressively, which includes several Online Random forests (ORFs) as the weak learners. Then, it performs graph cuts optimization to segment the 3D shape by using the trained/updated segmentation model as the optimal data term. There exist three features of our framework. Firstly, the segmentation model can be trained gradually during the collection of the shapes. Secondly, the segmentation results can be refined progressively until users\u2019 requirements are met. Thirdly, the segmentation model can be updated incrementally without retraining all shapes when users add new shapes. Experimental results demonstrate the effectiveness of our approach."]},
{"title": "Precise gouging-free tool orientations for 5-axis CNC machining", "highlights": ["We present an algorithm for generating optimized gouging-free tool path for 5-Axis CNC machining.", "We employ analysis of hyper-osculating circles that provides third order approximation of the surface.", "Double tangential contact between the tool and the target surface is employed to connect feasible hyper-osculating tool paths.", "A robust collision and gouging detection algorithm is provided.", "We introduce a global optimization algorithm that maximizes the geometric matching between the tool and the target surface."], "abstract": ["We present a precise approach to the generation of optimized collision-free and gouging-free tool paths for 5-axis CNC machining of freeform NURBS surfaces using flat-end and rounded-end (bull nose) tools having cylindrical shank. To achieve high approximation quality, we employ analysis of hyper-osculating circles (HOCs) (Wang et\u00a0al., 1993a,b), that have third order contact with the target surface, and lead to a locally collision-free configuration between the tool and the target surface. At locations where an HOC is not possible, we aim at a double tangential contact among the tool and the target surface, and use it as a bridge between the feasible HOC tool paths. We formulate all such possible two-contact configurations as systems of algebraic constraints and solve them. For all feasible HOCs and two-contact configurations, we perform a global optimization to find the tool path that maximizes the approximation quality of the machining, while being gouge-free and possibly satisfying constraints on the tool tilt and the tool acceleration. We demonstrate the effectiveness of our approach via several experimental results."]},
{"title": "A highly solid model boundary preserving method for large-scale parallel 3D Delaunay meshing on parallel computers", "highlights": ["We propose a novel parallel 3D Delaunay meshing algorithm for large-scale simulations.", "The model information is kept during parallel triangulation process.", "A 3D local non-Delaunay mesh repair algorithm is proposed.", "The meshing results can be very approaching to the model boundary.", "The method can achieve high parallel performance and perfect scalability."], "abstract": ["In this paper, we propose a novel parallel 3D Delaunay triangulation algorithm for large-scale simulations on parallel computers. Our method keeps the 3D boundary representation model information during the whole parallel 3D Delaunay triangulation process running on parallel computers so that the solid model information can be accessed dynamically and the meshing results can be very approaching to the model boundary with the increase of meshing scale. The model is coarsely meshed at first and distributed on CPUs with consistent partitioned shared interfaces and partitioned model boundary meshes across processors. The domain partition aims at minimizing the edge-cuts across different processors for minimum communication cost and distributing roughly equal number of mesh vertices for load balance. Then a parallel multi-scale surface mesh refinement phase is iteratively performed to meet the mesh density criteria followed by a parallel surface mesh optimization phase moving vertices to the model boundary so as to fit model geometry feature dynamically. A dynamic load balancing algorithm is performed to change the partition interfaces if necessary. A 3D local non-Delaunay mesh repair algorithm is finally done on the shared interfaces across processors and model boundaries. The experimental results demonstrate our method can achieve high parallel performance and perfect scalability, at the same time preserve model boundary feature and generate high quality 3D Delaunay mesh as well."]},
{"title": "ECISER: Efficient Clip-art Image SEgmentation by Re-rasterization", "highlights": ["We propose ECISER, a method for clip-art image segmentation.", "It achieves dramatic computational speedups over the state-of-the-art approaches.", "It preserves almost the same quality of results.", "The basic idea is to connect image segmentation with aliased rasterization.", "We also present a clip-art image segmentation database with ground truth labeling."], "abstract": ["Clip-art image segmentation is widely used as an essential step to solve many vision problems such as colorization and vectorization. Many of these applications not only demand accurate segmentation results, but also have little tolerance for time cost, which leads to the main challenge of this kind of segmentation. However, most existing segmentation techniques are found not sufficient for this purpose due to either their high computation cost or low accuracy. To address such issues, we propose a novel segmentation approach, ECISER, which is well-suited in this context. The basic idea of ECISER is to take advantage of the particular nature of cartoon images and connect image segmentation with aliased rasterization. Based on such relationship, a clip-art image can be quickly segmented into regions by re-rasterization of the original image and several other computationally efficient techniques developed in this paper. Experimental results show that our method achieves dramatic computational speedups over the current state-of-the-art approaches, while preserving almost the same quality of results."]},
{"title": "Progressive point set surface compression based on planar reflective symmetry analysis", "highlights": ["We propose a progressive point set surface compression algorithm.", "The algorithm analyzes high-level semantic information existent in a 3D model.", "The original surface is partitioned and encoded by different encoding techniques.", "The proposed algorithm is able to process surfaces of arbitrary topology types."], "abstract": ["In this work we propose an algorithm for progressive point set surface compression based on planar reflective symmetry analysis. For a given point set surface, we detect the primary symmetry plane and project the surface onto three orthogonal planes including the primary symmetry one. Then, on each projection plane, we adaptively subdivide the support domain into rectangular sub-domains. Analyzing the projected geometry on each sub-domain, we partition the original surface into portion(s), if any, that each can be modeled as a height field and the remaining portion, if any, that cannot. Further, we identify symmetric pairs of height-field portions, if any. Finally, different encoding techniques are designed for different types of surface portions, resulting in a generic progressive point set surface encoder that processes surfaces of arbitrary topological complexity and yields outstanding rate\u2013distortion performance."]},
{"title": "Morphology-preserving smoothing on polygonized isosurfaces of inhomogeneous binary volumes", "highlights": ["We design an effective mesh smoothing framework that focuses on medical data.", "The LSQ gives an isosurface mesh a compact approximation to the underlying surface.", "We improve the staircase detection strategy by a novel cascaded operation."], "abstract": ["Polygonized isosurfaces of anatomical structures commonly suffer from severe artifacts (e.g.,\u00a0noise and staircases), due to inhomogeneous binary volumes. Most state-of-the-art techniques can reduce these artifacts but inevitably ruining anatomical structures\u2019 morphology. Given an initial polygonization of an isosurface, we first eliminate these apparent staircases based on a context-aware Laplace filter, and then solve the morphology-preserving problem of anatomical structures as an optimization of the local spatial quadrics (LSQ) of fitted B\u00e9zier surfaces during mesh evolution. This results in a conceptually simple approach that provides a unified framework for not only handling artifacts, but also for enabling the morphology preservation of anatomical structures."]},
{"title": "Feature-preserving T-mesh construction using skeleton-based polycubes", "highlights": ["We present a novel algorithm which uses skeleton-based polycube generation to construct T-meshes.", "Three kinds of features are preserved: open curves, closed curves, singularity features.", "With a valid T-mesh, we calculate trivariate T-splines for isogeometric analysis."], "abstract": ["This paper presents a novel algorithm which uses skeleton-based polycube generation to construct feature-preserving T-meshes. From the skeleton of the input model, we first construct initial cubes in the interior. By projecting corners of interior cubes onto the surface and generating a new layer of boundary cubes, we split the entire interior domain into different cubic regions. With the splitting result, we perform octree subdivision to obtain T-spline control mesh or T-mesh. Surface features are classified into three groups: open curves, closed curves and singularity features. For features without introducing new singularities like open or closed curves, we preserve them by aligning to the parametric lines during subdivision, performing volumetric parameterization from frame field, or modifying the skeleton. For features introducing new singularities, we design templates to handle them. With a valid T-mesh, we calculate rational trivariate T-splines and extract B\u00e9zier elements for isogeometric analysis."]},
{"title": "Correct resolution rendering of trimmed spline surfaces", "highlights": ["Tight estimates relate domain resolution to screen resolution of trimmed surfaces.", "Based on the estimates, sub-pixel accuracy of a display algorithm is proven.", "The algorithm has been implemented within the standard graphics pipeline.", "The implementation enables interactive editing of trimmed surfaces.", "The implementation has a small memory footprint."], "abstract": ["Current strategies for real-time rendering of trimmed spline surfaces re-approximate the data, pre-process extensively or introduce visual artifacts. This paper presents a new approach to rendering trimmed spline surfaces that guarantees visual accuracy efficiently, even under interactive adjustment of trim curves and spline surfaces. The technique achieves robustness and speed by discretizing at a near-minimal correct resolution based on a tight, low-cost estimate of adaptive domain griding. The algorithm is highly parallel, with each trim curve writing itself into a slim lookup table. Each surface fragment then makes its trim decision robustly by comparing its parameters against the sorted table entries. Adding the table-and-test to the rendering pass of a modern graphics pipeline achieves anti-aliased sub-pixel accuracy at high render-speed, while using little additional memory and fragment shader effort, even during interactive trim manipulation."]},
{"title": "Reliable detection and separation of components for solid objects defined with scalar fields", "highlights": ["A technique for find the number of disjoint components for a model defined implicitly.", "Various methods for the separation of components for these models.", "An adaptive spatial continuation for the fast and reliable enumeration."], "abstract": ["The detection of the number of disjoint components is a well-known procedure for surface objects. However, this problem has not been solved for solid models defined with scalar fields in the so-called implicit form. In this paper, we present a technique which allows for detection of the number of disjoint components with a predefined tolerance for an object defined with a single scalar function. The core of the technique is a reliable continuation of the spatial enumeration based on the interval methods. We also present several methods for separation of components using set-theoretic operations for further handling these components individually in a solid modelling system dealing with objects defined with scalar fields."]},
{"title": "Reconstruction of water-tight surfaces through Delaunay sculpting", "highlights": ["Delaunay-based surface reconstruction algorithm has been proposed.", "It is a non-parametric and single stage approach.", "Theoretical guarantee has been discussed."], "abstract": ["Given a finite set of points ", ", we define a proximity graph called as shape-hull graph (", ") that contains all Gabriel edges and a few non-Gabriel edges of Delaunay triangulation of ", ". For any ", ", ", " is topologically regular with its boundary (referred to as shape-hull (", ")) homeomorphic to a simple closed curve. We introduce the concept of divergent concavity for simple, closed, planar curves based on the alignment of curves in concave portions and discuss various measures to characterize curves having divergent concavity. Under sufficiently dense sampling, we prove that ", ", where ", " is sampled from a divergent concave curve ", ", represents a piece-wise linear approximation of ", ". We extend this result to provide a sculpting algorithm for closed surface reconstruction from a set of raw samples. The surface is constructed through a repeated elimination of Delaunay tetrahedra subjected to circumcenter and topological constraints. Theoretically, we justify our algorithm by establishing a topological guarantee on the 3D shape-hull with the help of topological rules. We demonstrate the effectiveness of our approach with experimental results on models with sharp features and sparsely distributed point clouds. Compared to existing sculpting approaches for surface reconstruction that require either a parameter tuning or several stages, our approach is simple, non-parametric, single stage and reconstructs topologically correct piece-wise linear approximation for divergent concave surfaces."]},
{"title": "On modeling with rational ringed surfaces", "highlights": ["The paper focuses on modeling with rational ringed surfaces, mainly for blending purposes.", "We answer the question of their rationality and use P-curves for constructing rational ringed surfaces.", "The method for constructing blends that satisfy certain prescribed constraints is presented.", "The designed approach can be easily modified also for computing ", "-way blends.", "The contour curves are used for computing approximate parameterizations of implicitly given blends by ringed surfaces."], "abstract": ["A surface in Euclidean space is called ringed (or cyclic) if there exists a one-parameter family of planes that intersects this surface in circles. Well-known examples of ringed surfaces are the surfaces of revolution, (not only rotational) quadrics, canal surfaces, or Darboux cyclides. This paper focuses on modeling with rational ringed surfaces, mainly for blending purposes. We will deal with the question of rationality of ringed surfaces and discuss the usefulness of the so called P-curves for constructing rational ringed-surface-blends. The method of constructing blending surfaces that satisfy certain prescribed constraints, e.g. a necessity to avoid some obstacles, will be presented. The designed approach can be easily modified also for computing ", "-way blends. In addition, we will study the contour curves on ringed surfaces and use them for computing approximate parameterizations of implicitly given blends by ringed surfaces. The designed techniques and their implementations are verified on several examples."]},
{"title": "Lightweight wrinkle synthesis for 3D facial modeling and animation", "highlights": ["We present a lightweight non-parametric approach to generate wrinkles for 3D facial modeling and animation.", "Our method represents a convenient approach for generating plausible facial wrinkles with low-cost.", "Our method enables the reconstruction of captured expressions with wrinkles in real-time."], "abstract": ["We present a lightweight non-parametric method to generate wrinkles for 3D facial modeling and animation. The key ", " feature of the method is that it can generate plausible wrinkles using a single low-cost Kinect camera and one high quality 3D face model with details as the example. Our method works in two stages: (1) offline personalized wrinkled blendshape construction. User-specific expressions are recorded using the RGB-Depth camera, and the wrinkles are generated through example-based synthesis of geometric details. (2) Online 3D facial performance capturing. These reconstructed expressions are used as blendshapes to capture facial animations in real-time. Experiments on a variety of facial performance videos show that our method can produce plausible results, approximating the wrinkles in an accurate way. Furthermore, our technique is low-cost and convenient for common users."]},
{"title": "Control vectors for splines", "highlights": ["We extend traditional splines based on control points by incorporating control vectors.", "Our paradigm allows combining several spline constructions into one formulation.", "We can model curves and surfaces that are not possible with existing techniques."], "abstract": ["Traditionally, modelling using spline curves and surfaces is facilitated by control points. We propose to enhance the modelling process by the use of ", ". This improves upon existing spline representations by providing such facilities as modelling with local (semi-sharp) creases, vanishing and diagonal features, and hierarchical editing. While our prime interest is in surfaces, most of the ideas are more simply described in the curve context. We demonstrate the advantages provided by control vectors on several curve and surface examples and explore avenues for future research on control vectors in the contexts of geometric modelling and finite element analysis based on splines, and B-splines and subdivision in particular."]},
{"title": "Brain morphometry on congenital hand deformities based on Teichm\u00fcller space theory", "highlights": ["We propose a novel Teichm\u00fcller space theory approach to study brain morphometry.", "Conformal welding signature reflects the geometric relations of different regions.", "The invertible method encodes complete information of the functional area boundaries.", "We evaluate signatures of pre-central and post-central gyrus on subjects and control.", "Congenital Hand Deformities may make a greater impact on post-central gyrus."], "abstract": ["Congenital Hand Deformities (CHD) usually occurred between the fourth and the eighth week after the embryo is formed. Failure of the transformation from arm bud cells to upper limb can lead to an abnormal appearing/functioning upper extremity which is presented at birth. Some causes are linked to genetics while others are affected by the environment, and the rest have remained unknown. CHD patients develop prehension through the use of their hands, which affects the brain as time passes. In recent years, CHD have gained increasing attention and researches have been conducted on CHD, both surgically and psychologically. However, the impacts of CHD on the brain structure are not well-understood so far. Here, we propose a novel approach to apply Teichm\u00fcller space theory and conformal welding method to study brain morphometry in CHD patients. Conformal welding signature reflects the geometric relations among different functional areas on the cortex surface, which is intrinsic to the Riemannian metric, invariant under conformal deformation, and encodes complete information of the functional area boundaries. The computational algorithm is based on discrete surface Ricci flow, which has theoretic guarantees for the existence and uniqueness of the solutions. In practice, discrete Ricci flow is equivalent to a convex optimization problem, therefore has high numerically stability. In this paper, we compute the signatures of contours on general 3D surfaces with the surface Ricci flow method, which encodes both global and local surface contour information. Then we evaluated the signatures of pre-central and post-central gyrus on healthy control and CHD subjects for analyzing brain cortical morphometry. Preliminary experimental results from 3D MRI data of CHD/control data demonstrate the effectiveness of our method. The statistical comparison between left and right brain gives us a better understanding on brain morphometry of subjects with Congenital Hand Deformities, in particular, missing the distal part of the upper limb."]},
{"title": "Defining Simulation Intent", "highlights": ["A novel approach to capturing \u201cSimulation Intent\u201d is described.", "The design space is partitioned into cells of analysis significance.", "Analysis attributes are attached to these cells and their interfaces.", "Models are derived using Simulation Intent, design geometry and analysis attributes.", "By specifying a different Simulation Intent, different analysis models are obtained."], "abstract": ["Defining Simulation Intent involves capturing high level modelling and idealisation decisions in order to create an efficient and fit-for-purpose analysis. These decisions are recorded as attributes of the decomposed design space.", "An approach to defining Simulation Intent is described utilising three known technologies: Cellular Modelling, the subdivision of space into volumes of simulation significance (structures, gas paths, internal and external airflows, etc.); Equivalencing, maintaining functional links between different analysis representations of the same region of design space across multiple analysis models; and Virtual Topology, which offers tools for partitioning and de-partitioning the model without disturbing the manufacturing oriented design geometry. The end result is a convenient framework to which high-level analysis attributes can be applied, and from which detailed analysis models can be generated with a high degree of controllability, repeatability and automation. There are multiple novel aspects to the approach, including its reusability, robustness to changes in model topology and the inherent links created between analysis models at different levels of fidelity and physics.", "By utilising Simulation Intent, CAD modelling for simulation can be fully exploited and simulation work-flows can be more readily automated, reducing many repetitive manual tasks (e.g. the definition of appropriate coupling between elements of different types and the application of boundary conditions). The approach has been implemented and tested with practical examples, and significant benefits are demonstrated."]},
{"title": "Knot calculation for spline fitting via sparse optimization", "highlights": ["We reduce the computation time dramatically by solving convex optimization problem.", "We can simultaneously find a good combination of the knot number and knot locations.", "The algorithm has less knots with good fitting performance compared to other methods.", "We can recover the ground truth knots when data is sampled enough from a B-spline."], "abstract": ["Curve fitting with splines is a fundamental problem in computer-aided design and engineering. However, how to choose the number of knots and how to place the knots in spline fitting remain a difficult issue. This paper presents a framework for computing knots (including the number and positions) in curve fitting based on a sparse optimization model. The framework consists of two steps: first, from a dense initial knot vector, a set of active knots is selected at which certain order derivative of the spline is discontinuous by solving a sparse optimization problem; second, we further remove redundant knots and adjust the positions of active knots to obtain the final knot vector. Our experiments show that the approximation spline curve obtained by our approach has less number of knots compared to existing methods. Particularly, when the data points are sampled dense enough from a spline, our algorithm can recover the ground truth knot vector and reproduce the spline."]},
{"title": "Constructing developable surfaces by wrapping cones and cylinders", "highlights": ["An effective technique for modeling the bending of paper using cones and cylinders.", "A methodology for producing complicated developable surfaces from a planar figure.", "Interactive control of paper bending from a user-specified displacement."], "abstract": ["We model developable surfaces by wrapping a planar figure around cones and cylinders. Complicated developables can be constructed by successive mappings using cones and cylinders of different sizes and shapes. We also propose an intuitive control mechanism, which allows a user to select an arbitrary point on the planar figure and move it to a new position. Numerical techniques are then used to find a cone or cylinder that produces the required mapping. Several examples demonstrate the effectiveness of our technique."]},
{"title": "Topologically guaranteed bivariate solutions of under-constrained multivariate piecewise polynomial systems", "highlights": ["A subdivision algorithm for 2-DOF nonlinear algebraic systems.", "Topologically guaranteed subdivision termination criteria in ", ", ", ".", "A tessellation method for two-manifolds in ", "."], "abstract": ["We present a subdivision based algorithm to compute the solution of an under-constrained piecewise polynomial system of ", " equations with ", " unknowns, exploiting properties of B-spline basis functions. The solution of such systems is, typically, a two-manifold in ", ". To guarantee the topology of the approximated solution in each sub-domain, we provide subdivision termination criteria, based on the (known) topology of the univariate solution on the domain\u2019s boundary, and the existence of a one-to-one projection of the unknown solution on a two dimensional plane, in ", ". We assume the equation solving problem is regular, while sub-domains containing points that violate the regularity assumption are detected, bounded, and returned as singular locations of small (subdivision tolerance) size. This work extends (and makes extensive use of) topological guarantee results for systems with zero and one dimensional solution sets. Test results in ", " and ", " are also demonstrated, using error-bounded piecewise linear approximations of the two-manifolds."]},
{"title": "Intrinsic computation of centroidal Voronoi tessellation (CVT) on meshes", "highlights": ["We propose two intrinsic methods for computing centroidal Voronoi tessellation (CVT) on triangle meshes.", "Thanks to their intrinsic nature, our methods compute CVT using metric only.", "Our results are independent of the embedding space."], "abstract": ["Centroidal Voronoi tessellation (CVT) is a special type of Voronoi diagram such that the generating point of each Voronoi cell is also its center of mass. The CVT has broad applications in computer graphics, such as meshing, stippling, sampling, etc. The existing methods for computing CVTs on meshes either require a global parameterization or compute it in the restricted sense (that is, intersecting a 3D CVT with the surface). Therefore, these approaches often fail on models with complicated geometry and/or topology. This paper presents two intrinsic algorithms for computing CVT on triangle meshes. The first algorithm adopts the Lloyd framework, which iteratively moves the generator of each geodesic Voronoi diagram to its mass center. Based on the discrete exponential map, our method can efficiently compute the Riemannian center and the center of mass for any geodesic Voronoi diagram. The second algorithm uses the L-BFGS method to accelerate the intrinsic CVT computation. Thanks to the intrinsic feature, our methods are independent of the embedding space, and work well for models with arbitrary topology and complicated geometry, where the existing extrinsic approaches often fail. The promising experimental results show the advantages of our method."]},
{"title": "Hybrid CAD/E platform supporting exploratory architectural design", "highlights": ["Hybrid CAD/E platform for early-stage exploratory architectural design is proposed.", "Platform supports human intuition-based and computation-based workflows.", "An experiment indicates the potential of the platform to yield creative designs.", "Technical aspects of CAD/E tool interoperability are presented.", "Features of platform-inspired future architectural software are pointed out."], "abstract": ["Although computers prove useful in aiding human creativity, there is still shortage of adequate software for early-stage explorations in the design disciplines. One of such under-supported disciplines is architectural design. In response to this lack of proper software, architects adopted tools from other industries with a large level of success. However, despite the fact that several types of such tools have now been employed in conceptual architectural design, their application indicates that they are seldom used altogether.", "This paper presents a hybrid software platform, bringing these various CAD and CAE tools into one design toolkit. Each platform tool introduces a different possibility of designing, and supports computation-based activities, human intuitive ones, or a mixture of both. The platform user can freely switch between various working styles: algorithmic on the one hand, and fuzzy on the other. Moreover, the designer can perform tasks impossible to execute in traditional design, such as quick generation, exploration and evaluation of large design spaces, containing geometrically-complex solutions. The testing of the hybrid platform in a design experiment indicates its ability to support meaningful early-stage explorations and to yield a highly creative design.", "One of the aims of investigating a CAD/E platform based on the existing software is to lay the foundations for the development of a future CAD/E system for early-stage architectural design \u2014 a tool which is currently missing in the architectural practice. By revealing the potentials of the ready-made software, and by pointing out the technical challenges associated with its integration, we wish to suggest a possible direction for future studies, so that solutions developed by academia have higher chances of being noticed and more widely implemented in praxis."]},
{"title": "Solving the pentahedron problem", "highlights": ["Reduction of the pentahedron problem to a well-constrained system of 3 equations in 3 unknowns.", "A considerable performance enhancement (", ") over classical formulation.", "Existence of 3D parallel solutions for generic 3D pentahedron problems is shown.", "Interesting properties of the solution set are studied.", "Discussion of how the pentahedron interesting properties generalize for other polyhedra."], "abstract": ["Nowadays, all geometric modelers provide some tools for specifying geometric constraints. The 3D pentahedron problem is an example of a 3D Geometric Constraint Solving Problem (GCSP), composed of six vertices, nine edges, five faces (two triangles and three quadrilaterals), and defined by the lengths of its edges and the planarity of its quadrilateral faces. This problem seems to be the simplest non-trivial problem, as the methods used to solve the Stewart platform or octahedron problem fail to solve it. The naive algebraic formulation of the pentahedron yields an under-constrained system of twelve equations in eighteen unknowns. Even if the use of placement rules transforms the pentahedron into a well-constrained problem of twelve equations in twelve unknowns, the resulting system is still hard to solve for interval solvers. In this work, we focus on solving the pentahedron problem in a more efficient and robust way, by reducing it to a well-constrained system of three equations in three unknowns, which can be solved by any interval solver, avoiding by the way the use of placement rules since the new formulation is already well-constrained. Several experiments showing a considerable performance enhancement (", ") are reported in this paper to consolidate our theoretical findings. Throughout this paper, we also emphasize some interesting properties of the solution set, by showing that for a generic set of parameters, solutions in the form of 3D parallel edge pentahedra do exist almost all the time, and by providing a geometric construction for these solutions. The pentahedron problem also admits degenerate 2D solutions in finite number. This work also studies how these interesting properties generalize for other polyhedra."]},
{"title": "Hexahedral mesh smoothing via local element regularization and global mesh optimization", "highlights": ["A novel local to global hexahedral mesh smoothing algorithm is proposed.", "An element size adjustment method is proposed to scale transformed ideal elements.", "The volumetric Laplacian operator is used to stitch the regularized elements.", "Geometric constraints of surface meshes are introduced to global optimization."], "abstract": ["The quality of finite element meshes is one of the key factors that affect the accuracy and reliability of finite element analysis results. In order to improve the quality of hexahedral meshes, we present a novel hexahedral mesh smoothing algorithm which combines a local regularization for each hexahedral mesh, using dual element based geometric transformation, with a global optimization operator for all hexahedral meshes. The global optimization operator is composed of three main terms, including the volumetric Laplacian operator of hexahedral meshes and the geometric constraints of surface meshes which keep the volumetric details and the surface details, and another is the transformed node displacements condition which maintains the regularity of all elements. The global optimization operator is formulated as a quadratic optimization problem, which is easily solved by solving a sparse linear system. Several experimental results are presented to demonstrate that our method obtains higher quality results than other state-of-the-art approaches."]},
{"title": "Efficient offset trimming for deformable planar curves using a dynamic hierarchy of bounding circular arcs", "highlights": ["Offset trimming for planar freeform curves under deformation.", "Dynamic BVH generation using a hierarchy of recursive bisections.", "BV generation using the bounding circular arcs (BCA) of Meek and Walton (1995).", "Performance improvement over the biarc-based algorithm of Kim et al. (2012)."], "abstract": ["We present an efficient algorithm for computing a family of trimmed offsets for planar freeform curves under deformation. The algorithm is based on a dynamic bounding volume hierarchy (BVH) for the untrimmed offsets of a given planar curve, which can be generated efficiently using a hierarchy of recursive bisections of the given curve. The proposed algorithm is effective for deformable planar curves. At each time frame, we segment the input curve into monotone spiral pieces (Barton and Elber, 2011), which is the only pre-processing needed for the dynamic BVH construction. To speed up the on-line generation of dynamic BVH, we employ the bounding circular arcs (BCA) of Meek and Walton (1995) that can be computed very efficiently using the position and tangent information at the endpoints of each monotone spiral curve segment. Using several experimental results, we demonstrate the performance improvement of our algorithm over the previous biarc-based algorithm of Kim et\u00a0al. (2012)."]},
{"title": "Polynomial local shape descriptor on interest points for 3D part-in-whole matching", "highlights": ["We solve the part-in-whole matching problem for 3D models in engineering domain.", "A local descriptor is proposed to describe the local shape of the interest points.", "The proposed descriptor is sensitive to the geodesic radius.", "Interest points are extracted based on the local saliency of the sharp features.", "We report the properties and the matching performance of the proposed method."], "abstract": ["Part-in-whole 3D shape matching is to recognize query shapes as sub-parts of a target intact 3D object. It plays a pivotal role in a large number of engineering applications. The most critical component in a part-in-whole search system is the local shape descriptor which encapsulates the identified local feature on the query part and is matched with the local shape descriptors of the parts in the database. We propose a novel local shape descriptor based on the concept that the evolution pattern of geodesic iso-contour\u2019s length is a good representative for surface features. Our local shape descriptor enjoys a unique advantage over most existing ones by being sensitive to the geodesic radius of the local region, and thus is able to capture more comprehensive shape information if the query portion of the shape is larger and includes more complicated surface features. Through a simple approximation scheme, our local shape descriptor is defined as a vector piecewise polynomial function of the geodesic radius of the interest point, thus enabling local matching to be performed quickly by simple curve evaluations. We also introduce a new schema of interest points sampling so that we can reserve the most corresponding information of the model by a small number of local feature descriptors. The proposed part-in-whole matching approach outperforms many existing approaches in matching efficiency and requiring a smaller input region. It is a shortcut solution for incomplete model matching/retrieval."]},
{"title": "Geometric segmentation of 3D scanned surfaces", "highlights": ["A new method to segment geometric features in discrete geometric models is proposed.", "Sharp edges, defective zones and 10 different types of regular points are recognized.", "The method requires just a few setting parameters that are not critical.", "It works with real scanned geometries, highly noised and not well-sampled models.", "The point type association is not affected by the singular properties of the point."], "abstract": ["The geometric segmentation of a discrete geometric model obtained by the scanning of real objects is affected by various problems that make the segmentation difficult to perform without uncertainties. Certain factors, such as point location noise (coming from the acquisition process) and the coarse representation of continuous surfaces due to triangular approximations, introduce ambiguity into the recognition process of the geometric shape. To overcome these problems, a new method for geometric point identification and surface segmentation is proposed.", "The point classification is based on a fuzzy parameterization using three shape indexes: the ", ", ", " and ", ". A total of 11 fuzzy domain intervals have been identified and comprise sharp edges, defective zones and 10 different types of regular points. For each point of the discrete surface, the related membership functions are dynamically evaluated to be adapted to consider, point by point, those properties of the geometric model that affects uncertainty in point type attribution.", "The methodology has been verified in many test cases designed to represent critical conditions for any method in geometric recognition and has been compared with one of the most robust methods described in the related literature."]},
{"title": "Approach for developing coordinated rubrics to convey quality criteria in MCAD training", "highlights": ["Six main quality dimensions in CAD models for inexperienced CAD trainees.", "Approach to convey quality strategies to CAD trainees through rubrics.", "Assertions Map that adapts the rubrics to CAD trainee progress.", "Lessons learned on rubrics (separate, accurate timing, support with lecture notes)."], "abstract": ["This paper describes an approach to convey quality-oriented strategies to CAD trainees by embedding quality criteria into rubrics so as to force CAD trainees to understand them early in their instruction. To this end, the paper analyzes how CAD quality criteria can be organized around ", " and embedded into rubrics in order to enforce quality modeling during the CAD training of novice product designers. Hence, the ambit of this study is training with history-based parametric feature-based MCAD systems, although the general conclusions are believed to be adaptable to other CAD training scenarios. Furthermore, it introduces an approach based on progressive refinement, which results in an ", " that indicates quality dimensions vs. sequence of tasks for CAD models. The map illustrates how the expand\u2013contract strategy adapts the rubrics to CAD trainee progress and assists them in comprehending the different dimensions of the rubrics. The paper also highlights lessons learned on the suitability of using separate rubrics for different tasks, the need of accurately timing the expand\u2013contract process, and on the convenience of supporting rubrics with suitable teaching, which must convey good practices and evaluation tools through rubrics. The experiments describe the different lessons learned and illustrate the suggested process for replicating our approach for further developing rubrics adapted to other scenarios."]},
{"title": "Potential of support vector regression for optimization of lens system", "highlights": ["Lens system design represents a crucial factor for good image quality.", "Optimization procedure is the main part of the lens system design methodology.", "Soft computing methodologies optimization application.", "Adaptive neuro-fuzzy inference system (ANFIS) application.", "Support vector regression (SVR application)."], "abstract": ["Lens system design is an important factor in image quality. The main aspect of the lens system design methodology is the optimization procedure. Since optimization is a complex, non-linear task, soft computing optimization algorithms can be used. There are many tools that can be employed to measure optical performance, but the spot diagram is the most useful. The spot diagram gives an indication of the image of a point object. In this paper, the spot size radius is considered an optimization criterion. Intelligent soft computing scheme Support Vector Regression (SVR) is implemented. In this study, the polynomial and radial basis functions (RBF) are applied as the SVR kernel function to estimate the optimal lens system parameters. The performance of the proposed estimators is confirmed with the simulation results. The SVR results are then compared with other soft computing techniques. According to the results, a greater improvement in estimation accuracy can be achieved through the SVR with polynomial basis function compared to other soft computing methodologies. The SVR coefficient of determination ", " with the polynomial function was 0.9975 and with the radial basis function the ", " was 0.964. The new optimization methods benefit from the soft computing capabilities of global optimization and multi-objective optimization rather than choosing a starting point by trial and error and combining multiple criteria into a single criterion in conventional lens design techniques."]},
{"title": "Imposing angle boundary conditions on B-spline/NURBS surfaces", "highlights": ["We study the construction of a B-spline surface satisfying prescribed angle distribution of tangent planes along its boundary curve.", "We prove that for a given B-spline curve, the exact solution exists only in very special cases (for a special form of an angle function).", "We propose an algorithm for finding an approximate solution, derive a bound on its approximation error and study the approximation order of the proposed algorithm."], "abstract": ["In this paper, we study the construction of a B-spline surface satisfying prescribed angle distribution (with respect to a chosen vector) of tangent planes along its boundary curve. This problem arises e.g. in a creation of a parametric geometric model of a Pelton turbine bucket, where specific angle distributions along a splitter and an outlet curve have to be fulfilled in order to control the flow of water into and out of the bucket. We prove that for a given B-spline curve ", ", ", ", the exact solution exists only in very special cases (for a special form of an angle function ", "). Further, we formulate an algorithm for finding an approximate solution. We also derive a bound on its approximation error and give a numerical evidence that the approximation order of the proposed algorithm is four. Finally, the method is demonstrated on several examples."]},
{"title": "A TRIZ-based Trimming method for Patent design around", "highlights": ["Establish an integrated TRIZ-based framework for Trimming strategy to design around competitive patent.", "Present a step by step Trimming Technical Features process to generate Trimming scenarios and Trimming problems for innovation inspiration.", "Generate two Trimming scenarios for Core Ejector System improvement."], "abstract": ["Patent design around is an effective way to walk around competitive patents and avoid patent infringement. This paper proposes a TRIZ-based Trimming method for Patent design around. The 4 stages of Patent design around include design around target definition, design around problem identification, problem solving, and solution evaluation. In the 4 stages, Technical Features (TF) are identified to define design around target based on patent claim decomposition. In order to avoid literal infringement and doctrine of equivalents, the design around problem identification process of Trimming one or more patent Technical Features is developed. Then, the problem solving tools based on Modern TRIZ tools (such as Physical Contradiction resolution, and Function-Oriented Search) are introduced to solve the design around problem. Two Trimming scenarios of Core Ejector System is investigated to illustrate the method. The project successfully circumvents target patent and generates several new granted patents in China."]},
{"title": "Developing multiagent systems for design activity analysis", "highlights": ["A theory for design activity reasoning is proposed.", "The multiagent system ADEA is modeled and implemented for the theory analysis.", "Design space is modeled as a network of design parameter agents.", "Emergence is the result of a non-programmed behavior of role agents.", "Design parameter agents help to overcome the cognitive limitation of role agents."], "abstract": ["Engineering design is a complex socio-technical activity characterized by co-evolution of problem and solution. However, the actual design theories are not well-suited to represent and model the complexity of design activity, the co-evolution and its dynamics. Therefore, there is a need to develop design activity reasoning theories and tools, which can theorize and simulate the model of co-evolution and its dynamics. Multiagent systems have the capacity to play an important role in developing and analyzing models and theories of interactivity in socio-technical societies, particularly in design. This paper first addresses a theory for design activity reasoning. Then, it will present a multiagent system, called ADEA (", "), in order to model, simulate and analyze this theory. The agents of the ADEA platform formalize the necessary design roles, characterizing the design activity as well as the relationship between design parameters in the design space. ADEA\u2019s platform shows that cognitive limitation of role agents has been overcome, considering their relationship with the design space modeled as a network of design parameter agents."]},
{"title": "Identifying and constructing elemental parts of shafts based on conditional random fields model", "highlights": ["Our work improves the level of semantic understanding of 2D projections in 3D solids reconstruction.", "It is the first trial to formulate the parts identification task into a classification problem.", "We employ an advanced classification model, CRFs, to identify the elemental parts."], "abstract": ["Semantic information is very important for understanding 2D engineering drawings. However, this kind of information is implicit so that it is hard to be extracted and understood by computers. In this paper, we aim to identify the semantic information of shafts from their 2D drawings, and then reconstruct the 3D models. The 2D representations of shafts are diverse. By analyzing the characteristics of 2D drawings of shafts, we find that there is always a view which represents the projected outline of the shaft, and each loop in this view corresponds to an elemental part. The conditional random fields (CRFs) model is a classification technique which can automatically integrate various features, rather than manually organizing of heuristic rules. We first use a CRFs model to identify elemental parts with semantic information. The 3D elemental parts are then constructed by a parameters template method. Compared with the existing 3D reconstruction methods, our approach can obtain both geometrical information and semantic information of each part of shafts from 2D drawings. Several examples are provided to demonstrate that our algorithm can accurately handle diverse 2D drawings of shafts."]},
{"title": "A flexible and effective NC machining process reuse approach for similar subparts", "highlights": ["A feature-based parameter-driven model to guide NC process reuse is established.", "The NC process parameter-driven characteristic of similar feature is revealed.", "An NC process reusability assessment approach of similar pocket/subpart is proposed.", "We extend feature-based NC process reuse to a greater granularity of subpart."], "abstract": ["NC machining process reuse is widely accepted as an effective strategy for engineers to generate the process plan with less time and lower cost. However, there has been very little research on how to reuse the NC machining process of similar subparts. As a result, most reusable NC machining process still has to remain in the repository as tacit knowledge, which can easily get lost due to oblivion. This paper proposes a novel NC machining process reuse approach for similar subparts in which existing NC machining process cases are described in association with machining features. First, a feature-based parameter-driven model is established to formalize the links between information imbedded in the machining feature and the parameters of cutting tools, drive geometries, and machining strategies. Then, the NC process parameter-driven characteristic of similar feature is revealed from the perspective of machining geometry, machining precision of the feature, and cutter geometry. Moreover, an NC process reusability assessment approach of similar pocket/subpart is presented using the pocket\u2019s medial axis transform. Finally, the NC machining process inheritance mechanisms are explored to implement the NC machining process reuse automatically and efficiently. A prototype system based on CATIA has been developed to verify the effectiveness of the proposed approach."]},
{"title": "Ubiquitous computer aided design: A broken promise or a Sleeping Beauty?", "highlights": ["An overall account of the status of ubiquitous computing and technologies in CAD.", "Penetration of ubiquitous computing remained insignificant in most applications.", "Application of ubiquitous technologies did not lead to radically new functionalities.", "Computer aided design steps over the paradigm of ubiquitous computing.", "New CAD functionalities expected from the emerging new computer paradigms."], "abstract": ["As a novel computational approach, ubiquitous computing was emerging at the beginning of the 1980s and has reached a rather mature level by now. It assumes that computing can be available anywhere, anytime and in any context due to technological developments, social demands and calm implementations. Over the years, the opportunities of this computing paradigm have been explored and the benefits have been exploited successfully in many application fields. This survey paper addresses ubiquitous computing from the perspective of enabling computer aided design. The specific objectives of the reported survey are to: (i) give an overall account of the current status of ubiquitous computing and technologies, (ii) cast light on how ubiquitous computing has influenced the development of CAD systems, tools, and methods, and (iii) critically investigate future development opportunities of ubiquitous computing enabled computer aided design. First, the paper discusses the principles and typical technologies of ubiquitous computing. Then, the development and spectrum of the so-called standard computer aided design tasks are analyzed from a computational point of view. Afterwards, the already implemented design enabling functionalities are discussed and some additional functional possibilities are considered. The literature provides evidence that ubiquitous computing has not managed to revolutionize the methodologies or the systems of computer aided design so far, though many researchers intensively studied the affordances and the application possibilities of ubiquitous technologies. One reason is that ubiquitous computing technologies had in the last two decades to compete with other kinds of computational technologies, such as high-capacity computing, high-speed networking, immersive virtual reality, knowledge ontologies, smart software agents, mobile communication, etc., which had a much stronger influence on the development of computer aided design methods and systems. In combination with the rather conservative and conventionalist industrial practice of CAD system development and application, this may explain why the ubiquitous computing revolution remained weak in computer aided design. The literature clearly indicates that application of ubiquitous technologies did not lead to radically new functionalities that could have been exploited by the concerned industries. Consequently, it seems to be possible that computer aided design simply steps over the paradigm of ubiquitous computing and expects new functionalities from the emerging new computing paradigms, such as brain\u2013computer interfacing, cyber\u2013physical computing, biological computing, or quantum computing."]},
{"title": "Impact of a behavior model linearization strategy on the tolerance analysis of over-constrained mechanisms", "highlights": ["A tolerance analysis approaches overview is proposed.", "A linearization procedure of the behavior model is required for both approaches.", "Some linearization strategies provide conservative probability of failure results.", "A confidence interval is obtained using two different linearization strategies.", "The order of magnitude of the probability has an effect on the convergence speed."], "abstract": ["All manufactured products have geometrical variations which may impact their functional behavior. Tolerance analysis aims at analyzing the influence of these variations on product behavior, the goal being to evaluate the quality level of the product during its design stage. Analysis methods must verify whether specified tolerances enable the assembly and functional requirements. This paper first focuses on a literature overview of tolerance analysis methods which need to deal with a linearized model of the mechanical behavior. Secondly, the paper shows that the linearization impacts the computed quality level and thus may mislead the conclusion about the analysis. Different linearization strategies are considered, it is shown on an over-constrained mechanism in 3D that the strategy must be carefully chosen in order to not over-estimate the quality level. Finally, combining several strategies allows to define a confidence interval containing the true quality level."]},
{"title": "Constructive generation of the medial axis for solid models", "highlights": ["A novel constructive method is given based on reuse of the existing MA.", "Only the MA in the local re-dilation region needs to be regenerated.", "Some new properties of MA are analyzed for the constructive MA generation.", "The local re-dilation region is incrementally determined for Boolean subtraction operations."], "abstract": ["The medial axis (MA) is a simplified representation of complicated models which is widely used in current research. However, the efficient generation of the MA for complicated solid models continues to pose a challenge. In this study, a constructive approach for the generation of the MA is proposed for solid models after they are voxelized. With this method, the MA of the model constructed from two operand models via a Boolean operation is efficiently generated by merging the MAs of the operand models in a certain way, instead of regenerating them from scratch. To support the proposed method, the affected region of the resultant model is computed first using a Boolean operation. Second, only the MA in the affected region is regenerated by distance dilation. Third, the complete MA of the resultant model is constructed by combining the newly generated MA with the unchanged MAs of the operand models. In this study, the accuracy and complexity are analyzed for the final MA and some examples are given to illustrate the performance of the proposed method."]},
{"title": "Outlier detection for scanned point clouds using majority voting", "highlights": ["A robust method to detect and remove all types of outliers in scanned point clouds.", "Ability to preserve valid point clusters of small size.", "Effectiveness validated with a variety of scanned point clouds."], "abstract": ["When scanning an object using a 3D laser scanner, the collected scanned point cloud is usually contaminated by numerous measurement outliers. These outliers can be sparse outliers, isolated or non-isolated outlier clusters. The non-isolated outlier clusters pose a great challenge to the development of an automatic outlier detection method since such outliers are attached to the scanned data points from the object surface and difficult to be distinguished from these valid surface measurement points. This paper presents an effective outlier detection method based on the principle of majority voting. The method is able to detect non-isolated outlier clusters as well as the other types of outliers in a scanned point cloud. The key component is a majority voting scheme that can cut the connection between non-isolated outlier clusters and the scanned surface so that non-isolated outliers become isolated. An expandable boundary criterion is also proposed to remove isolated outliers and preserve valid point clusters more reliably than a simple cluster size threshold. The effectiveness of the proposed method has been validated by comparing with several existing methods using a variety of scanned point clouds."]},
{"title": "Automatic shape adaptation for parametric solid models", "highlights": ["Automatic shape adaptation is achieved based on corresponding faces and dimensions.", "A new method of determining corresponding faces is put forward.", "An algorithm to identify corresponding dimensions by constraint graph is proposed."], "abstract": ["Adaptation, as is well known, plays a fundamental role in Case-Based Design. However, after decades of efforts, automatic adaptation approach is still rare. In common design works, the first thing one will usually do is choosing a start-up model (a candidate model) of moderate complexity based on a simple query model possessing primary design constraints. To enable the candidate model to smartly adapt its shape to that of the query model according to the embedded constraints, a novel automatic shape adaptation approach is proposed in this paper. First, to determine the corresponding faces between two non-preregistered models as relevant elements, a shape frame concept and its quantitative descriptor are defined. Second, to unify the representation of seemingly different but inherently consistent dimensions, a promotion method is adopted. Third, based on the corresponding faces and the promoted dimension representation, the corresponding dimensions between the two parametric solid models are identified. Finally, the parametric information is smoothly transferred from the query model to the candidate model as design constraints, and the shape of the candidate model is automatically adapted to the query model. Besides that, a prototype system is also implemented to verify the effectiveness of the automatic shape adaptation approach."]},
{"title": "A mapping-based approach to eliminating self-intersection of offset paths on mesh surfaces for CNC machining", "highlights": ["A method of eliminating self-intersection of offset paths on mesh surfaces is proposed.", "The new orientation rule based on the local loop is developed and discussed in detail.", "The method of rounding the sharp corner is preliminarily investigated.", "Simulation and real machining are performed."], "abstract": ["Geometrically, a tool path can be generated by successively offsetting its adjacent path on the surface with a given path interval, which preferably starts from one of the surface boundaries or a primary curve. The key issues involved in offset path planning are the generation of raw offset paths and the elimination of the self-intersection of raw offset paths. Most researches available in this area are focused on how to generate the raw offset paths, however, the latter, especially how to eliminate the self-intersection of the offset paths on mesh surfaces, has not been sufficiently addressed. In this paper, a mapping-based approach to eliminating the self-intersection of offset paths is proposed for the CNC machining of mesh surfaces. The method first flattens the mesh surface onto a predefined plane by using a mesh mapping technique, and then taking the mapping as a guide, the offset paths are also naturally mapped onto the plane, from which those invalid self-intersection loops can be effectively identified and eliminated. To handle the issue of self-intersection for all types of offset path, a notion of local loop is introduced to detect and eliminate the invalid self-intersection loops. After that the planar paths are inversely mapped into the physical space and the final tool paths used for the machining of mesh surface are obtained. Meanwhile, in order to improve the kinematic and dynamic performance of the machine tool when machining along the generated offset paths, a method for rounding the sharp corners of tool paths, which result from the process of eliminating the self-intersection of raw offset paths, is also preliminarily investigated. Finally, the proposed method is validated by the results of simulations and machining experiments."]},
{"title": "Adaptive meshing for finite element analysis of heterogeneous materials", "highlights": ["An adaptive meshing scheme for versatile heterogeneous materials.", "A mesh adaptation algorithm based on centroidal Voronoi tessellation.", "A specific density function aiming to equalize the material variation over mesh elements.", "An adaptive sampling technique for calculating material variations."], "abstract": ["Adaptive meshing of ", ", ", " as well as ", " has been extensively studied in Finite Element Analysis (FEA) in the past few decades. Despite the maturity of these adaptive meshing approaches, most of the existing schemes assume the domain or sub-domain of interest is ", ". In the context of ", ", traditional adaptive mesh generation strategies become inadequate as they fail to take into account the material heterogeneities. This paper is motivated to tackle such problems and propose an adaptive mesh generation scheme for FEA of versatile heterogeneous materials. The proposed approach takes full advantages of the material heterogeneity information, and the mesh density is formulated with a specific function of the material variations. Dual triangulation of centroidal Voronoi tessellation is then constructed and necessary mesh subdivision is applied with respect to a predefined material threshold. Experiments show that the proposed approach distributes the material composition variation over mesh elements as equally as possible and thus minimizes the number of elements in terms of the given material threshold. FEA results show that the proposed method can significantly decrease the mesh complexities as well as computational resources in FEA of heterogeneous objects and compared with existing approaches, significant mesh reduction can be achieved without sacrifice in FEA qualities."]},
{"title": "A polynomial Hermite interpolant for ", "highlights": ["Transcendental curves and most offsets do not admit exact NURBS representation.", "We apply Hermite interpolation to achieve ", " quasi arc-length approximation.", "Two alternative tools are considered: piecewise B\u00e9zier quintics and cubic B-splines.", "The quintic displays simple control points, with locally nonparametric arrangement.", "We approximate offsets and clothoids and compare our results with existing software."], "abstract": ["Transcendental curves, or in general those resulting from offsetting, do not admit an exact rational or polynomial representation and must hence be approximated in order to incorporate them into most commercial CAD systems. We present a simple, yet general geometric tool for polynomial approximation, based on piecewise Hermite interpolation with ", " quasi arc-length parameterization, a desirable property for robotics or CNC. We take the osculatory Hermite interpolation, prescribing position, tangent direction and curvature at the endpoints, and add quasi arc-length conditions, by imposing unit speed and vanishing tangential acceleration. These new conditions fit naturally into this scheme, yielding a quintic with B\u00e9zier points that turn out to display extremely simple geometry. In addition we consider a lower degree alternative to the quintic, namely a cubic B-spline. Finally, we include two examples of applications (the approximations of regular offsets and the clothoid) and compare our results with those from commercial systems or existing methods."]},
{"title": "Tolerance analysis by polytopes: Taking into account degrees of freedom with cap half-spaces", "highlights": ["In tolerance analysis, sets of constraints can be compliant with operand polyhedra.", "These operands are generally unbounded due to the inclusion of degrees of freedom.", "Cap half-spaces are added to each polyhedron to make it compliant with a polytope.", "The influence of the cap half-spaces on the topology of polytopes must be controlled.", "It is necessary to ensure the compliance of a mechanism in terms of requirements."], "abstract": ["To determine the relative position of any two surfaces in a system, one approach is to use operations (Minkowski sum and intersection) on sets of constraints. These constraints are made compliant with half-spaces of ", " where each set of half-spaces defines an operand polyhedron. These operands are generally unbounded due to the inclusion of degrees of invariance for surfaces and degrees of freedom for joints defining theoretically unlimited displacements. To solve operations on operands, Minkowski sums in particular, \u201ccap\u201d half-spaces are added to each polyhedron to make it compliant with a polytope which is by definition a bounded polyhedron. The difficulty of this method lies in controlling the influence of these additional half-spaces on the topology of polytopes calculated by sum or intersection. This is necessary to validate the geometric tolerances that ensure the compliance of a mechanical system in terms of functional requirements."]},
{"title": "A non-parametric approach to shape reconstruction from planar point sets through Delaunay filtering", "highlights": ["A fully automatic algorithm using the structural properties of Delaunay triangles.", "External boundary as well as an internal hole detection have been addressed.", "Demonstrate the efficacy with varying point set densities and distributions.", "Theoretical guarantee of the algorithm has been presented."], "abstract": ["In this paper, we present a fully automatic Delaunay based sculpting algorithm for approximating the shape of a finite set of points ", " in ", ". The algorithm generates a relaxed Gabriel graph (", ") that consists of most of the Gabriel edges and a few non-Gabriel edges induced by the Delaunay triangulation. Holes are characterized through a structural pattern called as ", " formed by the Delaunay triangles in the void regions. ", " is constructed through an iterative removal of Delaunay triangles subjected to circumcenter (of triangle) and topological regularity constraints in ", " time using ", " space.", "We introduce the notion of directed boundary samples which characterizes the two dimensional objects based on the alignment of their boundaries in the cavities. Theoretically, we justify our algorithm by showing that under given sampling conditions, the boundary of ", " captures the topological properties of objects having directed boundary samples. Unlike many other approaches, our algorithm does not require tuning of any external parameter to approximate the geometric shape of point set and hence human intervention is completely eliminated. Experimental evaluations of the proposed technique are done using ", " error norm measure, which is the symmetric difference between the boundaries of reconstructed shape and the original shape. We demonstrate the efficacy of our automatic shape reconstruction technique by showing several examples and experiments with varying point set densities and distributions."]},
{"title": "Free-form surface machining error compensation applying 3D CAD machining pattern model", "highlights": ["I propose the method for compensating for systematic errors of free-form surfaces.", "The basis for performing compensation is machining pattern model (MPM).", "MPM represents the most probable shape of errors in the machining process.", "MPM is available in a very convenient CAD format.", "The application of the MPM provides a high effectiveness of error compensation."], "abstract": ["The article presents a methodology for compensating for systematic influences of computer numerical control (CNC) machining processes on free-form surfaces. The proposed procedure is performed off-line by introducing corrections compensating for these influences to machining programs. The effect of systematic influences of machining are deterministic deviations of surfaces. CAD models of these deviations, averaged for a number of surfaces machined under repeatable conditions, represent a machining pattern model (MPM) which serves as the basis for performing compensation. The basis for developing such models is surface deviations determined during coordinate measurements carried out along a regular grid of points. For estimating surface MPMs, a methodology is proposed in which regression analysis, spatial statistics methods, an iterative procedure, and NURBS modeling are applied. An MPM with the opposite sign was used for compensating systematic influences of the ball-end milling process by modifying the nominal geometry data and correcting the machining program. The results of machining error compensation carried out on the basis of a previously developed MPM were compared to the results of compensation performed on the basis of raw measurement data as well as to the results these after compensation on the basis of a model of deterministic deviations for the surface under study."]},
{"title": "Heterogeneous object modeling with material convolution surfaces", "highlights": ["A material convolution surface-based approach is presented for modeling complex heterogeneous objects.", "Complex one-dimensional material-distributions are modeled with material primitives and field functions.", "Schema for compound and irregular heterogeneities in two-and three-dimensions is formulated and outlined.", "We report a few examples of complex heterogeneous object modeling for the validation of proposed approach."], "abstract": ["The possibility to attain diverse applications from heterogeneous objects calls for a generic and systematic modeling approach for design, analysis and rapid manufacturing of heterogeneous objects. The available heterogeneous object modeling techniques model simple material-distributions only and just a few of them are capable of modeling heterogeneous objects with complex geometries. Even these approaches have also, at time, shown some glitches while modeling complex objects with compound and irregular material variations. This paper unfolds the development of a stand-alone convolution surface-based modeling approach to model complex heterogeneous objects with multi-functional heterogeneities, entailing stratified sub-analytic boundary-representation, convolution material primitives, membership functions and material-potential functions. One-dimensional (associative and non-associative) and compound two-and three-dimensional material-distribution schemas are formulated and outlined to model simple, compound and irregular material-distributions in simple/complex geometry objects. The paper also illustrates a few examples of modeling complex heterogeneous objects by implementing the approach using specialized languages and software tools."]},
{"title": "An attribute-based and object-oriented approach with system implementation for change impact analysis in variant product design", "highlights": ["An integrated product model for identifying contextual impacts of design changes.", "An OO mechanism for handling dynamic and recursive change propagation processes.", "A tool to handle multiple change attempts and show the collective impact result."], "abstract": ["Variant product design is critical to the continued survival of a product in the demand-changing market. As variant product design involves numerous and concurrent attempts at requirement and product modification, a change impact analysis (CIA) becomes essential in order to capture both the potential and contextual impacts of a change proposal, and thereby ensure the consistency of the product\u2019s integrated content throughout the process of carrying out any changes. In this regard, this paper presents an attribute-based, object-oriented approach for effectively and comprehensively performing the CIA tasks in variant product design. This approach models the integrated content of a product by characterizing its components and associated requirements with attributes and linkages. It then features an object-oriented change propagation design in order to handle the dynamic and recursive CIA loops. This approach is also able to show the collective impact when multiple changes are attempted. A computerized prototype, EPCII_EC, is implemented in order to realize the approach, and an illustrative product case with a comparative evaluation is provided in order to validate the work. Issues and limitations of this study are also discussed and suggestions are provided for future studies."]},
{"title": "Degree reduction of B\u00e9zier curves with restricted control points area", "highlights": ["We propose a new approach to the problem of degree reduction of B\u00e9zier curves.", "We impose constraints of the control points area.", "The degree reduced curve is suitable for further modification and applications.", "We give and compare two methods of solving the new problem."], "abstract": ["In this paper, we propose a new approach to the problem of degree reduction of B\u00e9zier curves with respect to the least squares norm. We impose restrictions of the control points area to get more intuitive location of the control points. As a result, the degree reduced curve is suitable for further modification and applications. We present and compare two methods of solving the new problem. First one is based on quadratic programming approach and the other on BVLS algorithm. Some illustrative examples are given."]},
{"title": "A virtual fixture using a FE-based transformation model embedded into a constrained optimization for the dimensional inspection of nonrigid parts", "highlights": ["The virtual fixture method allows for the inspection of nonrigid parts.", "It does not necessitate the pre-processing of the point cloud into a FE mesh.", "It takes into account the part\u2019s specification limiting the restraining forces.", "It infers the part\u2019s structural behavior from the FE model of the nominal CAD.", "Two case studies on physical parts are performed."], "abstract": ["Virtually mounting nonrigid parts onto their fixture is proposed by researchers to remove the need for the use of complex physical inspection fixtures during the measurement process. Current approaches necessitate the pre-processing of the free-state nonrigid part\u2019s point cloud into a suitable finite element\u00a0(FE) mesh and are limited by the use of the boundary conditions setting methods available in FE software. In addition to these limits, these approaches do not take into account the forces used to restrain the part during the inspection, as commonly mandated for aerospace panels. To address these shortcomings, this paper presents a virtual fixture method that predicts the fixed shape of the part without the aforementioned drawbacks of current approaches. This is achieved by embedding information retrieved from a FE analysis of the nominal CAD model into a boundary displacement constrained optimization. To evaluate the proposed method, two case studies on physical parts are performed using the proposed virtual fixture method to evaluate the profile and assembly force specifications of each part."]},
{"title": "Enriching the semantics of variational geometric constraint data with ontology", "highlights": ["The semantics of product data cannot be automatically exchanged by existing standards.", "An ontology is constructed by formalizing VGC specifications in OWL and SWRL.", "The ontology is instantiated by the VGC data extracted from CAD systems.", "The ontology can well express the explicit semantics of the VGC data.", "Consistency checking, knowledge reasoning and semantic queries can be performed."], "abstract": ["Lack of explicit semantics in the product data to be exchanged among product development systems is a major problem for existing product data exchange standards. To solve this problem, the semantics of product data should be enriched to form a basis for exchanging them. This paper proposes an ontology-based approach to enrich the semantics of variational geometric constraint data, one of the most important kinds of product data in product development systems. In this approach, an ontology for variational geometric constraint specifications is constructed by formalizing the specifications in the variational geometric constraint network theory in Web Ontology Language 2 Description Logic (OWL 2 DL) and Semantic Web Rule Language (SWRL). This ontology has rigorous computer-interpretable semantics due to the mathematic logic-based semantics of OWL 2 DL and SWRL. It is capable of providing a semantic enrichment model for the variational geometric constraint data extracted from CAD systems. The ontology is implemented with the use of the OWL 2 DL/SWRL ontology based technologies. As the benefits of the implemented ontology, consistency checking, knowledge reasoning and semantic queries can be automatically performed. These benefits will lay a basis for further exchanging the explicit semantics of variational geometric constraint data among heterogeneous product development systems."]},
{"title": "A robust conforming NURBS tessellation for industrial applications based on a mesh generation approach", "highlights": ["A tessellation technique based on a mesh generation is described.", "A minimum number of elements is generated to encode the shape of a model as compressed as possible.", "Conformity is guaranteed by construction.", "NURBS singularities are commented and handled properly.", "Drawbacks of parametric methods are highlighted."], "abstract": ["A NURBS tessellation technique is presented with the goal to robustly approximate CAD surfaces that define the boundary of complicated three dimensional geometric shapes with a minimum number of triangles. The minimization is achieved by generating anisotropic triangles in the three dimensional space. New procedures are presented to handle numerical stability issues due to the anisotropy. The tessellation is generated using a mesh generation viewpoint, as opposed to the more classical viewpoint of graphical visualization of surfaces in CAD. This ensures topological conformity of the resulting mesh. A tiered approximation approach is used for speed and robustness. Degeneracies associated with NURBS curves and surfaces are given special attention as they occur frequently in naval and aerospace conceptual-to-early design process. Analogies with a classical mesh generation process are discussed and several numerical examples illustrate the method."]},
{"title": "An effective algorithm for constrained optimization based on optics inspired optimization (OIO)", "highlights": ["Introducing a new algorithm for constrained optimization inspired from Optics.", "The mechanism of algorithm is simple which allows its implementation easily.", "Investigating the application of the algorithm on mechanical engineering design.", "The algorithm is capable to find the global optimum of many investigated problems.", "The algorithm behaves constantly and performs more reliable than other algorithms."], "abstract": ["Due to the law of reflection, the converging and diverging behavior of concave and convex mirrors causes that curved mirrors show different image types. The optics inspired optimization (OIO) is a recently proposed algorithm for unconstrained optimization which treats the surface of the function to be optimized as a wavy mirror in which each peak is assumed to reflect as a convex mirror and each valley to reflect as a concave one. Each individual is treated as an artificial light point that its glittered ray is reflected back by the function surface, given that the surface is convex or concave, and the artificial image (a new solution) is formed based on mirror equations adopted from Optics. There are several constraint handling techniques which have been proposed for handling infeasible solutions. However, these techniques may suffer from problem dependency, no unique way for designing their operators, no unique way for updating their internal parameters, increasing the computational complexity, etc. To equip OIO with a mechanism to handle constraints and to avoid the drawbacks of typical techniques, a feasibility measure is used beside the objective function value to bias the search toward feasible regions. Such a consideration requires to modify several modules in the basic OIO algorithm. To increase the probability to generate better solutions, a number of alternative solutions are produced from each individual and one is selected based on the sequential use of modified Deb\u2019s tournament selection. Besides, Deb\u2019s tournament selection rule is used in place of the greedy selection in basic OIO, along with allowing the survival of individuals with a good value of the objective function, regardless of their feasibility. Performance of the proposed algorithm is compared with a number of noticeable algorithms such as COPSO, ECHT-EP2, ", "Simplex etc, on CEC 2006 and CEC 2010 set of benchmark problems and on a set of mechanical design optimization problems. Results demonstrate that the proposed algorithm performs the global optimization task very well and competitive. Such an outcome encourages that further developments and applications of OIO would be worth to realize its full potency in the future studies."]},
{"title": "Multiple views system to support awareness for cooperative design", "highlights": ["We devise a cooperative awareness model to describe cooperative awareness information in product design.", "We propose concept of awareness intensity and an object-oriented method to identify and filter overloaded cooperative awareness model.", "A mechanism which responds to changes of lean cooperative awareness information is proposed to plan and execute task in product design.", "A navigation net managed by a multiple views system is proposed to browser lean cooperative awareness information set."], "abstract": ["It is acknowledged that multiple views technology improves designer\u2019s work efficiency by filtering redundant information. However, the increased need of cooperative activities in product design process requires views incorporating Cooperative awareness information (CAI), content of which should be lean for understanding and sortable according to importance. To achieve this target, this paper proposes an Object-based cooperative awareness model (OBCWM) and corresponding multiple views system. Requirements of CAI in product design are firstly analyzed. Then OBCWM is proposed, which consists of a CPM (Core product model) based concept framework. By use of CPM, an algorithm to calculate awareness intensity is developed based on object-oriented method. To support the mechanism of OBCWM, a multiple views system is then designed with \u201cflexible view\u201d, content of which is adjustable according to awareness intensity between CAI objects. After introduction to the system, application on a case is presented, followed by the evaluation of this system."]},
{"title": "A new Steiner patch based file format for Additive Manufacturing processes", "highlights": ["A new Steiner patch based Additive Manufacturing file format has been developed.", "Steiner format uses triangular rational Bezier representation of Steiner patches.", "Steiner format has high geometric fidelity and low approximation error.", "The Steiner patches can be easily sliced and closed form solutions can be obtained.", "AM parts manufactured using Steiner format has very low profile and form errors."], "abstract": ["Additive Manufacturing (AM) processes adopt a layering approach for building parts in continuous slices and use the Standard Tessellation Language (STL) file format as an input to generate the slices during part manufacturing. However, the current STL format uses planar triangular facets to approximate the surfaces of the parts. This approximation introduces errors in the part representation which leads to additional errors downstream in the parts produced by AM processes. Recently, another file format called Additive Manufacturing File (AMF) was introduced by ASTM which seeks to use curved triangles based on second degree Hermite curves. However, while generating the slices for manufacturing the part, the curved triangles are recursively sub-divided back to planar triangles which may lead to the same approximation error present in the STL file. This paper introduces a new file format which uses curved Steiner patches instead of planar triangles for not only approximating the part surfaces but also for generating the slices. Steiner patches are bounded Roman surfaces and can be parametrically represented by rational Bezier equations. Since Steiner surfaces are of higher order, this new Steiner file format will have a better accuracy than the traditional STL and AMF formats and will lead to lower Geometric Dimensioning and Tolerancing (GD&T) errors in parts manufactured by AM processes. Since the intersection of a plane and the Steiner patch is a closed form mathematical solution, the slicing of the Steiner format can be accomplished with very little computational complexity. The Steiner representation has been used to approximate the surfaces of two test parts and the chordal errors in the surfaces are calculated. The chordal errors in the Steiner format are compared with the STL and AMF formats of the test surfaces and the results have been presented. Further, an error based adaptive tessellation algorithm is developed for generating the Steiner representation which reduces the number of curved facets while still improving the accuracy of the Steiner format. The test parts are virtually manufactured using the adaptive Steiner, STL and AMF format representations and the GD&T errors of the manufactured parts are calculated and compared. The results demonstrate that the modified Steiner format is able to significantly reduce the chordal and profile errors as compared to the STL and AMF formats."]},
{"title": "Spiral and conformal cooling in plastic injection molding", "highlights": ["Spiral conformal cooling channels with high speed coolants.", "An efficient algorithm to generate smooth spiral curves on free-form surfaces.", "Spiral curves are governed by an approximated boundary-distance-map (BDM).", "Region decomposition for covering by contours of BDM."], "abstract": ["Designing cooling channels for the thermoplastic injection process is a very important step in mold design. A conformal cooling channel can significantly improve the efficiency and the quality of production in plastic injection molding. This paper introduces an approach to generate spiral channels for conformal cooling. The cooling channels designed by our algorithms has very simple connectivity and can achieve effective conformal cooling for the models with complex shapes. The axial curves of cooling channels are constructed on a free-form surface conformal to the mold surface. With the help of boundary-distance maps, algorithms are investigated to generate evenly distributed spiral curves on the surface. The cooling channels derived from these spiral curves are conformal to the plastic part and introduce nearly no reduction at the rate of coolant flow. Therefore, the channels are able to achieve uniform mold cooling. Moreover, by having simple connectivity, these spiral channels can be fabricated by copper duct bending instead of expensive selective laser sintering."]},
{"title": "Optimizing conformality of NURBS surfaces by general bilinear transformations", "highlights": ["We give explicit representations of the general bilinear reparameterized surfaces.", "A scheme is given to construct the general bilinear reparameterized NURBS surfaces.", "An optimization method is given to improve the conformality of NURBS surfaces."], "abstract": ["The conformality of NURBS surfaces greatly affects the results of rendering and tessellation applications. To improve the conformality of NURBS surfaces, an optimization algorithm using general bilinear transformations is presented in this paper. The conformality energy is first formulated and its numerical approximation is then constructed using the composite simpson\u2019s rule. The initial general bilinear transformation is obtained by approximating the conformal mapping of its 3D discretized mesh using a least square method, which is further optimized by the Levenberg\u2013Marquardt method. Examples are given to show the performance of our algorithm for rendering and tessellation applications."]},
{"title": "Shape reconstruction from a normal map in terms of uniform bi-quadratic B-spline surfaces", "highlights": ["The mathematical formulation of our method is simple.", "The computation is not only accurate but also fast.", "Less memory is required to store the data.", "Aesthetic surfaces are generated via direct editing of normal vectors."], "abstract": ["This paper studies the recovery of the height field function of an object from its surface normal vectors in terms of uniform bi-quadratic B-spline surfaces. The study is motivated by two modern technological needs namely, the recovery of the height field function of an object from its surface normal vectors obtained by photometric stereo techniques, and the introduction of new design methodology of streamlined aesthetic surfaces through the direct editing of normal vectors. Our mathematical formulation for explicit surface reconstruction is based on differential geometry and geometric modeling techniques. Complex examples are provided to demonstrate the effectiveness of the proposed method."]},
{"title": "An analytical representation of conformal mapping for genus-zero implicit surfaces and its application to surface shape similarity assessment", "highlights": ["We develop an analytical representation of conformal mapping for implicit surfaces.", "An analytical conformal mapping describes a continuous one-to-one correspondence.", "Conformal mappings from complex surfaces to simple ones are effectively obtained.", "Conformal mappings between two similar complex surfaces are effectively obtained."], "abstract": ["This paper develops an analytical representation of conformal mapping for genus-zero implicit surfaces based on algebraic polynomial functions, and its application to surface shape similarity assessment. Generally, the conformal mapping often works as a tool of planar or spherical parameterization for triangle mesh surfaces. It is further exploited for implicit surface matching in this study. The method begins with discretizing one implicit surface by triangle mesh, where a discrete harmonic energy model related to both the mesh and the other implicit surface is established based on a polynomial-function mapping. Then both the zero-center constraint and the landmark constraints are added to the model to ensure the uniqueness of mapping result with the M\u00f6bius transformation. By searching optimal polynomial coefficients with the Lagrange\u2013Newton method, the analytical representation of conformal mapping is obtained, which reveals all global and continuous one-to-one correspondent point pairs between two implicit surfaces. Finally, a shape similarity assessment index for (two) implicit surfaces is proposed through calculating the differences of all the shape index values among those corresponding points. The proposed analytical representation method of conformal mapping and the shape assessment index are both verified by the simulation cases for the closed genus-zero implicit surfaces. Experimental results show that the method is effective for genus-zero implicit surfaces, which will offer a new way for object retrieval and manufactured surface inspection."]},
{"title": "A physically-based model for global collision avoidance in 5-axis point milling", "highlights": ["A new approach is proposed for collision avoidance in 5-axis end milling.", "The method is based on a physical modeling to compute the tool axis orientation.", "The ball-end tool is considered as a rigid body moving in 3D space.", "Repulsive forces are deriving from a scalar potential linked to check surfaces.", "Check surfaces tessellation ensures smooth variations of the tool axis orientation."], "abstract": ["Although 5-axis free form surface machining is commonly proposed in CAD/CAM software, several issues still need to be addressed and especially collision avoidance between the tool and the part. Indeed, advanced user skills are often required to define smooth tool axis orientations along the tool path in high speed machining. In the literature, the problem of collision avoidance is mainly treated as an iterative process based on local and global collision tests with a geometrical method. In this paper, an innovative method based on physical modeling is used to generate 5-axis collision-free smooth tool paths. In the proposed approach, the ball-end tool is considered as a rigid body moving in the 3D space on which repulsive forces, deriving from a scalar potential field attached to the check surfaces, and attractive forces are acting. A study of the check surface tessellation is carried out to ensure smooth variations of the tool axis orientation. The proposed algorithm is applied to open pocket parts such as an impeller to emphasize the effectiveness of this method to avoid collision."]},
{"title": "Direct diffeomorphic reparameterization for correspondence optimization in statistical shape modeling", "highlights": ["We propose an approach for optimizing shape correspondence across a population.", "B-splines are used for shape representation and reparameterization.", "The quality measure of the statistical shape model is the description length.", "An adjoint method for deriving analytical sensitivity is developed.", "The approach improves shape correspondence in a group-wise manner."], "abstract": ["In this paper, we propose an efficient optimization approach for obtaining shape correspondence across a group of objects for statistical shape modeling. With each shape represented in a B-spline based parametric form, the correspondence across the shape population is cast as an issue of seeking a reparameterization for each shape so that a quality measure of the resulting shape correspondence across the group is optimized. The quality measure is the description length of the covariance matrix of the shape population, with landmarks sampled on each shape. The movement of landmarks on each B-spline shape is controlled by the reparameterization of the B-spline shape. The reparameterization itself is also represented with B-splines and B-spline coefficients are used as optimization parameters. We have developed formulations for ensuring the bijectivity of the reparameterization. A gradient-based optimization approach is developed, including techniques such as constraint aggregation and adjoint sensitivity for efficient, direct diffeomorphic reparameterization of landmarks to improve the group-wise shape correspondence. Numerical experiments on both synthetic and real 2D and 3D data sets demonstrate the efficiency and effectiveness of the proposed approach."]},
{"title": "Analysis of an EMST-based path for 3D meshes", "highlights": ["We analysed sensitivity of the EMST structure to obtain a more robust synchronization.", "We computed how a vertex can be moved without changing the connections.", "We present a new theoretical analysis and a way to visualize EMST robustness.", "We detect fragile area and to predict the 3D object robustness.", "Keywords are Euclidean minimum spanning tree, Sensitivity analysis, Synchronization."], "abstract": ["For several 3D data applications such as data-hiding or compression, data ordering is a major problem. We need to know how to achieve the same 3D mesh path between the coding and decoding stages. Various algorithms have been proposed in recent years, but we focus on methods based on Euclidean Minimum Spanning Trees (EMST). In this paper, we analyse the sensitivity of the EMST structure to obtain a more robust synchronization. We present a new theoretical analysis and a way to visualize EMST robustness. Moreover, this analysis can be useful in 3D data-hiding in order to detect fragile area and to predict the 3D object robustness during transmission on a noisy channel."]},
{"title": "Canal surfaces with rational contour curves and blends bypassing the obstacles", "highlights": ["The rationality of generalized contours on rational canal surfaces is studied.", "The contour method is used for computing PN blends between two canal surfaces.", "The constructed blends can easily satisfy certain constrains, e.g. avoiding obstacles.", "Only one SOS decomposition for all canal surfaces with the same silhouette is needed."], "abstract": ["In this paper, we will present an algebraic condition, see ", ", which guarantees that a canal surface, given by its rational medial axis transform (MAT), possesses rational generalized contours (i.e., contour curves with respect to a given viewpoint). The remaining computational problem of this approach is how to find the right viewpoint. The canal surfaces fulfilling this distinguished property are suitable for being taken as modeling primitives when some rational approximations of canal surfaces are required. Mainly, we will focus on the low-degree cases such as quadratic and cubic MATs that are especially useful for applications. To document a practical usefulness of the presented approach, we designed and implemented two simple algorithms for computing rational offset blends between two canal surfaces based on the contour method which do not need any further advanced formalism (as e.g. interpolations with MPH curves). A main advantage of the designed blending technique is its simplicity and also an adaptivity to choose a suitable blend satisfying certain constrains (avoiding obstacles, bypassing other objects, etc.). Compared to other similar methods, our approach requires only one SOS decomposition for the whole family of rational canal surfaces sharing the same silhouette, which significantly simplifies the computational complexity."]},
{"title": "Approaches for the assembly simulation of skin model shapes", "highlights": ["Skin Model Shapes are digital part representatives comprising geometric deviations.", "Approaches for the relative positioning of point-based Skin Model Shapes are proposed.", "The approaches ground on algorithms from computational geometry and computer graphics.", "Applications for the assembly simulation in tolerancing are given."], "abstract": ["Even though they are weakly noticed, geometric part deviations accompany our everyday life. These geometric deviations affect the assemblability and functional compliance of products, since small part variations accumulate through large-scale assemblies and lead to malfunction as well as decreased product reliability and safety. However, the consideration of part deviations in the virtual modelling of mechanical assemblies is an ongoing challenge in computer-aided tolerancing research. This is because the resulting assembly configurations for variant parts are far more complicated than for nominal assemblies. In this contribution, two approaches for the relative positioning of point based models are highlighted and adapted to the assembly simulation of Skin Model Shapes, which are specific workpiece representatives considering geometric deviations. The first approach employs constrained registration techniques to determine the position of variant parts in an assembly considering multiple assembly steps simultaneously, whereas the second utilizes the difference surface to solve the positioning problem sequentially. The application of these approaches to computer-aided tolerancing is demonstrated, though their applicability reaches various fields of industrial geometry."]},
{"title": "On the equilibrium of funicular polyhedral frames and convex polyhedral force diagrams", "highlights": ["Three-dimensional extension of graphic statics using polyhedral form and force diagrams.", "Defining the topological and geometrical relationships of 3D reciprocal diagrams.", "Design of compression and tension-only spatial structures with externally applied loads.", "Designing complex funicular spatial forms by aggregating convex force polyhedral cells.", "CAD implementation to manipulate the geometry of the force and explore its effects on the forms."], "abstract": ["This paper presents a three-dimensional extension of graphic statics using polyhedral form and force diagrams for the design of compression-only and tension-only spatial structures with externally applied loads. It explains the concept of 3D structural reciprocity based on Rankine\u2019s original proposition for the equilibrium of spatial frames. It provides a definition for polyhedral reciprocal form and force diagrams that allows including external forces and discusses their geometrical and topological characteristics. This paper furthermore provides a geometrical procedure for constructing a pair of reciprocal polyhedral diagrams from a given polyhedron representing either the form or force diagram of a structural system. Using this method, this paper furthermore suggests a design strategy for finding complex funicular spatial forms in pure compression (or tension), based on the construction of force diagrams through the aggregation of convex polyhedral cells. Finally, it discusses the effect of changes in the geometry of the force diagram on the geometry of the form diagram and the distribution of forces in it."]},
{"title": "Support slimming for single material based additive manufacturing", "highlights": ["Optimize the shape of a designed model into a \u2018self-supported\u2019 state for AM.", "Global shape of a model is preserved by minimizing the energy of rigidity.", "A closed-form solution for minimal rotation to drive the optimization.", "Tackle the shape optimization problem for reducing supporting structures."], "abstract": ["In layer-based additive manufacturing (AM), supporting structures need to be inserted to support the overhanging regions. The adding of supporting structures slows down the speed of fabrication and introduces artifacts onto the finished surface. We present an orientation-driven shape optimizer to slim down the supporting structures used in single material-based AM. The optimizer can be employed as a tool to help designers to optimize the original model to achieve a more self-supported shape, which can be used as a reference for their further design. The model to be optimized is first enclosed in a volumetric mesh, which is employed as the domain of computation. The optimizer is driven by the operations of reorientation taken on tetrahedra with \u2018facing-down\u2019 surface facets. We formulate the demand on minimizing shape variation as global rigidity energy. The local optimization problem for determining a minimal rotation is analyzed on the Gauss sphere, which leads to a closed-form solution. Moreover, we also extend our approach to create the functions of controlling the deformation and searching for optimal printing directions."]},
{"title": "HiGeoM: A symbolic framework for a unified function space representation of trivariate solids for isogeometric analysis", "highlights": ["We propose a unified function space representation of trivariate solids and its attributes.", "We develop an approach for ab-initio construction of trivariate description of geometry using generative modeling techniques.", "The analysis of the constructed geometries enables isogeometric analysis without need for finite element mesh.", "We develop a symbolic modeling framework to construct trivariate solids and carry out isogeometric analysis."], "abstract": ["In this paper, a procedural description of solids is proposed with representational domain that encompasses the needs in Computer Aided Design (CAD), heterogeneous object modeling and Computer Aided Engineering (CAE). Specifically, a function space formalism, that unifies the representation of geometry and physical attributes, is proposed along with an algebra that operates on elements in those spaces. A declarative programming language is provided, which accepts a subset of mathematical semantics specified using the Mathematical Markup Language (MathML) as well as a fortran-like syntax. The language enables a high-level definition of the geometric model using continuous affine transformation and boolean operators leading to a mixed-dimensional representation. Thus, it is possible to construct analytically defined shapes as well as spline representation of complex geometry. The framework is demonstrated by constructing complex engineered solids and isogeometrically solving boundary value problems on the constructed solid."]},
{"title": "Resource based process planning for additive manufacturing", "highlights": ["Resource based process planning model is proposed for Additive manufacturing.", "Optimum build direction is determined for free-form object.", "Deposition directions are optimized for generated slices.", "Proposed methodology minimizes the resource consumption during part fabrication.", "Better build time and length are found during implementation."], "abstract": ["Resource consumption in additive manufacturing (AM) is often tied with the physical attribute of the fabricated part. Thus, optimizing the processes plan for minimum part fabrication resource requirement is a matter of great interest. In this work, the hierarchical nature of the AM process plan steps are emphasized and both build direction and material deposition direction are optimized while considering the resource requirement. A novel combined two-step optimization methodology is presented to determine optimal build direction for the object and material deposition direction for layers while considering minimum contour plurality, surface quality, build height, fabrication factor, and layer contour concavity to compensate the fabrication and resource limitations. The proposed methodology is implemented for an example object and the results indicate that the total build time can be significantly reduced compared to the total build time associated with arbitrary build and deposition directions."]},
{"title": "The status, challenges, and future of additive manufacturing in engineering", "highlights": ["The fundamental attributes and challenges/barriers of Additive Manufacturing (AM).", "The evolution of research on AM with a focus on engineering capabilities.", "The affordances enabled by AM such as geometry, material and tools design.", "The developments in industry, intellectual property, and education-related aspects.", "The important future trends of AM technologies."], "abstract": ["Additive manufacturing (AM) is poised to bring about a revolution in the way products are designed, manufactured, and distributed to end users. This technology has gained significant academic as well as industry interest due to its ability to create complex geometries with customizable material properties. AM has also inspired the development of the maker movement by democratizing design and manufacturing. Due to the rapid proliferation of a wide variety of technologies associated with AM, there is a lack of a comprehensive set of design principles, manufacturing guidelines, and standardization of best practices. These challenges are compounded by the fact that advancements in multiple technologies (for example materials processing, topology optimization) generate a \u201cpositive feedback loop\u201d effect in advancing AM. In order to advance research interest and investment in AM technologies, some fundamental questions and trends about the dependencies existing in these avenues need highlighting. The goal of our review paper is to organize this body of knowledge surrounding AM, and present current barriers, findings, and future trends significantly to the researchers. We also discuss fundamental attributes of AM processes, evolution of the AM industry, and the affordances enabled by the emergence of AM in a variety of areas such as geometry processing, material design, and education. We conclude our paper by pointing out future directions such as the \u201cprint-it-all\u201d paradigm, that have the potential to re-imagine current research and spawn completely new avenues for exploration."]},
{"title": "A tool path generation method for freeform surface machining by introducing the tensor property of machining strip width", "highlights": ["A regional based tool path generation method is proposed and tested.", "A tensor is derived to evaluate machining strip width using ball end mill.", "A surface division method is presented based on machining strip width tensor field."], "abstract": ["Due to the complexity of geometry, the feed direction with maximal machining strip width usually varies among different regions over a freeform surface or a shell of surfaces. However, in most traditional tool path generation methods, the surface is treated as one machining region thus only local optimisation might be achieved. This paper presents a new region-based tool path generation method. To achieve the full effect of the optimal feed direction, a surface is divided into several sub-surface regions before tool path computation. Different from the scalar field representation of the machining strip width, a rank-two tensor field is derived to evaluate the machining strip width using ball end mill. The continuous tensor field is able to represent the machining strip widths in all feed directions at each cutter contact point, except at the boundaries between sub-regions. Critical points where the tensor field is discontinuous are defined and classified. By applying critical points in the freeform surface as the start for constructing inside boundaries, the surface could be accurately divided to such that each region contain continuous distribution of feed directions with maximal machining strip width. As a result, tool paths are generated in each sub-surface separately to achieve better machining efficiency. The proposed method was tested using two freeform surfaces and the comparison to several leading existing tool path generation methods is also provided."]},
{"title": "Conformance checking of PMI representation in CAD model STEP data exchange files", "highlights": ["Geometric and dimensional tolerances (PMI) are represented in STEP files.", "Correct implementation of PMI ensures interoperability with downstream applications.", "A software tool has been developed for conformance checking of PMI in STEP files."], "abstract": ["Recommended practices supplement data exchange standards by providing common implementation guidance associated with specific requirements. ISO 10303 (STEP) product data exchange files that conform to recommended practices ensure interoperability between computer-aided design (CAD) systems and with downstream applications such as manufacturing and inspection. Correct implementation of product and manufacturing information (PMI)\u2013annotations associated with a CAD model\u2019s edges and faces such as geometric tolerances, dimensional tolerances, and datum features\u2013in CAD authoring systems and translators is essential for interoperability. This paper discusses an approach implemented in a software tool for checking the conformance of STEP files to the recommended practice for PMI representation."]},
{"title": "Representation and analysis of additively manufactured parts", "highlights": ["Representation of as-manufactured models using convolution.", "Incorporating manufacturing uncertainty into representation.", "Computing as-manufactured models using planar morphological operations.", "Interoperable analysis using query based simulation on sliced as-manufacturable models."], "abstract": ["Representations of solid models were initially formulated partially in response to the need to support automation for numerically controlled machining processes. The assumed equivalence between shape, topology, and material properties of manufactured components and their computer representations led to the practice of modeling and simulating the behavior of physical parts before manufacture. In particular, representations of shape and material properties are treated in distinct nominal models for most unit manufacturing processes. Additively manufactured parts usually exhibit deviations from their nominal geometry in the form of stair-stepping artifacts and topological irregularities in the vicinity of small features. Furthermore, structural properties of additively manufactured parts have experimentally been shown to be dependent on the build orientation defining the cross sections where material is accumulated. Therefore geometric models of additively manufactured parts cannot be decoupled from the manufacturing process plan.", "In this paper we show that ", " shapes may be represented in terms of the convolution operation to capture the additive deposition of material, measure the conformance to nominal geometry in terms of overlap volume, and model uncertainties involved in material flow and process control. We then demonstrate a novel interoperable approach to physical analysis on as-manufactured part geometry represented as a collection of machine-specific cross sections augmented with boundary conditions defined on the nominal geometry. The analysis only relies on fundamental queries of point membership classification and distance to boundary and therefore does not involve the overhead of model preparation required in approaches such as finite element analysis. Results are shown for non-trivial geometries to validate the proposed approach."]},
{"title": "Extending CSG with projections: Towards formally certified geometric modeling", "highlights": ["Extension of classical CSG with the projection operator.", "Support for gradient computations.", "Topological property computation.", "Application of formal methods like interval analysis and proof assistants to CSG models."], "abstract": ["We extend traditional Constructive Solid Geometry (CSG) trees to support the projection operator. Existing algorithms in the literature prove various topological properties of CSG sets. Our extension readily allows these algorithms to work on a greater variety of sets, in particular parametric sets, which are extensively used in CAD/CAM systems. Constructive Solid Geometry allows for algebraic representation which makes it easy for certification tools to apply. A geometric primitive may be defined in terms of a characteristic function, which can be seen as the zero-set of a corresponding system along with inequality constraints. To handle projections, we exploit the Disjunctive Normal Form, since projection distributes over union. To handle intersections, we transform them into disjoint unions. Each point in the projected space is mapped to a contributing primitive in the original space. This way we are able to perform gradient computations on the boundary of the projected set through equivalent gradient computations in the original space. By traversing the final expression tree, we are able to automatically generate a set of equations and inequalities that express either the geometric solid or the conditions to be tested for computing various topological properties, such as homotopy equivalence. We conclude by presenting our prototype implementation and several examples."]},
{"title": "Topology authentication for piping isometric drawings", "highlights": ["The topology authentication problem of piping isometric drawings is addressed.", "A semi-fragile watermarking scheme for the topology authentication is proposed.", "The proposed scheme is robust against local similarity transformations.", "The proposed scheme is invariant to the stretching operation on pipes.", "The proposed scheme provides good tamper localization accuracy."], "abstract": ["Piping isometric drawings, which feature their intrinsical topological relation rather than just geometrical shape, are important industrial art works in the field of Computer-Aided Design (CAD). This paper takes a fresh look at the topology integrity authentication of piping isometric drawings, which has not been mentioned before in the literature, from the digital watermarking perspective. A blind and semi-fragile watermarking based algorithm is proposed to address the referred interesting issue. The topology authentication problem of piping isometric drawings is investigated. In addition to the stretching operation, both global and local similarity transformation operations, which are critical problems in the case of watermarking embedding and extraction, are analyzed in detail. The topological graph is extracted and constructed from the drawing firstly. Then, similarity transformation invariants are constructed as watermarks carriers for each node. After that, the topological relation among joint components is encoded into singular watermarks for each node of the graph. These generated topology sensitive watermarks are embedded into geometrical invariants of each node via quantization index modulation. Theoretical analysis and experimental results demonstrate that our approach yields a strong ability in detecting and locating unauthorized topology attacks while achieves robustness against both global and local similarity transformations especially the stretching operation. The proposed scheme can be employed to authenticate topology integrity for each of the drawings derived from the model individually in industry practices."]},
{"title": "Conformal bubbler cooling for molds by metal deposition process", "highlights": ["Mold design with conformal cooling have been developed.", "Simulations have been done to make mold hollow but strong enough for all forces.", "Manufacturability is tested using metal deposition technique.", "Temperature distributions are uniform over the mold surfaces from the tests."], "abstract": ["Molds with conformal bubbler cooling channels have been developed in order to reduce the cycle time of the plastic injection process, which in turn has increased the production rate. In addition, part warpage can be greatly reduced in the injection process because the temperature distribution is uniform over the mold surfaces. Molds with conformal cooling designs, however, are still limited to computer simulations for generating the cooling channels or calculating the cooling rates ", ", ", ", ", ", ", ". This is mainly because there is \u201cvirtually\u201d no fabrication technique that can effectively make the complicated conformal cooling channels. Even though metal deposition processes have the potential to create such complex mold shapes, most of the metal deposition processes are still in the developmental stage in laboratories. Molds formed by some of the metal deposition processes are not suitable for real industry use because the costs are still too expensive. This paper describes how to create a mold with conformal bubbler cooling tunnels through a metal deposition process that can actually be used in plastic injection industries. Experiments are conducted to confirm the normal temperature distribution over the mold surfaces."]},
{"title": "Missing sets in rational parametrizations of surfaces of revolution", "highlights": ["We give a simple description of the missing area of ruled surface parametrization.", "We provide algorithms to compute, in each case, the missing area.", "We analyze the real and the complex case."], "abstract": ["Parametric representations do not cover, in general, the whole geometric object that they parametrize. This can be a problem in practical applications. In this paper we analyze the question for surfaces of revolution generated by real rational profile curves, and we describe a simple small superset of the real zone of the surface not covered by the parametrization. This superset consists, in the worst case, of the union of a circle and the mirror curve of the profile curve."]},
{"title": "Next viewing directions for the scanning of dental impressions", "highlights": ["Dental impression scanning with the minimum number of scanning operations.", "Graphics board utilization to accelerate the evaluation of visibility.", "Missing area identification from range images.", "Next viewing directions from the visibility of missing areas.", "Proposed procedure is more than twice faster compared to conventional method."], "abstract": ["This paper proposes a scanning procedure for a structured light system (SLS) to measure dental impressions. Although increasing the number of scanning orientations may improve the quality of a scanned model, it is desirable to minimize the number of scanning operations to time and data storage. We attempt to reduce the number of scanning operations to the least number that will still acquire a complete model. The proposed procedure must resolve two sub-problems: (1) identification of missing areas from given range images, and (2) determination of the next viewing directions to fill those missing areas. If we consider range images as triangular meshes, the sub-problems can be solved by using well-known geometric algorithms. The triangular meshes, however, may consist of tens of millions of triangles, which require an unacceptably long time to compute. To cope with this problem, we propose two key ideas: (1) utilizing an inherent attribute of a range image, the map structure; (2) utilizing a graphics board to accelerate the evaluation of visibility. Our demonstration proves that the proposed approach improves the quality of scanned models and reduces the number of scanning operations."]},
{"title": "Laser path calculation method on triangulated mesh for repair process on turbine parts", "highlights": ["Apply geodesic methods in laser path generation for repair of turbine components.", "Improve MMP and FMM algorithms on STL considering requirements of laser process.", "Propose an approximate algorithm to rapidly generate laser paths within tolerance.", "Methods are applied to repair cases on tip, platform and aerofoil of turbine blade."], "abstract": ["Laser metal deposition is a very effective and precise technology applied by the manufacturers of turbomachinery parts to repair damages on parts in service or reclaim quality during manufacturing of new parts. The process chains to repair or refurbish turbomachinery parts are based upon a 3D laser scanning process which acquires digitalized models in the format of triangulated-meshes to represent the geometry of parts. This paper firstly studies a discrete geodesic method proposed by Mitchell, Mount and Papadimitriou (MMP) and based on this method proposes an optimized exact algorithm to generate precise laser paths on triangulated-mesh data of part geometry. Furthermore this paper presents an approximate method based on Sethian\u2019s Fast Marching Method (FMM) and constructs vertex-based geodesic distance field on triangulated-meshes to divide the part surface in equal-distant stripes morphed from boundary curves. These two methods are compared and error condition is evaluated. During the research the methods are implemented into an automated LMD process planning software, which is utilized into industrial applications and obtained satisfactory results. The results from application of LMD process planning software are also demonstrated in this paper."]},
{"title": "Curve fitting and optimal interpolation for CNC machining under confined error using quadratic B-splines", "highlights": ["The explicit Hausdorff distance of a line segment and a quadratic curve is given.", "G01 codes can be fitted by quadratic B-splines with confined error.", "We combine the tool path generating and optimal velocity planning in one step.", "We simulate the manufacture process with our proposed method."], "abstract": ["In CNC machining, fitting the polyline machining tool path with parametric curves can be used for smooth tool path generation and data compression. In this paper, an optimization problem is solved to find a quadratic B-spline curve whose Hausdorff distance to the given polyline tool path is within a given precision. Furthermore, adopting time parameter for the fitting curve, we combine the usual two stages of tool path generation and optimal velocity planning to derive a one-step solution for the CNC optimal interpolation problem of polyline tool paths. Compared with the traditional decoupled model of curve fitting and velocity planning, experimental results show that our method generates a smoother path with minimal machining time."]},
{"title": "Preferred feed direction field: A new tool path generation method for efficient sculptured surface machining", "highlights": ["Tool path generation according to a preferred feed direction field of the surface.", "Generated tool paths having comparatively short overall tool path length.", "Surface segmentation by identifying degenerate points and forming their separatrices."], "abstract": ["This paper presents a new method to generate efficient ball-end milling tool paths for three-axis sculptured surface machining. The fundamental principle of the presented method is to generate the tool paths according to a preferred feed direction (PFD) field derived from the surface to be machined. The PFD at any point on the surface is the feed direction that maximizes the machining strip width. Theoretically, tool paths that always follow the direction of maximum machining strip width at every cutter contact point on the surface would result in shorter overall tool path length. Unfortunately, overlaps of adjacent machining strips commonly exist for tool paths that follow the preferred directions exactly. Such redundant machining can be reduced by iso-scallop tool paths. Nonetheless, iso-scallop tool paths do not in general follow the preferred feed directions. To improve machining efficiency via generating short overall tool path length, the presented method analyzes the PFD field of the surface and segments the surface into distinct regions by identifying the degenerate points and forming their separatrices. The resulting segmented regions are characterized by similar PFD\u2019s and iso-scallop tool paths are then generated for each region to mitigate redundant machining. The developed method has been validated with numerous case studies. The results have shown that the generated tool paths consistently have shorter overall length than those generated by the existing methods."]},
{"title": "Combining volumetric dental CT and optical scan data for teeth modeling", "highlights": ["A novel teeth modeling framework by combining optical scan data and dental CT images is introduced.", "Co-segmentation between optical scan data and dental CT images is performed by the graph-cut method simultaneously.", "The proposed algorithm is automatic and time efficient, and shows high fidelity.", "Teeth data with defects such as metal artifacts can be completed successfully."], "abstract": ["Dental computer-aided design (CAD) systems have been intensively introduced to digital dentistry in recent years. As basic digital models, volumetric computed tomography (CT) images or optical surface scan data are used in most dental fields. In many fields, including orthodontics, complete teeth models are required for the diagnosis, planning and treatment purposes. In this research, we introduce a novel modeling approach combining dental CT images and an optically scanned surface to create complete individual teeth models. First, to classify crown and root regions for each set of data, corresponding pairs between two different data are determined based on their spatial relationship. The pairs are used to define the co-segmentation energy by introducing similarity and dissimilarity terms for each corresponding pair. Efficient global optimization can be performed by formulating a graph-cut problem to find the segmentation result that minimizes the energy. After classifying crown and root regions for each data set, complete individual teeth are obtained by merging the two different data sets. The advancing front method was successfully applied for merging purposes by considering the signed distance from the crown boundary of the surface mesh to the root surface of the CT. The teeth models which have detailed geometries obtained from the optically scanned surface and interstice regions recovered from volumetric data can be obtained using the proposed method. In addition, the suggested merging approach makes it possible to obtain complete teeth models from incomplete CT data with metal artifacts."]},
{"title": "Review and taxonomies of assembly and disassembly path planning problems and approaches", "highlights": ["State-of-the-art review of the Assembly/Disassembly Path Planning (APP/DAPP) field.", "New taxonomies for categorizing APP/DAPP problem types and solution methods.", "Critical discussions on research trends, applications and open problems in APP/DAPP."], "abstract": ["Assembly Planning (AP) is one of the most important elements of process planning in manufacturing industries, and is defined as the process of creating a detailed assembly plan to craft a whole product from separate parts considering the final product geometry, available resources, fixture design, feeder and tool descriptions, etc. AP has three main subproblems: (1) Assembly Sequence Planning (ASP), in which a sequence of collision-free operations is computed for bringing assembly parts together, (2) Assembly Line Balancing (ALB), in which some groups of subassemblies are formed and assigned to assembly stations in a way that their workloads are balanced, and (3) Assembly Path Planning (APP), in which collision-free paths for adding parts to a subassembly are computed. Each of the above subproblems has a disassembly version, creating DASP, DALB, and DAPP problems. All of the above problems have proven to be either NP-hard or NP-Complete, and many researches have been conducted to solve them efficiently. While some surveys and reviews exist on the ASP/DASP and ALB/DALB problems, no comprehensive survey exists for APP/DAPP problems, despite their important role in the design process of products as invaluable tools for deploying concurrent engineering, end-of-life processing, maintenance and repair, and decreasing the cost and time of manufacturing products. This paper investigates the relations between the above six subproblems and reviews the state-of-the-art of the APP and DAPP problems and their solution approaches. Through two new taxonomies the properties and categories of APP/DAPP problems and solution approaches are identified and described, the characteristics and applications of the reviewed 60 most relevant works are exposed and analyzed comprehensively, and open problems in the field are identified."]},
{"title": "Separation force analysis and prediction based on cohesive element model for constrained-surface Stereolithography processes", "highlights": ["We formulate cohesive zone model to characterize the separation process.", "We establish an optimization model to evaluate the mechanical parameters.", "The effectiveness of the proposed technique is validated by computer-simulated experiments.", "Fabrication performance can be significantly improved by the proposed approach."], "abstract": ["Constrained-surface based Additive Manufacturing (AM) processes have been widely used in both academia and industry for the past few years. Despite the advantages of constrained-surface based AM processes, it has not been widely used in practice. A main reason for this is that a substantial separation force is required to separate the cured part from the material vat during the pulling-up stage, which may damage the cured part and reduce the reliability of the process. The solutions proposed previously to reduce this separation force recommend using an intermediate coating material (e.g., Teflon and silicone films) between the cured part and the vat. This, however, has only negligible effects in reducing the separation force. In this work, the pulling-up process is modeled within the framework of mechanics-based principles. In particular, the cohesive zone model (CZM) is adopted to characterize the separation mechanism, and finite element (FE) simulation is carried out to investigate the separation process using the commercially available FE software, Abaqus. A new simple optimization scheme is also proposed to estimate the constitutive cohesive stiffness parameters from experimental measurements. These constitutive parameters are very difficult to estimate using the standard mechanical tests. The proposed work based on sound mechanics-based principles can be used for reliable prediction of pulling-up speed, and thus, is likely to be useful in devising an adaptive closed-loop system to control the pulling-up process and achieve a reliable AM approach."]},
{"title": "h-graphs: A new representation for tree decompositions of graphs", "highlights": ["h-graphs, a new representation for tree decompositions of constraints graph is presented.", "h-graphs explicitly capture construction steps dependencies in a tree decomposition.", "An application to speed up computing feasibility ranges for constraint parameters is described."], "abstract": ["In geometric constraint solving, 2D well constrained geometric problems can be abstracted as Laman graphs. If the graph is tree decomposable, the constraint-based geometric problem can be solved by a Decomposition\u2013Recombination planner based solver. In general decomposition and recombination steps can be completed only when steps on which they are dependent have already been completed. This fact naturally defines a hierarchy in the decomposition\u2013recombination steps that traditional tree decomposition representations do not capture explicitly.", "In this work we introduce h-graphs, a new representation for decompositions of tree decomposable Laman graphs, which captures dependence relations between different tree decomposition steps. We show how h-graphs help in efficiently computing parameter ranges for which solution instances to well constrained, tree decomposable geometric constraint problems with one degree of freedom can actually be constructed."]},
{"title": "Robust polyhedral Minkowski sums with GPU implementation", "highlights": ["We compute Minkowski sum boundaries of two input polyhedra.", "Sum polygons from convolution are created, subdivided, and traversed for boundary construction.", "The algorithm is robustly implemented on CPU and GPU both.", "The performance on GPU is orders-of-magnitude higher than previous works."], "abstract": ["We present a Minkowski sum algorithm for polyhedra based on convolution. We develop robust CPU and GPU implementations, using our ACP strategy to eliminate degeneracy and to enforce a user-specified backward error bound. We test the programs on 45 inputs with an error bound of ", ". The CPU program outperforms prior work, including non-robust programs. The GPU program using 2688 CUDA cores exhibits a median speedup factor of 36, which increases to 68 on the 6 hardest tests. For example, it computes a Minkowski sum with a million features in 20 seconds."]},
{"title": "Quality guaranteed all-hex mesh generation by a constrained volume iterative fitting algorithm", "highlights": ["An iterative algorithm is developed to fill a triangular mesh with an all-hex mesh.", "The Jacobian values of the all-hex mesh are guaranteed to be positive.", "The convergence of the iterative algorithm is proved."], "abstract": ["The hexahedral mesh (hex mesh) is usually preferred to the tetrahedral mesh (tet mesh) in finite element methods for numerical simulation. In finite element analysis, a valid hex mesh requires that the scaled Jacobian value at each mesh vertex is larger than ", ". However, the hex mesh produced by lots of prevailing hex mesh generation methods cannot be guaranteed to be a valid hex mesh. In this paper, we develop a constrained volume iterative fitting (CVIF) algorithm to fill a given triangular mesh model with an all-hex volume mesh. Starting from an initial all-hex mesh model, which is generated by voxelizing the given triangular mesh model, CVIF algorithm fits the boundary mesh of the initial all-hex mesh to the given triangular mesh model by iteratively adjusting the boundary mesh vertices. In each iteration, the movements of the boundary mesh vertices are diffused to the inner all-hex mesh vertices. After the iteration stops, an all-hex volume mesh that fills the given triangular mesh model can be generated. In the CVIF algorithm, the movement of each all-hex mesh vertex is constrained to ensure that the scaled Jacobian value at each mesh vertex is larger than ", ", etc. Therefore, the all-hex mesh generated by the CVIF algorithm is guaranteed to be a valid all-hex mesh."]},
{"title": "CNC double spiral tool-path generation based on parametric surface mapping", "highlights": ["We study a new double spiral tool-path generation algorithm for 5-axis HSM.", "We call a smoothness optimization method during solving PDEs to smooth curves.", "Machining parameters (e.g., path interval and step length) can be guaranteed.", "Mapping rules are based on the same ranges of NURBS curve and parametric domain.", "Our tool-paths have a self-complementary structure and can be suitably linked."], "abstract": ["High-speed machining (HSM) has been an important method for machining complex parametric surface. Tool-path planning for HSM has a significant impact on processing efficiency and surface quality. A new double spiral tool-path generation algorithm for HSM is proposed in this paper. First, the isothermal lines which satisfy the machining parameters in the mapping parametric domain are computed by means of constructing a thermal conductivity model and solving partial differential equations (PEDs). Furthermore, a smoothness optimization method is proposed to improve the smoothness of the isothermal lines and avoid taking up too much memory. Then, the mapping rules are constructed and the trajectory is planned out in the standard parametric domain in order to generate double spiral trajectory in the corresponding parametric domain. Finally, the trajectory is mapped onto the parametric surface to obtain the tool-paths, and the tool-paths linking method is planned for complex multi-domains. This method can realize the precision milling of complicated parametric surface without tool retractions, and meanwhile it improves the uniformity of the tool-paths and machining efficiency. Our method has been experimented in several simulations and validated successfully through the actual machining of a complicated pocket. The results indicate that this method is superior to other existing machining methods, and it can realize HSM of complicate-shaped pocket based on parametric surface."]},
{"title": "A gesture-free geometric approach for mid-air expression of design intent in 3D virtual pottery", "highlights": ["We introduce a geometric approach for mid-air virtual pottery design.", "A user can design virtual pots without the need to remember gestures.", "The shape of a pot gradually converges to the point-cloud of the user\u2019s hands.", "Applications are shown with two depth sensors, Leap Motion and SoftKinetic DepthSense.", "User evaluation demonstrates strengths and weaknesses of our approach."], "abstract": ["The advent of depth cameras has enabled mid-air interactions for shape modeling with bare hands. Typically, these interactions employ a finite set of pre-defined hand gestures to allow users to specify modeling operations in virtual space. However, human interactions in real world shaping processes (such as pottery or sculpting) are complex, iterative, and continuous. In this paper, we show that the expression of user intent in shaping processes can be derived from the geometry of contact between the hand and the manipulated object. Specifically, we describe the design and evaluation of a geometric interaction technique for bare-hand mid-air virtual pottery. We model the shaping of a pot as a gradual and progressive convergence of the pot\u2019s profile to the shape of the user\u2019s hand represented as a point-cloud (PCL). Thus, a user does not need to learn, know, or remember any gestures to interact with our system. Our choice of pottery simplifies the geometric representation, allowing us to systematically study how users use their hands and fingers to express the intent of deformation during a shaping process. Our evaluations demonstrate that it is possible to enable users to express their intent for shape deformation without the need for a fixed set of gestures for clutching and deforming a shape."]},
{"title": "Bidirectional Evolutionary Structural Optimization (BESO) based design method for lattice structure to be fabricated by additive manufacturing", "highlights": ["Both functional roles of solid volume and skin structure are considered.", "Lattice orientation is introduced that may be adjusted to improve performance.", "Increased speed of lattice frame generation.", "Structural stiffness is increased by the proposed optimization algorithm."], "abstract": ["Unlike traditional manufacturing methods, additive manufacturing can produce parts with complex geometric structures without significant increases in fabrication time and cost. One application of additive manufacturing technologies is the fabrication of customized lattice-skin structures which can enhance performance of products while minimizing material or weight. In this paper, a novel design method for the creation of periodic lattice structures is proposed. In this method, Functional Volumes (FVs) and Functional Surfaces (FSs) are first determined based on an analysis of the functional requirements. FVs can be further decomposed into several sub-FVs. These sub-FVs can be divided into two types: FV with solid and FV with lattice. The initial design parameters of the lattice are selected based on the proposed guidelines. Based on these parameters, a kernel based lattice frame generation algorithm is used to generate lattice wireframes within the given FVs. At last, traditional bidirectional evolutionary structural optimization is modified to optimize distribution of lattice struts\u2019 thickness. The design method proposed in this paper is validated through a case study, and provides an important foundation for the wide adoption of additive manufacturing technologies in the industry."]},
{"title": "A simple strategy for defining polynomial spline spaces over hierarchical T-meshes", "highlights": ["A strategy for defining cubic tensor product spline functions is proposed.", "Simple rules for inferring local knot vectors to define blending functions for a given T-mesh.", "Examples of application of the strategy for adaptive refinement in isogeometric analysis and CAD."], "abstract": ["We present a new strategy for constructing spline spaces over hierarchical T-meshes with quad- and octree subdivision schemes. The proposed technique includes some simple rules for inferring local knot vectors to define ", "-continuous cubic tensor product spline blending functions. Our conjecture is that these rules allow to obtain, for a given T-mesh, a set of linearly independent spline functions with the property that spaces spanned by nested T-meshes are also nested, and therefore, the functions can reproduce cubic polynomials. In order to span spaces with these properties applying the proposed rules, the T-mesh should fulfill the only requirement of being a ", " mesh. The straightforward implementation of the proposed strategy can make it an attractive tool for its use in geometric design and isogeometric analysis. In this paper we give a detailed description of our technique and illustrate some examples of its application in isogeometric analysis performing adaptive refinement for 2D and 3D problems."]},
{"title": "Statistical geometric computation on tolerances for dimensioning", "highlights": ["A generalized RSS method is proposed for modeling geometric representations of tolerances in the statistical way.", "A set of basic operations over the new tolerance model are proposed to enable tolerance compositing and cascading.", "A set of examples demonstrate applications of the new model in tolerance estimation.", "A tolerance allocation framework based on optimization is also proposed by utilizing analytical forms of the new model."], "abstract": ["Dimensions are used to specify the distances between different features in geometric models. These dimensions will often be expressed as a range of allowable dimensions. When considering a group of toleranced dimensions, these ranges can be analyzed as either a worst-case bound on allowable ranges, or as a statistical measure of expected distribution. This paper presents a new geometric model for representing statistically-based tolerance regions. Methods for tolerance estimation and allocation on a geometric model are provided by generalizing root sum square (RSS) methods for compositing and cascading over tolerance zones. This gives us a geometric interpretation of a statistical analysis. Tolerance regions are determined by probabilities of variations of dimensions falling into the region. A dependency graph over dimensions can be represented by a topological graph on which the tolerance cascading and tolerance allocation can be processed. To illustrate applications of this geometric method, we provide examples of tolerance estimation and tolerance allocation on our model. The estimation examples utilize the compositing and cascading operations provided in the analysis method. The allocation examples present an automatic tolerance allocation procedure on the tolerance model. As opposed to existing methods, our allocation method allows us to specify not only a numerical objective of the optimization, but also a statistically-based objective for the geometric shape of the tolerance."]},
{"title": "High-order mesh curving by distortion minimization with boundary nodes free to slide on a 3D CAD representation", "highlights": ["We present a method to generate curved high-order meshes for industrial applications.", "The method is based on the minimization of a single and global objective function.", "The objective function is defined as the distortion of the volume mesh.", "To enlarge the feasible region, boundary nodes are free to slide on the CAD model.", "A block iterative implementation of the global objective function is presented."], "abstract": ["We propose a 3D mesh curving method that converts a straight-sided mesh to an optimal-quality curved high-order mesh that interpolates a CAD boundary representation. The main application of this method is the generation of discrete approximations of curved domains that are valid for simulation analysis with unstructured high-order methods. We devise the method as follows. First, the boundary of a straight-sided high-order mesh is curved to match the curves and surfaces of a CAD model. Second, the method minimizes the volume mesh distortion with respect to the coordinates of the inner nodes and the parametric coordinates of the curve and surface nodes. The proposed minimization features untangling capabilities and therefore, it repairs the invalid elements that may arise from the initial curving step. Compared with other mesh curving methods, the only goal of the proposed residual system is to minimize the volume mesh distortion. Furthermore, it is less constrained since the boundary nodes are free to slide on the CAD curves and surfaces. Hence, the proposed method is well suited to generate curved high-order meshes of optimal quality from CAD models that contain thin parts or high-curvature entities. To illustrate these capabilities, we generate several curved high-order meshes from CAD models with the implementation detailed in this work. Specifically, we detail a node-by-node non-linear iterative solver that minimizes the proposed objective function in a block Gauss\u2013Seidel manner."]},
{"title": "Isogeometric segmentation: Construction of auxiliary curves", "highlights": ["A method is proposed for splitting a trimmed surface into two with a curve.", "The curve is required to have specified endpoints and tangents at the endpoints.", "The splitting is central to an algorithm for isogeometric segmentation of 3D models.", "The curve optimizes a penalty function that measures the quality of the shapes.", "We study regularity properties and methods for computing the penalty function."], "abstract": ["In the context of segmenting a boundary represented solid into topological hexahedra suitable for isogeometric analysis, it is often necessary to split an existing face by constructing auxiliary curves. We consider solids represented as a collection of trimmed spline surfaces, and design a curve which can split the domain of a trimmed surface into two pieces satisfying the following criteria: the curve must not intersect the boundary of the original domain, it must not intersect itself, the two resulting pieces should have good shape, and the endpoints and the tangents of the curve at the endpoints must be equal to specified values."]},
{"title": "Efficient data-parallel tree-traversal for ", "highlights": ["We show how to improve ", " traversal time by one order of magnitude.", "We demonstrate how to incorporate Warp Transformations within the ", " into new traversal.", "The performance is measured on computer generated and hand built models."], "abstract": ["The hierarchical implicit modelling paradigm, as exemplified by the ", ", makes it possible to support not only Boolean operations and affine transformations, but also various forms of blending and space warping. Typically, the resulting solid is converted to a boundary representation, a triangle mesh approximation, for rendering. These triangles are obtained by evaluating the corresponding implicit function (field) at the samples of a dense regular three-dimensional grid and by performing a local iso-surface extraction at each voxel. The performance bottleneck of this rendering process lies in the cost of the tree traversal (which typically must be executed hundreds of millions of times) and in the cost of applying the inverses of the space transformations associated with some of the nodes of the tree to the grid samples.", "Tree pruning is commonly used to reduce the number of samples for which the field value must be computed. Here, we propose a complementary strategy, which reduces the costs of both the traversal and of applying the inverses of the blending and warping transformations that are associated with each evaluation.", "Without blending or warping, a ", " can be reduced to a CSG tree only containing Boolean nodes and affine transformations, which can be reordered to increase memory coherence. Furthermore, the cumulative effects of the affine transformations can be precomputed via matrix multiplication. We propose extensions of these techniques from CSG trees to the fully general ", ". These extensions are based on tree reordering, bottom-up traversal, and caching of the combined matrix for uninterrupted runs of affine transformations in the ", ".", "We show that these new techniques result in an order of magnitude performance improvement for rendering large ", " on modern Single Program Multiple Data (SPMD) devices."]},
{"title": "Precise contact motion planning for deformable planar curved shapes", "highlights": ["We present an efficient motion planning algorithm for a planar deformable robot.", "We employ ", "-contact motion analysis to reduce the degrees of freedom of the robot.", "Our algorithm efficiently finds a feasible path via a graph searching algorithm."], "abstract": ["We present a precise contact motion planning algorithm for a deformable robot in a planar environment with stationary obstacles. The robot and obstacles are both represented with ", "-continuous implicit or parametric curves. The robot is changing its shape using a single degree of freedom (via a one-parameter family of deformable curves). In order to reduce the dimensionality of the configuration space, geometrically constrained yet collision free contact motions are sought, that have ", " simultaneous tangential contact points between the robot and the obstacles. The ", "-contact motion analysis effectively reduces the degrees of freedom of the robot, which enables a more efficient motion planning. The geometric conditions for the ", "-contact motions can be formulated as a system of non-linear polynomial equations, which can be solved precisely using a multivariate equation solver. The solutions for ", "-contact motions are represented as curves in a 4-dimensional ", " space, where ", " are the degrees of freedom of the rigid motion and ", " is the deformation\u2019s parameter. Using the graph structure of the solution curves for the ", "-contact motions, our algorithm efficiently finds a feasible path connecting two configurations via a graph searching algorithm, whenever available. We demonstrate the effectiveness of the proposed approach using several examples."]},
{"title": "Analytic methods for geometric modeling via spherical decomposition", "highlights": ["We propose a grid-free discretization scheme for analytic geometric modeling.", "Solids are approximated with countable unions of 3D balls cut from 4D cones.", "The unions turn into 3D slices of 4D Minkowski sums of knots and a template cone.", "The Minkowski formulation embeds well into cross-correlations between solids.", "The analytic formulation follows using convolution algebra and Fourier Transform."], "abstract": ["Analytic methods are emerging in solid and configuration modeling, while providing new insights into a variety of shape and motion related problems by exploiting tools from group morphology, convolution algebras, and harmonic analysis. However, most convolution-based methods have used uniform grid-based sampling to take advantage of the fast Fourier transform (FFT) algorithm. We propose a new paradigm for more efficient computation of analytic correlations that relies on a grid-free discretization of arbitrary shapes as countable unions of balls, in turn described as sublevel sets of summations of smooth radial kernels at adaptively sampled \u2018knots\u2019. Using a simple geometric lifting trick, we interpret this combination as a convolution of an impulsive skeletal density and primitive kernels with conical support, which faithfully embeds into the convolution formulation of interactions across different objects. Our approach enables fusion of search-efficient combinatorial data structures prevalent in time-critical collision and proximity queries with analytic methods popular in path planning and protein docking, and outperforms uniform grid-based FFT methods by leveraging nonequispaced FFTs. We provide example applications in formulating holonomic collision constraints, shape complementarity metrics, and morphological operations, unified within a single analytic framework."]},
{"title": "Geometric consideration of support structures in part overhang fabrications by electron beam additive manufacturing", "highlights": ["The developed thermomechanical model is able to simulate the deformation of overhang parts in EBAM.", "The overhang length noticeably affects the overhang deformation.", "Solid columns can avoid a serious overhang warping defect.", "A solid piece beneath the overhang in the powder bed can reduce the overhang deformation."], "abstract": ["Powder bed electron beam additive manufacturing (EBAM) has emerged as a potentially cost-effective process for high-value, small-batch productions for biomedical and aerospace applications. In EBAM, the process would not require support structures for overhang geometry because a build part is immersed in the powder bed. However, support structures are indeed needed in practice for an overhang; without it, the overhang area will have defects such as warping, which are due to the complex thermomechanical process in EBAM. In this study, a numerical approach is introduced to simulate the thermomechanical responses in the EBAM process of overhang structures. The objective of this study was to develop a 2D thermomechanical model, using a finite element method (FEM), to evaluate temperature induced deformation on different overhang support patterns in the EBAM process. The major results are summarized as follows. (1) The thermomechanical model is able to simulate the deformation of overhang parts in EBAM. The overhang length noticeably affects the overhang deformation. (2) As a traditional support structure, solid columns can reduce the overhang warping; further, the size of the column may be minimized to satisfy a deformation constraint, and meanwhile, reduce the amount of support materials. (3) Including a solid piece beneath the overhang, acting as a heat sink, may also reduce the overhang deformation; however, an appropriate gap must be incorporated so as not to fuse to the overhang area, while still effectively reducing the deformation."]},
{"title": "Implementation and evaluation of automated tetrahedral\u2013prismatic mesh generation software", "highlights": ["An open-source implementation for prismatic\u2013tetrahedral mesh generation is presented.", "Global constrained optimization is employed to create prismatic layer envelope.", "We find that total mesh generation time is substantially reduced.", "Manual user intervention effort is smaller due to large degree of automation.", "Comparisons of RANS solutions demonstrate adequate mesh quality."], "abstract": ["An open-source implementation of an efficient mesh generation procedure for hybrid prismatic\u2013tetrahedral meshes intended for use in Reynolds-averaged Navier\u2013Stokes solutions is presented. The method employed combines the established, and very fast, Delaunay-based tetrahedral mesh generator ", " with a novel technique for the creation of a prismatic layer, where constrained global optimization of the envelope is employed. Once a well-shaped envelope is thus obtained, a semi-structured layer of pentahedral elements is extruded between wall and envelope surface. Satisfactory mesh quality is demonstrated by comparing solutions obtained using the new meshes with reference data computed on high-quality advancing-front grids. Mesh generation time is shown to be substantially smaller than with many other methods. Overall, the presented implementation is deemed a valuable tool for cases where many meshes need to be generated for routine analyses and turnaround time is critical.", "This is an extended version of the paper presented at the 23rd International Meshing Roundtable in London, October 2014."]},
{"title": "Smoothness driven frame field generation for hexahedral meshing", "highlights": ["We propose a novel algorithm to generate block-structured hexahedral meshes for any CAD domain.", "The proposed approach does not require a pre-meshed boundary.", "The frame field is built so that singularities are better positioned.", "A skeleton is built using the frame field.", "Each part defined by the skeleton can be meshed with a structured mesh."], "abstract": ["For many years, providing an algorithm to generate hexahedral meshes that fulfill minimal geometric criteria (boundary-alignment, minimum of singularity vertices) and that is not limited to a category of geometries has been an open issue. In the past couple of years, techniques using 3D frame fields have emerged to design such meshes (Huang et\u00a0al., 2011; Li et\u00a0al., 2012). Those methods are based on a two-step process where a 3D frame field is built by assigning a frame to each cell of a tetrahedral mesh, then a parametrization algorithm is applied to generate a hexahedral mesh. In this work, we propose a novel algorithm to generate block-structured hexahedral meshes for any CAD domain ", ". This work differs from previous ones in many points: (1) the proposed approach does not need to start from a pre-meshed quad boundary; (2) The frame field initialization does not put singularity lines around the medial object of ", "; (3) Conceptually, frames are assigned to the vertices and not to the cells of the tetrahedral mesh; (4) The parametrization process is replaced by a constructive algorithm that generates a block structure, which partitions ", " in meshable regions."]},
{"title": "Average curve of ", "highlights": ["We define the Average Curve (AC) of any compatible set of ", " smooth Jordan curves.", "We define the inflation of the AC that conveys local variability.", "We present a linear-cost algorithm for computing a polygonal approximation of the AC.", "The AC and the inflation form a natural extension of the Medial Axis Transform."], "abstract": ["We define the ", " (AC) of a compatible set of two or more smooth and planar, Jordan curves. It is independent of their order and representation. We compare two variants: the ", " (vAC), defined in terms of the valley of the field that sums the squared distances to the input curves, and the ", " (zAC), defined as the zero set of the field that sums the signed distances to the input curves. Our formulation provides an orthogonal projection homeomorphism from the AC to each input curve. We use it to define compatibility. We propose a fast tracing algorithm for computing a polygonal approximation (PAC) of the AC and for testing compatibility. We provide a linear-cost implementation for tracing the PAC of polygonal approximations of smooth input curves. We also define the ", " of the AC and use it to visualize the local variability in the set of input curves. We argue that the AC and its inflation form a natural extension of the Medial Axis Transform to an arbitrary number of curves. We propose extensions to open curves and to weighted averages of curves, which can be used to design animations."]},
{"title": "SURGEM: A solid modeling tool for planning and optimizing pediatric heart surgeries", "highlights": ["We describe solid modeling challenges faced while developing a surgery planning tool.", "We support three surgeries: (1) DORV, (2) Fontan procedure and (3) Stenosis repair.", "DORV and Stenosis repair procedures utilize a simple sketch based interface.", "For Fontan, the surgeon specifies endpoint locations by clicking.", "More direct and precise control allows quick exploration of number of surgical options."], "abstract": ["Approximately 1% of children are born with a moderate to severe congenital heart defect, and half of them undergo one or more surgeries to fix it. SURGEM, a solid modeling environment, is used to improve surgical outcome by allowing the surgeon to design the geometry for several possible surgical options before the operation and to evaluate their relative merits using computational fluid simulation. We describe here the solid modeling and graphical user interface challenges that we have encountered while developing support for three surgeries: (1) repair of double-outlet right ventricle, which adds a graft wall within the cardiac chambers to split the solid model of the unique ventricle, (2) the Fontan procedure, which routes a graft tube to connect the inferior vena cava to the pulmonary arteries, and (3) stenosis repair, which adds a stent to expand a constricted artery. We describe several solutions that we have developed to address these challenges and to improve the performance, reliability, and usability of SURGEM for these tasks."]},
{"title": "Effective contact measures", "highlights": ["Introduces a new concept of effective contact measure as an approximation of surface contact area.", "Proposes 3 new concepts of effective contact measures.", "Discusses application to alignment problems."], "abstract": ["Contact area is an important geometric measurement in many physical systems. It is also difficult to compute due to its extreme sensitivity to infinitesimal perturbations. In this paper, we propose a new concept called an ", ", which acts as a smooth version of contact area. Effective contact measures incorporate a notion of scale into the definition of contact area, allowing one to consider the degree of contact at different sizes. We show how effective contact measures can yield useful statistics for a number of applications, including analysis of multiphase materials and docking/alignment problems."]},
{"title": "Tool path generation for multi-axis freeform surface finishing with the LKH TSP solver", "highlights": ["A new way of planning SFC type tool path is proposed.", "Cutting simulation method is proposed to evaluate the scallop error.", "Tool path planning task is formulated as a TSP and LKH is applied for solution.", "In LKH, the distance function is redefined to avoid incorrect linking problem."], "abstract": ["In freeform surface finishing, there are three major types of tool path topologies: the direction-parallel type, the contour\u2013parallel type and the space-filling curve (SFC) type. The SFC topology is capable of covering the whole surface with only one path. In this paper, we present a new way of planning the SFC type tool path by formulating the planning task as a traveling salesman problem (TSP). The optimal path is generated in two steps. Firstly, a set of regular cutter contact (CC) points is generated on the input surface. A cutting simulation method is developed to evaluate the scallop error and determine the position of the next CC point in cross-feed direction. This method is free of local surface curvature assumptions and is therefore accurate for big cutters. Secondly, the obtained CC points are input into an efficient TSP solver LHK for the optimal CC point linking sequences. To stop the CC points from diagonal linking or penetrating linking, the Euclidean distance evaluation function for two CC points is redefined in LHK. The proposed tool path generation method is verified with several freeform surface examples; the results show that the method can automatically find the optimal feed direction and it can generate shorter tool path than the traditional SFC method. The feasibility of the proposed method is also verified by a cutting experiment."]},
{"title": "Enhanced medial-axis-based block-structured meshing in 2-D", "highlights": ["A novel method is described for using the medial axis for block-structured meshing.", "The hitherto neglected angular information in the medial axis is shown to be useful.", "Geometry concavities are dealt with effectively.", "The surface is decomposed into m-sided and submappable subregions.", "The final meshes are of high quality for all types of 2-D geometry."], "abstract": ["New techniques are presented for using the medial axis to generate decompositions on which high quality block-structured meshes with well-placed mesh singularities can be generated. Established medial-axis-based meshing algorithms are effective for some geometries, but in general, they do not produce the most favourable decompositions, particularly when there are geometric concavities. This new approach uses both the topological and geometric information in the medial axis to establish a valid and effective arrangement of mesh singularities for any 2-D surface. It deals with concavities effectively and finds solutions that are most appropriate to the geometric shapes. Resulting meshes are shown for a number of example models."]},
{"title": "Automatic generation of LEGO building instructions from multiple photographic images of real objects", "highlights": ["Even beginners are able to build realistic complex LEGO models.", "Our system allows reconstruction of portable large scale models with color information.", "Easy adjustment of the trade-off between model strength and the total number of bricks is possible."], "abstract": ["We introduce a system to reconstruct large scale LEGO models from multiple two dimensional images of objects taken from different views. We employ a unit voxel with an edge length ratio of 5:5:6 for the shape from silhouette method that reconstructs an octree voxel-based three dimensional model with color information from images. We then convert the resulting voxel model with color information into a LEGO sculpture. In order to minimize the number of LEGO bricks, we use a stochastic global optimization method, simulated annealing, to hollow the model as much as possible but keep its strength for portability. Several real complex LEGO models are provided to demonstrate the effectiveness of the proposed method."]},
{"title": "A statistical atlas based approach to automated subject-specific FE modeling", "highlights": ["A statistical atlas is constructed to account for shape variations.", "Shape correspondence is obtained through the eigenspace search.", "Subject-specific FE mesh is obtained from the deformed mesh of the mean shape.", "The method is automated and is applicable to shapes with large variations."], "abstract": ["Subject-specific modeling is increasingly important in biomechanics simulation. However, how to automatically create high-quality finite element (FE) mesh and how to automatically impose boundary condition are challenging.", "This paper presents a statistical atlas based approach for automatic meshing of subject-specific shapes. In our approach, shape variations among a shape population are explicitly modeled and the correspondence between a given subject-specific shape and the statistical atlas is sought within the \u201clegal\u201d shape variations. This approach involves three parts: (1) constructing a statistical atlas from a shape population, including the statistical shape model and the FE model of the mean shape; (2) establishing the correspondence between a given subject shape and the atlas; and (3) deforming the atlas to the subject shape based on the shape correspondence. Numerical results on 2D hands, 3D femur bones and 3D aorta demonstrate the effectiveness of the proposed approach."]},
{"title": "Constrained space deformation techniques for design optimization", "highlights": ["High quality space deformation for design optimization based on MLS approximation.", "High level of modeling flexibility through explicit energy minimization.", "Maintenance of geometric constraints during deformation."], "abstract": ["We present a novel shape deformation method for its use in design optimization tasks. Our space deformation technique based on moving least squares approximation improves upon existing approaches in crucial aspects: It offers the same level of modeling flexibility as surface-based deformations, but it is independent of the underlying geometry representation and therefore highly robust against defects in the input data. It overcomes the scalability limitations of existing space deformation techniques based on globally supported radial basis functions while providing the same high level of deformation quality. Finally, unlike existing space deformation approaches, our technique directly incorporates geometric constraints\u2013such as preservation of critical feature lines, circular couplings, planar or cylindrical construction parts\u2013into the deformation, thereby fostering the exploration of more favorable and producible shape variations during the design optimization process."]},
{"title": "Geometric characteristics of a class of cubic curves with rational offsets", "highlights": ["This paper presents a geometric characterization of cubic indirect-PH curves.", "We give a construction of ", " Hermite interpolation using indirect-PH curves.", "The optimal indirect-PH curve to ", " Hermite interpolation is investigated."], "abstract": ["Planar B\u00e9zier curves that have rationally parameterized offsets can be classified into two classes. The first class is composed of curves that have Pythagorean hodographs (PH) and the second class is composed of curves that do not have PHs but can have rational PHs after reparameterization by a fractional quadratic transformation. This paper reveals a geometric characterization for all properly-parameterized cubic B\u00e9zier curves in the second class. The characterization is given in terms of B\u00e9zier control polygon geometry. Based on the derived conditions, we also present a simple geometric construction of ", " Hermite interpolation using such B\u00e9zier", "\u00a0", "curves. The construction results in a one-parameter family of curves if a solution exists. We further prove that there exists a unique value of the parameter which minimizes the integral of the squared norm of the second order derivative of the curves."]},
{"title": "Efficient global penetration depth computation for articulated models", "highlights": ["Novelty: The first global PD approach for high-DOF articulated models.", "Generality: Handling hybrid joints and links represented using polygonal models.", "Conservativeness: Guaranteeing that the configuration realizing PD is penetration free.", "Efficiency: Taking about 0.03\u20133 ms per runtime PD query in our experiments."], "abstract": ["We present an algorithm for computing the global penetration depth between an articulated model and an obstacle or between the distinctive links of an articulated model. In so doing, we use a formulation of penetration depth derived in configuration space. We first compute an approximation of the boundary of the obstacle regions using a support vector machine in a learning stage. Then, we employ a nearest neighbor search to perform a runtime query for penetration depth. The computational complexity of the runtime query depends on the number of support vectors, and its computational time varies from 0.03 to 3 milliseconds in our benchmarks. We can guarantee that the configuration realizing the penetration depth is penetration free, and the algorithm can handle general articulated models. We tested our algorithm in robot motion planning and grasping simulations using many high degree of freedom (DOF) articulated models. Our algorithm is the first to efficiently compute global penetration depth for high-DOF articulated models."]},
{"title": "Analytic construction and analysis of spiral pocketing via linear morphing", "highlights": ["We define spiral tool-paths via linear interpolation between closed curves.", "The algorithm is simple and provides an analytic solution in B-spline form.", "The tool path is smooth and maintains constant radial distances between laps.", "For non-convex pockets, the interpolation of its boundary and medial axis provides the spiral.", "Cutter engagement, feed direction, time and forces help to analyse the spiral."], "abstract": ["In numerical control, pocketing is a widely extended machining operation with different industrial applications. Conventional strategies (directional and contour parallel) provide a uniform material removal rate, but they show discontinuities and undesirable stops. However, smooth spiral paths overcome discontinuities, although the removal rate is not constant, and their implementation is complex. In order to provide an in-between solution, our algorithm embeds an Archimedean spiral into a linear morphing definition of the pocket. The solution is smooth, simple, analytic, and leads to a B-spline curve. Different tests were performed to compare the proposed spiral to other conventional and spiral strategies. To study the influence of the tool-path geometry, we computed engagement angle and feed direction, and measured force and time. The results demonstrate that our spiral is a committed, analytic and easy to compute solution."]},
{"title": "An improved star test for implicit polynomial objects", "highlights": ["A homogeneous star test method for implicit polynomial objects.", "A linear programming optimization method to improve the efficiency of star test.", "Comparison of the naive star test and the homogeneous star test using different arithmetic."], "abstract": ["For a given point set, a particular point is called a star if it can see all the boundary points of the set. The star test determines whether a candidate point is a star for a given set. It is a key component of some topology computing algorithms such as Connected components via Interval Analysis (CIA), Homotopy type via Interval Analysis (HIA), etc. Those algorithms decompose the input object using axis-aligned boxes, so that each box is either not intersecting or intersecting with the object and in this later case its center is a star point of the intersection. Graphs or simplicial complexes describing the topology of the objects can be obtained by connecting these star points following different rules. The star test is performed for simple primitive geometric objects, because complex objects can be constructed using Constructive Solid Geometry (CSG), and the star property is preserved via union and intersection. In this paper, we improve the method to perform the test for implicit objects. For a primitive set defined by an implicit polynomial equation, the polynomial is made homogeneous with the introduction of an auxiliary variable, thus the degree of the star condition is reduced. A linear programming optimization is introduced to further improve the performance. Several examples are given to show the experimental results of our method."]},
{"title": "Secondary Laplace operator and generalized Giaquinta\u2013Hildebrandt operator with applications on surface segmentation and smoothing", "highlights": ["Two new geometric operators are introduced based on the second fundamental form.", "The new operators, SLO and GGHO, are sensitive to the curvature-related features.", "A segmentation method is introduced based on the SLO eigenfunctions.", "A geometric flow method is developed based on the GGHO for surface smoothing."], "abstract": ["Various geometric operators have been playing an important role in surface processing. For example, many shape analysis algorithms have been developed based on eigenfunctions of the \u200bLaplace\u2013Beltrami operator (LBO), which is defined based on the first fundamental form of the surface. In this paper, we introduce two new geometric operators based on the second fundamental form of the surface, namely the secondary Laplace operator (SLO) and generalized Giaquinta\u2013Hildebrandt operator (GGHO). Surface features such as concave creases/regions and convex ridges can be captured by eigenfunctions of the SLO, which can be used in surface segmentation with concave and convex features detected. Moreover, a new geometric flow method is developed based on the GGHO, providing an effective tool for sharp feature-preserving surface smoothing."]},
{"title": "Consistent quadrangulation for shape collections via feature line co-extraction", "highlights": ["We propose a framework to produce consistent quadrangulations for a shape collection.", "We propose an efficient method for extracting feature lines from the shape collection.", "We propose a practical greedy algorithm for extracting the aligned cut graphs."], "abstract": ["This paper presents a method that takes a collection of 3D surface shapes, and produces a consistent and individually feature preserving quadrangulation of each shape. By exploring the correspondence among shapes within a collection, we coherently extract a set of representative feature lines as the key characteristics for the given shapes. Then we compute a smooth cross-field interpolating sparsely distributed directional constraints induced from the feature lines and apply the mixed-integer quadrangulation to generate the quad meshes. We develop a greedy algorithm to extract aligned cut graphs across the shape collection so that the meshes can be aligned in a common parametric domain. Computational results demonstrate that our approach not only produces consistent quad meshes across the entire collection with significant geometry variation but also achieves a trade-off between global structural simplicity for the collection and local geometry fidelity for each shape."]},
{"title": "Solving the initial value problem of discrete geodesics", "highlights": ["Shortest geodesic is not able to solve the initial value problem of discrete geodesic.", "Geodesic equation are second-order ODEs.", "We solve the initial value problem on triangle meshes by solving a first-order ODE", "The computed discrete geodesic path converges to the one on the smooth surface."], "abstract": ["Computing geodesic paths and distances is a common operation in computer graphics and computer-aided geometric design. The existing discrete geodesic algorithms are mainly designed to solve the boundary value problem, i.e., to find the shortest path between two given points. In this paper, we focus on the initial value problem, i.e., finding a uniquely determined geodesic path from a given point in any direction. Since the ", " paths do not provide the unique solution on triangle meshes, we solve the initial value problem in an indirect manner: given a fixed point and an initial tangent direction on a triangle mesh ", ", we first compute a geodesic curve ", " on a piecewise smooth surface ", ", which well approximates the input mesh ", " and can be constructed at little cost. Then, we solve a first-order ODE of the tangent vector using the fourth-order Runge\u2013Kutta method, and parallel transport it along ", ". When the geodesic curve reaches the boundary of the current patch, its tangent can be directly transported to the neighboring patch, thanks to the ", "-continuity along the common boundary of two adjacent patches. Finally, once the geodesic curve ", " is available, we project it onto the underlying mesh ", ", producing the discrete geodesic path ", ", which is guaranteed to be unique on ", ". It is worth noting that our method is different from the conventional methods of directly solving the geodesic equation (i.e., a second-order ODE of the position) on piecewise smooth surfaces, which are difficult to implement due to the complicated representation of the geodesic equation involving Christoffel symbols. The proposed method, based on the first-order ODE of the tangent vector, is intuitive and easy for implementation. Our method is particularly useful for computing geodesic paths on low-resolution meshes which may have large and/or skinny triangles, since the conventional ", " geodesic paths are usually far from the ground truth."]},
{"title": "Re-parameterization reduces irreducible geometric constraint systems", "highlights": ["A new re-parameterization for reducing and unlocking irreducible geometric systems.", "No need for the values of the key unknowns and no limit on their number.", "Enabling the usage of decomposition methods on irreducible re-parameterized systems.", "Usage at the lowest linear Algebra level and significant performance improvement.", "Benefits for numerous solvers (Newton\u2013Raphson, homotopy, ", "-adic methods, etc.)"], "abstract": ["You recklessly told your boss that solving a non-linear system of size ", " (", " unknowns and ", " equations) requires a time proportional to ", ", as you were not very attentive during algorithmic complexity lectures. So now, you have only one night to solve a problem of big size (e.g., 1000 equations/unknowns), otherwise you will be fired in the next morning. The system is well-constrained and structurally irreducible: it does not contain any strictly smaller well-constrained subsystems. Its size is big, so the Newton\u2013Raphson method is too slow and impractical. The most frustrating thing is that if you knew the values of a small number ", " of key unknowns, then the system would be reducible to small square subsystems and easily solved. You wonder if it would be possible to exploit this reducibility, even without knowing the values of these few key unknowns. This article shows that it is indeed possible. This is done at the lowest level, at the linear algebra routines level, so that numerous solvers (Newton\u2013Raphson, homotopy, and also ", "-adic methods relying on Hensel lifting) widely involved in geometric constraint solving and CAD applications can benefit from this decomposition with minor modifications. For instance, with ", " key unknowns, the cost of a Newton iteration becomes ", " instead of ", ". Several experiments showing a significant performance gain of our re-parameterization technique are reported in this paper to consolidate our theoretical findings and to motivate its practical usage for bigger systems."]},
{"title": "Rapidly finding CAD features using database optimization", "highlights": ["This paper describes a declarative feature recognizer which utilizes concepts from database query optimization.", "It gives a general way to translate feature definitions to efficient SQL query.", "It uses lazy evaluation to reduce the work performed by the CAD modeler.", "It also uses estimated cost of reorder various geometric computations to further improve performance.", "Our approach provides linear time performance with respect to model size for common features."], "abstract": ["Automatic feature recognition aids downstream processes such as engineering analysis and manufacturing planning. Not all features can be defined in advance; a declarative approach allows engineers to specify new features without having to design algorithms to find them. Naive translation of declarations leads to executable algorithms with high time complexity. Database queries are also expressed declaratively; there is a large literature on optimizing query plans for efficient execution of database queries. Our earlier work investigated applying such technology to feature recognition, using a testbed interfacing a database system (SQLite) to a CAD modeler (CADfix). Feature declarations were translated into SQL queries which are then executed.", "The current paper extends this approach, using the PostgreSQL database, and provides several new insights: (i) query optimization works quite differently in these two databases, (ii) with care, an approach to query translation can be devised that works well for both databases, and (iii) when finding various simple common features, ", " time performance can be achieved with respect to model size, with acceptable times for real industrial models. Further results also show how (i) lazy evaluation can be used to reduce the work performed by the CAD modeler, and (ii) estimating the time taken to compute various geometric operations can further improve the query plan. Experimental results are presented to validate our main conclusions."]},
{"title": "A Total Order Heuristic-Based Convex Hull Algorithm for Points in the Plane", "highlights": ["We propose a 2D convex hull algorithm based on comparison operators.", "We propose a 2D convex hull algorithm that outperforms Quickhull.", "We propose a 2D non-convex hull algorithm."], "abstract": ["Computing the convex hull of a set of points is a fundamental operation in many research fields, including geometric computing, computer graphics, computer vision, robotics, and so forth. This problem is particularly challenging when the number of points goes beyond some millions. In this article, we describe a very fast algorithm that copes with millions of points in a short period of time without using any kind of parallel computing. This has been made possible because the algorithm reduces to a sorting problem of the input point set, what dramatically minimizes the geometric computations (e.g., angles, distances, and so forth) that are typical in other algorithms. When compared with popular convex hull algorithms (namely, Graham\u2019s scan, Andrew\u2019s monotone chain, Jarvis\u2019 gift wrapping, Chan\u2019s, and Quickhull), our algorithm is capable of generating the convex hull of a point set in the plane much faster than those five algorithms without penalties in memory space."]},
{"title": "Generalizing bicubic splines for modeling and IGA with irregular layout", "highlights": ["Bi-3 tensor-product splines are complemented by bi-4 splines near irregular points.", "The vertices of the irregular quad mesh serve as spline-like control points.", "The resulting surfaces have a good distribution of highlight lines.", "The resulting surfaces have a increased smoothness and reproduction at irregular points."], "abstract": ["Quad meshes can be interpreted as tensor-product spline control meshes as long as they form a regular grid, locally. We present a new option for complementing bi-3 splines by bi-4 splines near irregularities in the mesh layout, where less or more than four quadrilaterals join. These new generalized surface and IGA (isogeometric\u00a0analysis) elements have as their degrees of freedom the vertices of the irregular quad mesh. From a geometric design point of view, the new construction distinguishes itself from earlier work by a notably better distribution of highlight lines. From the IGA point of view, increased smoothness and reproduction at the irregular point\u00a0yield fast convergence."]},
{"title": "A method for improving measurement accuracy of cylinders in dimensional CT metrology", "highlights": ["The method can fit cylindrical surface from a projection image directly.", "The method can measure cylindrical surface with a higher accuracy.", "The method can complete measurement by use of much less measurement time."], "abstract": ["Measurement quality of industrial cone beam X-ray computed tomography (CT) is influenced by many types of artefacts, therefore, industrial-level accuracy is difficult to attain. In order to avoid the influences of these artefacts, this paper proposes a new method for measuring cylindrical surface that is a common geometric structure in mechanical parts. The proposed method fits cylindrical surface from a sinogram image directly. Compared with a standard way for dimensional CT metrology, higher measurement accuracy and less measurement time can be achieved. Mainly, the method consists of edge points detection of a sinogram image and cylindrical surface fitting. The performance of the method is evaluated by use of both simulation data and actual data."]},
{"title": "Can local NURBS refinement be achieved by modifying only the user interface?", "highlights": ["A mechanism providing local NURBS refinement by changing only the user interface.", "Provides the user with local editing without changing the underlying representation.", "A middle way between global refinement and changing to a different representation.", "The method will degenerate to non-local refinement in certain cases."], "abstract": ["NURBS patches have a serious restriction: they are constrained to a strict rectangular topology. This means that a request to insert a single new control point will cause a row of control points to appear across the NURBS patch, a global refinement of control. We investigate a method that can hide unwanted control points from the user so that the user\u2019s interaction is with local, rather than global, refinement. Our method requires only straightforward modification of the user interface and the data structures that represent the control mesh, making it simpler than alternatives that use hierarchical or T-constructions. Our results show that our method is effective in many cases but has limitations where inserting a single new control point in certain cases will still cause a cascade of new control points to appear across the NURBS patch."]},
{"title": "High-order curvilinear meshing using a thermo-elastic analogy", "highlights": ["We present a method to generate curved meshes using a thermo-elastic model.", "A linear elastic analogy is amended to include thermal stress terms.", "These terms \u2018heat\u2019 or \u2018cool\u2019 elements to allow for increased deformation.", "Both isotropic and anisotropic forms of the thermal stress tensor are presented.", "The method is demonstrated to have benefits in both two and three dimensions."], "abstract": ["With high-order methods becoming increasingly popular in both academia and industry, generating curvilinear meshes that align with the boundaries of complex geometries continues to present a significant challenge. Whereas traditional low-order methods use planar-faced elements, high-order methods introduce curvature into elements that may, if added naively, cause the element to self-intersect. Over the last few years, several curvilinear mesh generation techniques have been designed to tackle this issue, utilizing mesh deformation to move the interior nodes of the mesh in order to accommodate curvature at the boundary. Many of these are based on elastic models, where the mesh is treated as a solid body and deformed according to a linear or non-linear stress tensor. However, such methods typically have no explicit control over the validity of the elements in the resulting mesh. In this article, we present an extension of this elastic formulation, whereby a thermal stress term is introduced to \u2018heat\u2019 or \u2018cool\u2019 elements as they deform. We outline a proof-of-concept implementation and show that the adoption of a thermo-elastic analogy leads to an additional degree of robustness, by considering examples in both two and three dimensions."]},
{"title": "LayTracks3D: A new approach for meshing general solids using medial axis transform", "highlights": ["The proposed meshing algorithm handles general solids.", "Output mesh is boundary sensitive and preserves sharp features.", "Output mesh has very low percentage of non-hex elements.", "LayTracks3D is based on a mathematically sound formulation of MAT.", "Extension to all-hex meshing is discussed."], "abstract": ["This paper presents an extension of the all-quad meshing algorithm called LayTracks to generate high quality hex-dominant meshes of general solids. LayTracks3D uses the mapping between the Medial Axis (MA) and the boundary of the 3D domain to decompose complex 3D domains into simpler domains called Tracks. Tracks in 3D have no branches and are symmetric, non-intersecting, orthogonal to the boundary, and the shortest path from the MA to the boundary. These properties of tracks result in desired meshes with near cube shape elements at the boundary, structured mesh along the boundary normal with any irregular nodes restricted to the MA, and sharp boundary feature preservation. The algorithm has been tested on a few industrial CAD models and hex-dominant meshes are shown in the Results section. Work is underway to extend LayTracks3D to generate all-hex meshes."]},
{"title": "Dimensional perturbation of rigidity and mobility", "highlights": ["Unified state transition framework.", "Iso-constrained, over-constrained and paradoxical mechanisms in the same framework.", "Understanding over constrained vs. paradoxical through dimensional perturbation.", "Non ambiguous definition of paradoxical mechanism."], "abstract": ["Mechanisms, defined as assemblies of dimensioned rigid bodies linked by ideal joints, can be partitioned in three mobility states: the rigid state (where bodies can have only one position relative to each other), the mobile state (where bodies can move relatively to each other) and the impossible state (where bodies dimensions and specified joints cannot lead to a feasible assembly). It is also clear that although bodies dimensions can vary in a continuous way, assemblies may experience quite abrupt changes across those states. This paper proposes a new approach to this problem with the goal of being able to predict the mobility class of an assembly of arbitrary complexity, and how it can be affected by a perturbation of the dimensions of its bodies. It does so by proposing a simple and general state transition framework including the three above defined states and seven transitions describing how a dimensional perturbation can affect them. Using this framework, the mobility of a mechanism is easier to capture and predict, using only dimensional (", ") and positional (", ") parameters involved in an appropriate equation (", "). This is achieved by focusing on how ", " behaves when ", " and ", " get perturbed, and the impact of this reaction on the mobility state of the assembly. As a result of this more mathematic approach to the problem, previously used notions of iso-constraint, over-constraint and paradoxical assembly, traditionally used to describe such assemblies, can be rigorously defined and thus clarified."]},
{"title": "A decade of progress on anisotropic mesh adaptation for computational fluid dynamics", "highlights": ["Steady and unsteady error estimates.", "Adaptive mesh adaptation techniques.", "Adjoint-based error estimates for Euler equations.", "Remaining potential challenges for mesh generation."], "abstract": ["In the context of scientific computing, the mesh is used as a discrete support for the considered numerical methods. As a consequence, the mesh greatly impacts the efficiency, the stability and the accuracy of numerical methods. The goal of anisotropic mesh adaptation is to generate a mesh which fits the application and the numerical scheme in order to achieve the best possible solution. It is thus an active field of research which is progressing continuously. This review article proposes a synthesis of the research activity of the INRIA Gamma3 team in the field of anisotropic mesh adaptation applied to inviscid flows in computational fluid dynamics since 2000. It shows the evolution of the theoretical and numerical results during this period. Finally, challenges for the next decade are discussed."]},
{"title": "Fragmentary shape recognition: A BCI study", "highlights": ["BCI based fragmentary shape recognition process.", "Innovative use of BCI in shape recognition domain.", "Power spectral density and cognitive load estimation.", "IRB approved study to assess the fragmentary shape recognition process."], "abstract": ["Recently, Brain\u2013Computer Interface (BCI) has emerged as a potential modality that utilizes natural and intuitive human mechanisms of thinking process to enable interactions in CAD interfaces. Before BCI could become a mainstream mode of HCI for CAD interfaces; fundamental studies directed towards understanding how humans mentally represent and process the geometry are needed. The outlined work in this paper presents an objective user study to understand shape recognition process in the humans. Specifically, we focus on the fundamental task of fragmentary shape identification. The problem of fragmentary shape recognition can be defined as follows: given a partial and incomplete minimalistic representation of a given shape, can one recognize the actual complete shape or object? In user studies, each subject was progressively (in stages) shown more informative fragmented images of an object to be recognized. During each stage of the experiment, the brain activity of users in the form of electroencephalogram (EEG) signals was recorded with a BCI headset. The recorded signals are then processed to objectively study the fragmentary shape recognition process. The results of user studies conclusively show that the measured brain activities of subjects can serve as a very accurate proxy to estimate subjects fragmentary shape recognition process."]},
{"title": "Assembly-based conceptual 3D modeling with unlabeled components using probabilistic factor graph", "highlights": ["We present a novel 3D CAD tool for conceptual design exploration.", "Interactive concept exploration through assembly-based 3D modeling paradigm.", "Automated component suggestion algorithm based on probabilistic factor graph.", "Creative reuse of 3D models available on vast online repositories."], "abstract": ["This work presents a novel and intuitive assembly based 3D modeling interface to support conceptual design exploration activities. In the presented modeling interface, unlabeled segmented components of the objects are assembled to create new 3D models. The development of the interface is motivated by two aspects. First, the focus is on novice users since they stand to gain the most from intuitive interfaces. Second, the intent is on creative reuse of a growing number and variety of 3D models available on vast online repositories like Turbosquid and Trimble 3D warehouse. Specifically, we have devised an automated component suggestion algorithm based on a probabilistic factor graph. This algorithm helps the user to easily browse and select components from a database that are most compatible with the current state of 3D models being assembled. The component suggestion algorithm incorporates various aspects such as shape similarity, repetitions of shapes, and adjacency relationships. Our new suggestive interface overcomes several limitations of traditional CAD interfaces by helping the users to quickly create and explore new conceptual designs. We present results on the conceptual design of several products."]},
{"title": "Frame field smoothness-based approach for hex-dominant meshing", "highlights": ["A frame field smoothness-based algorithm for bulk point insertion is proposed.", "An iterative procedure for smoothing the frame field is proposed.", "The impact of geometric singularities on the final mesh is reduced.", "Volumic ratio of hexahedra is increased up to twenty percents."], "abstract": ["An indirect approach for building hex-dominant meshes is proposed: a tetrahedral mesh is constructed at first and is recombined to create a maximum amount of hexahedra. The efficiency of the recombination process is known to significantly depend on the quality of the sampling of the vertices. A good vertex sampling depends itself on the quality of the underlying frame field that has been used to locate the vertices. An iterative procedure to obtain a high quality three-dimensional frame field is presented. Then, a new point insertion algorithm based on a frame field smoothness is developed. Points are inserted in priority in smooth frame field regions. The new approach is tested and compared with simpler strategies on various geometries. The new method leads to hex-dominant meshes exhibiting either an equivalent or a larger volume ratio of hexahedra (up to 20%) compared to the frontal point insertion approach."]},
{"title": "Robustness and efficiency of geometric programs: The Predicate Construction Kit (PCK)", "highlights": ["A system to automatically generate C++ code for robust predicates from their formulas.", "The predicates involved in the intersection between a Voronoi diagram and a tetrahedral solid.", "A companion open-source implementation (Predicate Construction Kit) in the GEOGRAM library.", "Experimental validation with several synthetic and real industrial datasets."], "abstract": ["In this article, I focus on the robustness of ", " (e.g.,\u00a0Delaunay triangulation, intersection between surfacic or volumetric meshes, Voronoi-based meshing \u2026) w.r.t. numerical degeneracies. Some of these geometric programs require \u201cexotic\u201d predicates, not available in standard libraries (e.g.,\u00a0J.-R. Shewchuk\u2019s implementation and CGAL). I propose a complete methodology and a sample Open Source implementation of a toolset (PCK: Predicate Construction Kit) that makes it reasonably easy to design geometric programs free of numerical errors. The C++ code of the predicates is automatically generated from its formula, written in a simple specification language. Robustness is obtained through a combination of arithmetic filters, expansion arithmetics and symbolic perturbation.", "As an example of my approach, I give the formulas and PCK source-code for the 4 predicates used to compute the intersection between a 3d Voronoi diagram and a tetrahedral mesh, as well as symbolic perturbations that provably escapes the corner cases. This allows to robustly compute the intersection between a Voronoi diagram and a triangle mesh, or the intersection between a Voronoi diagram and a tetrahedral mesh. Such an algorithm may have several applications, including surface and volume meshing based on Lloyd relaxation."]},
{"title": "High-quality quadratic curve fitting for scanned data of styling design", "highlights": ["We propose a method for fitting a ", " quadratic B-spline curve to planar styling design data.", " continuity is attained by using a non-uniform knot vector of the B-spline curve.", "High-quality curves have been obtained from noisy styling design data."], "abstract": ["We propose a new method for fitting a high-quality planar curve to styling design data by using a curvature continuous (", ") quadratic B-spline curve. In order to attain ", " continuity of the B-spline curve, we use a non-uniform knot vector, which also enables the curve to be composed of fewer segments as compared to a uniform curve. In our method, control points and the knot vector of the B-spline curve are calculated separately; therefore, we can avoid solving a complicated nonlinear optimization problem. By conducting experiments, we demonstrate that high-quality curves can be generated from both artificial noisy data and real-world scanned data."]},
{"title": "Arc\u2013surface intersection method to calculate cutter\u2013workpiece engagements for generic cutter in five-axis milling", "highlights": ["Arc\u2013surface intersection method to obtain cutter\u2013workpiece engagements is proposed.", "Feasible contact arc is extracted to calculate the engagement boundaries.", "Analytical solution can be obtained for flat workpiece by using this method.", "This method is compatible well with the force model without loss of precision."], "abstract": ["Calculating cutter\u2013workpiece engagements (CWEs) is essential to the physical simulation of milling process that starts with the prediction of cutting forces. As for five-axis milling of free form surfaces, the calculation of CWEs remains a challenge due to the complicated and varying engagement geometries that occur between the cutter and the in-process workpiece. In this paper, a new arc\u2013surface intersection method (ASIM) is proposed to obtain CWEs for generic cutter in five-axis milling. The cutter rotary surface is first represented by the family of section circles which are generated by slicing the cutter with planes perpendicular to the tool axis. Based on the envelope condition, two grazing points on each section circle are analytically derived, which divide the circle into two arcs. The feasible contact arc (FCA) is then extracted to intersect with workpiece surfaces. Using arc/surface intersection and distance fields based approach, the boundary of the closed CWEs is accurately and efficiently calculated. Compared with the solid modeler based method and the discrete method, the ASIM has higher computational efficiency and accuracy. Moreover, an analytical solution for calculating CWEs can be obtained with this method in five-axis milling of the workpiece merely comprising of flat and quadric surfaces. Finally, two case tests are implemented to confirm the validity of the ASIM and comparisons have been made with a Vericut based system which utilizes the Z-buffer method. The results indicate that the ASIM is computationally efficient, accurate and robust."]},
{"title": "Vectorizing NURBS surface evaluation with basis functions in power basis", "highlights": ["Direct evaluation of NURBS curves and surfaces.", "Basis functions precomputed in shifted power basis.", "Branchless vectorized linear search to compute span index.", "Vectorized evaluation of NURBS surface: x 3 speedup."], "abstract": ["Several known methods for direct evaluation of NURBS curves and surfaces are described. Runtime performance and simplicity of vectorization are discussed. An evaluation method, which uses basis functions precomputed in shifted power basis, is shown to be promising. This method for surfaces is vectorized with SSE2 intrinsics, yielding 3 times performance improvement over the non-vectorized version. Branchless vectorized linear search is proposed for span search, being most efficient for small number of knots. Binary search ending with a small linear search is shown to be most efficient for large number of knots, and good for general case. Performance comparison of the evaluation method and its equivalents from available geometric kernels is included."]},
{"title": "Off-centre Steiner points for Delaunay-refinement on curved surfaces", "highlights": ["Development of a new unstructured triangulation algorithm for smooth surfaces.", "Hybridisation of existing Delaunay-refinement and advancing-front techniques.", "Combines desirable aspects of both methods: high element quality, robustness.", "Provably good behaviour: guaranteed termination, bounded element quality."], "abstract": ["An extension of the restricted Delaunay-refinement algorithm for surface mesh generation is described, where a new point-placement scheme is introduced to improve element quality in the presence of mesh size constraints. Specifically, it is shown that the use of ", " Steiner points, positioned on the faces of the associated Voronoi diagram, typically leads to significant improvements in the shape- and size-quality of the resulting surface tessellations. The new algorithm can be viewed as a Frontal-Delaunay approach \u2014 a hybridisation of conventional Delaunay-refinement and advancing-front techniques in which new vertices are positioned to satisfy both element size and shape constraints. The performance of the new scheme is investigated experimentally via a series of comparative studies that contrast its performance with that of a typical Delaunay-refinement technique. It is shown that the new method inherits many of the best features of classical Delaunay-refinement and advancing-front type methods, leading to the construction of smooth, high quality surface triangulations with bounded radius-edge ratios and convergence guarantees. Experiments are conducted using a range of complex benchmarks, verifying the robustness and practical performance of the proposed scheme."]},
{"title": "Towards an automated robotic arc-welding-based additive manufacturing system from CAD to finished part", "highlights": ["An automated arc-welding-based additive manufacturing system was reported.", "Integrated additive and subtractive manufacturing methodology was developed.", "Deposition paths and welding parameters were automatically generated.", "User interface using only CAD models as inputs was developed.", "The proposed automated system was verified experimentally."], "abstract": ["Arc welding has been widely explored for additive manufacturing of large metal components over the last three decades due to its lower capital cost, an unlimited build envelope, and higher deposition rates. Although significant improvements have been made, an arc welding process has yet to be incorporated in a commercially available additive manufacturing system. The next step in exploiting \u201ctrue\u201d arc-welding-based additive manufacturing is to develop the automation software required to produce CAD-to-part capability. This study focuses on developing a fully automated system using robotic gas metal arc welding to additively manufacture metal components. The system contains several modules, including bead modelling, slicing, deposition path planning, weld setting, and post-process machining. Among these modules, bead modelling provides the essential database for process control, and an innovative path planning strategy fulfils the requirements of the automated system. A user friendly interface has been developed for non-experts to operate the developed system. Finally, a thin-walled aluminium structure has been fabricated automatically using only a CAD model as the informational input to the system. This exercise demonstrates that the developed system is a significant contribution towards the ultimate goal of producing a practical and highly automated arc-welding-based additive manufacturing system for industrial application."]},
{"title": "A semi-analytical approach to un-deformed chip boundary theory and cutting force prediction in face-hobbing of bevel gears", "highlights": ["Semi-analytical representation of the chip geometry in face-hobbing is presented.", "The method is computationally efficient and more accurate than numerical methods.", "Cutting forces in face-hobbing are predicted.", "Regions where are more prone to be worn out along the cutting edge are predicted."], "abstract": ["Rule-of-thumb based design for cutting tools and machining settings in face-hobbing of bevel gears result in cutting tool failures and quality issues. Lack of a virtual machining environment, to efficiently obtain the instantaneous un-deformed chip geometry and predict cutting forces in face-hobbing, causes undesirable production costs in industries. In the present paper, semi-analytical representation of the projection of the un-deformed chip on the rake face of the cutting blades is presented. The proposed approach is drastically fast and more accurate in comparison with numerical methods and can be implemented in a virtual gear machining environment. The cutting system intricate geometry, multi-axis machine tool kinematic chains and the variant cutting velocity along the cutting edge are taken into consideration to obtain the chip geometry efficiently. Then, cutting forces are predicted during face-hobbing by implementing oblique cutting theory using the derived chip geometry and converting face-hobbing into oblique cutting. The proposed methods are applied on two case studies of face-hobbing of bevel gears, and the chip geometry is derived and the cutting forces are predicted."]},
{"title": "Finite element mesh deformation with the skeleton-section template", "highlights": ["The skeleton-section template is designed to parameterize the input FE mesh hierarchically.", "FE meshes can be deformed globally with the skeleton and locally with the cross-sections.", "Local mesh deformations along prescribed directions are realized by anisotropic metric in the embedding space."], "abstract": ["To develop fast finite element (FE) adaptation methods for simulation-driven design optimization, we propose a radial basis functions (RBF) method with a skeleton-section template to globally and locally deform FE meshes of thin-walled beam structures.", "The skeleton-section template is automatically formulated from the input mesh and serves as a hierarchical parameterization for the FE meshes. With this hierarchical parameterization, both the global and the local geometries of a thin-walled beam can be processed in the same framework, which is of importance for designing engineering components. The curve skeleton of the mesh is constructed with Voronoi decomposition, while the cross-sections are extracted from the mesh based on the curve skeleton.", "The RBF method is employed to locally and globally deform the mesh model with the cross-sections and the skeleton, respectively. The RBF method solves the spatial deformation field given prescribed deformations at the cross-sections. At the local scale, the user modifies the cross-sections to deform a region of the surface mesh. At the global level, the skeleton is manipulated and its deformation is transferred to all cross-sections to induce the mesh deformation.", "In order to handle curved mesh models and attain flexible local deformations, the input mesh is embedded into its skeleton frame field using an anisotropic distance metric. In this way, even strip-like features along arbitrary directions can be created on the mesh model using only a few cross-sections as the deformation handles. In addition, form features can be rigidly preserved at both deformation levels.", "Numerical examples demonstrate that intuitive and qualified FE mesh deformations can be obtained with manipulation of the skeleton-section template."]},
{"title": "An evolutionary approach to the extraction of object construction trees from 3D point clouds", "highlights": ["An algorithm for extracting an object from a 3D point-cloud.", "An evolutionary approach for combining primitives, fitted to the input point-cloud, by modeling operations.", "A technique for limiting the size of the evolved model to allow its reusability."], "abstract": ["In order to extract a construction tree from a finite set of points sampled on the surface of an object, we present an evolutionary algorithm that evolves set-theoretic expressions made of primitives fitted to the input point-set and modeling operations. To keep relatively simple trees, we use a penalty term in the objective function optimized by the evolutionary algorithm. We show with experiments successes but also limitations of this approach."]},
{"title": "A level-set based multi-material topology optimization method using a reaction diffusion equation", "highlights": ["A multi-material topology optimization method using a reaction diffusion equation.", "Modified Multi-material Level Set (MM-LS) topology description model.", "Controllable geometrical complexity of optimal solutions."], "abstract": ["A level-set based multi-material topology optimization method using a reaction diffusion equation is proposed in this paper. Each phase is represented by a combined formulation of different level set functions. This description model is modified from Multi-Material Level Set (MM-LS) topology description model. With a total number of ", " level set functions, this approach provides a representation of ", " materials and one void phase (totally ", " phases). By this approach, the mathematic model of the multi-material topology optimization problem using a reaction diffusion equation is established. With this model, the geometrical complexity of optimal solutions can be easily controlled by appropriately setting a regularization parameter. Some implementation details for solving this model are also presented in this paper. Finally, several typical numerical examples are shown to confirm the effectiveness of the proposed method."]},
{"title": "Parametric CAD modeling: An analysis of strategies for design reusability", "highlights": ["We analyze three formal CAD modeling strategies for history-based parametric design.", "User performance was studied using three industrial CAD models with different levels of complexity.", "Formal modeling methodologies offer significant advantages over non-structured approaches.", "Modeling methodologies significantly affect reusability."], "abstract": ["CAD model quality in parametric design scenarios largely determines the level of flexibility and adaptability of a 3D model (how easy it is to alter the geometry) as well as its reusability (the ability to use existing geometry in other contexts and applications). In the context of mechanical CAD systems, the nature of the feature-based parametric modeling paradigm, which is based on parent\u2013child interdependencies between features, allows a wide selection of approaches for creating a specific model. Despite the virtually unlimited range of possible strategies for modeling a part, only a small number of them can guarantee an appropriate internal structure which results in a truly reusable CAD model. In this paper, we present an analysis of formal CAD modeling strategies and best practices for history-based parametric design: Delphi\u2019s horizontal modeling, explicit reference modeling, and resilient modeling. Aspects considered in our study include the rationale to avoid the creation of unnecessary feature interdependencies, the sequence and selection criteria for those features, and the effects of parent/child relations on model alteration. We provide a comparative evaluation of these strategies in the form of a series of experiments using three industrial CAD models with different levels of complexity. We analyze the internal structure of the models and compare their robustness and flexibility when the geometry is modified. The results reveal significant advantages of formal modeling methodologies, particularly resilient techniques, over non-structured approaches as well as the unexpected problems of the horizontal strategy in numerous modeling situations."]},
{"title": "Path planning with obstacle avoidance by ", "highlights": ["A two-step approach to design planar smooth collision-free paths is presented.", "The construction of piecewise linear paths with angle-based criteria is investigated.", "The smooth path is based on PH spline interpolation schemes with tension parameters.", "A selection of test cases demonstrates the quality of the new motion planning scheme."], "abstract": ["We propose a two-step approach for the construction of planar smooth collision-free navigation paths. Obstacle avoidance techniques that rely on classical data structures are initially considered for the identification of piecewise linear paths having no intersection with the obstacles of a given scenario. Variations of the shortest piecewise linear path with angle-based criteria are proposed and discussed. In the second part of the scheme we rely on spline interpolation algorithms with tension parameters to provide a smooth planar control strategy. In particular, we consider the class of curves with Pythagorean structures, because they provide an exact computation of fundamental geometric quantities. A selection of test cases demonstrates the quality of the new motion planning scheme."]},
{"title": "Efficient simulation and rendering of realistic motion of one-dimensional flexible objects", "highlights": ["Natural and realistic motion of flexible 1D objects.", "Smoother and higher order continuity by use of splines.", "Tractrix based motion applied to control polygon instead of the polyline.", "Approximate length preservation by modifying control polygon.", "Adaptive subdivision and merging to reduce computation effort."], "abstract": ["In gross motion of flexible one-dimensional (1D) objects such as cables, ropes, chains, ribbons and hair, the assumption of constant length is realistic and reasonable. The motion of the object also appears more natural if the motion or disturbance given at one end attenuates along the length of the object. In an earlier work, variational calculus was used to derive natural and length-preserving transformation of planar and spatial curves and implemented for flexible 1D objects discretized with a large number of straight segments. This paper proposes a novel idea to reduce computational effort and enable real-time and realistic simulation of the motion of flexible 1D objects. The key idea is to represent the flexible 1D object as a spline and move the underlying control polygon with much smaller number of segments. To preserve the length of the curve to within a prescribed tolerance as the control polygon is moved, the control polygon is adaptively modified by subdivision and merging. New theoretical results relating the length of the curve and the angle between the adjacent segments of the control polygon are derived for quadratic and cubic splines. Depending on the prescribed tolerance on length error, the theoretical results are used to obtain threshold angles for subdivision and merging. Simulation results for arbitrarily chosen planar and spatial curves whose one end is subjected to generic input motions are provided to illustrate the approach."]},
{"title": "Reconstruction of B-spline curves and surfaces by adaptive group testing", "highlights": ["The B-spline by Adaptive Group testing Estimation (B-AGE) is proposed.", "B-AGE derives a B-spline curve or surface estimation only from salient 3D points.", "The salient points are found sequentially by means of group testing.", "B-AGE finds a unique solution with a minimum Akaike Information Criteria (AIC) value."], "abstract": ["Point clouds as measurements of 3D sensors have many applications in various fields such as object modeling, environment mapping and surface representation. Storage and processing of raw point clouds is time consuming and computationally expensive. In addition, their high dimensionality shall be considered, which results in the well known ", ". Conventional methods either apply reduction or approximation to the captured point clouds in order to make the data processing tractable. B-spline curves and surfaces can effectively represent 2D data points and 3D point clouds for most applications. Since processing all available data for B-spline curve or surface fitting is not efficient, based on the ", " theory an algorithm is developed that finds salient points sequentially. The B-spline curve or surface models are updated by adding a new salient point to the fitting process iteratively until the Akaike Information Criterion (AIC) is met. Also, it has been proved that the proposed method finds a unique solution so as what is defined in the group testing theory. From the experimental results the applicability and performance improvement of the proposed method in relation to some state-of-the-art B-spline curve and surface fitting methods, may be concluded."]},
{"title": "Slice coherence in a query-based architecture for 3D heterogeneous printing", "highlights": ["We explore 3D Voronoi-based heterogeneous printing.", "We utilize Euler loops to minimize non-extruding fast travels.", "The motion paths of consecutive slices are almost the same.", "The motion paths are not generated from scratch at every slice.", "Only 3 local cases can arise when updating the motion paths."], "abstract": ["We report on 3D printing of artifacts with a structured, inhomogeneous interior. The interior is decomposed into cells defined by a 3D Voronoi diagram and their sites. When printing such objects, most slices the printer deposits are topologically the same and change only locally in the interior. The slicing algorithm capitalizes on this coherence and minimizes print head moves that do not deposit material. This approach has been implemented on a client/server architecture that computes the slices on the geometry side. The slices are printed by fused deposition, and are communicated upon demand."]},
{"title": "Variational geometric modeling with black box constraints and DAGs", "highlights": ["Extending geometric constraints with black box constraints which have no equation.", "Solving and optimizing without equation.", "This approach applies to all history-based or parametric modelers."], "abstract": ["CAD modelers enable designers to construct complex 3D shapes with high-level B-Rep operators. This avoids the burden of low level geometric manipulations. However a gap still exists between the shape that the designers have in mind and the way they have to decompose it into a sequence of modeling steps. To bridge this gap, Variational Modeling enables designers to specify constraints the shape must respect. The constraints are converted into an explicit system of mathematical equations (potentially with some inequalities) which the modeler numerically solves. However, most of available programs are 2D sketchers, basically because in higher dimension some constraints may have complex mathematical expressions. This paper introduces a new approach to sketch constrained 3D shapes. The main idea is to replace explicit systems of mathematical equations with (mainly) Computer Graphics routines considered as Black Box Constraints. The obvious difficulty is that the arguments of all routines must have known numerical values. The paper shows how to solve this issue, ", " \u00a0 how to solve and optimize without equations. The feasibility and promises of this approach are illustrated with the developed DECO (Deformation by Constraints) prototype."]},
{"title": "Data driven webpage color design", "highlights": ["The first attempt to automate webpage coloring through a data driven approach.", "We addressed the three fundamental design objectives of webpage coloring.", "We introduced novel probabilistic models capturing color contrasts and semantics.", "The models coordinated with the lexicographic strategy prove effective in demos.", "User tests verify the system-generated designs are more preferable."], "abstract": ["This paper presents a design framework for automatic webpage coloring regarding several fundamental design objectives: proper visual contrasts, multi-color compatibility and semantic associations. The objective functions are formulated with data-driven probabilistic models: the Color Contrast model concerning visual saliencies is trained on 52,000 basic components parsed from 500 popular webpages. Color Compatibility and Semantics are modeled from a dataset of manually tagged and rated color schemes from Adobe Kuler. To incorporate the multi-objectives in optimization, the framework adopts a lexicographic strategy, which determines the best choices by optimizing the objectives one by one in a user specified sequence. We demonstrate the effectiveness of the models and the flexibility of the framework in two typical web color design scenarios: fine tuning a colored page and recoloring a page with a specified palette. Independent perception experiments verify that the system-generated designs are preferable to those generated by nonprofessionals."]},
{"title": "A framework for geometry acquisition, 3-D printing, simulation, and measurement of head-related transfer functions with a focus on hearing-assistive devices", "highlights": ["We present a pipeline of geometry acquisition, printing, and HRTF determination.", "HRTFs are determined from both acoustical measurements and FEM simulations.", "Monaural spectral features were more similar between measurements than simulations.", "Binaural ITD cues were very similar among all three HRTF sets."], "abstract": ["Individual head-related transfer functions (HRTFs) are essential in applications like fitting hearing-assistive devices (HADs) for providing accurate sound localization performance. Individual HRTFs are usually obtained through intricate acoustic measurements. This paper investigates the use of a three-dimensional (3D) head model for acquisition of individual HRTFs. Two aspects were investigated; whether a 3D-printed model can replace measurements on a human listener and whether numerical simulations can replace acoustic measurements. For this purpose, HRTFs were acoustically measured for four human listeners and for a 3D printed head model of one of these listeners. Further, HRTFs were simulated by applying the finite element method to the 3D head model. The monaural spectral features and spectral distortions were very similar between re-measurements and between human and printed measurements, however larger deviations were observed between measurement and simulation. The binaural cues were in agreement among all HRTFs of the same listener, indicating that the 3D model is able to provide localization cues potentially accessible to HAD users. Hence, the pipeline of geometry acquisition, printing, and acoustic measurements or simulations, seems to be a promising step forward towards in-silico design of HADs."]},
{"title": "Knitted fabrics design and manufacture: A novel CAD system for qualifying bagging performance based on geometric-mechanical models", "highlights": ["Develop a novel CAD system to evaluate and qualify the bagging performance of knitted fabrics.", "Simulate the bagging rheological behaviors in knitted fabrics by a set of geometric-mechanical models.", "Characterize the parameters of fibers, yarns and fabrics provided with available measurement methods.", "Rapidly achieve the knitted fabric products with desirable mechanical functions."], "abstract": ["Knitted fabrics have excellent formability and tensile ability and are widely used in textile-related products and industrial applications. The traditional quality control on fabric\u2019s performance is undertaken by repeated measurement and testing which is very time-consuming and has great expense. There are urgent requirements for the designers and manufacturers to validate and control the mechanical performance of knitted fabrics. In this paper, we present a novel simulation-based CAD system for evaluating and qualifying the bagging performance of knitted fabrics. A set of geometric-mechanical models are developed with characterization of the fibers/yarns and fabrics for available inputs, which make it feasible for practical applications. Through encapsulating the models and presenting with a series of friendly interfaces, the CAD system offers users the abilities of data management, numerical design, bagging simulation and performance preview of knitted fabrics. The simulation capability of the models is validated by comparing the predicted results with measured data from experiments under same bagging testing conditions. The potential industrial applications of this system is demonstrated, and the designers and manufacturers can achieve the knitted fabric products with desirable mechanical functions effectively and economically."]},
{"title": "A new approach to automatic and a priori mesh adaptation around circular holes for finite element analysis", "highlights": ["A new approach to a priori mesh adaptation around circular holes in 2D.", "Based on offline FEA results and error distributions obtained on a reference case.", "A heuristic relationship between element size and FEA error.", "Potential extension to 3D holes and to other 2D features."], "abstract": ["Through our research on the integration of finite element analysis in the design and manufacturing process with CAD, we have proposed the concept of mesh pre-optimization. This concept consists in converting shape and analysis information in a size map (a mesh sizing function) with respect to various adaptation criteria (refining the mesh around geometric form features, minimizing the geometric discretization error, boundary conditions, etc.). This size map then represents a constraint that has to be respected by automatic mesh generation procedures. This paper introduces a new approach to automatic mesh adaptation around circular holes. This tool aims at optimizing, before any FEA, the mesh of a CAD model around circular holes. This approach, referred to as \u201ca priori\u201d mesh adaptation, should not be regarded as an alternative to adaptive a posteriori mesh refinement but as an efficient way to obtain reasonably accurate FEA results before a posteriori adaptation, which is particularly interesting when evaluating design scenarios. The approach is based on performing many offline FEA analyses on a reference case and deriving, from results and error distributions obtained, a relationship between mesh size and FEA error. This relationship can then be extended to target user specified FEA accuracy objectives in a priori mesh adaptation for any distribution of circular holes. The approach being purely heuristic, fulfilling FEA accuracy objectives, in all cases, cannot be theoretically guaranteed. However, results obtained using varying hole diameters and distributions in 2D show that this heuristic approach is reliable and useful. Preliminary results also show that extension of the method can be foreseen towards a priori mesh adaptation in 3D and mesh adaptation around other types of 2D features."]},
{"title": "Tool orientation optimization for ", "highlights": ["Average strip width estimation method and sample points selection method for ", "-axis sculptured surface machining.", "The quasi-feasible sector (QFS) in a projection plane for determining the domain that contains the optimal tool orientation.", "Novel tool orientation optimization method based on a bunch of projection planes.", "Numerical examples for tool orientation optimization in ", "-axis machining of a sculptured surface."], "abstract": ["This paper presented an optimization method to select a tool orientation for machining a sculptured surface by the ", "-axis machining strategy. The optimization method could select the tool orientation for the maximum average strip width in ", "-axis machining. The method could also be used to determine the workpiece setup for general 3-axis machining. The average strip width estimation method was presented as well. Quasi-feasible sectors containing the optimal tool orientation could be found according to the projection planes and the normal vectors of sample points. And the method can find the optimal tool orientation based on projection planes. A freeform surface was parted into 9 sub-surfaces firstly, and then the presented method was applied on those sub-surfaces to determine the optimal tool orientations. The tool paths were generated with the optimized tool orientations and used to mill the sub-surfaces without interference. The method presented could also be applied on the trimmed surface, the surface with a boss, and the blade on a blisk. The machining results indicate that our method can improve machining efficiency through reducing the number of tool paths for ", "-axis sculptured surface machining."]},
{"title": "Implicit slicing for functionally tailored additive manufacturing", "highlights": ["A novel implicitly-defined slicer for additive manufacturing (AM) is developed.", "The implict formulation allows for slicing based on design intent.", "The slicer is intended to produce components with tailored functional properties.", "A large improvement in the mechanical properties of AM components is demonstrated."], "abstract": ["One crucial component of the additive manufacturing software toolchain is a class of geometric algorithms known as \u201cslicers.\u201d The purpose of the slicer is to compute a parametric toolpath and associated commands, which direct an additive manufacturing system to produce a physical realization of a three-dimensional input model. Existing slicing algorithms operate by application of geometric transformations upon the input geometry in order to produce the toolpath. In this paper we introduce a new implicit slicing algorithm based on the computation of toolpaths derived from the level sets of arbitrary heuristics-based or physics-based fields defined over the input geometry. This enables computationally efficient slicing of arbitrarily complex geometries in a straight forward fashion. Additionally, the calculation of component \u201cinfill\u201d (as a process control parameter) is explored due to its crucial effect on functional performance fields of interest such as strain and stress distributions. Several examples of the application of the proposed implicit slicer are presented. Finally, an example demonstrating improved structural performance during physical testing is presented. We conclude with remarks regarding the strengths of the implicit approach relative to existing explicit approaches, and discuss future work required in order to extend the methodology."]},
{"title": "Sweep scan path planning for efficient freeform surface inspection on five-axis CMM", "highlights": ["A method for planning sweep scan path for freeform surfaces on a 5-axis CMM is proposed.", "The generated scan path utilizes kinematic advantages of 5-axis CMM to improve scanning efficiency.", "Experiments show significant improvement in scanning efficiency compared to some existing methods."], "abstract": ["Compared with the traditional 3-axis coordinate measuring machine (CMM), a 5-axis CMM equipped with the capability of continues sweep scanning can provide much denser data points while taking much shorter time. This paper presents an automatic sweep scan path planning system that generates a continuous sweep scanning path for the inspection of an arbitrary free-form surface using a 5-axis CMM with three translational axes and a rotary head with two very light rotary axes. The system strives to significantly improve the scanning efficiency by utilizing the superb kinematic advantages of the two rotary axes, which have very low moment of inertia, to cover a larger area, while tremendously reducing the speed and acceleration demand on the three translational axes which have much larger inertia. The path is generated from a mesh model of the freeform surface and an iterative approach is used to ensure that the stylus contacts the surface at an acceptable angle during the entire scan. Physical scanning experiments are performed and the test results show significant improvement in scanning efficiency by the proposed sweep scan path planning method when compared with some existing continuous scanning path planning approaches such as the standard isoparametric or zigzag method."]},
{"title": "Molecular dynamics-based unstructured grid generation method for aerodynamic applications", "highlights": [], "abstract": ["A new approach to triangular "]},
{"title": "A new approach to modeling of selected human respiratory system diseases, directed to computer simulations", "highlights": ["We model respiratory system with 11-coefficient electrical analogue.", "New method for diseases modeling is proposed, based on laws of mechanics.", "Some common, severe diseases are modeled on the basis of proposed method.", "Developed modeling method provide great time saving and acceptable results."], "abstract": ["This paper presents a new versatile approach to model severe human respiratory diseases via computer simulation. The proposed approach enables one to predict the time histories of various diseases via information accessible in medical publications. This knowledge is useful to bioengineers involved in the design and construction of medical devices that are employed for monitoring of respiratory condition. The approach provides the data that are crucial for testing diagnostic systems. This can be achieved without the necessity of probing the physiological details of the respiratory system as well as without identification of parameters that are based on measurement data."]},
{"title": "GPU-based simulation of the long-range Potts model via parallel tempering", "highlights": [], "abstract": []},
{"title": "Quadcopter flight control using a low-cost hybrid interface with EEG-based classification and eye tracking", "highlights": ["We present a noninvasive and wearable interface to control a quadcopter in 3D space.", "The hybrid system is low cost and easily wearable.", "The system hybridizes eye tracking and EEG-based classification.", "The system allows users to complete their tasks easily with various commands.", "People can control their flight naturally in everyday life."], "abstract": ["We propose a wearable hybrid interface where eye movements and mental concentration directly influence the control of a quadcopter in three-dimensional space. This noninvasive and low-cost interface addresses limitations of previous work by supporting users to complete their complicated tasks in a constrained environment in which only visual feedback is provided. The combination of the two inputs augments the number of control commands to enable the flying robot to travel in eight different directions within the physical environment. Five human subjects participated in the experiments to test the feasibility of the hybrid interface. A front view camera on the hull of the quadcopter provided the only visual feedback to each remote subject on a laptop display. Based on the visual feedback, the subjects used the interface to navigate along pre-set target locations in the air. The flight performance was evaluated by comparing with a keyboard-based interface. We demonstrate the applicability of the hybrid interface to explore and interact with a three-dimensional physical space through a flying robot."]},
{"title": "An investigation of molecular dynamics simulation and molecular docking: Interaction of citrus flavonoids and bovine \u03b2-lactoglobulin in focus", "highlights": ["The interaction of citrus flavonoids and BLG was studied computationally.", "Flavonoids bind in the calyx of BLG by hydrophobic, H-bond and \u03c0\u2013\u03c0 interactions.", "There was not any conformational change as for BLG\u2013flavonoid complexes.", "The ligand binding site remains rigid during the simulation."], "abstract": ["Citrus flavonoids are natural compounds with important health benefits. The study of their interaction with a transport protein, such as bovine \u03b2-lactoglobulin (BLG), at the atomic level could be a valuable factor to control their transport to biological sites. In the present study, molecular docking and molecular dynamics simulation methods were used to investigate the interaction of hesperetin, naringenin, nobiletin and tangeretin as citrus flavonoids and BLG as transport protein. The molecular docking results revealed that these flavonoids bind in the internal cavity of BLG and the BLG affinity for binding the flavonoids follows naringenin>hesperetin>tangeretin>nobiletin. The docking results also indicated that the BLG\u2013flavonoid complexes are stabilized through hydrophobic interactions, hydrogen bond interactions and \u03c0\u2013\u03c0 stacking interactions. The analysis of molecular dynamics (MD) simulation trajectories showed that the root mean square deviation (RMSD) of various systems reaches equilibrium and fluctuates around the mean value at various times. Time evolution of the radius of gyration, total solvent accessible surface of the protein and the second structure of protein showed as well that BLG and BLG\u2013flavonoid complexes were stable around 2500", "\u00a0", "ps, and there was not any conformational change as for BLG\u2013flavonoid complexes. Further, the profiles of atomic fluctuations indicated the rigidity of the ligand binding site during the simulation."]},
{"title": "Prediction of temperature and damage in an irradiated human eye\u2014Utilization of a detailed computer model which includes a vectorial blood stream in the choroid", "highlights": ["We created an eye model with a very precise geometry.", "The blood stream in the choroid is implemented by the use of a vectorial flow.", "This model predicts temperatures and damages of the human eye under laser irradiation.", "The calculated temperatures and damages are in good agreement with measurements."], "abstract": ["The work presented here describes the development and use of a three-dimensional thermo-dynamic model of the human eye for the prediction of temperatures and damage thresholds under irradiation. This model takes into account the blood flow by the implementation of a vectorial blood stream in the choroid and also uses the actual physiological extensions and tissue parameters of the eye. Furthermore it considers evaporation, radiation and convection at the cornea as well as the eye lid. The predicted temperatures were successfully validated against existing eye models in terms of corneal and global thermal behaviour. The model\u05f3s predictions were additionally checked for consistency with in-vivo temperature measurements of the cornea, the irradiated retina and its damage thresholds. These thresholds were calculated from the retinal temperatures using the Arrhenius integral. Hence the model can be used to predict the temperature increase and irradiation hazard within the human eye as long as the absorption values and the Arrhenius coefficients are known and the damage mechanism is in the thermal regime."]},
{"title": "Coverage planning in computer-assisted ablation based on Genetic Algorithm", "highlights": ["Ablation planning to avoid over-ablation, over-perforation or under-ablation.", "Complete tumor coverage with minimal number of ablations and trajectories.", "Genetic Algorithm with exponential weight-criterion fitness function and constraints.", "Candidate plans can be encoded in chromosomes, evolving based on a fitness function."], "abstract": ["An ablation planning system plays a pivotal role in tumor ablation procedures, as it provides a dry run to guide the surgeons in a complicated anatomical environment. Over-ablation, over-perforation or under-ablation may result in complications during the treatments. An optimal solution is desired to have complete tumor coverage with minimal invasiveness, including minimal number of ablations and minimal number of perforation trajectories. As the planning of tumor ablation is a multi-objective problem, it is challenging to obtain optimal covering solutions based on clinician\u05f3s experiences. Meanwhile, it is effective for computer-assisted systems to decide a set of optimal plans. This paper proposes a novel approach of integrating a computational optimization algorithm into the ablation planning system. The proposed ablation planning system is designed based on the following objectives: to achieve complete tumor coverage and to minimize the number of ablations, number of needle trajectories and over-ablation to the healthy tissue. These objectives are taken into account using a Genetic Algorithm, which is capable of generating feasible solutions within a constrained search space. The candidate ablation plans can be encoded in generations of chromosomes, which subsequently evolve based on a fitness function. In this paper, an exponential weight-criterion fitness function has been designed by incorporating constraint parameters that were reflective of the different objectives. According to the test results, the proposed planner is able to generate the set of optimal solutions for tumor ablation problem, thereby fulfilling the aforementioned multiple objectives."]},
{"title": "Segmentation of colon tissue sample images using multiple graphics accelerators", "highlights": ["Our goal is to find all cell nuclei in a HE stained colon tissue image.", "We develop a GPGPU based data-parallel region growing.", "We can start more than one process in the same GPU parallel.", "We can use all CPU cores and more than one GPUs parallel.", "This method is 6\u00d7 faster than the sequential algorithm (the accuracy is the same)."], "abstract": ["Nowadays, processing medical images is increasingly done through using digital imagery and custom software solutions. The distributed algorithm presented in this paper is used to detect special tissue parts, the nuclei on haematoxylin and eosin stained colon tissue sample images. The main aim of this work is the development of a new data-parallel region growing algorithm that can be implemented even in an environment using multiple video accelerators. This new method has three levels of parallelism: (a) the parallel region growing itself, (b) starting more region growing in the device, and (c) using more than one accelerator. We use the split-and-merge technique based on our already existing data-parallel cell nuclei segmentation algorithm extended with a fast, backtracking-based, non-overlapping cell filter method. This extension does not cause significant degradation of the accuracy; the results are practically the same as those of the original sequential region growing method. However, as expected, using more devices usually means that less time is needed to process the tissue image; in the case of the configuration of one central processing unit and two graphics cards, the average speed-up is about 4\u20136\u00d7. The implemented algorithm has the additional advantage of efficiently processing very large images with high memory requirements."]},
{"title": "Averaging of diffusion tensor imaging direction-encoded color maps for localizing substantia nigra", "highlights": ["A method for averaging diffusion direction-encoded color maps is presented.", "Application in a large sample of healthy controls shows improved signal-to-noise.", "Averaging in standard space helps distinguish substantia nigra from neighboring nuclei.", "Substantia nigra on average maps agrees with atlas locations and individual raters."], "abstract": ["Diffusion tensor imaging (DTI) is a form of MRI that has been used extensively to map in vivo the white matter architecture of the human brain. It is also used for mapping subcortical nuclei because of its general sensitivity to tissue orientation differences and effects of iron accumulation on the diffusion signal. While DTI provides excellent spatial resolution in individual subjects, a challenge is visualizing consistent patterns of diffusion orientation across subjects. Here we present a simple method for averaging direction-encoded color anisotropy maps in standard space, explore this technique for visualizing the substantia nigra (SN) in relation to other midbrain structures, and show with signal-to-noise analysis that averaging improves the direction-encoded color signature. SN is distinguished on averaged maps from neighboring structures, including red nucleus (RN) and cerebral crus, and is proximal to SN location from existing brain atlases and volume of interest (VOI) delineation on individual scans using two blinded raters."]},
{"title": "A compartment model of alveolar\u2013capillary oxygen diffusion with ventilation\u2013perfusion gradient and dynamics of air transport through the respiratory tract", "highlights": ["We present compartment model of alveolar\u2013capillary oxygen diffusion.", "Model of respiratory mechanics and impact of ", "\u2013", " gradient are incorporated.", "A stream of oxygen mass transported to alveoli is modeled.", "Oxygen saturation is calculated for various levels of physical effort.", "Results of oxygen saturation are compared to results of various clinical trials."], "abstract": ["This paper presents a model of alveolar\u2013capillary oxygen diffusion with dynamics of air transport through the respiratory tract. For this purpose electrical model representing the respiratory tract mechanics and differential equations representing oxygen membrane diffusion are combined. Relevant thermodynamic relations describing the mass of oxygen transported into the human body are proposed as the connection between these models, as well as the influence of ventilation\u2013perfusion mismatch on the oxygen diffusion. The model is verified based on simulation results of varying exercise intensities and statistical calculations of the results obtained during various clinical trials. The benefit of the approach proposed is its application in simulation-based research aimed to generate quantitative data of normal and pathological conditions. Based on the model presented, taking into account many essential physiological processes and air transport dynamics, comprehensive and combined studies of the respiratory efficiency can be performed. The impact of physical exercise, precise changes in respiratory tract mechanics and alterations in breathing pattern can be analyzed together with the impact of various changes in alveolar\u2013capillary oxygen diffusion. This may be useful in simulation of effects of many severe medical conditions and increased activity level."]},
{"title": "A new feature extraction framework based on wavelets for breast cancer diagnosis", "highlights": ["All healthy mammographic patches are correctly classified by a proposed framework.", "The LCP descriptor is very successful on both microcalcifications and masses.", "LCP parameters are directly computed on frequency domain representation of images.", "Proposed framework is effective on separation of malignant cases from benign ones.", "The new framework can be developed to aid radiologists for more accurate diagnosis."], "abstract": ["This paper investigates a pattern recognition framework in order to determine and classify breast cancer cases. Initially, a two-class separation study classifying normal and abnormal (cancerous) breast tissues is achieved. The Histogram of Oriented Gradients (HOG), Dense Scale Invariant Feature Transform (DSIFT), and Local Configuration Pattern (LCP) methods are used to extract the rotation- and scale-invariant features for all tissue patches. A classification is made utilizing Support Vector Machine (SVM), k-Nearest Neighborhood (k-NN), Decision Tree, and Fisher Linear Discriminant Analysis (FLDA) via 10-fold cross validation. Then, a three-class study (normal, benign, and malignant cancerous cases) is carried out using similar procedures in a two-class case; however, the attained classification accuracies are not sufficiently satisfied. Therefore, a new feature extraction framework is proposed. The feature vectors are again extracted with this new framework, and more satisfactory results are obtained. Our new framework achieved a remarkable increase in recognition performance for the three-class study."]},
{"title": "Three dimensional quantitative structure\u2013toxicity relationship modeling and prediction of acute toxicity for organic contaminants to algae", "highlights": ["A new alignment-free method is described to characterize organic contaminants.", "A general-purpose 3D-QSTR predictor is developed based on a large contaminant pool.", "The predictor is used to virtually evaluate a random substituted benzene library.", "Four substituted benzenes are determined to have high acute toxicity to ", "."], "abstract": ["Although numerous chemicals have been identified to have significant toxicological effect on aquatic organisms, there is still lack of a reliable, high-throughput approach to evaluate, screen and monitor the presence of organic contaminants in aquatic system. In the current study, we proposed a synthetic pipeline to automatically model and predict the acute toxicity of chemicals to algae. In the procedure, a new alignment-free three dimensional (3D) structure characterization method was described and, with this method, several 3D-quantitative structure\u2013toxicity relationship (3D-QSTR) models were developed, from which two were found to exhibit strong internal fitting ability and high external predictive power. The best model was established by Gaussian process (GP), which was further employed to perform extrapolation on a random compound library consisting of 1014 virtually generated substituted benzenes. It was found that (i) substitution number can only exert slight influence on chemical\u05f3s toxicity, but low-substituted benzenes seem to have higher toxicity than those of high-substituted entities, and (ii) benzenes substituted by nitro group and halogens exhibit high acute toxicity as compared to other substituents such as methyl and carboxyl groups. Subsequently, several promising candidates suggested by computational prediction were assayed by using a standard algal growth inhibition test. Consequently, four substituted benzenes, namely 2,3-dinitrophenol, 2-chloro-4-nitroaniline, 1,2,3-trinitrobenzene and 3-bromophenol, were determined to have high acute toxicity to ", ", with their EC", " values of 2.5\u00b10.8, 10.5\u00b12.1, 1.4\u00b10.2 and 42.7\u00b15.4", "\u00a0", "\u03bcmol/L, respectively."]},
{"title": "Cascaded-Automatic Segmentation for ", "highlights": ["The cascaded method applies suitable methods in each step for particular segmentation objects.", "We use the method of Radon-Like Features for noise reduction and boundary enhancement.", "We construct one suitable method for removing irregular noisy points from the images.", "A classification model based on SVM is built for identifying eggs."], "abstract": ["To recognize parasite eggs automatically, the automatic segmentation of parasite egg images is very important for the extraction of characteristics and genera classification.", "A Cascaded-Automatic Segmentation approach was proposed. Firstly, image contrast between the border of an egg and its background for all samples was strengthened by the Radon-Like Features algorithm and the enhanced image was processed into a binary image to get an initial set. Then, the elliptical targets are located with Randomized Hough Transform (RHT). The fitted data of an elliptical border are considered the initial border data and the accurate border of a ", " egg can be finally segmented using an Active Contour Model (Snake).", "Seventy-three cases of ", " eggs in fecal samples were found; 61 images contained a parasite egg and 12 did not. Although the illumination, noise pollution, boundary definitions of eggs, and egg position are different, they are all segmented and labeled accurately.", "The results proved that accurate borders of ", " eggs could be recognized precisely using the proposed method, and the robustness of the method is good even in images with heavy noise. This indicates that the proposed method can overcome the disadvantages of the traditional threshold segmentation method, which has limited adaptability to images with heavy background noise."]},
{"title": "Finding multivariate outliers in fMRI time-series data", "highlights": ["Multivariate outlier detection methods are applicable to fMRI time-series data.", "Removing outliers increases spatial specificity without hurting classification.", "Simulation shows PCOut is more sensitivity to small outliers than HD BACON."], "abstract": ["A fundamental challenge for researchers studying the brain is to explain how distributed patterns of brain activity relate to a specific representation or computation. Multivariate techniques are therefore becoming increasingly popular for pattern localization of functional magnetic resonance imaging (fMRI) data. The increased power of these techniques can be offset by their susceptibility to multivariate outliers, a problem not directly encountered when fMRI data are analyzed in more common univariate analysis techniques. We test how two algorithms, ", " (HD BACON) and ", " (PCOut), can detect multivariate outliers in high-dimensional fMRI data, in which the number of variables is larger than the number of observations. We show how these methods can be applied to individual, voxel time-series to identify outlying voxels within a region of interest. Finally, we compare these methods with simulated data to identify which aspects of the data each method is most sensitive to. Voxels identified by both algorithms were primarily on the edges of univariate activation clusters and near the boundaries between different tissue types. Simulation results showed the PCOut outperformed HD BACON, maintaining both high sensitivity and specificity across a wide range of outlier contamination percentages. Our results suggest that multivariate analysis of fMRI can benefit from including multivariate outlier detection as a routine data quality check prior to model fitting."]},
{"title": "Systems pharmacology-based approach for dissecting the addition and subtraction theory of traditional Chinese medicine: An example using Xiao-Chaihu-Decoction and Da-Chaihu-Decoction", "highlights": ["Candidate compounds, targets and interaction network involved in XCHD and DCHD.", "\u201cFundamental formula\u201d is mainly responsible for basic therapeutic effects.", "\u201cAdditive herbs\u201d exhibit reinforced function to the foundational formula."], "abstract": ["Addition and subtraction theory (AST), a basic theory of herb combination in traditional Chinese medicine (TCM), is often used to add or subtract the \u201cfundamental formulae\u201d to generate more targeted prescriptions. This theory plays a core role in individualized medicine and compound compatibility of TCM. However, the mechanisms underlying AST have largely remained elusive.", "An integrated platform of systems pharmacology was proposed for revealing how the oral administration, drug half-life, and target interactions affect the pharmacological functions of herbal medicines. This platform was further applied on two classical prescriptions, i.e., Xiao Chaihu decoction (XCHD) and Da Chaihu decoction (DCHD) to dissect the addition and subtraction theory (AST).", "We uncovered the candidate compounds, key molecular targets and interaction network involved in XCHD and DCHD, and summarized its pharmacological characters and therapeutic indications. The results show that the \u201cfundamental formula\u201d is responsible for the major therapeutic effects, whereas the \u201cadditive herbs\u201d synergistically enhance the treatment outcomes by targeting the same or complementary proteins between the foundational and additive herbs.", "This work has established a novel method to comprehensively understand the mechanism of AST, which would be beneficial for the TCM recipe optimization as well as the production of new herbal formula with desirable therapeutic effects."]},
{"title": "A compact fourth-order finite difference scheme for the three-dimensional Cahn\u2013Hilliard equation", "highlights": [], "abstract": ["This work extends the previous two-dimensional compact scheme for the Cahn\u2013Hilliard equation (Lee et\u00a0al., 2014) to three-dimensional space. The proposed scheme, derived by combining a compact formula and a linearly stabilized splitting scheme, has second-order accuracy in time and fourth-order accuracy in space. The discrete system is conservative and practically stable. We also implement the compact scheme in a three-dimensional adaptive mesh refinement framework. The resulting system of discrete equations is solved by using a multigrid. We demonstrate the performance of our proposed algorithm by several numerical experiments."]},
{"title": "A prediction model of drug-induced ototoxicity developed by an optimal support vector machine (SVM) method", "highlights": ["A classification model of drug-induced ototoxicity is established by GA-CG-SVM.", "The na\u00efve Bayesian method is used to develop prediction models of ototoxicity.", "The RP method is used to develop prediction models of ototoxicity.", "The established GA-CG-SVM model II outperforms models developed by other methods."], "abstract": ["Drug-induced ototoxicity, as a toxic side effect, is an important issue needed to be considered in drug discovery. Nevertheless, current experimental methods used to evaluate drug-induced ototoxicity are often time-consuming and expensive, indicating that they are not suitable for a large-scale evaluation of drug-induced ototoxicity in the early stage of drug discovery. We thus, in this investigation, established an effective computational prediction model of drug-induced ototoxicity using an optimal support vector machine (SVM) method, GA-CG-SVM. Three GA-CG-SVM models were developed based on three training sets containing agents bearing different risk levels of drug-induced ototoxicity. For comparison, models based on na\u00efve Bayesian (NB) and recursive partitioning (RP) methods were also used on the same training sets. Among all the prediction models, the GA-CG-SVM model II showed the best performance, which offered prediction accuracies of 85.33% and 83.05% for two independent test sets, respectively. Overall, the good performance of the GA-CG-SVM model II indicates that it could be used for the prediction of drug-induced ototoxicity in the early stage of drug discovery."]},
{"title": "Effects of offset values for artificial teeth positions in CAD/CAM complete denture", "highlights": ["We evaluated the optimal offset required for artificial teeth positioning during fabrication of CAD/CAM complete dentures.", "The optimal offset values differ with the basal shape of artificial teeth.", "Optimal offset values were 0.15\u20130.25", "\u00a0", "mm for upper left central incisor, 0.15 and 0.25", "\u00a0", "mm for upper left canine, 0.25", "\u00a0", "mm for upper left first premolar, and 0.10\u20130.25", "\u00a0", "mm for upper left first molar."], "abstract": ["Recently, computer-aided design/computer-aided manufacturing (CAD/CAM) technology has been applied to the field of removable complete denture prosthodontics. We developed a system for fabricating complete dentures applying CAD/CAM technology. In this system, artificial teeth were bonded to the recesses of a milled denture base. However, the offset values needed for the recesses are not known. The purpose of the present study was to evaluate the accuracy of bonded artificial teeth positions in 0.00 (control), 0.10, 0.15, 0.20, and 0.25", "\u00a0", "mm offset recess groups. Four types of artificial teeth, upper left central incisor (UL1), upper left canine (UL3), upper left first premolar (UL4), and upper left first molar (UL6), were used. Each type of artificial tooth was arranged at regular intervals on the denture base model with the CAD software. These data were defined as the master data. The artificial teeth parts were subtracted from the denture base model by Boolean logic operations in order to make recesses, and the recesses were then offset in five values. Based on these denture base data, prepolymerized resin blocks were milled (", "=3). After bonding artificial teeth on the milled denture base model, a cone beam computed tomography (CBCT) scan was performed to obtain scanned data. Deviations between the master data and the scanned data were calculated. Based on the results, the optimal offset values were found to be 0.15\u20130.25", "\u00a0", "mm for UL1, 0.15 and 0.25", "\u00a0", "mm for UL3, 0.25", "\u00a0", "mm for UL4, and 0.10\u20130.25", "\u00a0", "mm for UL6."]},
{"title": "Application of an anatomically-detailed finite element thorax model to investigate pediatric cardiopulmonary resuscitation techniques on hard bed", "highlights": ["A 10 years old child thorax FE model was used to evaluate CPR loading techniques and compression rates.", "The rib responses were studied and considered as a complementary evaluation for CPR quality.", "The effect of chest stiffness was studied by introducing a softer and stiffer chest.", "The child FE model provided the research community a more child-like surrogate for studying CPR."], "abstract": ["Improved Cardiopulmonary Resuscitation (CPR) approaches will largely benefit the children in need. The constant peak displacement and constant peak force loading methods were analyzed on hard bed for pediatric CPR by an anatomically-detailed 10 year-old (YO) child thorax finite element (FE) model. The chest compression and rib injury risk were studied for children with various levels of thorax stiffness.", "We created three thorax models with different chest stiffness. Simulated CPR\u05f3s in the above two conditions were performed. Three different compression rates were considered under the constant peak displacement condition. The model-calculated deflections and forces were analyzed. The rib maximum principle strains (MPS\u05f3s) were used to predict the potential risk of rib injury.", "Under the constant peak force condition, the chest deflection ranged from 34.2 to 42.2", "\u00a0", "mm. The highest rib MPS was 0.75%, predicted by the compliant thorax model. Under the normal constant peak displacement condition, the highest rib MPS was 0.52%, predicted by the compliant thorax model. The compression rate did not affect the highest rib MPS.", "Results revealed that the thoracic stiffness had great effects on the quality of CPR. To maintain CPR quality for various children, the constant peak displacement technique is recommended when the CPR is performed on the hard bed. Furthermore, the outcome of CPR in terms of rib strains and total work are not sensitive to the compression rate. The FE model-predicted high strains were in the ribs, which have been found to be vulnerable to CPR in the literature."]},
{"title": "SimFlu: A simulation tool for predicting the variation pattern of influenza A virus", "highlights": ["We developed a simulation tool, named SimFlu, for influenza virus evolution.", "SimFlu predicts possible future variants of influenza viruses.", "SimFlu creates variants using the codon variation parameters from a seed sequence."], "abstract": ["Since the first pandemic outbreak of avian influenza A virus (H5N1 subtype) in 1997, the National Center for Biotechnology Information (NCBI) has provided a large number of influenza virus sequences with well-organized annotations. Using the time-series sequences of influenza A viruses, we developed a simulation tool for influenza virus, named SimFlu, to predict possible future variants of influenza viruses. SimFlu can create variants from a seed nucleotide sequence of influenza A virus using the codon variation parameters included in the SimFlu package. The SimFlu library provides pre-calculated codon variation parameters for the H1N1, H3N2, and H5N1 subtypes of influenza A virus isolated from 2000 to 2011, allowing the users to simulate their own nucleotide sequences by selecting their preferred parameter options. SimFlu supports three operating systems \u2013 Windows, Linux, and Mac OS X. SimFlu is publicly available at ", "."]},
{"title": "Experimental validation of 3D printed patient-specific implants using digital image correlation and finite element analysis", "highlights": ["We design patient-specific bone replacement shapes using topology optimization.", "We perform mechanical testing on a 3D printed bone replacement by applying masticatory forces.", "We validate the strains of the tests with digital image correlation and finite element analysis.", "We show that the topology optimized implants provide adequate load-transfer mechanism.", "This technique has the potential to alleviate the uncertainty for bone replacement designs."], "abstract": ["With the dawn of 3D printing technology, patient-specific implant designs are set to have a paradigm shift. A topology optimization method in designing patient-specific craniofacial implants has been developed to ensure adequate load transfer mechanism and restore the form and function of the mid-face. Patient-specific finite element models are used to design these implants and to validate whether they are viable for physiological loading such as mastication. Validation of these topology optimized finite element models using mechanical testing is a critical step. Instead of inserting the implants into a cadaver or patient, we embed the implants into the computer-aided skull model of a patient and, fuse them together to 3D print the complete skull model with the implant. Masticatory forces are applied in the molar region to simulate chewing and measure the stress\u2013strain trajectory. Until recently, strain gages have been used to measure strains for validation. Digital Image Correlation (DIC) method is a relatively new technique for full-field strain measurement which provides a continuous deformation field data. The main objective of this study is to validate the finite element model of patient-specific craniofacial implants against the strain data from the DIC obtained during the mastication simulation and show that the optimized shapes provide adequate load-transfer mechanism. Patient-specific models are obtained from CT scans. The principal maximum and minimum strains are compared. The computational and experimental approach to designing patient-specific implants proved to be a viable technique for mid-face craniofacial reconstruction."]},
{"title": "A computational study of the respiratory airflow characteristics in normal and obstructed human airways", "highlights": ["We developed 3-D airflow models to study lower airway diseases.", "We compared airflow characteristics in normal and obstructed airways.", "We developed a new method to quantitatively compare airflow patterns.", "High expiratory flow patterns differentiated lower airway conditions.", "Wall shear stresses depended on breathing rates and distribution of obstructions."], "abstract": ["Obstructive lung diseases in the lower airways are a leading health concern worldwide. To improve our understanding of the pathophysiology of lower airways, we studied airflow characteristics in the lung between the 8th and the 14th generations using a three-dimensional computational fluid dynamics model, where we compared normal and obstructed airways for a range of breathing conditions. We employed a novel technique based on computing the Pearson\u05f3s correlation coefficient to quantitatively characterize the differences in airflow patterns between the normal and obstructed airways. We found that the airflow patterns demonstrated clear differences between normal and diseased conditions for high expiratory flow rates (>2300", "\u00a0", "ml/s), but not for inspiratory flow rates. Moreover, airflow patterns subjected to filtering demonstrated higher sensitivity than airway resistance for differentiating normal and diseased conditions. Further, we showed that wall shear stresses were not only dependent on breathing rates, but also on the distribution of the obstructed sites in the lung: for the same degree of obstruction and breathing rate, we observed as much as two-fold differences in shear stresses. In contrast to previous studies that suggest increased wall shear stress due to obstructions as a possible damage mechanism for small airways, our model demonstrated that for flow rates corresponding to heavy activities, the wall shear stress in both normal and obstructed airways was <0.3", "\u00a0", "Pa, which is within the physiological limit needed to promote respiratory defense mechanisms. In summary, our model enables the study of airflow characteristics that may be impractical to assess experimentally."]},
{"title": "Recovery of gastrointestinal tract motility detection using Naive Bayesian and minimum statistics", "highlights": ["A method for detection of recovery of gastrointestinal track motility was proposed.", "Bowel activity statistics were obtained for patients who underwent abdominal surgery.", "Total burst duration and power in 100\u2013200", "\u00a0", "Hz band showed high predictive power.", "Tested on 59", "\u00a0", "h of recordings and 94.15% recognition accuracy was observed."], "abstract": ["Loss of gastrointestinal motility is a significant medical setback for patients who experience abdominal surgery and contributes to the most common reason for prolonged hospital stays. Recent clinical studies suggest that initiating feeding early after abdominal surgery is beneficial. Early feeding is possible when the patients demonstrate bowel motility in the form of bowel sounds (BS). This work provides a data collection, processing and analysis methodology for detection of recovery of gastrointestinal track motility by observing BSs in auscultation recordings. The approach is suitable for real-time long-term continuous monitoring in clinical environments. The system was developed using a Naive Bayesian algorithm for pattern classification, and Minimum Statistics and spectral subtraction for noise attenuation. The solution was tested on 59", "\u00a0", "h of recordings and 94.15% recognition accuracy was observed."]},
{"title": "Exploring the effects of intervention for those at high risk of developing type 2 diabetes using a computer simulation", "highlights": ["In order to reflect the effect of intervention for those at high risk of type 2 diabetes, a computer simulation was conducted.", "The high risk was classified upon Hierarchy Support Vector Machines algorithm.", "The proportion transitioning from the high risk state to moderate state, low risk state or the normal state was calculated.", "The method could help to determine risk transition by the adjustment of sensitive risk factors."], "abstract": ["A simulation based computational method was conducted to reflect the effect of intervention for those at high risk of type 2 diabetes. Hierarchy Support Vector Machines (H-SVMs) were used to classify high risk. The proportion transitioning from the high risk state to moderate state, low state or the normal state was calculated. When Body Mass Index (BMI) decreased by 5% (weight loss 3\u20135", "\u00a0", "kg), the proportion of Class A transferring to a lower state was 15\u201325%, and risk also appeared reduced for Class B1. In Class C, when cholesterol (CHOL) was decreased by 2.5% (0.13\u20130.34", "\u00a0", "mmol/L), 10\u201325% transitioned to a lower risk state. The method could help determine risk transition by the adjustment of sensitive risk factors. This might provide the basis for implementing intervention in cases in a high risk state."]},
{"title": "A correction method using a support vector machine to minimize hematocrit interference in blood glucose measurements", "highlights": ["A decrease in hematocrit levels increases glucose concentrations.", "An increase in hematocrit levels decreases glucose concentrations.", "We develop hematocrit compensation method that minimizes the effects of hematocrit on glucose measurements.", "Glucose concentrations were calculated with linear fitting prediction and a support vector machine."], "abstract": ["Point-of-care testing glucose meters are widely used, important tools for determining the blood glucose levels of people with diabetes, patients in intensive care units, pregnant women, and newborn infants. However, a number of studies have concluded that a change in hematocrit (Hct) levels can seriously affect the accuracy of glucose measurements. The aim of this study was to develop an algorithm for glucose calculation with improved accuracy using the Hct compensation method that minimizes the effects of Hct on glucose measurements. The glucose concentrations in this study were calculated with an adaptive calibration curve using linear fitting prediction and a support vector machine, which minimized the bias in the glucose concentrations caused by the Hct interference. This was followed by an evaluation of performance according to the international organization for standardization (ISO) 15197:2013 based on bias with respect to the reference method, the coefficient of variation, and the valid blood samples/total blood samples within the \u00b120% and 15% error grids. Chronoamperometry was performed to verify the effect of Hct variation and to compare the proposed method. As a result, the average coefficients of variation for chronoamperometry and the Hct compensation method were 2.43% and 3.71%, respectively, while the average biases (%) for these methods were 12.08% and 5.69%, respectively. The results of chronoamperometry demonstrated that a decrease in Hct levels increases glucose concentrations, whereas an increase in Hct levels reduces glucose concentrations. Finally, the proposed method has improved the accuracy of glucose measurements compared to existing chronoamperometry methods."]},
{"title": "Computer-aided diagnosis system for the Acute Respiratory Distress Syndrome from chest radiographs", "highlights": ["A computer-aided diagnosis system is proposed for the detection of the ARDS.", "The system is based on chest radiograph analysis using texture features.", "An automatic selection of intercostal regions is used for the analysis.", "A combination of statistical and spectral features is extracted.", "An SVM classifier is used to achieve the ARDS detection."], "abstract": ["This paper presents a computer-aided diagnosis (CAD) system for the assessment of Acute Respiratory Distress Syndrome (ARDS) from chest radiographs. Our method consists in automatically extracting intercostal patches from chest radiographs belonging to the test database using a semiautomatic segmentation method of the ribs. Statistical and spectral features are computed from each patch then a method of feature transformation is applied using the Linear Discriminant Analysis (LDA). A training database of 321 patches was classified by an expert in two classes, a class of normal patches and a class of abnormal patches. Patches belonging to the test database are then classified using the SVM classifier. Finally, the rate of abnormal patches is calculated for each quadrant to decide if the chest radiograph presents an ARDS. The method has been evaluated on 90 radiographs where 53 images present ARDS. The results show a sensitivity of 90.6% at a specificity of 86.5%."]},
{"title": "Numerical investigation of regional particle deposition in the upper airway of a standing male mannequin in calm air surroundings", "highlights": ["A 3-D realistic model of upper airway integrated into a full size standing mannequin was developed.", "The results for deposition of microparticles are compared with those for a separate airway model.", "The total and regional deposition of microparticles are mainly different between two models."], "abstract": ["A 3-D realistic computational model of the airway system integrated into a standing male mannequin was developed. The computational domain includes the regions around the mannequin and the inside of the airway passages. The simulation was performed for low activity breathing rates with calm air around the mannequin. The flowfield of the inhaled air was first obtained from solving the Navier\u2013Stokes and continuity equations. Then the particles were released in the domain around the mannequin and their trajectories were evaluated by using the Lagrangian approach for solving the particle equation of motion. The regional aerosols deposition was evaluated for different parts of the human airway system and the results were compared with those obtained from the separate modeling of the airway system without the interaction of the airflow with the mannequin external face. The results showed when the upper airway is integrated into the mannequin, the regional deposition of inhaled particles mainly changes in the airway system."]},
{"title": "New method for geometric calibration and distortion correction of conventional C-arm", "highlights": ["A new method is proposed to calibrate and correct the image for conventional C-arm.", "C-arm is calibrated with non-linear model before image distortion correction.", "An automatic method for marker extraction in C-arm image and matching is realized.", "The new method can simplify the installment of the calibration phantom onto C-arm.", "Increasing polynomial order will increase both the precision and time-consumption."], "abstract": ["Image distortion correction and geometric calibration are critical operations for using C-arm DSA (Digital Subtraction Angiography) images to digitally navigate vascular interventional surgery. In traditional ways, C-arm images are corrected with global or local correction methods where a supposed virtual ideal image is needed, and then the corrected images are utilized to calibrate the C-arm with a pin-hole model. In this paper, we propose a new method to calibrate the C-arm with a nonlinear model and to improve navigation performance. We first calibrate the C-arm with a nonlinear model and then the distortion correction is accomplished without virtual ideal image. In this paper, the nonlinear model of C-arm imaging system is addressed at first, and then the C-arm is calibrated with a two-stage method. In the first stage, the C-arm is calibrated with the markers in image center by RAC (radial alignment constraint) method, and in the second stage the calibration parameters are optimized with Levenberg-Marquadt algorithm by minimizing the sum of the square of difference between all markers\u05f3 real distorted positions and their theoretical distorted positions in the phantom image. Based on the calibration result, the image distortion can be corrected. To verify our method, experiments were conducted with a conventional DSA C-arm machine in hospital. The errors in distortion correction and 3D (three-dimensional) reconstruction were quantitatively compared with the global polynomial correction method and visual model method, and the results showed that the proposed method had better performance in distortion correction and 3D reconstruction."]},
{"title": "Wavelet-based denoising method for real phonocardiography signal recorded by mobile devices in noisy environment", "highlights": ["We show that the noise occurring during PCG signals measure has variable character.", "We adapt a wavelet denoising algorithm for the filtration of real PCG signal.", "The algorithm denoise PCG signals recorded in noisy environments by mobile devices.", "The performance of the algorithm was also tested on pathological heart sounds."], "abstract": ["The main obstacle in development of intelligent autodiagnosis medical systems based on the analysis of phonocardiography (PCG) signals is noise. The noise can be caused by digestive and respiration sounds, movements or even signals from the surrounding environment and it is characterized by wide frequency and intensity spectrum. This spectrum overlaps the heart tones spectrum, which makes the problem of PCG signal filtrating complex. The most common method for filtering such signals are wavelet denoising algorithms. In previous studies, in order to determine the optimum wavelet denoising parameters the disturbances were simulated by Gaussian white noise. However, this paper shows that this noise has a variable character. Therefore, the purpose of this paper is adaptation of a wavelet denoising algorithm for the filtration of real PCG signal disturbances from signals recorded by a mobile devices in a noisy environment. The best results were obtained for Coif 5 wavelet at the 10th decomposition level with the use of a minimaxi threshold selection algorithm and mln rescaling function. The performance of the algorithm was tested on four pathological heart sounds: early systolic murmur, ejection click, late systolic murmur and pansystolic murmur."]},
{"title": "Ventricular fibrillation mechanisms and cardiac restitutions: An investigation by simulation study on whole-heart model", "highlights": ["Effects of restitutions on ventricular fibrillation (VF) were examined ", " ", ".", "Action potential duration restitution (APDR) heterogeneity causes mother-rotor VF.", "The effect of conduction velocity restitution on VF depends on the APDR", "Conversion of two types of VF was successfully simulated."], "abstract": ["The action potential duration (APD) and the conduction velocity (CV) restitution have been reported to be important in the maintenance and conversion of ventricular fibrillation (VF), whose mechanisms remain poorly understood. Multiple-wavelet and/or mother-rotor have been regarded as the main VF mechanisms, and APD restitution (APDR) and CV restitution (CVR) properties are involved in the mutual conversion or transition between VF and ventricular tachycardia (VT).", "The effects of APDR (both its slope and heterogeneity) and CVR on VF organization and conversion were examined using a \u201crule-based\u201d whole-heart model. The results showed that different organizations of simulated VF were manifestations of different restitution configurations. Multiple-wavelet and mother-rotor VF mechanisms could recur in models with steep and heterogeneous APDR, respectively. Suppressing the excitability either decreased or increased the VF complexity under the steep or shallow APDR, respectively. The multiple-wavelet VF changed into a VT in response to a flattening of the APDR, and the VT degenerated into a mother-rotor VF due to the APDR heterogeneity.", "Our results suggest that the mechanisms of VF are tightly related to cardiac restitution properties. From a viewpoint of the \u201crule-based\u201d whole-heart model, our work supports the hypothesis that the synergy between APDR and CVR contributes to transitions between multiple-wavelet and mother-rotor mechanisms in the VF."]},
{"title": "Numerical models of net-structure stents inserted into arteries", "highlights": ["Analyses of stresses developed on artery wall after net structure stent implantation.", "Planar 2D numerical models are in good agreement with complex 3D models.", "2D cases can be used as a simplified and convenient tool for calculating the arterial wall stresses in complex cases.", "Axisymmetric models may provide the worst-case estimation values for a stent of interest."], "abstract": ["Restenosis is strongly attributed to stresses caused by stent\u2013artery interactions generated in the artery after balloon angioplasty. Numerical methods are often used to examine the stent\u2013artery mechanical interactions. To overcome the extensive computational requirements demanded by these simulations, simplifications are needed.", "We introduce simplified models to calculate the mechanical interactions between net-structured stents and arteries, and discuss their validity and implications.", "2D simplified numerical models are suggested, which allow cost effective assessment of arterial stresses and the potential damage factor (DF). In these models, several contact problems were solved for arteries with hyper elastic mechanical properties. Stresses were calculated for a large range of cases and for different numerical model types. The effects of model simplifications, oversizing mismatch and stenosis rate and length and symmetry on the resulting stresses were analyzed.", "Results obtained from planar 2D models were found in good agreement with results obtained from complex 3D models for cases with axisymmetric constant or varying stenosis. This high correlation between the results of 3D cases with varying stenosis and the more simple 2D cases can be used as a simplified and convenient tool for calculating the arterial wall stresses in complex cases. Maximal stresses obtained by the 2D model with an asymmetric stenosis are lower than the maximal stresses obtained in the axisymmetric case with the same stenosis percentage. Therefore, axisymmetric models may provide the worst-case estimation values for a stent of interest."]},
{"title": "An in silico case study of idiopathic dilated cardiomyopathy via a multi-scale model of the cardiovascular system", "highlights": ["A multi-scale model of the left ventricle and circulation is presented.", "This model helps studying idiopathic dilated cardiomyopathy (IDC) in a multi-scale context.", "It can reproduce well-known characteristics of the disease by varying key parameters.", "It provides insight on IDC by trying to explain potential mechanisms at multiple biological scales.", "It could be applied to simulate other pathological cases or pharmacological intervention."], "abstract": ["Mathematical modelling has been used to comprehend the pathology and the assessment of different treatment techniques such as heart failure and left ventricular assist device therapy in the cardiovascular field. In this study, an in-silico model of the heart is developed to understand the effects of idiopathic dilated cardiomyopathy (IDC) as a pathological scenario, with mechanisms described at the cellular, protein and organ levels. This model includes the right and left atria and ventricles, as well as the systemic and pulmonary arteries and veins.", "First, a multi-scale model of the whole heart is simulated for healthy conditions. Subsequently, the model is modified at its microscopic and macroscopic spatial scale to obtain the characteristics of IDC. The extracellular calcium concentration, the binding affinity of calcium binding proteins and the maximum and minimum elastances have been identified as key parameters across all relevant scales. The modified parameters cause a change in (a) intracellular calcium concentration characterising cellular properties, such as calcium channel currents or the action potential, (b) the proteins being involved in the sliding filament mechanism and the proportion of the attached crossbridges at the protein level, as well as (c) the pressure and volume values at the organ level. This model allows to obtain insight and understanding of the effects of the treatment techniques, from a physiological and biological point of view."]},
{"title": "Decoding the EGFR mutation-induced drug resistance in lung cancer treatment by local surface geometric properties", "highlights": ["A method for predicting the EGFR mutation-induced drug resistance is proposed.", "Four types of local surface changes are defined for the mutants compared with wild-type EGFR.", "Three types of the changes have strong correlation with the progression-free survival (PFS).", "The number of convex shapes also shows a correlation with the PFS."], "abstract": ["Epidermal growth factor receptor (EGFR) mutation-induced drug resistance leads to a limited efficacy of tyrosine kinase inhibitors during lung cancer treatments. In this study, we explore the correlations between the local surface geometric properties of EGFR mutants and the progression-free survival (PFS). The geometric properties include local surface changes (four types) of the EGFR mutants compared with the wild-type EGFR, and the convex degrees of these local surfaces. Our analysis results show that the Spearman\u05f3s rank correlation coefficients between the PFS and three types of local surface properties are all greater than 0.6 with small ", "-values, implying a high significance. Moreover, the number of atoms with solid angles in the ranges of [0.71, 1], [0.61, 1] or [0.5, 1], indicating the convex degree of a local EGFR surface, also shows a strong correlation with the PFS. Overall, these characteristics can be efficiently applied to the prediction of drug resistance in lung cancer treatments, and easily extended to other cancer treatments."]},
{"title": "Alpha-plane based automatic general type-2 fuzzy clustering based on simulated annealing meta-heuristic algorithm for analyzing gene expression data", "highlights": ["Presenting a new two-stage meta-heuristic clustering algorithm based on general type-2 fuzzy sets.", "Incorporating a new similarity-based objective function using alpha-plane representation of general type-2 fuzzy sets.", "Implementing the proposed approach on real microarray gene expression datasets."], "abstract": ["This paper considers microarray gene expression data clustering using a novel two stage meta-heuristic algorithm based on the concept of \u03b1-planes in general type-2 fuzzy sets. The main aim of this research is to present a powerful data clustering approach capable of dealing with highly uncertain environments. In this regard, first, a new objective function using \u03b1-planes for general type-2 fuzzy c-means clustering algorithm is represented. Then, based on the philosophy of the meta-heuristic optimization framework \u2018Simulated Annealing\u2019, a two stage optimization algorithm is proposed. The first stage of the proposed approach is devoted to the annealing process accompanied by its proposed perturbation mechanisms. After termination of the first stage, its output is inserted to the second stage where it is checked with other possible local optima through a heuristic algorithm. The output of this stage is then re-entered to the first stage until no better solution is obtained. The proposed approach has been evaluated using several synthesized datasets and three microarray gene expression datasets. Extensive experiments demonstrate the capabilities of the proposed approach compared with some of the state-of-the-art techniques in the literature."]},
{"title": "A new algorithm and problems in automatic anterior eye chamber volume determining", "highlights": ["We showed the problem of automatic determination of the anterior eye chamber volume.", "60,000 images were obtained using OCT SS-1000 CASIA and Zeiss Visante OCT.", "The proposed algorithm gave the anterior chamber surface measurement error at the level of 4.3%.", "The proposed algorithm enables to obtain reproducible results fully automatically."], "abstract": ["This study investigates the problem of automatic determination of the anterior eye chamber volume using previously published as well as new algorithms of image analysis and processing proposed by the authors. A new method for determining the anterior eye chamber volume that provides more accurate results has been proposed. The entire algorithm was implemented in Matlab and C language. 60,000 images were obtained using OCT SS-1000 CASIA and Zeiss Visante OCT. The acquired images of the anterior segment of the eye had a resolution of 256\u00d71024 pixels with a measuring range of 8\u00d716", "\u00a0", "mm", ". The images were acquired during routine medical examination at the Clinical Department of Ophthalmology, District Railway Hospital in Katowice, Poland, and were analysed in accordance with the Declaration of Helsinki. The new algorithm uses edge detection, morphological operations, binarization and filtration. The proposed algorithm gave the anterior chamber surface measurement error at the level of 4.3% and the anterior chamber volume measurement error of 12%. For comparison, the surface measurement error of the tomograph software was at 6.7%. Thus the obtained results were better by 2.4%. The proposed algorithm provides reproducible results automatically at a runtime of 3", "\u00a0", "s per patient using a Core i7 PC computer with 8", "\u00a0", "GB of RAM."]},
{"title": "ODE/PDE analysis of corneal curvature", "highlights": ["We solve numerically a nonlinear boundary-value problem modeling corneal topography.", "We use the method of lines to facilitate fast computation and include the R routines.", "We derive some estimates which demonstrate the order of convergence of the algorithm."], "abstract": ["The starting point for this paper is a nonlinear, two-point boundary value ordinary differential equation (BVODE) that defines corneal curvature according to a static force balance. A numerical solution to the BVODE is computed by first converting the BVODE to a parabolic partial differential equation (PDE) by adding an initial value (", ", pseudo-time) derivative to the BVODE. A numerical solution to the PDE is then computed by the method of lines (MOL) with the calculation proceeding to a sufficiently large value of ", " such that the derivative in ", " reduces to essentially zero. The PDE solution at this point is also the solution for the BVODE. This procedure is implemented in R (an open source scientific programming system) and the programming is discussed in some detail. A series approximation to the solution is derived from which an estimate for the rate of convergence is obtained. This is compared to a fitted exponential model. Also, two linear approximations are derived, one of which leads to a closed form solution. Both provide solutions very close to that obtained from the full nonlinear model. An estimate for the cornea radius of curvature is also derived. The paper concludes with a discussion of the features of the solution to the ODE/PDE system."]},
{"title": "Real-time electrocardiogram P-QRS-T detection\u2013delineation algorithm based on quality-supported analysis of characteristic templates", "highlights": ["A low-latency and accurate algorithm for real-time detection of the ECG P-QRS-T waves.", "Response time about 8-ms for detection of the QRS complexes.", "Response time about 198-ms and 177-ms for detection of P- and T-waves, respectively.", "Relative high operating characteristics for real-time detection of P-, QRS-, and T-waves.", "Evolving correction of generated clusters for increasing the final accuracy of the algorithm."], "abstract": ["The main objective of this study is to introduce a simple, low-latency, and accurate algorithm for real-time detection of P-QRS-T waves in the electrocardiogram (ECG) signal. In the proposed method, real-time signal preprocessing, which includes high frequency noise filtering and baseline wander reduction, is performed by applying discrete wavelet transform (DWT). A method based on signal first-order derivative and adaptive threshold adjustment is employed for real-time detection of the QRS complex. Moreover, detection and delineation of P- and T-waves are achieved by correlation analysis conducted between signal and their templates. Besides, signal quality is investigated online, and if the quality of the analysis window is unacceptable, then the algorithm will guess (estimate) the locations of P- and T-waves.", "The operating characteristics of the proposed algorithm are evaluated by its implementation to an artificially generated ECG signal whose quality is adjustable from the best (Quality, 100%) to the worst (Quality, \u226440%) cases based on the random-walk noise theory. The algorithm was applied to the MIT-BIH arrhythmia database, QT database, and Physionet/CinC challenge 2011competition database. The obtained results, which were based on the QT database, showed sensitivity and positive predictivity of Se=99.63% and P+=99.83%, Se=99.83% and P+=99.98%, and Se=99.74% and P+=99.89% for the detection of P-, QRS-, and T-waves, respectively, and the obtained results, which were based on the MIT-BIH arrhythmia database, showed Se=99.81% and P+=99.70% for the detection of the QRS complex. Moreover, it will be shown that the results of the proposed method are reliable for a minimum signal quality value of 70%. According to numerical assessments, 8-ms after the occurrence of R-wave, its location will be identified by the computer code of the proposed algorithm. This parameter is 198-ms and 177-ms for P- and T-waves, respectively."]},
{"title": "Identifying high-cost patients using data mining techniques and a small set of non-trivial attributes", "highlights": ["We used data mining techniques to build predictive models to identify ", " patients.", "A refined ", " dataset of 31,704 records was modeled.", "CHAID was the top performing predictive model.", "Created a set of 5 non-trivial attributes to identify the top 5% of high cost patients.", "The results of this study can improve the delivery of health services."], "abstract": ["In this paper, we use data mining techniques, namely ", " and ", ", to build predictive models to identify ", " patients in the top 5 percentile among the general population. A large empirical dataset from the ", " with 98,175 records was used in our study. After pre-processing, partitioning and balancing the data, the refined dataset of 31,704 records was modeled by Decision Trees (including C5.0 and CHAID), and Neural Networks. The performances of the models are analyzed using various measures including ", "y, ", ", and ", ". We concluded that the CHAID classifier returns the best ", "-mean and AUC measures for top performing predictive models ranging from 76% to 85%, and 0.812 to 0.942 units, respectively. We also identify a small set of 5 non-trivial attributes among a primary set of 66 attributes to identify the top 5% of the high cost population. The attributes are the individual\u05f3s overall health perception, age, history of blood cholesterol check, history of physical/sensory/mental limitations, and history of colonic prevention measures. The small set of attributes are what we call non-trivial and does not include visits to care providers, doctors or hospitals, which are highly correlated with expenditures and does not offer new insight to the data. The results of this study can be used by healthcare data analysts, policy makers, insurer, and healthcare planners to improve the delivery of health services."]},
{"title": "A user-operated test of suprathreshold acuity in noise for adult hearing screening: The SUN (SPEECH UNDERSTANDING IN NOISE) test", "highlights": ["The development of adult hearing screening methods is a key technological challenge.", "Adults frequently have difficulties in speech communication, particularly in noise.", "The speech understanding in noise (SUN) is a supra-threshold acuity in noise test.", "The SUN test can be implemented on a stand-alone, portable, easy-to-use device.", "The SUN test may be viable for adult screening in clinical and nonclinical settings."], "abstract": ["A novel, user-operated test of suprathreshold acuity in noise for use in adult hearing screening (AHS) was developed.", "The Speech Understanding in Noise test (SUN) is a speech-in-noise test that makes use of a list of vowel\u2013consonant\u2013vowel (VCV) stimuli in background noise presented in a three-alternative forced choice (3AFC) paradigm by means of a touch sensitive screen. The test is automated, easy-to-use, and provides self-explanatory results (i.e., \u2018no hearing difficulties\u2019, or \u2018a hearing check would be advisable\u2019, or \u2018a hearing check is recommended\u2019). The test was developed from its building blocks (VCVs and speech-shaped noise) through two main steps: (i) ", " through equalization of the intelligibility of test stimuli across the set and (ii) ", " through maximization of the test sensitivity and specificity. The test had 82.9% sensitivity and 85.9% specificity compared to conventional pure-tone screening, and 83.8% sensitivity and 83.9% specificity to identify individuals with disabling hearing impairment. Results obtained so far showed that the test could be easily performed by adults and older adults in less than one minute per ear and that its results were not influenced by ambient noise (up to 65", "\u00a0", "dBA), suggesting that the test might be a viable method for AHS in clinical as well as non-clinical settings."]},
{"title": "Modelling and simulating reaction\u2013diffusion systems using coloured Petri nets", "highlights": ["We present a new coloured Petri net approach to model reaction\u2013diffusion systems.", "Spatial attributes of such systems are represented using colour definitions.", "We give a method to represent state- and/or space-dependent diffusion rate."], "abstract": ["Reaction\u2013diffusion systems often play an important role in systems biology when developmental processes are involved. Traditional methods of modelling and simulating such systems require substantial prior knowledge of mathematics and/or simulation algorithms. Such skills may impose a challenge for biologists, when they are not equally well-trained in mathematics and computer science. Coloured Petri nets as a high-level and graphical language offer an attractive alternative, which is easily approachable. In this paper, we investigate a coloured Petri net framework integrating deterministic, stochastic and hybrid modelling formalisms and corresponding simulation algorithms for the modelling and simulation of reaction\u2013diffusion processes that may be closely coupled with signalling pathways, metabolic reactions and/or gene expression. Such systems often manifest multiscaleness in time, space and/or concentration. We introduce our approach by means of some basic diffusion scenarios, and test it against an established case study, the Brusselator model."]},
{"title": "The hemodynamic alterations induced by the vascular angular deformation in stent-assisted coiling of bifurcation aneurysms", "highlights": ["The hemodynamic changes induced by vascular remodeling were investigated.", "The intra-aneurysmal flow activity was reduced by stent-induced vessel deformation.", "The maximum wall shear stress and oscillatory shear index were also increased.", "Bifurcation angle remodeling provided an unfavorable hemodynamic environment."], "abstract": ["The hemodynamic changes induced by stent deployment and vascular remodeling in bifurcation aneurysms were investigated using computational fluid dynamics. The stent deployment reduced the intra-aneurysmal flow activity by decreasing the mean velocity, mean kinetic energy, mean wall shear stress, and mean vorticity. These hemodynamic parameters increased with an increase in the branching angle because of the vessel deformation caused by stent straightening. The maximum wall shear stress and its spatial gradient occurred near the neck of the aneurysm in the stented left daughter vessel, whereas a maximum oscillatory shear index was detected near the neck of the right aneurysm of the right daughter vessel. Theses parameters, which might be related to the recurrence of aneurysms, were also increased by stent-induced vessel deformation."]},
{"title": "A content and structural assessment of oxidative motifs across a diverse set of life forms", "highlights": ["Protein is oxidized at RKPT and PEST motifs in protein during stress conditions.", "Mt respiration creates ROSs which are linked to oxidative damage.", "The frequencies of motifs attracting oxidation are shown to be reduced in Mt.", "These motifs are shown to be joints between turns, sheets, coils, and helices."], "abstract": ["Exposure to weightlessness (microgravity) or other protein stresses are detrimental to animal and human protein tissue health. Protein damage has been associated with stress and is linked to aging and the onset of diseases such as Alzheimer\u05f3s, Parkinson\u05f3s, sepsis, and others. Protein stresses may cause alterations to physical protein structure, altering its functional identity. Alterations from stresses such as microgravity may be responsible for forms of muscle atrophy (as noted in returning astronauts), however, protein stresses come from other sources as well.", "Oxidative carbonylation is a protein stress which is a driving force behind protein decay and is attracted to protein segments enriched in R, K, P, T, E and S residues. Since mitochondria apply oxidative processes to produce ATP, their proteins may be placed in the same danger as those that are exposed to stresses. However, they do not appear to be impacted in the same way.", "Across 14 diverse organisms, we evaluate the coverage of motifs which are high in the amino acids thought to be affected by protein stresses such as oxidation. For this study, we study RKPT and PEST motifs which are both responsible for attracting forms of oxidation across mitochondrial and non-mitochondrial proteins. We show that mitochondrial proteins have fewer of these oxidative sites compared to non-mitochondrial proteins. Additionally, we analyze the oxidative regions to determine that their motifs preferentially tend to make up the connection points between the four kinds of structures of folded proteins (helices, turns, sheets, and coils)."]},
{"title": "Visualizing in vivo brain neural structures using volume rendered feature spaces", "highlights": ["Interactive visualization software for 3D microscope images is developed.", "A new transfer function design using volume rendered feature spaces is proposed.", "Multidimensional features are directly utilized for volume exploration.", "Two-photon microscope images of live mice are applied to the developed software.", "Soma, dendrites and apical dendrites are visualized using 3D feature spaces."], "abstract": ["Dendrites of cortical neurons are widely spread across several layers of the cortex. Recently developed two-photon microscopy systems are capable of visualizing the morphology of neurons within deeper layers of the brain and generate large amounts of volumetric imaging data from living tissue.", "For visual exploration of the three-dimensional (3D) structure of dendrites and the connectivity among neurons in the brain, we propose a visualization software and interface for 3D images based on a new transfer function design using volume rendered feature spaces. This software enables the visualization of multidimensional descriptors of shape and texture extracted from imaging data to characterize tissue. It also allows the efficient analysis and visualization of large data sets.", "We apply and demonstrate the software to two-photon microscopy images of a living mouse brain. By applying the developed visualization software and algorithms to two-photon microscope images of the mouse brain, we identified a set of feature values that distinguish characteristic structures such as soma, dendrites and apical dendrites in mouse brain. Also, the visualization interface was compared to conventional 1D/2D transfer function system.", "We have developed a visualization tool and interface that can represent 3D feature values as textures and shapes. This visualization system allows the analysis and characterization of the higher-dimensional feature values of living tissues at the micron level and will contribute to new discoveries in basic biology and clinical medicine."]},
{"title": "OvaSpec \u2013 A vision-based instrument for assessing concentration and developmental stage of ", "highlights": ["A scalable, objective method for in-process drug testing was developed.", "OvaSpec automatically assess concentration and embryonation percentage of TSO.", "Algorithms were tested on a large, annotated dataset of 42,894 eggs in 2970 images.", "Darkfield scattering and larval morphology enables classification accuracies of 99%.", "Computer vision agrees with manual microscopy but has superior precision."], "abstract": ["OvaSpec is a new, fully automated, vision-based instrument for assessing the quantity (concentration) and quality (embryonation percentage) of ", " parasite eggs in liquid suspension. The eggs constitute the active pharmaceutical ingredient in a medicinal drug for the treatment of immune-mediated diseases such as Crohn\u05f3s disease, ulcerative colitis, and multiple sclerosis.", "This paper describes the development of an automated microscopy technology, including methodological challenges and design decisions of relevance for the future development of comparable vision-based instruments. Morphological properties are used to distinguish eggs from impurities and two features of the egg contents under brightfield and darkfield illumination are used in a statistical classification to distinguish eggs with undifferentiated contents (non-embryonated eggs) from eggs with fully developed larvae inside (embryonated eggs).", "For assessment of the instrument\u05f3s performance, six egg suspensions of varying quality were used to generate a dataset of unseen images. Subsequently, annotation of the detected eggs and impurities revealed a high agreement with the manual, image-based assessments for both concentration and embryonation percentage (both error rates <1.0%). Similarly, a strong correlation was demonstrated in a final, blinded comparison with traditional microscopic assessments performed by an experienced laboratory technician.", "The present study demonstrates the applicability of computer vision in the production, analysis, and quality control of ", " eggs used as an active pharmaceutical ingredient for the treatment of autoimmune diseases."]},
{"title": "Anonymization of DICOM electronic medical records for radiation therapy", "highlights": ["We extended an open-source code to process multiple EMRs automatically.", "We tested commercial optical character recognition (OCR) algorithm for the detection of burned-in text on a test image.", "OCR was unable to recognize the burned-in text reliably.", "We also developed and tested an image filtering algorithm to redact burned-in text from the test radiograph.", "Validation tests verified that PHI was anonymized and data integrity was preserved."], "abstract": ["Electronic medical records (EMR) and treatment plans are used in research on patient outcomes and radiation effects. In many situations researchers must remove protected health information (PHI) from EMRs. The literature contains several studies describing the anonymization of generic Digital Imaging and Communication in Medicine (DICOM) files and DICOM image sets but no publications were found that discuss the anonymization of DICOM radiation therapy plans, a key component of an EMR in a cancer clinic. In addition to this we were unable to find a commercial software tool that met the minimum requirements for anonymization and preservation of data integrity for radiation therapy research. The purpose of this study was to develop a prototype software code to meet the requirements for the anonymization of radiation therapy treatment plans and to develop a way to validate that code and demonstrate that it properly anonymized treatment plans and preserved data integrity. We extended an open-source code to process all relevant PHI and to allow for the automatic anonymization of multiple EMRs. The prototype code successfully anonymized multiple treatment plans in less than 1", "\u00a0", "min/patient. We also tested commercial optical character recognition (OCR) algorithms for the detection of burned-in text on the images, but they were unable to reliably recognize text. In addition, we developed and tested an image filtering algorithm that allowed us to isolate and redact alpha-numeric text from a test radiograph. Validation tests verified that PHI was anonymized and data integrity, such as the relationship between DICOM unique identifiers (UID) was preserved."]},
{"title": "Dynamic analysis of a needle insertion for soft materials: Arbitrary Lagrangian\u2013Eulerian-based three-dimensional finite element analysis", "highlights": ["We performed ALE-based finite element analysis for agar gel and copper needles.", "We compared simulation results with corresponding experimental results.", "Deflections of each needle between both sets of results were different.", "There was no significant difference for mismatching area error.", "Our results have a potential to use as pre-operative surgical planning."], "abstract": ["Our goal was to develop a three-dimensional finite element model that enables dynamic analysis of needle insertion for soft materials. To demonstrate large deformation and fracture, we used the arbitrary Lagrangian\u2013Eulerian (ALE) method for fluid analysis. We performed ALE-based finite element analysis for 3% agar gel and three types of copper needle with bevel tips.", "To evaluate simulation results, we compared the needle deflection and insertion force with corresponding experimental results acquired with a uniaxial manipulator. We studied the shear stress distribution of agar gel on various time scales.", "For 30\u00b0, 45\u00b0, and 60\u00b0, differences in deflections of each needle between both sets of results were 2.424, 2.981, and 3.737", "\u00a0", "mm, respectively. For the insertion force, there was no significant difference for mismatching area error (", "<0.05) between simulation and experimental results.", "Our results have the potential to be a stepping stone to develop pre-operative surgical planning to estimate an optimal needle insertion path for MR image-guided microwave coagulation therapy and for analyzing large deformation and fracture in biological tissues."]},
{"title": "Remote detection of mental workload changes using cardiac parameters assessed with a low-cost webcam", "highlights": ["A mental workload quantification using a low-cost webcam is proposed.", "An adaptive filtering was developed to track pulse frequency evolutions in time.", "Two parameters are combined from the PPG signal to form a stress curve.", "The stress inductor is based on a computerized version of the Stroop test.", "Our results showed high agreement with a reference skin conductance sensor."], "abstract": ["We introduce a new framework for detecting mental workload changes using video frames obtained from a low-cost webcam. Image processing in addition to a continuous wavelet transform filtering method were developed and applied to remove major artifacts and trends on raw webcam photoplethysmographic signals. The measurements are performed on human faces. To induce stress, we have employed a computerized and interactive Stroop color word test on a set composed by twelve participants. The electrodermal activity of the participants was recorded and compared to the mental workload curve assessed by merging two parameters derived from the pulse rate variability and photoplethysmographic amplitude fluctuations, which reflect peripheral vasoconstriction changes. The results exhibit strong correlation between the two measurement techniques. This study offers further support for the applicability of mental workload detection by remote and low-cost means, providing an alternative to conventional contact techniques."]},
{"title": "A computational pipeline for quantification of mouse myocardial stiffness parameters", "highlights": ["Deformation of passively inflated mouse heart was measured with echo speckle tracking.", "The observed deformation was compared with simulations of a finite element model.", "Simulations spanned the range of published parameter estimates.", "Two lack-of-fit criteria were tried: node positions or gross phenotypes.", "Estimation turned out to be more precise when based on changes in gross phenotypes."], "abstract": ["The mouse is an important model for theoretical\u2013experimental cardiac research, and biophysically based whole organ models of the mouse heart are now within reach. However, the passive material properties of mouse myocardium have not been much studied.", "We present an experimental setup and associated computational pipeline to quantify these stiffness properties. A mouse heart was excised and the left ventricle experimentally inflated from 0 to 1.44", "\u00a0", "kPa in eleven steps, and the resulting deformation was estimated by echocardiography and speckle tracking. An in silico counterpart to this experiment was built using finite element methods and data on ventricular tissue microstructure from diffusion tensor MRI. This model assumed a hyperelastic, transversely isotropic material law to describe the force\u2013deformation relationship, and was simulated for many parameter scenarios, covering the relevant range of parameter space. To identify well-fitting parameter scenarios, we compared experimental and simulated outcomes across the whole range of pressures, based partly on gross phenotypes (volume, elastic energy, and short- and long-axis diameter), and partly on node positions in the geometrical mesh. This identified a narrow region of experimentally compatible values of the material parameters. Estimation turned out to be more precise when based on changes in gross phenotypes, compared to the prevailing practice of using displacements of the material points. We conclude that the presented experimental setup and computational pipeline is a viable method that deserves wider application."]},
{"title": "Automated diagnosis of Age-related Macular Degeneration using greyscale features from digital fundus images", "highlights": ["We have developed automated Age-Related Macular Degeneration diagnosis system.", "Entropies, HOS, FD and Gabor wavelet features are extracted from fundus images.", "Various feature ranking methods are used to identify optimum features.", "The proposed system was evaluated using private, ARIA and STARE datasets.", "It yielded the highest average classification accuracies of 90.19%, 95.07% and 95%."], "abstract": ["Age-related Macular Degeneration (AMD) is one of the major causes of vision loss and blindness in ageing population. Currently, there is no cure for AMD, however early detection and subsequent treatment may prevent the severe vision loss or slow the progression of the disease. AMD can be classified into two types: dry and wet AMDs. The people with macular degeneration are mostly affected by dry AMD. Early symptoms of AMD are formation of drusen and yellow pigmentation. These lesions are identified by manual inspection of fundus images by the ophthalmologists. It is a time consuming, tiresome process, and hence an automated diagnosis of AMD screening tool can aid clinicians in their diagnosis significantly. This study proposes an automated dry AMD detection system using various entropies (Shannon, Kapur, Renyi and Yager), Higher Order Spectra (HOS) bispectra features, Fractional Dimension (FD), and Gabor wavelet features extracted from greyscale fundus images. The features are ranked using ", "-test, Kullback\u2013Lieber Divergence (KLD), Chernoff Bound and Bhattacharyya Distance (CBBD), Receiver Operating Characteristics (ROC) curve-based and Wilcoxon ranking methods in order to select optimum features and classified into normal and AMD classes using Naive Bayes (NB), ", "-Nearest Neighbour (", "-NN), Probabilistic Neural Network (PNN), Decision Tree (DT) and Support Vector Machine (SVM) classifiers. The performance of the proposed system is evaluated using private (Kasturba Medical Hospital, Manipal, India), Automated Retinal Image Analysis (ARIA) and STructured Analysis of the Retina (STARE) datasets. The proposed system yielded the highest average classification accuracies of 90.19%, 95.07% and 95% with 42, 54 and 38 optimal ranked features using SVM classifier for private, ARIA and STARE datasets respectively. This automated AMD detection system can be used for mass fundus image screening and aid clinicians by making better use of their expertise on selected images that require further examination."]},
{"title": "Classification algorithms for the identification of structural injury in TBI using brain electrical activity", "highlights": ["The derivation of algorithms for the identification of structural TBI are described.", "EEG recordings were obtained using only forehead electrodes on a handheld device.", "Classifiers methodologies used included genetic algorithms and LASSO logistic regression.", "The classifiers achieved an average sensitivity/specificity of 97.5%/59.5%.", "Performance well surpasses that of standard clinical practice for CT scan referrals in TBI."], "abstract": ["There is an urgent need for objective criteria adjunctive to standard clinical assessment of acute Traumatic Brain Injury (TBI). Details of the development of a quantitative index to identify structural brain injury based on brain electrical activity will be described.", "Acute closed head injured and normal patients (", "=1470) were recruited from 16 US Emergency Departments and evaluated using brain electrical activity (EEG) recorded from forehead electrodes. Patients had high GCS (median=15), and most presented with low suspicion of brain injury. Patients were divided into a CT positive (CT+) group and a group with CT negative findings or where CT scans were not ordered according to standard assessment (CT\u2212/CT_NR). Three different classifier methodologies, Ensemble Harmony, Least Absolute Shrinkage and Selection Operator (LASSO), and Genetic Algorithm (GA), were utilized.", "Similar performance accuracy was obtained for all three methodologies with an average sensitivity/specificity of 97.5%/59.5%, area under the curves (AUC) of 0.90 and average Negative Predictive Validity (NPV)>99%. Sensitivity was highest for CT+ cases with potentially life threatening hematomas, where two of three classifiers were 100%.", "Similar performance of these classifiers suggests that the optimal separation of the populations was obtained given the overlap of the underlying distributions of features of brain activity. High sensitivity to CT+ injuries (highest in hematomas) and specificity significantly higher than that obtained using ED guidelines for imaging, supports the enhanced clinical utility of this technology and suggests the potential role in the objective, rapid and more optimal triage of TBI patients."]},
{"title": "Blood glucose level reconstruction as a function of transcapillary glucose transport", "highlights": ["We test the widely used Steil\u2013Rebrin model for blood glucose level reconstruction.", "We propose an alternative for the Steil\u2013Rebrin model.", "The proposed model covers effects that the Steil\u2013Rebrin model does not cover.", "We compare both models using frequent sampling of glucose levels.", "The proposed model outperforms the Steil\u2013Rebrin model."], "abstract": ["A diabetic patient occasionally undergoes a detailed monitoring of their glucose levels. Over the course of a few days, a monitoring system provides a detailed track of their interstitial fluid glucose levels measured in their subcutaneous tissue. A discrepancy in the blood and interstitial fluid glucose levels is unimportant because the blood glucose levels are not measured continuously. Approximately five blood glucose level samples are taken per day, and the interstitial fluid glucose level is usually measured every 5", "\u00a0", "min. An increased frequency of blood glucose level sampling would cause discomfort for the patient; thus, there is a need for methods to estimate blood glucose levels from the glucose levels measured in subcutaneous tissue. The Steil\u2013Rebrin model is widely used to describe the relationship between blood and interstitial fluid glucose dynamics. However, we measured glucose level patterns for which the Steil\u2013Rebrin model does not hold. Therefore, we based our research on a different model that relates present blood and interstitial fluid glucose levels to future interstitial fluid glucose levels. Using this model, we derived an improved model for calculating blood glucose levels. In the experiments conducted, this model outperformed the Steil\u2013Rebrin model while introducing no additional requirements for glucose sample collection. In subcutaneous tissue, 26.71% of the calculated blood glucose levels had absolute values of relative differences from smoothed measured blood glucose levels less than or equal to 5% using the Steil\u2013Rebrin model. However, the same difference interval was encountered in 63.01% of the calculated blood glucose levels using the proposed model. In addition, 79.45% of the levels calculated with the Steil\u2013Rebrin model compared with 95.21% of the levels calculated with the proposed model had 20% difference intervals."]},
{"title": "LRRsearch: An asynchronous server-based application for the prediction of leucine-rich repeat motifs and an integrative database of NOD-like receptors", "highlights": ["LRRsearch is a new platform to identify LRR motifs in the wide variety of proteins.", "It is based on position specific scoring matrix of 11 residue LRR-HCS.", "It is integrated with a data library of 421 proteins of NLR families.", "The access to the \u201cLRRsearch\u201d is freely available at ", "."], "abstract": ["The leucine-rich repeat (LRR) motifs of the nucleotide-binding oligomerization domain like receptors (NLRs) play key roles in recognizing and binding various pathogen associated molecular patterns (PAMPs) resulting in the activation of downstream signaling and innate immunity. Therefore, identification of LRR motifs is very important to study ligand\u2013receptor interaction. To date, available resources pose restrictions including both false negative and false positive prediction of LRR motifs from the primary protein sequence as their algorithms are relied either only on sequence based comparison or alignment techniques or are over biased for a particular LRR containing protein family. Therefore, to minimize the error (\u22645%) and to identify a maximum number of LRR motifs in the wide range of proteins, we have developed \u201cLRRsearch\u201d web-server using position specific scoring matrix (PSSM) of 11 residue LRR-HCS (highly conserved segment) which are frequently observed motifs in the most divergent classes of LRR containing proteins. A data library of 421 proteins, distributed among five known NLR families has also been integrated with the \u201cLRRsearch\u201d for the rich user experience. The access to the \u201cLRRsearch\u201d program is freely available at ", "."]},
{"title": "Locally linear representation Fisher criterion based tumor gene expressive data classification", "highlights": ["Based on the class information, an intra-class graph and inter-class graph are constructed.", "In the inter-class graph, the reconstruction error denotes the shortest inter-class distance.", "In the intra-class graph, the reconstruction error means the intra-class data compactness.", "Experiments on some tumor gene expressive data validate LLRFC\u05f3s superiority."], "abstract": ["Tumor gene expressive data are characterized by a large amount of genes with only a small amount of observations, which always appear with high dimensionality. So it is necessary to reduce the dimensionality before identifying their genre. In this paper, a discriminant manifold learning method, named locally linear representation Fisher criterion (LLRFC), is applied to extract features from tumor gene expressive data. In LLRFC, an inter-class graph and an intra-class graph are constructed based on their genre information, where any tumor gene expressive data in the inter-class graph should select ", " nearest neighbors with different class labels and in the intra-class graph the ", " nearest neighbors for any tumor gene expressive data must be sampled from those with the same class. And then the locally least linear reconstruction is introduced to optimize the corresponding weights in both graphs. Moreover, a Fisher criterion is modeled to explore a low dimensional subspace where the reconstruction errors in the inter-class graph can be maximized and the reconstruction errors in the intra-class graph can be minimized, simultaneously. Experiments on some benchmark tumor gene expressive data have been conducted with some related algorithms, by which the proposed LLRFC has been validated to be efficient."]},
{"title": "A survey of prostate modeling for image analysis", "highlights": ["Complete and up to date review of prostate modeling.", "Description as three main steps: characteristics extraction, analyses and modeling.", "Classification and linkage with the main clinical applications."], "abstract": ["Computer technology is widely used for multimodal image analysis of the prostate gland. Several techniques have been developed, most of which incorporate ", " knowledge extracted from organ features. Knowledge extraction and modeling are multi-step tasks. Here, we review these steps and classify the modeling according to the data analysis methods employed and the features used. We conclude with a survey of some clinical applications where these techniques are employed."]},
{"title": "An automatic tool to facilitate the statistical group analysis of DTI", "highlights": ["We design an easy-to-use tool for DTI analysis using a user-centred methodology.", "Our tool leads to a 96% reduction in analysis time compared to traditional pipeline.", "Our tool minimises the user errors and automates the DTI statistical group analysis.", "Results in the study of depression in the elderly demonstrate its accuracy.", "The tool is currently being used by clinicians in a University Hospital."], "abstract": ["Users may have difficulty calculating DTI group statistics since they need to master several complex tools that require high user intervention. A tool called DTIStatistics for the automatic and easy calculation of DTI group statistics was developed to reduce analysis times and possible errors.", "The proposed software was designed by using a user-centred methodology in which we performed an iterative usability evaluation with an expert committee. Once the experts\u05f3 requirements were fulfilled, we performed a validation of the final version of DTIStatistics with target users, comparing the execution time of this tool and the standard pipeline normally used.", "Target users needed significantly less time to complete the tasks with DTIStatistics, reducing the analysis time from 1383.78 to 57.2", "\u00a0", "s. They were able to complete all the tasks and barely made errors. Moreover, target users were not able to display the analysis results with the standard pipeline, but when using our tool they only needed 34", "\u00a0", "s. Target users found DTIStatistics easy to learn, use and interact with, and they concluded that they could effectively complete the tasks with it. Additionally, we present example results in the study of depression to demonstrate the validity of DTIStatistics for clinical research.", "DTIStatistics facilitates and significantly automates the calculation of DTI group statistics by reducing the analysis times, which implies lower costs. DTIStatistics is highly applicable in clinical research, as demonstrated by the fact that it is currently being used at the University Hospital, University of Navarra (Spain)."]},
{"title": "A molecular prospective provides new insights into implication of ", "highlights": ["SNPs alter an individual\u05f3s susceptibility for alcohol addiction.", "Transcription factors bind to ", " and ", " SNPs in an allele-specific manner.", " SNP rs1997794 has a high deleterious effect on transcriptional regulation."], "abstract": ["Single nucleotide polymorphisms (SNPs) both in coding and non-coding regions govern gene functions prompting differential vulnerability to diseases, heterogeneous response to pharmaceutical regimes and environmental anomalies. These genetic variations, SNPs, may alter an individual\u05f3s susceptibility for alcohol dependence by remodeling DNA\u2013protein interaction patterns in prodynorphin (", ") and the \u03ba-opioid receptor (", ") genes. In order to elaborate the underlying molecular mechanism behind these susceptibility differences we used bioinformatics tools to retrieve differential DNA\u2013protein interactions at ", " and ", " SNPs significantly associated with alcohol dependence. Our results show allele-specific DNA\u2013protein interactions depicting allele-specific mechanisms implicated in differential regulation of gene expression. Several transcription factors, for instance, VDR, RXR-alpha, NFYA, CTF family, USF-1, USF2, ER, AR and predominantly SP family show an allele-specific binding affinity with ", " gene; likewise, GATA, TBP, AP-1, USF-2, C/EBPbeta, Cart-1 and ER interact with ", " SNPs on intron 2 in an allele-specific manner. In a nutshell, transition of a single nucleotide may modify differential DNA\u2013protein interactions at ", " and ", " SNPs, significantly associated with pathology that may lead to altered individual vulnerability for alcohol dependence."]},
{"title": "Transport and deposition of pharmaceutical particles in three commercial spacer\u2013MDI combinations", "highlights": ["Numerical modeling of particle deposition in three commercial spacers.", "Introduction and demonstration of a novel particle injection method.", "Experimental validation of numerical results.", "Introduction of some design suggestions to improve spacers\u05f3 effectiveness."], "abstract": ["Respiratory drug delivery has been under the research spotlight for the past few decades, mainly due to the high incidence of pulmonary diseases and the fact that this type of delivery offers the highest efficiency for treatment. Despite its invaluable benefits, there are some major drawbacks to respiratory drug delivery, the most important of which being poor delivery efficiency and relatively high drug deposition in undesirable regions, such as the mouth cavity. One way to improve the efficiency of respiratory drug delivery with metered-dose inhalers is placing a respiratory spacer between the inhaler exit and the mouth. It is argued that high drug deposition in the immediate airways of the respiratory system is strongly affected by relatively high initial momentum of pharmaceutical particles leaving the inhaler. A respiratory spacer, however, can provide an expansion region in which the initial momentum of particles can subside. As a result, particles enter the patient\u05f3s oral cavity more gradually and are more likely to reach the desired regions. In this study, the effectiveness of using three commercial spacers paired with a commercial inhaler is examined through numerical investigation of fluid flow and particle transport phenomena. Particles ranging from 1 to 50", "\u00a0", "\u00b5m in diameter are tracked using a Lagrangian point of view and fluid flow fields are resolved using the LRN ", " turbulence model. A novel particle injection method is introduced and is demonstrated to be able to adequately capture the effects of particle initial momentum. Lastly, a few design suggestions are made."]},
{"title": "Thermographic evaluation of early melanoma within the vascularized skin using combined non-Newtonian blood flow and bioheat models", "highlights": ["Coupled bioheat and blood flow model has been proposed for thermal imaging of melanoma.", "Blood rheological properties have been studied and their importance has been discussed.", "Thermal performance of dynamic thermal imaging reduces due to regional vessels.", "Thermal evaluation criteria for detection of early melanoma has been predicted."], "abstract": ["A theoretical study on vascularized skin model to predict the thermal evaluation criteria of early melanoma using the dynamic thermal imaging technique is presented in this article. Thermographic evaluation of melanoma has been carried out during the thermal recovery of skin from undercooled condition. During thermal recovery, the skin has been exposed to natural convection, radiation, and evaporation. The thermal responses of melanoma have been evaluated by integrating the bioheat model for multi-layered skin with the momentum as well as energy conservation equations for blood flow. Differential changes in the surface thermal response of various melanoma stages except that of the early stage have been determined. It has been predicted that the thermal response due to subsurface blood flow overpowers the response of early melanoma. Hence, the study suggests that the quantification of early melanoma diagnosis using thermography has not reached a matured stage yet. Therefore, the study presents a systematic analysis of various intermediate melanoma stages to determine the thermal evaluation criteria of early melanoma. The comprehensive modeling effort made in this work supports the prediction of the disease outcome and relates the thermal response with the variation in patho-physiological, thermal and geometrical parameters."]},
{"title": "Manual and automated intima-media thickness and diameter measurements of the common carotid artery in patients with renal failure disease", "highlights": ["We use two different automated segmentation systems, based on snakes and on active contour models.", "We performed measurements of the carotid intima-media thickness (IMT) and diameter (", ") in renal failure disease patients.", "Statistical significant differences for the IMT and the ", ", between the normal and the RFD patients were found.", "The ACM segmentation was slightly more accurate than segmentation based on snakes."], "abstract": ["The objective of this study was to investigate differences in intima-media thickness (IMT) and diameter (", ") measurements of the common carotid artery (CCA) in ultrasound imaging in normal subjects and renal failure disease (RFD) patients. Manual measurements by two experts and automated segmentation measurements (based on snakes and active contour models (ACM)) were carried out on 73 normal subjects, and 80 RFD patients. Statistical analysis was carried out using the Wilcoxon rank-sum test at ", "<0.05. Results demonstrated that the mean IMT and ", " measurements were significantly higher for the RFD group versus the normal group. Moreover, there was no significant difference between the manual and automated measurements. The ACM segmentation was slightly more accurate than segmentation based on snakes. Further work is needed to validate these findings on a larger group of subjects."]},
{"title": "Soft computing approach to 3D lung nodule segmentation in CT", "highlights": ["We propose a multistage algorithm for 3D lung nodule segmentation.", "Soft computing: evolutionary computation and fuzzy connectedness is employed.", "The system is designed to extract various types of lung nodules.", "Two LIDC datasets with over 500 nodules have been employed for evaluation.", "A comprehensive efficiency analysis with original metrics has been proposed."], "abstract": ["This paper presents a novel, multilevel approach to the segmentation of various types of pulmonary nodules in computed tomography studies. It is based on two branches of computational intelligence: the fuzzy connectedness (FC) and the evolutionary computation. First, the image and auxiliary data are prepared for the 3D FC analysis during the first stage of an algorithm \u2013 the masks generation. Its main goal is to process some specific types of nodules connected to the pleura or vessels. It consists of some basic image processing operations as well as dedicated routines for the specific cases of nodules. The evolutionary computation is performed on the image and seed points in order to shorten the FC analysis and improve its accuracy. After the FC application, the remaining vessels are removed during the postprocessing stage. The method has been validated using the first dataset of studies acquired and described by the Lung Image Database Consortium (LIDC) and by its latest release \u2013 the LIDC\u2013IDRI (Image Database Resource Initiative) database."]},
{"title": "Computational diagnosis and risk evaluation for canine lymphoma", "highlights": ["Acute phase proteins, C-Reactive Protein and Haptoglobin, are used for the canine lymphoma blood test.", "This test can be used for diagnostics, screening, and for remission monitoring.", "We compare various decision trees, KNN (and advanced KNN) and algorithms for probability density evaluation.", "For the differential diagnosis the best solution gives the sensitivity 83.5% and specificity 77%."], "abstract": ["The canine lymphoma blood test detects the levels of two biomarkers, the acute phase proteins (C-Reactive Protein and Haptoglobin). This test can be used for diagnostics, for screening, and for remission monitoring as well. We analyze clinical data, test various machine learning methods and select the best approach to these oblems. Three families of methods, decision trees, kNN (including advanced and adaptive kNN) and probability density evaluation with radial basis functions, are used for classification and risk estimation. Several pre-processing approaches were implemented and compared. The best of them are used to create the diagnostic system. For the differential diagnosis the best solution gives the sensitivity and specificity of 83.5% and 77%, respectively (using three input features, CRP, Haptoglobin and standard clinical symptom). For the screening task, the decision tree method provides the best result, with sensitivity and specificity of 81.4% and ", ", respectively (using the same input features). If the clinical symptoms (Lymphadenopathy) are considered as unknown then a decision tree with CRP and Hapt only provides sensitivity 69% and specificity 83.5%. The lymphoma risk evaluation problem is formulated and solved. The best models are selected as the system for computational lymphoma diagnosis and evaluation of the risk of lymphoma as well. These methods are implemented into a special web-accessed software and are applied to the problem of monitoring dogs with lymphoma after treatment. It detects recurrence of lymphoma up to two months prior to the appearance of clinical signs. The risk map visualization provides a friendly tool for exploratory data analysis."]},
{"title": "Informatics can identify systemic sclerosis (SSc) patients at risk for scleroderma renal crisis", "highlights": ["Electronic medical records provide an opportunity to improve patient care.", "Natural language processing allows the identification of patients potentially at risk.", "Informatics provides the opportunity educate providers when patients at risk are identified."], "abstract": ["Electronic medical records (EMR) provide an ideal opportunity for the detection, diagnosis, and management of systemic sclerosis (SSc) patients within the Veterans Health Administration (VHA). The ", " of this project was to use informatics to identify potential SSc patients in the VHA that were on prednisone, in order to inform an outreach project to prevent scleroderma renal crisis (SRC).", "The electronic medical data for this study came from Veterans Informatics and Computing Infrastructure (VINCI). For natural language processing (NLP) analysis, a set of retrieval criteria was developed for documents expected to have a high correlation to SSc. The two annotators reviewed the ratings to assemble a single adjudicated set of ratings, from which a support vector machine (SVM) based document classifier was trained. Any patient having at least one document positively classified for SSc was considered positive for SSc and the use of prednisone\u226510", "\u00a0", "mg in the clinical document was reviewed to determine whether it was an active medication on the prescription list.", "In the VHA, there were 4272 patients that have a diagnosis of SSc determined by the presence of an ICD-9 code. From these patients, 1118 patients (21%) had the use of prednisone\u226510", "\u00a0", "mg. Of these patients, 26 had a concurrent diagnosis of hypertension, thus these patients should not be on prednisone. By the use of natural language processing (NLP) an additional 16,522 patients were identified as possible SSc, highlighting that cases of SSc in the VHA may exist that are unidentified by ICD-9. A 10-fold cross validation of the classifier resulted in a precision (positive predictive value) of 0.814, recall (sensitivity) of 0.973, and f-measure of 0.873.", "Our study demonstrated that current clinical practice in the VHA includes the potentially dangerous use of prednisone for veterans with SSc. This present study also suggests there may be many undetected cases of SSc and NLP can successfully identify these patients."]},
{"title": "Time series for blind biosignal classification model", "highlights": ["A blind biosignal classification model is proposed to benefit the diagnosis.", "The approach can automatically identify the type of a blind biosignal.", "The model classifies a disease or symptom without knowing the source signal type.", "Enable non-skillful home users to operate the biosignal acquisition devices easily.", "Refinements over time series algorithm improve the performance and efficiency."], "abstract": ["Biosignals such as electrocardiograms (ECG), electroencephalograms (EEG), and electromyograms (EMG), are important noninvasive measurements useful for making diagnostic decisions. Recently, considerable research has been conducted in order to potentially automate signal classification for assisting in disease diagnosis. However, the biosignal type (ECG, EEG, EMG or other) needs to be known prior to the classification process. If the given biosignal is of an unknown type, none of the existing methodologies can be utilized. In this paper, a blind biosignal classification model (", ") is proposed in order to identify the source biosignal type automatically, and thus ultimately benefit the diagnostic decision. The approach employs time series algorithms for constructing the model. It uses a dynamic time warping (DTW) algorithm with clustering to discover the similarity between two biosignals, and consequently classifies disease without prior knowledge of the source signal type. The empirical experiments presented in this paper demonstrate the effectiveness of the method as well as the scalability of the approach."]},
{"title": "The identification of the relationship between chemical and electrical parameters of honeys using artificial neural networks", "highlights": ["Relationships between chemical and electrical parameters of honeys were modeled.", "The neural networks sensitivity analysis was conducted.", "The influence of chemical on electrical honeys parameters depends on frequency.", "Electrical parameters suitability for honey quality assessment depends on frequency."], "abstract": ["A number of significant scientific studies have confirmed the health benefits of honey. Due to the high price of natural honey, it is a common target for adulteration which reduces its medicinal value. Adulteration detection methods require specific laboratory equipment and are very expensive. The development of measurement techniques enables the measurement of electrical characteristics of strained honey. Honey electrical parameters can possibly be used for its quality assessment. The identification of the relationship between chemical and electrical parameters of honeys and analysis to determine if there are frequency-dependent changes, can help in developing of that group of methods. The aim of this research was to determine how the chemical parameters of certain honeys influence the dielectric loss factor and the permittivity of strained honey measured in various frequencies. Another aim was to determine whether the percentage influence of certain chemical parameters of honeys on electrical characteristics significantly depends on frequency value. The research was based on neural network models and sensitivity analysis. The percentage influence of certain chemical parameters on electrical characteristics significantly depends on frequency value."]},
{"title": "A novel electrocardiogram parameterization algorithm and its application in myocardial infarction detection", "highlights": ["This study proposes a novel electrocardiogram (ECG) parameterization algorithm.", "An ECG cycle is optimally fit with a 20th order polynomial function.", "The coefficients of the fitting function are used for efficient MI detection."], "abstract": ["The electrocardiogram (ECG) is a biophysical electric signal generated by the heart muscle, and is one of the major measurements of how well a heart functions. Automatic ECG analysis algorithms usually extract the geometric or frequency-domain features of the ECG signals and have already significantly facilitated automatic ECG-based cardiac disease diagnosis. We propose a novel ECG feature by fitting a given ECG signal with a 20th order polynomial function, defined as ", ". The ", " feature is almost identical to the fitted ECG curve, measured by the Akaike information criterion (AIC), and achieved a 94.4% accuracy in detecting the Myocardial Infarction (MI) on the test dataset. Currently ST segment elongation is one of the major ways to detect MI (ST-elevation myocardial infarction, STEMI). However, many ECG signals have weak or even undetectable ST segments. Since ", " does not rely on the information of ST waves, it can be used as a complementary MI detection algorithm with the STEMI strategy. Overall, our results suggest that the ", " feature may satisfactorily reconstruct the fitted ECG curve, and is complementary to the existing ECG features for automatic cardiac function analysis."]},
{"title": "Mining approximate temporal functional dependencies with pure temporal grouping in clinical databases", "highlights": ["We focus on dependencies over facts (e.g. diagnosis vs therapy) stored in a database.", "Dependencies may generally hold: deviating facts set up an approximate dependency.", "We group data in sets, either by temporal granules or by fixed-length sliding windows", "Within every set, we look for temporal approximate dependencies.", "We mine psychiatry and pharmacovigilance data sets and derive new knowledge."], "abstract": ["Functional dependencies (", "s) typically represent associations over facts stored by a database, such as \u201c", ".\u201d In more recent years, some extensions have been introduced to represent both temporal constraints (temporal functional dependencies \u2013 ", "s), as \u201c", ",\u201d and approximate properties (approximate functional dependencies \u2013 ", "s), as \u201c", " ", " ", ".\u201d An ", " holds most of the facts stored by the database, enabling some data to deviate from the defined property: the percentage of data which violate the given property is user-defined.", "According to this scenario, in this paper we introduce approximate temporal functional dependencies (", "s) and use them to mine clinical data. Specifically, we considered the need for deriving new knowledge from psychiatric and pharmacovigilance data.", "s may be defined and measured either on temporal granules (e.g.", "\u00a0", "grouping data by day, week, month, year) or on sliding windows (e.g.", "\u00a0", "a fixed-length time interval which moves over the time axis): in this regard, we propose and discuss some specific and efficient data mining techniques for ", "s. We also developed two running prototypes and showed the feasibility of our proposal by mining two real-world clinical data sets. The clinical interest of the dependencies derived considering the psychiatry and pharmacovigilance domains confirms the soundness and the usefulness of the proposed techniques."]},
{"title": "Evaluation of automatic feature detection algorithms in EEG: Application to interburst intervals", "highlights": ["A software to improve algorithms for feature detection in neonatal EEG is proposed.", "The clustering quality of feature detectors with EEG rating is quantified.", "The method is tested on a modular definition of interburst intervals detectors.", "The power supply filter can be removed with little effect on the detection quality.", "Detectors are robust compared with the standard deviation thresholding of the EEG."], "abstract": ["In this paper, we present a new method to compare and improve algorithms for feature detection in neonatal EEG. The method is based on the algorithm\u05f3s ability to compute accurate statistics to predict the results of EEG visual analysis. This method is implemented inside a Java software called ", ", as part of an e-health Web portal dedicated to neonatal EEG.", " encapsulates a component-based implementation of the detection algorithms called analyzers. Each analyzer is defined by a list of modules executed sequentially. As the libraries of modules are intended to be enriched by its users, we developed a process to evaluate the performance of new modules and analyzers using a database of expertized and categorized EEGs. The evaluation is based on the Davies\u2013Bouldin index (DBI) which measures the quality of cluster separation, so that it will ease the building of classifiers on risk categories. For the first application we tested this method on the detection of interburst intervals (IBI) using a database of 394 EEG acquired on premature newborns. We have defined a class of IBI detectors based on a threshold of the standard deviation on contiguous short time windows, inspired by previous work. Then we determine which detector and what threshold values are the best regarding DBI, as well as the robustness of this choice. This method allows us to make counter-intuitive choices, such as removing the 50", "\u00a0", "Hz filter (power supply) to save time."]},
{"title": "Fractal-like correlations of the fluctuating inter-spike membrane potential of a ", "highlights": ["We found long-term correlations in pacemaker neural inter-spike voltage fluctuations.", "Such behavior is probably linked to mechanisms that regulate the membrane potential.", "Our study endorses critical-like phenomena occurring at a single-neuron level."], "abstract": ["We analyzed the voltage fluctuations of the membrane potential manifested along the inter-spike segment of a pacemaker neuron. Time series of intracellular inter-spike voltage fluctuations were obtained in the current-clamp configuration from the F1 neuron of 12 ", " specimens. To assess the dynamic or stochastic nature of the voltage fluctuations these series were analyzed by Detrended Fluctuation Analysis (DFA), providing the scaling exponent ", ". The median ", " result obtained for the inter-spike segments was 0.971 ([0.963, 0.995] lower and upper quartiles). Our results indicate a critical-like dynamic behavior in the inter-spike membrane potential that, far from being random, shows long-term correlations probably linked to the dynamics of the mechanisms involved in the regulation of the membrane potential, thereby endorsing the occurrence of critical-like phenomena at a single-neuron level."]},
{"title": "Fully automated liver segmentation from SPIR image series", "highlights": ["A novel fully automated liver segmentation method is proposed.", "The accuracy of the proposed method is 96%.", "Processing all slices in a data set without any user interaction requires less time.", "The proposed segmentation method is successful even if there is a cyst in the liver."], "abstract": ["Accurate liver segmentation is an important component of surgery planning for liver transplantation, which enables patients with liver disease a chance to survive. Spectral pre-saturation inversion recovery (SPIR) image sequences are useful for liver vessel segmentation because vascular structures in the liver are clearly visible in these sequences. Although level-set based segmentation techniques are frequently used in liver segmentation due to their flexibility to adapt to different problems by incorporating prior knowledge, the need to initialize the contours on each slice is a common drawback of such techniques. In this paper, we present a fully automated variational level set approach for liver segmentation from SPIR image sequences. Our approach is designed to be efficient while achieving high accuracy. The efficiency is achieved by (1) automatically defining an initial contour for each slice, and (2) automatically computing weight values of each term in the applied energy functional at each iteration during evolution. Automated detection and exclusion of spurious structures (e.g. cysts and other bright white regions on the skin) in the pre-processing stage increases the accuracy and robustness. We also present a novel approach to reduce computational cost by employing binary regularization of level set function. A signed pressure force function controls the evolution of the active contour. The method was applied to ten data sets. In each image, the performance of the algorithm was measured using the receiver operating characteristics method in terms of accuracy, sensitivity and specificity. The accuracy of the proposed method was 96%. Quantitative analyses of results indicate that the proposed method can accurately, efficiently and consistently segment liver images."]},
{"title": "Heart monitoring systems\u2014A review", "highlights": ["We classified portable heart monitoring systems in two manners, on-site and off-site.", "Off-site heart monitoring systems have been introduced in five modules.", "A detailed review of recent advancements in each module has been provided.", "Phonocardiography, electrocardiography, photoplethysmography and seismocardiography are discussed for long term heart monitoring.", "Smart phones based heart monitoring system and their challenges have been discussed."], "abstract": ["To diagnose health status of the heart, heart monitoring systems use heart signals produced during each cardiac cycle. Many types of signals are acquired to analyze heart functionality and hence several heart monitoring systems such as phonocardiography, electrocardiography, photoplethysmography and seismocardiography are used in practice. Recently, focus on the at-home monitoring of the heart is increasing for long term monitoring, which minimizes risks associated with the patients diagnosed with cardiovascular diseases. It leads to increasing research interest in portable systems having features such as signal transmission capability, unobtrusiveness, and low power consumption. In this paper we intend to provide a detailed review of recent advancements of such heart monitoring systems. We introduce the heart monitoring system in five modules: (1) body sensors, (2) signal conditioning, (3) analog to digital converter (ADC) and compression, (4) wireless transmission, and (5) analysis and classification. In each module, we provide a brief introduction about the function of the module, recent developments, and their limitation and challenges."]},
{"title": "Investigation of global and local network properties of music perception with culturally different styles of music", "highlights": ["Ten untrained subjects listened to 3 culturally different styles of music stimuli.", "Electroencephalograms were measured and global and local network properties compared.", "Light music and western classical music perception manifested small world properties.", "Chinese and western classical music perception showed right lateralization.", "Further research is needed to understand network properties of music perception."], "abstract": ["Graph theoretical analysis has recently become a popular research tool in neuroscience, however, there have been very few studies on brain responses to music perception, especially when culturally different styles of music are involved.", "Electroencephalograms were recorded from ten subjects listening to Chinese traditional music, light music and western classical music. For event-related potentials, phase coherence was calculated in the alpha band and then constructed into correlation matrices. Clustering coefficients and characteristic path lengths were evaluated for global properties, while clustering coefficients and efficiency were assessed for local network properties.", "Perception of light music and western classical music manifested small-world network properties, especially with a relatively low proportion of weights of correlation matrices. For local analysis, efficiency was more discernible than clustering coefficient. Nevertheless, there was no significant discrimination between Chinese traditional and western classical music perception.", "Perception of different styles of music introduces different network properties, both globally and locally. Research into both global and local network properties has been carried out in other areas; however, this is a preliminary investigation aimed at suggesting a possible new approach to brain network properties in music perception."]},
{"title": "PcHD: Personalized classification of heartbeat types using a decision tree", "highlights": ["We propose a new method for personalized arrhythmia classification of an individual\u05f3s heartbeats by Holter monitoring.", "We use a decision tree to classify beats.", "We demonstrate the efficacy of this classifier by means of experiments against the MIT-BIH arrhythmia database.", "Our classifier is very accurate; it reduces the number of false alarms and missing events."], "abstract": ["The computer-aided interpretation of electrocardiogram ", "ECG", " signals provides a non-invasive and inexpensive technique for analyzing heart activity under various cardiac conditions. Further, the proliferation of smartphones and wireless networks makes it possible to perform continuous Holter monitoring. However, although considerable attention has been paid to automated detection and classification of heartbeats from ECG data, classifier learning strategies have never been used to deal with individual variations in cardiac activity. In this paper, we propose a novel method for automatic classification of an individual\u05f3s ECG beats for Holter monitoring. We use the Pan-Tompkins algorithm to accurately extract features such as the QRS complex and P wave, and employ a decision tree to classify each beat in terms of these features. Evaluations conducted against the MIT-BIH arrhythmia database before and after personalization of the decision tree using a patient\u05f3s own ECG data yield heartbeat classification accuracies of 94.6% and 99%, respectively. These are comparable to results obtained from state", "of", "the-art schemes, validating the efficacy of our proposed method."]},
{"title": "Bayesian reconstruction of projection reconstruction NMR (PR-NMR)", "highlights": ["We design a mathematical model for reconstructing multidimensional NMR spectra.", "We compare the performance of several statistical algorithms for PR-NMR.", "A generalized Monte Carlo algorithm is applied to restore the NMR spectra."], "abstract": ["Projection reconstruction nuclear magnetic resonance (PR-NMR) is a technique for generating multidimensional NMR spectra. A small number of projections from lower-dimensional NMR spectra are used to reconstruct the multidimensional NMR spectra. In our previous work ", ", ", ", it was shown that multidimensional NMR spectra are efficiently reconstructed using peak-by-peak based reversible jump Markov chain Monte Carlo (RJMCMC) algorithm. We propose an extended and generalized RJMCMC algorithm replacing a simple linear model with a linear mixed model to reconstruct close NMR spectra into true spectra. This statistical method generates samples in a Bayesian scheme. Our proposed algorithm is tested on a set of six projections derived from the three-dimensional 700", "\u00a0", "MHz HNCO spectrum of a protein HasA."]},
{"title": "Quantification of the acute effect of a low dose of red wine by nonlinear measures of RR and QT interval series in healthy subjects", "highlights": ["We compared the acute effect of red wine and control alcoholic drink on the heart rhythm properties.", "Nonlinear analysis of RR and QT series reveals the difference between the effects of wine and the control drink.", "Wine changed the complexity of series, while the control drink prolonged QT and RR intervals."], "abstract": ["The measures of nonlinear properties of RR interval and QT interval time series are sensitive to physiologically- or pathologically-induced complexity/regularity changes, but were not used to estimate the effect of alcohol intake. We wanted to examine the potential of these measures to quantify the acute effect of a low dose of red wine in healthy subjects. In separate experiments, fourteen young volunteers drank 200", "\u00a0", "ml of red wine and a control drink with equal concentration of ethanol. ECG in supine position was recorded 20", "\u00a0", "min before and 60", "\u00a0", "min after drink intake. RR interval and QT interval series were extracted from ECG and we calculated variability, scaling exponents (", " and ", ") and sample entropy (SampEn) for both series. Systolic and diastolic blood pressures (BP) were measured every 10", "\u00a0", "min. The immediate effect of both the drinks was equal: HR, BP and QT variability exhibited a sudden increase and then a decrease. However, the prolonged effect of wine and the control drink was different. Wine decreased both BP (", "<0.05) and reduced complexity of RR and QT series (increased scaling exponents and decreased SampEn). The control drink prolonged QT and RR intervals (", "<0.05). These results point out that the nonlinear properties of RR and QT interval series could be used to differentiate the effect of wine and ethanol. Changes in RR and QT interval series induced by a low dose of red wine are more detectable by methods that quantify the structure of the series than by methods that quantify their variability."]},
{"title": "Biological-data-based finite-element stress analysis of mandibular bone with implant-supported overdenture", "highlights": ["We clarify the stress distribution in peri-implant mandibular bone of a subject with an implant-supported overdenture.", "FEA models of mandible with two and four implants were constructed by CT images and ", " loading data in the subject.", "Stress reduction was observed by increasing the number of implants.", "Stress concentrated at thin cortical bone around the implants.", "Stress concentrated around the implant aligned with a large deviation from load direction."], "abstract": ["This study aimed to evaluate the stress distribution in a mandibular bone with an implant-supported overdenture by a biological-data-based finite element analysis (FEA) utilizing personal CT images and ", " loading data, and to evaluate the influence of the number and alignment of implants and bone conditions on the stress in peri-implant bone.", "FEA models of a mandible were constructed for two types of overdentures: 4 implants supported overdenture (4-OD) and 2 implants supported overdenture (2-OD). The geometry of these models was constructed from CT images of a subject, who wore an implant-supported overdenture. The magnitude and direction of the loads on the implants for two types of overdentures during the maximal voluntary clenching were measured with 3D force transducers. FEA using these loads was carried out to observe stress distributions in peri-implant bone.", "Higher stress was observed in cortical bone around the implant neck. Stress in peri-implant bone for 4-OD was reduced in comparison with those for the 2-OD. For the 4-OD, notwithstanding such reduction of the stress, the stress concentrated at the cortical bone around the implant aligned with large deviation from load direction.", "In this study, biological data from a certain subject was successfully duplicated to the FEA models. The results demonstrate the mechanical prominence of using more implants. Even in 4 implants model, high stress was found around an implant with a large inclination and with thin cortical bone. This suffices to demonstrate the capability and usefulness of the biological-data-based FEA."]},
{"title": "Network-based approach reveals Y chromosome influences prostate cancer susceptibility", "highlights": ["We developed an integrative network-based framework for prostate cancer.", "We examined the role of Y-chromosome genes through different states.", "The new definition of modulation score is proposed for detecting novel pathways and processes.", "Candidate genes are introduced for future research in the field of cancer studies as key factors."], "abstract": ["The human Y chromosome contains a small number of genes that play a critical role in the determination of male-specific organs. Today\u2019s advances have provided valuable resources for defining the functions of this chromosome in both normal and cancerous prostates. Despite the fact that generation of high-throughput expression data is becoming usual; the systematic methods of data analysis in a biological context are still an impediment.", "Here we have shown that constructing co-expression networks using Y-chromosome genes provides an alternative strategy for the detection of new candidate genes involved in prostate cancer. In our approach, independent co-expression networks from normal and cancerous stages are reconstructed using a reverse engineering approach. We then highlight crucial pathways, biological processes, and genes involved in the prostate cancer by analyzing each network individually and in concert. Thus, we have identified 18 critical pathways and processes related to prostate cancer, many of which have previously been shown to be involved in cancer. In particular, we identify 22 Y-chromosome genes putatively linked to prostate cancer, 13 of which have been already verified experimentally.", "Our novel network-based approach is useful for accurate inference of processes and essential regulators that mediate molecular changes during cancer progression."]},
{"title": "New layers in understanding and predicting \u03b1-linolenic acid content in plants using amino acid characteristics of omega-3 fatty acid desaturase", "highlights": ["Discovery of key protein attributes of omega-3 desaturase in high omega-3 plants.", "Predicting \u03b1-linolenic acid using amino acid characteristics of omega-3 desaturase.", "Model discovery by large scale feature extraction coupled with machine learning.", "High performance of feature discovery via comparison of attribute weighting models.", "Developing a software for prediction of high and low content of \u03b1-linolenic acid."], "abstract": ["\u03b1-linolenic acid (ALA) is the most frequent omega-3 in plants. The content of ALA is highly variable, ranging from 0 to 1% in rice and corn to >50% in perilla and flax. ALA production is strongly correlated with the enzymatic activity of omega-3 fatty acid desaturase. To unravel the underlying mechanisms of omega-3 diversity, 895 protein features of omega-3 fatty acid desaturase were compared between plants with high and low omega-3. Attribute weighting showed that this enzyme in plants with high omega-3 content has higher amounts of Lys, Lys-Phe, and Pro-Asn but lower Aliphatic index, Gly-His, and Pro-Leu. The ", " model with ", " criterion when run on the dataset pre-filtered with ", " algorithm was the best model in distinguishing high omega-3 content based on the frequency of Lys\u2013Lys in the structure of fatty acid desaturase. Interestingly, the discriminant function algorithm could predict the level of omega-3 only based on the six important selected attributes (out of 895 protein attributes) of fatty acid desaturase with 75% accuracy. We developed \u201cPlant omega3 predictor\u201d to predict the content of \u03b1-linolenic acid based on structural features of omega-3 fatty acid desaturase. The software calculates the 6 key structural protein features from imported Fasta sequence of omega-3 fatty acid desaturase or utilizes the imported features and predicts the ALA content using discriminant function formula. This work unravels an underpinning mechanism of omega-3 diversity via discovery of the key protein attributes in the structure of omega-3 desaturase offering a new approach to obtain higher omega-3 content."]},
{"title": "Design, analysis and verification of a knee joint oncological prosthesis finite element model", "highlights": ["PROSPON oncological implant FE model developed and validated.", "The PEEK-OPTIMA", " hinge pin bushing analysed in knee-bent position.", "Stress distribution in accordance with contact pressure distribution.", "Maximal Von Mises stress (46.64", "\u00a0", "MPa) in accordance with material yield strength.", "The final model in accordance with the real endoprosthesis behaviour."], "abstract": ["The aim of this paper was to design a finite element model for a hinged PROSPON oncological knee endoprosthesis and to verify the model by comparison with ankle flexion angle using knee-bending experimental data obtained previously.", "Visible Human Project CT scans were used to create a general lower extremity bones model and to compose a 3D CAD knee joint model to which muscles and ligaments were added. Into the assembly the designed finite element PROSPON prosthesis model was integrated and an analysis focused on the PEEK-OPTIMA", " hinge pin bushing stress state was carried out. To confirm the stress state analysis results, contact pressure was investigated. The analysis was performed in the knee-bending position within 15.4\u201369.4\u00b0 hip joint flexion range.", "The results showed that the maximum stress achieved during the analysis (46.6", "\u00a0", "MPa) did not exceed the yield strength of the material (90", "\u00a0", "MPa); the condition of plastic stability was therefore met. The stress state analysis results were confirmed by the distribution of contact pressure during knee-bending.", "The applicability of our designed finite element model for the real implant behaviour prediction was proven on the basis of good correlation of the analytical and experimental ankle flexion angle data."]},
{"title": "Computer-aided design of the human aortic root", "highlights": ["We have created four typical models of human aortic root based on ECHO and CT data.", "We performed detailed analysis of aortic root elements dynamic during cardiac cycle.", "The separate material description of annulus and sinuses can improve FEA results."], "abstract": ["The development of computer-based 3D models of the aortic root is one of the most important problems in constructing the prostheses for transcatheter aortic valve implantation. In the current study, we analyzed data from 117 patients with and without aortic valve disease and computed tomography data from 20 patients without aortic valvular diseases in order to estimate the average values of the diameter of the aortic annulus and other aortic root parameters. Based on these data, we developed a 3D model of human aortic root with unique geometry. Furthermore, in this study we show that by applying different material properties to the aortic annulus zone in our model, we can significantly improve the quality of the results of finite element analysis. To summarize, here we present four 3D models of human aortic root with unique geometry based on computational analysis of ECHO and CT data. We suggest that our models can be utilized for the development of better prostheses for transcatheter aortic valve implantation."]},
{"title": "Evaluation of robust wave image processing methods for magnetic resonance elastography", "highlights": ["Addressing wave image processing and enhancement in MRE.", "Investigating various solutions for phase unwrapping, directional filtering and noise suppression.", "Carrying out both quantitative and qualitative experiments for performance evaluation and comparison."], "abstract": ["Magnetic resonance elastography (MRE) is a promising modality for in vivo quantification and visualization of soft tissue elasticity. It involves three stages of processes for (1) external excitation, (2) wave imaging and (3) elasticity reconstruction. One of the important issues to be addressed in MRE is wave image processing and enhancement. In this study we approach it from three different ways including phase unwrapping, directional filtering and noise suppression. The relevant solutions were addressed briefly. Some of them were implemented and evaluated on both simulated and experimental MRE datasets. The results confirm that wave image enhancement is indispensable before carrying out MRE elasticity reconstruction."]},
{"title": "Modeling of short-term mechanism of arterial pressure control in the cardiovascular system: Object-oriented and acausal approach", "highlights": ["We reimplemented model of hemodynamics of cardiovascular system.", "We recommend utilizing object-oriented and acausal features of Modelica language.", "The models of components focus on single phenomenon.", "The models of subsystems separate a subsystem behavior from a control mechanism.", "The whole model, its components and subsystems are more understandable and reusable."], "abstract": ["This letter introduces an alternative approach to modeling the cardiovascular system with a short-term control mechanism published in Computers in Biology and Medicine, Vol. 47 (2014), pp. 104\u2013112. We recommend using abstract components on a distinct physical level, separating the model into hydraulic components, subsystems of the cardiovascular system and individual subsystems of the control mechanism and scenario. We recommend utilizing an acausal modeling feature of Modelica language, which allows model variables to be expressed declaratively. Furthermore, the Modelica tool identifies which are the dependent and independent variables upon compilation. An example of our approach is introduced on several elementary components representing the hydraulic resistance to fluid flow and the elastic response of the vessel, among others.", "The introduced model implementation can be more reusable and understandable for the general scientific community."]},
{"title": "Quantifying subtle locomotion phenotypes of ", "highlights": ["We extend the range of applications of the recently published FIM imaging technique.", "We demonstrate how to integrate internal organs into locomotion phenotyping.", "Rolling behavior is automatically quantified based on the trachea (located dorsal).", "Maximal muscle contractions are detected based on gaps between muscle fibers.", "Proof-of-principle: internal structures visible in FIM images can be utilized."], "abstract": ["Quantitative analysis of behavioral traits requires precise image acquisition and sophisticated image analysis to detect subtle locomotion phenotypes. In the past, we have established Frustrated Total Internal Reflection (FTIR) to improve the measurability of small animals like insects. This FTIR-based Imaging Method (FIM) results in an excellent foreground/background contrast and even internal organs and other structures are visible without any complicated imaging or labeling techniques. For example, the trachea and muscle organizations are detectable in FIM images. Here these morphological details are incorporated into phenotyping by performing cluster analysis using histogram-based statistics for the first time. We demonstrate that FIM enables the precise quantification of locomotion features namely rolling behavior or muscle contractions. Both were impossible to quantify automatically before. This approach extends the range of FIM applications by enabling advanced automatic phenotyping for particular locomotion patterns."]},
{"title": "Automated retinal layers segmentation in SD-OCT images using dual-gradient and spatial correlation smoothness constraint", "highlights": ["We develop an automatic method for segmenting retinal layers based on dual-gradient and spatial smoothness constraint.", "Experimental results demonstrate the effectiveness of our method.", "Qualitative and quantitative features extracted from images may be clinically useful for analyzing retinal diseases."], "abstract": ["Automatic segmentation of retinal layers in spectral domain optical coherence tomography (SD-OCT) images plays a vital role in the quantitative assessment of retinal disease, because it provides detailed information which is hard to process manually. A number of algorithms to automatically segment retinal layers have been developed; however, accurate edge detection is challenging. We developed an automatic algorithm for segmenting retinal layers based on dual-gradient and spatial correlation smoothness constraint. The proposed algorithm utilizes a customized edge flow to produce the edge map and a convolution operator to obtain local gradient map in the axial direction. A valid search region is then defined to identify layer boundaries. Finally, a spatial correlation smoothness constraint is applied to remove anomalous points at the layer boundaries. Our approach was tested on two datasets including 10 cubes from 10 healthy eyes and 15 cubes from 6 patients with age-related macular degeneration. A quantitative evaluation of our method was performed on more than 600 images from cubes obtained in five healthy eyes. Experimental results demonstrated that the proposed method can estimate six layer boundaries accurately. Mean absolute boundary positioning differences and mean absolute thickness differences (mean\u00b1SD) were 4.43\u00b13.32", "\u00a0", "\u03bcm and 0.22\u00b10.24", "\u00a0", "\u03bcm, respectively."]},
{"title": "Dynamic thermal imaging analysis in the effectiveness evaluation of warming and cooling formulations", "highlights": ["Image analysis allows qualitative assessment of the rate of skin temperature changes.", "Skin temperature perception is not correlated with the skin temperature.", "Major factor lowering the skin temperature is the weight of the applied formulation."], "abstract": ["Warming cosmetics and medicines are used to accelerate recovery from injuries whereas cooling preparations are used in the pains of muscles, joints, spine, bruises or edema. The paper verifies subjective heating or warming sensations with respect to the measured temperature changes. The influence of three formulations, labelled ", ", ", ", ", ", on skin reaction was tested. The first two formulations (", ", ", ") had a cooling effect while the formulation ", " had warming properties. Two hundred thermal images with a resolution of ", "\u00d7", "=120\u00d7120", "\u00a0", "pixel were acquired with the Flir i7 infrared camera. The paper also shows how to analyse low resolution thermal images and their practical usefulness. For this purpose, a dedicated algorithm for image analysis and processing, which uses morphological operations, segmentation and area analysis, was applied. Application of both ", " and ", " resulted in subjective perception of feeling cold. Approximately 7", "\u00a0", "min following application of the formulation ", ", the skin temperature returned to baseline levels. The minimum skin temperature after using the formulation ", " was 27.5", "\u00a0", "\u00b0C and it was registered at the time of application. Application of ", ", which by definition is a warming formulation, caused a sensation of coolness in the first minutes following the application. The perception of cool and warm sensations after the application of topical formulations is in no way correlated with the skin temperature assessed using a thermal imaging method."]},
{"title": "Biological activity of ", "highlights": ["Biological activity of the essential oils of ", " taxa from Serbia was analyzed.", "The black pine essential oils showed weak DPPH and ABTS scavenging effects.", "One fungal and two bacterial strains showed sensitivity against oils of all three taxa.", "Germacrene D-4-ol interacts with FtsZ binding pocket.", "Synergistic activity between major and minor compounds in the tested oils is proposed."], "abstract": ["In the current work, ", " antioxidant, antibacterial, and antifungal activites of the needle terpenes of three taxa of ", " from Serbia (ssp. ", ", ssp. ", ", and var. ", ") were analyzed. The black pine essential oils showed generally weak antioxidative properties tested by two methods (DPPH and ABTS scavenging assays), where the highest activity was identified in ", " var. ", " (IC", "=25.08", "\u00a0", "mg/mL and VitC=0.67", "\u00a0", "mg (vitamin C)/g when tested with the DPPH and ABTS reagents, respectively). In the antimicrobial assays, one fungal (", ") and two bacterial strains (", " and ", ") showed sensitivity against essential oils of all three ", " taxa. The tested oils have been shown to possess inhibitory action in the range from 20.00 to 0.62", "\u00a0", "mg/mL, where var. ", " exhibited the highest and ssp", " the lowest antimicrobial action. In order to determine potential compounds that are responsible for alternative mode of action, molecular docking simulations inside FtsZ (a prokaryotic homolog of tubulin) were performed. Tested compounds were the most abundant terpenoid (germacrene D-4-ol) and its structurally similar terpene (germacrene D), both present in all three essential oils. It was determined that the oxygenated form of the molecule creates stable bonds with investigated enzyme FtsZ, and that this compound, through this mechanism of action participates in the antimicrobial activity."]},
{"title": "The ", "highlights": ["We propose a L", " penalized accelerated failure time (AFT) model.", "A coordinate descent algorithm with renewed L1/2 threshold is developed.", "The L", " penalized AFT model is able to reduce the size of the predictor in practice.", "The classifier based on the model is suitable for the high dimension biological data."], "abstract": ["The analysis of high-dimensional and low-sample size microarray data for survival analysis of cancer patients is an important problem. It is a huge challenge to select the significantly relevant bio-marks from microarray gene expression datasets, in which the number of genes is far more than the size of samples. In this article, we develop a robust prediction approach for survival time of patient by a ", " regularization estimator with the accelerated failure time (AFT) model. The ", " regularization could be seen as a typical delegate of ", "(0<", "<1) regularization methods and it has shown many attractive features. In order to optimize the problem of the relevant gene selection in high-dimensional biological data, we implemented the ", " regularized AFT model by the coordinate descent algorithm with a renewed half thresholding operator. The results of the simulation experiment showed that we could obtain more accurate and sparse predictor for survival analysis by the ", " regularized AFT model compared with other ", " type regularization methods. The proposed procedures are applied to five real DNA microarray datasets to efficiently predict the survival time of patient based on a set of clinical prognostic factors and gene signatures."]},
{"title": "Recurring patterns of atrial fibrillation in surface ECG predict restoration of sinus rhythm by catheter ablation", "highlights": ["Analysis of surface ECG in 62 consecutive patients with atrial fibrillation (AF).", "Recurrence plot indices predict catheter ablation (CA) outcome at 6-month follow-up.", "Higher recurrence indices associated with decreased risk of AF relapse at follow-up.", "Recurrence indices could be used to select patients more likely to benefit from CA."], "abstract": ["Non-invasive tools to help identify patients likely to benefit from catheter ablation (CA) of atrial fibrillation (AF) would facilitate personalised treatment planning.", "To investigate atrial waveform organisation through recurrence plot indices (RPI) and their ability to predict CA outcome.", "One minute 12-lead ECG was recorded before CA from 62 patients with AF (32 paroxysmal AF; 45 men; age 57\u00b110 years). Organisation of atrial waveforms from i) TQ intervals in ", " and ii) QRST suppressed continuous AF waveforms (CAFW), were quantified using RPI: percentage recurrence (PR), percentage determinism (PD), entropy of recurrence (ER). Ability to predict acute (terminating vs. non-terminating AF), 3-month and 6-month postoperative outcome (AF vs. AF free) were assessed.", "RPI either by TQ or CAFW analysis did not change significantly with acute outcome. Patients arrhythmia-free at 6-month follow-up had higher organisation in TQ intervals by PD (", "<0.05) and ER (", "<0.005) and both were significant predictors of 6-month outcome (PD (AUC=0.67, ", "<0.05) and ER (AUC=0.72, ", "<0.005)). For paroxysmal AF cases, all RPI predicted 3-month (AUC(ER)=0.78, ", "<0.05; AUC(PD)=0.79, ", "<0.05; AUC(PR)=0.80, ", "<0.01) and 6-month (AUC(ER)=0.81, ", "<0.005; AUC(PD)=0.75, ", "<0.05; AUC(PR)=0.71, ", "<0.05) outcome. CAFW-derived RPIs did not predict acute or postoperative outcomes.", "Higher values of any RPI from TQ (values greater than 25th percentile of preoperative distribution) were associated with decreased risk of AF relapse at follow-up (hazard ratio \u22640.52, all ", "<0.05).", "Recurring patterns from preprocedural 1-minute recordings of ECG TQ intervals were significant predictors of CA 6-month outcome."]},
{"title": "Risk factors and prediction of very short term versus short/intermediate term post-stroke mortality: A data mining approach", "highlights": ["A data mining approach was used to analyse a clinical stroke data set.", "Differences in risk factors of short and longer term post-stroke mortality are found.", "Na\u00efve Bayes analysis of wide range of variables across different time ranges is performed.", "Age is not significant in very short term post-stroke mortality.", "Successful predictive classification models of post-stroke mortality are built."], "abstract": ["Data mining and knowledge discovery as an approach to examining medical data can limit some of the inherent bias in the hypothesis assumptions that can be found in traditional clinical data analysis. In this paper we illustrate the benefits of a data mining inspired approach to statistically analysing a bespoke data set, the academic multicentre randomised control trial, UK Glucose Insulin in Stroke Trial (GIST-UK), with a view to discovering new insights distinct from the original hypotheses of the trial. We consider post-stroke mortality prediction as a function of days since stroke onset, showing that the time scales that best characterise changes in mortality risk are most naturally defined by examination of the mortality curve. We show that certain risk factors differentiate between very short term and intermediate term mortality. In particular, we show that age is highly relevant for intermediate term risk but not for very short or short term mortality. We suggest that this is due to the concept of frailty. Other risk factors are highlighted across a range of variable types including socio-demographics, past medical histories and admission medication. Using the most statistically significant risk factors we build predictive classification models for very short term and short/intermediate term mortality."]},
{"title": "Segmentation of anterior cruciate ligament in knee MR images using graph cuts with patient-specific shape constraints and label refinement", "highlights": ["We propose an anterior cruciate ligament segmentation method in knee MRI.", "Patient-specific shape constraints for graph cuts are proposed to avoid leakage.", "Label refinement with superpixels is proposed to recover inhomogeneous region.", "Superpixel refinement significantly improves the accuracy of tibia-attached ACL.", "Experiments show that the proposed method improves the Boykov model by 15% in DSC."], "abstract": ["We propose a graph-cut-based segmentation method for the anterior cruciate ligament (ACL) in knee MRI with a novel shape prior and label refinement. As the initial seeds for graph cuts, candidates for the ACL and the background are extracted from knee MRI roughly by means of adaptive thresholding with Gaussian mixture model fitting. The extracted ACL candidate is segmented iteratively by graph cuts with patient-specific shape constraints. Two shape constraints termed fence and neighbor costs are suggested such that the graph cuts prevent any leakage into adjacent regions with similar intensity. The segmented ACL label is refined by means of superpixel classification. Superpixel classification makes the segmented label propagate into missing inhomogeneous regions inside the ACL. In the experiments, the proposed method segmented the ACL with Dice similarity coefficient of 66.47\u00b17.97%, average surface distance of 2.247\u00b10.869, and root mean squared error of 3.538\u00b11.633, which increased the accuracy by 14.8%, 40.3%, and 37.6% from the Boykov model, respectively."]},
{"title": "Region based stellate features combined with variable selection using AdaBoost learning in mammographic computer-aided detection", "highlights": ["Region-based stellate features for spiculated mass classification are proposed.", "A novel approach for selecting an optimal set of feature variables is proposed.", "Proposed features outperform other mammographic spiculated mass features.", "Investigating contributions of subregions of ROIs to extract discriminant features."], "abstract": ["In this paper, a new method is developed for extracting so-called region-based stellate features to correctly differentiate spiculated malignant masses from normal tissues on mammograms. In the proposed method, a given region of interest (ROI) for feature extraction is divided into three individual subregions, namely core, inner, and outer parts. The proposed region-based stellate features are then extracted to encode the different and complementary stellate pattern information by computing the statistical characteristics for each of the three different subregions. To further maximize classification performance, a novel variable selection algorithm based on AdaBoost learning is incorporated for choosing an optimal subset of variables of region-based stellate features. In particular, we develop a new variable selection metric (criteria) that effectively determines variable importance (ranking) within the conventional AdaBoost framework. Extensive and comparative experiments have been performed on the popular benchmark mammogram database (DB). Results show that our region-based stellate features (extracted from automatically segmented ROIs) considerably outperform other state-of-the-art features developed for mammographic spiculated mass detection or classification. Our results also indicate that combining region-based stellate features with the proposed variable selection strategy has an impressive effect on improving spiculated mass classification and detection."]},
{"title": "Reducing sojourn points from recurrence plots to improve transition detection: Application to fetal heart rate transitions", "highlights": ["Developing clean recurrence plots.", "Reducing the bias of recurrence quantification analysis.", "Improving the detection of dynamic transitions by reducing sojourn points.", "Improving the rate of discrimination of logistic map transitions and improving the discrimination of two fetal heart rate signals."], "abstract": ["The analysis of biomedical signals demonstrating complexity through recurrence plots is challenging. Quantification of recurrences is often biased by sojourn points that hide dynamic transitions. To overcome this problem, time series have previously been embedded at high dimensions. However, no one has quantified the elimination of sojourn points and rate of detection, nor the enhancement of transition detection has been investigated. This paper reports our on-going efforts to improve the detection of dynamic transitions from logistic maps and fetal hearts by reducing sojourn points. Three signal-based recurrence plots were developed, i.e. embedded with specific settings, derivative-based and m-time pattern. Determinism, cross-determinism and percentage of reduced sojourn points were computed to detect transitions. For logistic maps, an increase of 50% and 34.3% in sensitivity of detection over alternatives was achieved by m-time pattern and embedded recurrence plots with specific settings, respectively, and with a 100% specificity. For fetal heart rates, embedded recurrence plots with specific settings provided the best performance, followed by derivative-based recurrence plot, then unembedded recurrence plot using the determinism parameter. The relative errors between healthy and distressed fetuses were 153%, 95% and 91%. More than 50% of sojourn points were eliminated, allowing better detection of heart transitions triggered by gaseous exchange factors. This could be significant in improving the diagnosis of fetal state."]},
{"title": "Modeling multiple experiments using regularized optimization: A case study on bacterial glucose utilization dynamics", "highlights": ["A global model for glucose utilization is proposed based on time-varying parameters.", "Optimization with regularization allows identifying global models across experiments.", "Local and global parameters can be identified using particle swarm optimization.", "Results are validated by predicting new conditions and through model integration."], "abstract": ["The aim of inverse modeling is to capture the systems\u05f3 dynamics through a set of parameterized Ordinary Differential Equations (ODEs). Parameters are often required to fit multiple repeated measurements or different experimental conditions. This typically leads to a multi-objective optimization problem that can be formulated as a non-convex optimization problem. Modeling of glucose utilization of ", " bacteria is considered using in vivo Nuclear Magnetic Resonance (NMR) measurements in perturbation experiments. We propose an ODE model based on a modified time-varying exponential decay that is flexible enough to model several different experimental conditions. The starting point is an over-parameterized non-linear model that will be further simplified through an optimization procedure with regularization penalties. For the parameter estimation, a stochastic global optimization method, particle swarm optimization (PSO) is used. A regularization is introduced to the identification, imposing that parameters should be the same across several experiments in order to identify a general model. On the remaining parameter that varies across the experiments a function is fit in order to be able to predict new experiments for any initial condition. The method is cross-validated by fitting the model to two experiments and validating the third one. Finally, the proposed model is integrated with existing models of glycolysis in order to reconstruct the remaining metabolites. The method was found useful as a general procedure to reduce the number of parameters of unidentifiable and over-parameterized models, thus supporting feature selection methods for parametric models."]},
{"title": "Automatic exudate detection by fusing multiple active contours and regionwise classification", "highlights": ["We introduce an automatic exudate detection method.", "We take advantage of several image enhancement methods.", "We extract the precise contours of exudate candidates by an active contour method.", "We apply region-wise classifier for labeling of candidates.", "Our method outperforms several state-of-the-art approaches."], "abstract": ["In this paper, we propose a method for the automatic detection of exudates in digital fundus images. Our approach can be divided into three stages: candidate extraction, precise contour segmentation and the labeling of candidates as true or false exudates. For candidate detection, we borrow a grayscale morphology-based method to identify possible regions containing these bright lesions. Then, to extract the precise boundary of the candidates, we introduce a complex active contour-based method. Namely, to increase the accuracy of segmentation, we extract additional possible contours by taking advantage of the diverse behavior of different pre-processing methods. After selecting an appropriate combination of the extracted contours, a region-wise classifier is applied to remove the false exudate candidates. For this task, we consider several region-based features, and extract an appropriate feature subset to train a Na\u00efve\u2013Bayes classifier optimized further by an adaptive boosting technique. Regarding experimental studies, the method was tested on publicly available databases both to measure the accuracy of the segmentation of exudate regions and to recognize their presence at image-level. In a proper quantitative evaluation on publicly available datasets the proposed approach outperformed several state-of-the-art exudate detector algorithms."]},
{"title": "Gene expression microarray classification using PCA\u2013BEL", "highlights": ["The computational complexity of BEL model is ", "(", ").", "BEL is a suitable model for high dimensional feature vector classification.", "In this paper, BEL is applied for the classification tasks of gene-expression microarray data.", "Proposed method improves the detection accuracy of SRBCT, HGG and lung cancer.", "Our method has been able to effect a 30.18% improvement on HGG classification."], "abstract": ["In this paper, a novel hybrid method is proposed based on Principal Component Analysis (PCA) and Brain Emotional Learning (BEL) network for the classification tasks of gene-expression microarray data. BEL network is a computational neural model of the emotional brain which simulates its neuropsychological features. The distinctive feature of BEL is its low computational complexity which makes it suitable for high dimensional feature vector classification. Thus BEL can be adopted in pattern recognition in order to overcome the curse of dimensionality problem. In the experimental studies, the proposed model is utilized for the classification problems of the small round blue cell tumors (SRBCTs), high grade gliomas (HGG), lung, colon and breast cancer datasets. According to the results based on 5-fold cross validation, the PCA\u2013BEL provides an average accuracy of 100%, 96%, 98.32%, 87.40% and 88% in these datasets respectively. Therefore, they can be effectively used in gene-expression microarray classification tasks."]},
{"title": "Urodynamic characteristics of rats with detrusor instability", "highlights": ["Detrusor instability (DI) could be induced by three different causes.", "The frequency of DI in experimental group was significantly higher than control.", "Urodynamics reflect the pathophysiological characteristics of DI."], "abstract": ["The purpose of the present study is to investigate urodynamic characteristics of rats with detrusor instability (DI) induced by different causes. Forty-eight adult female Sprague\u2013Dawly rats were randomly divided into 4 groups: cyclophosphamide group, bladder outlet obstruction group, lipopolysaccharide group and control group. The BL-410 model bio-function experimental system was applied to monitor bladder pressure and a number of urodynamic parameters were recorded and calculated, including the frequency of detrusor instability, maximum voiding pressure (MVP), maximum cystometric capacity (MCC), intercontraction interval (ICI), voiding time (VT), postvoid residual (PVR) and bladder compliance (BC). The positive rates of DI in cyclophosphamide group, bladder outlet obstruction group and lipopolysaccharide group were 83.33%, 75.00% and 58.33%, respectively. And correspondingly the frequency of DI was 10.00\u00b12.00, 4.87\u00b11.24 and 3.50\u00b11.00", "\u00a0", "t", "\u00a0", "min", ", which was significantly higher than those of the control group (", "<0.05). Compared with the control group, the decrease of MVP, MCC, ICI, VT and BC was noted in the cyclophosphamide group and lipopolysaccharide group. Increased PVR, MVP, MCC, VT and BC were presented in the bladder outlet obstruction group. Therefore, we suggested that the urodynamic parameters could reflect the pathophysiological characteristics of DI induced by different causes, which could systematically benefit the diagnosis and treatment of overactive bladder."]},
{"title": "A classification study of kinematic gait trajectories in hip osteoarthritis", "highlights": ["Gait analysis can be used to discriminate hip OA patients and controls through the use of an SVM.", "The SVM can rank the discrimination capacities of the kinematic variables.", "The thigh, shank and foot sagittal angles and the foot frontal angles were the most discriminatory.", "Three dimensional gait analysis and WOMAC scores provide complementary information"], "abstract": ["The clinical evaluation of patients in hip osteoarthritis is often done using patient questionnaires. While this provides important information it is also necessary to continue developing objective measures. In this work we further investigate the studies concerning the use of 3D gait analysis to attain this goal. The gait analysis was associated with machine learning methods in order to provide a direct measure of patient control gait discrimination. The applied machine learning method was the support vector machine (SVM). Applying the SVM on all the measured kinematic trajectories, we were able to classify individual patient and control ", " with a mean success rate of 88%. With the use of an ROC curve to establish the threshold number of cycles necessary for a subject to be identified as a patient, this allowed for an accuracy of higher than 90% for discriminating patient and control ", ".", "We then went on to determine the importance of each trajectory. By ranking the capacity of each trajectory for this discrimination, we provided a guide on their order of importance in evaluating patient severity. In order to be clinically relevant, any measure of patient deficit must be compared with clinically validated scores of functional disability. In the case of hip osteoarthritis (OA), the WOMAC scores are currently one of the most widely accepted clinical scores for quantifying OA severity. The kinematic trajectories that provided the best patient\u2013control discrimination with the SVM were found to correlate well but imperfectly with the WOMAC scores, hence indicating the presence of complementary information in the two."]},
{"title": "Spike sorting paradigm for classification of multi-channel recorded fasciculation potentials", "highlights": ["Distribution of \u2018amplitude\u2019 and \u2018area\u2019 can be used to discriminate waveforms recorded by HDsEMG.", "Interactive modification module increases the accuracy of classification and is efficient time-wise.", "The spike sorting output, tested by two-source method, is reproducible.", "In each subject, only 1\u20132 classes were continuingly firing, albeit numerous classes had been derived.", "The final result of this program will further be subject to many other research studies."], "abstract": ["Fasciculation potentials (FPs) are important in supporting the electrodiagnosis of Amyotrophic Lateral Sclerosis (ALS). If classified by shape, FPs can also be very informative for laboratory-based neurophysiological investigations of the motor units.", "This study describes a Matlab program for classification of FPs recorded by multi-channel surface electromyogram (EMG) electrodes. The program applies Principal Component Analysis on a set of features recorded from all channels. Then, it registers unsupervised and supervised classification algorithms to sort the FP samples. Qualitative and quantitative evaluation of the results is provided for the operator to assess the outcome. The algorithm facilitates manual interactive modification of the results. Classification accuracy can be improved progressively until the user is satisfied. The program makes no assumptions regarding the occurrence times of the action potentials, in keeping with the rather sporadic and irregular nature of FP firings.", "Ten sets of experimental data recorded from subjects with ALS using a 20-channel surface electrode array were tested. A total of 11891 FPs were detected and classified into a total of 235 prototype template waveforms. Evaluation and correction of classification outcome of such a dataset with over 6000 FPs can be achieved within 1\u20132 days. Facilitated interactive evaluation and modification could expedite the process of gaining accurate final results.", "The developed Matlab program is an efficient toolbox for classification of FPs."]},
{"title": "Refinement of lung nodule candidates based on local geometric shape analysis and Laplacian of Gaussian kernels", "highlights": ["A new 3D, CAD for lung nodule candidates was introduced and compared with other works.", "Multiscale dot enhancement filter and Laplacian of Gaussian kernels were combined.", "Relatively a large number of FPs was reduced before using feature based classification.", "The performance of the proposed method was evaluated with 42 independent images from LIDC.", "The scheme detects nodules with wide variations in size, shape, intensity and location."], "abstract": ["This work is focused on application of a new technique in the first steps of computer-aided detection (CAD) of lung nodules. The scheme includes segmenting the lung volume and detecting most of the nodules with a low number of false positive (FP) objects. The juxtapleural nodules were properly included and the airways excluded in the lung segmentation. Among the suspicious regions obtained from the multiscale dot enhancement filter, those containing the center of nodule candidates, were determined. These center points were achieved from a 3D blob detector based on Laplacian of Gaussian kernels. Then the volumetric shape index (SI) that encodes the 3D local shape information was calculated for voxels in the determined regions. The performance of the scheme was evaluated by using 42 CT images from the Lung Image Database Consortium (LIDC). The results show that the average number of FPs reaches to 38.8 per scan with the sensitivity of 95.9% in the initial detections. The scheme is adaptable to detect nodules with wide variations in size, shape, intensity and location. Comparison of results with previously reported ones indicates that the proposed scheme can be satisfactory applied for initial detection of lung nodules in the chest CT images."]},
{"title": " Genome Database (HIGDB): A single point web resource for ", "highlights": ["The HIGDB act as a single access point source for ", " genomes.", "It provides a dynamic interface to execute varied searches for genes and proteins.", "The database facilitates the comparative study of ", " genomes.", "It allows dynamic viewing of the genomic content of ", " strains.", "It aids in the identification of DNA motifs within ", " genomes."], "abstract": [" (", ") is the causative agent of pneumonia, bacteraemia and meningitis. The organism is responsible for large number of deaths in both developed and developing countries. Even-though the first bacterial genome to be sequenced was that of ", ", there is no exclusive database dedicated for ", " This prompted us to develop the ", " Genome Database (HIGDB).", "All data of HIGDB are stored and managed in MySQL database. The HIGDB is hosted on Solaris server and developed using PERL modules. Ajax and JavaScript are used for the interface development.", "The HIGDB contains detailed information on 42,741 proteins, 18,077 genes including 10 whole genome sequences and also 284 three dimensional structures of proteins of ", " In addition, the database provides \u201cMotif search\u201d and \u201cGBrowse\u201d. The HIGDB is freely accessible through the URL: ", ".", "The HIGDB will be a single point access for bacteriological, clinical, genomic and proteomic information of ", " The database can also be used to identify DNA motifs within ", " genomes and to compare gene or protein sequences of a particular strain with other strains of ", "."]},
{"title": "Three-dimensional semiautomatic liver segmentation method for non-contrast computed tomography based on a correlation map of locoregional histogram and probabilistic atlas", "highlights": ["Liver segmentation for non-contrast CT would aid in diagnosis of disease/lesions.", "We evaluate the method against a hand-drawn standard in ten cases.", "We confirmed that our method was precise.", "Our method potentially offers radiologists a time-efficient segmentation aid."], "abstract": ["We sought to evaluate a new regional segmentation method for use with three-dimensional (3D) non-contrast abdominal CT images and to report the preliminary results.", "The proposed method was evaluated in ten cases. Manually segmented areas were used as the gold standard for evaluation. To compare the standard and the extracted liver regions, the degree of coincidence ", "% was redefined by transforming a volumetric overlap error. We also evaluated the influence of varying the density window size in terms of setting the starting points.", "We confirmed in ten cases that our method could segment the liver region more precisely than the conventional method. A size of window 15 voxels was optimal as the starting point in all cases.", "We demonstrated the accuracy of a 3D semiautomatic liver segmentation method for non-contrast CT. This method promises to offer radiologists a time-efficient segmentation aid."]},
{"title": "Benefits of a new Metropolis\u2013Hasting based algorithm, in non-linear regression for estimation of ", "highlights": ["We model the antimalarial sensitivity in a blood sample with two strains of parasite.", "We develop a Metropolis\u2013Hasting algorithm to estimate parameters of the model.", "We compare our estimation results with three other algorithms.", "We evaluate our algorithm on a simulation study, on ", " and ", " data.", "For poor concentration design, our algorithm gives accurate results."], "abstract": ["Malaria is one of the world\u05f3s most widespread parasitic diseases. The parasitic protozoans of the genus ", " have developed resistance to several antimalarial drugs. Some patients are therefore infected by two or more strains with different levels of antimalarial drug sensitivity. We previously developed a model to estimate the drug concentration ", " that inhibits 50% of the growth of the parasite isolated from a patient infected with one strain. We propose here a new Two-Slopes model for patients infected by two strains. This model involves four parameters: the proportion of each strain and their ", ", and the sigmoidicity parameter. To estimate the parameters of this model, we have developed a new algorithm called PGBO (Population Genetics-Based Optimizer). It is based on the Metropolis\u2013Hasting algorithm and is implemented in the statistical software R. We performed a simulation study and defined three evaluation criteria to evaluate its properties and compare it with three other algorithms (Gauss\u2013Newton, Levenberg\u2013Marquardt, and a simulated annealing). We also evaluated it using ", " data and three ", " datasets from the French Malaria Reference Center.", "Our evaluation criteria in the simulation show that PGBO gives good estimates of the parameters even if the concentration design is poor. Moreover, our algorithm is less sensitive than Gauss\u2013Newton algorithms to initial values. Although parameter estimation is good, interpretation of the results can be difficult if the proportion of the second strain is close to 0 or 1. For these reasons, this approach cannot yet be implemented routinely."]},
{"title": "Semi-automatic motion compensation of contrast-enhanced ultrasound images from abdominal organs for perfusion analysis", "highlights": ["This paper proposes a semi-automatic system to motion compensated CEUS perfusion data.", "Evaluation showed equal or improved performance compared to manual processing.", "It saves 41% of the time required for manual processing of the datasets."], "abstract": ["This paper presents a system for correcting motion influences in time-dependent 2D contrast-enhanced ultrasound (CEUS) images to assess tissue perfusion characteristics. The system consists of a semi-automatic frame selection method to find images with out-of-plane motion as well as a method for automatic motion compensation. Translational and non-rigid motion compensation is applied by introducing a temporal continuity assumption. A study consisting of 40 clinical datasets was conducted to compare the perfusion with simulated perfusion using pharmacokinetic modeling. Overall, the proposed approach decreased the mean average difference between the measured perfusion and the pharmacokinetic model estimation. It was non-inferior for three out of four patient cohorts to a manual approach and reduced the analysis time by 41% compared to manual processing."]},
{"title": "Computer-aided diagnosis system based on fuzzy logic for breast cancer categorization", "highlights": ["We develop a new method for categorizing findings in mammograms according to BIRADS.", "We use fuzzy logic to model a computer-aided diagnosis tool.", "We construct a fuzzy inference system.", "The system shows a degree of pertinence of a finding to each BIRADS category.", "The system achieved an accuracy of 76.7% for nodules and 83.3% for calcifications."], "abstract": ["Fuzzy logic can help reduce the difficulties faced by computational systems to represent and simulate the reasoning and the style adopted by radiologists in the process of medical image analysis. The study described in this paper consists of a new method that applies fuzzy logic concepts to improve the representation of features related to image description in order to make it semantically more consistent. Specifically, we have developed a computer-aided diagnosis tool for automatic BI-RADS categorization of breast lesions. The user provides parameters such as contour, shape and density and the system gives a suggestion about the BI-RADS classification.", "Initially, values of malignancy were defined for each image descriptor, according to the BI-RADS standard. When analyzing contour, for example, our method considers the matching of features and linguistic variables. Next, we created the fuzzy inference system. The generation of membership functions was carried out by the Fuzzy Omega algorithm, which is based on the statistical analysis of the dataset. This algorithm maps the distribution of different classes in a set.", "Images were analyzed by a group of physicians and the resulting evaluations were submitted to the Fuzzy Omega algorithm. The results were compared, achieving an accuracy of 76.67% for nodules and 83.34% for calcifications.", "The fit of definitions and linguistic rules to numerical models provided by our method can lead to a tighter connection between the specialist and the computer system, yielding more effective and reliable results."]},
{"title": "Dynamic finite element analysis and moving particle simulation of human enamel on a microscale", "highlights": ["We compared dynamic two-dimensional FEA and moving particle simulation (MPS).", "We assumed a plane strain condition in modeling human enamel on a reduced scale.", "We developed two-dimensional models with the same geometry was developed for both MPS and FEA.", "We tested the models tested in tension generated with a single step of displacement.", "The MPS and FEA were significantly correlated for all data sets."], "abstract": ["The study of biomechanics of deformation and fracture of hard biological tissues involving organic matrix remains a challenge as variations in mechanical properties and fracture mode may have time-dependency. Finite element analysis (FEA) has been widely used but the shortcomings of FEA such as the long computation time owing to re-meshing in simulating fracture mechanics have warranted the development of alternative computational methods with higher throughput. The aim of this study was to compare dynamic two-dimensional FEA and moving particle simulation (MPS) when assuming a plane strain condition in the modeling of human enamel on a reduced scale.", "Two-dimensional models with the same geometry were developed for MPS and FEA and tested in tension generated with a single step of displacement. The displacement, velocity, pressure, and stress levels were compared and Spearman\u05f3s rank-correlation coefficients ", " were calculated (", "<0.001).", "The MPS and FEA were significantly correlated for displacement, velocity, pressure, and ", "-stress.", "The MPS may be further developed as an alternative approach without mesh generation to simulate deformation and fracture phenomena of dental and potentially other hard tissues with complex microstructure."]},
{"title": "Establishing the macular grading grid by means of fovea centre detection using anatomical-based and visual-based features", "highlights": ["The paper has been reviewed again by an experienced native English speaker.", "Comments of reviewer 3 have been addressed.", "Comments of the editor have been addressed."], "abstract": ["This paper presents a methodology for establishing the macular grading grid in digital retinal images by means of fovea centre detection. To this effect, visual and anatomical feature-based criteria are combined with the aim of exploiting the benefits of both techniques. First, acceptable fovea centre estimation is obtained by using a priori known anatomical features with respect to the optic disc and the vascular tree. Second, a type of morphological processing is employed in an attempt to improve the obtained fovea centre estimation when the fovea is detectable in the image; otherwise, it is declared indistinguishable and the first result is retained. The methodology was tested on the MESSIDOR and DIARETDB1 databases making use of a distance criterion between the obtained and the real fovea centre. Fovea centres in the brackets between the categories Excellent and Fair (fovea centres primarily accepted as valid in the literature) made up for 98.24% and 94.38% of the cases in the MESSIDOR and DIARETDB1, respectively."]},
{"title": "POTAMOS mass spectrometry calculator: Computer aided mass spectrometry to the post-translational modifications of proteins. A focus on histones", "highlights": ["POTAMOS mass spectrometry calculator web tool aids to protein mass spectrometry.", "It calculates post-translational modifications qualitatively and quantitatively.", "It yields the full set of the unique primary structures.", "It calculates the masses and charges of a fragmented modified histone variant.", "It calculates the masses of cleaved peptide fragments."], "abstract": ["Mass spectrometry is a widely used technique for protein identification and it has also become the method of choice in order to detect and characterize the post-translational modifications (PTMs) of proteins. Many software tools have been developed to deal with this complication. In this paper we introduce a new, free and user friendly online software tool, named POTAMOS Mass Spectrometry Calculator, which was developed in the open source application framework Ruby on Rails. It can provide calculated mass spectrometry data in a time saving manner, independently of instrumentation. In this web application we have focused on a well known protein family of histones whose PTMs are believed to play a crucial role in gene regulation, as suggested by the so called \u201chistone code\u201d hypothesis. The PTMs implemented in this software are: methylations of arginines and lysines, acetylations of lysines and phosphorylations of serines and threonines. The application is able to calculate the kind, the number and the combinations of the possible PTMs corresponding to a given peptide sequence and a given mass along with the full set of the unique primary structures produced by the possible distributions along the amino acid sequence. It can also calculate the masses and charges of a fragmented histone variant, which carries predefined modifications already implemented. Additional functionality is provided by the calculation of the masses of fragments produced upon protein cleavage by the proteolytic enzymes that are most widely used in proteomics studies."]},
{"title": "Optic disc segmentation using the sliding band filter", "highlights": ["A fully automatic method for the optic disc segmentation in retinal images is proposed.", "This method is robust to variations of contrast and illumination, and insensitive to the presence of pathological conditions.", "The results are compared with manually extracted boundaries from three public databases.", "These results demonstrate that the proposed solution outperforms recent approaches."], "abstract": [": The optic disc (OD) centre and boundary are important landmarks in retinal images and are essential for automating the calculation of health biomarkers related with some prevalent systemic disorders, such as diabetes, hypertension, cerebrovascular and cardiovascular diseases.", ": This paper presents an automatic approach for OD segmentation using a multiresolution sliding band filter (SBF). After the preprocessing phase, a low-resolution SBF is applied on a downsampled retinal image and the locations of maximal filter response are used for focusing the analysis on a reduced region of interest (ROI). A high-resolution SBF is applied to obtain a set of pixels associated with the maximum response of the SBF, giving a coarse estimation of the OD boundary, which is regularized using a smoothing algorithm.", ": Our results are compared with manually extracted boundaries from public databases (ONHSD, MESSIDOR and INSPIRE-AVR datasets) outperforming recent approaches for OD segmentation. For the ONHSD, 44% of the results are classified as Excellent, while the remaining images are distributed between the Good (47%) and Fair (9%) categories. An average overlapping area of 83%, 89% and 85% is achieved for the images in ONHSD, MESSIDOR and INSPIR-AVR datasets, respectively, when comparing with the manually delineated OD regions.", ": The evaluation results on the images of three datasets demonstrate the better performance of the proposed method compared to recently published OD segmentation approaches and prove the independence of this method when from changes in image characteristics such as size, quality and camera field of view."]},
{"title": "Finding splitting lines for touching cell nuclei with a shortest path algorithm", "highlights": ["We utilize the ellipse fitting algorithm to obtain the markers of cell nuclei.", "The splitting line of touching cell nuclei is extracted by EDT.", "The concave points are adopted to adjust the endpoints of initial splitting line.", "We calculate the shortest path between the adjusted endpoints to find the accurate splitting line.", "Our algorithm achieves good experimental results on different types of touching cell nuclei."], "abstract": ["A shortest path-based algorithm is proposed in this paper to find splitting lines for touching cell nuclei. First, an initial splitting line is obtained through the distance transform of a marker image and the watershed algorithm. The initial splitting line is then separated into different line segments as necessary, and the endpoint positions of these line segments are adjusted to the concave points on the contour. Finally, a shortest path algorithm is used to find the accurate splitting line between the starting-point and the end-point, and the final split can be achieved by the contour of the touching cell nuclei and the splitting lines. Comparisons of experimental results show that the proposed algorithm is effective for segmentation of different types of touching cell nuclei."]},
{"title": "Degrees of separation as a statistical tool for evaluating candidate genes", "highlights": ["A novel method to validate candidate genes using network information is proposed.", "The package, CandidateBacon, enables GWA and QTL studies to utilize this method.", "Large variation between gene-networks are revealed when tested empirically.", "Proper validation will allow better interpretation of genome studies."], "abstract": ["Selection of candidate genes is an important step in the exploration of complex genetic architecture. The number of gene networks available is increasing and these can provide information to help with candidate gene selection. It is currently common to use the degree of connectedness in gene networks as validation in Genome Wide Association (GWA) and Quantitative Trait Locus (QTL) mapping studies. However, it can cause misleading results if not validated properly. Here we present a method and tool for validating the gene pairs from GWA studies given the context of the network they co-occur in. It ensures that proposed interactions and gene associations are not statistical artefacts inherent to the specific gene network architecture. The ", " package provides an easy and efficient method to calculate the average degree of separation (", ") between pairs of genes to currently available gene networks. We show how these empirical estimates of average connectedness are used to validate candidate gene pairs. Validation of interacting genes by comparing their connectedness with the average connectedness in the gene network will provide support for said interactions by utilising the growing amount of gene network information available."]},
{"title": "Odorant recognition using biological responses recorded in olfactory bulb of rats", "highlights": ["We present an analysis of pattern recognition and dimensionality reduction techniques applied to odor discrimination.", "The local field potential signals recorded in the olfactory bulb are stimulus specific in normal rats.", "Artificial olfaction systems discriminate odor information from local field potential recorded in rat olfactory bulb.", "PCA followed by SVM are able to discriminate odorant stimuli with accuracy of over 95%."], "abstract": ["In this study we applied pattern recognition (PR) techniques to extract odorant information from local field potential (LFP) signals recorded in the olfactory bulb (OB) of rats subjected to different odorant stimuli. We claim that LFP signals registered on the OB, the first stage of olfactory processing, are stimulus specific in animals with normal sensory experience, and that these patterns correspond to the neural substrate likely required for perceptual discrimination. Thus, these signals can be used as input to an artificial odorant classification system with great success. In this paper we have designed and compared the performance of several configurations of artificial olfaction systems (AOS) based on the combination of four feature extraction (FE) methods (Principal Component Analysis (PCA), Fisher Transformation (FT), Sammon NonLinear Map (NLM) and Wavelet Transform (WT)), and three PR techniques (Linear Discriminant Analysis (LDA), Multilayer Perceptron (MLP) and Support Vector Machine (SVM)), when four different stimuli are presented to rats. The best results were reached when PCA extraction followed by SVM as classifier were used, obtaining a classification accuracy of over 95% for all four stimuli."]},
{"title": "POPPER, a simple programming language for probabilistic semantic inference in medicine", "highlights": ["The POPPER language was developed to explore probabilistic semantics.", "The application was developed to assist in medical reasoning.", "The core theory is the same as the Hyperbolic Dirac Net HDN.", "Like an HDN, it enables bidirectional probabilistic logic and cyclic paths.", "Unlike the basic HDN, it allows verbal relationships.", "Unlike the basic HDN, it evolves by, e.g., syllogistic reasoning."], "abstract": ["Our previous reports described the use of the Hyperbolic Dirac Net (HDN) as a method for probabilistic inference from medical data, and a proposed probabilistic medical Semantic Web (SW) language Q-UEL to provide that data. Rather like a traditional Bayes Net, that HDN provided estimates of joint and conditional probabilities, and was static, with no need for evolution due to \u201creasoning\u201d. Use of the SW will require, however, (a) at least the semantic triple with more elaborate relations than conditional ones, as seen in use of most verbs and prepositions, and (b) rules for logical, grammatical, and definitional manipulation that can generate changes in the inference net. Here is described the simple POPPER language for medical inference. It can be automatically written by Q-UEL, or by hand. Based on studies with our medical students, it is believed that a tool like this may help in medical education and that a physician unfamiliar with SW science can understand it. It is here used to explore the considerable challenges of assigning probabilities, and not least what the meaning and utility of inference net evolution would be for a physician."]},
{"title": "A ternary model of decompression sickness in rats", "highlights": ["This study developed a ternary model of predicting probability of decompression sickness in rats.", "A dataset was compiled from 15 studies using 22 dive profiles and two strains of both sexes.", "Using ordinal logistic regression, model-fit was optimised by maximum log likelihood.", "The model predicted cases of DCS with 75% accuracy and 92.5% were within 95% confidence intervals.", "This model is reliable for predicting DCS within the range of parameters used to optimise the model."], "abstract": ["Decompression sickness (DCS) in rats is commonly modelled as a binary outcome. The present study aimed to develop a ternary model of predicting probability of DCS in rats, (as no-DCS, survivable-DCS or death), based upon the compression/decompression profile and physiological characteristics of each rat.", "A literature search identified dive profiles with outcomes no-DCS, survivable-DCS or death by DCS. Inclusion criteria were that at least one rat was represented in each DCS status, not treated with drugs or simulated ascent to altitude, that strain, sex, breathing gases and compression/decompression profile were described and that weight was reported. A dataset was compiled (", "=1602 rats) from 15 studies using 22 dive profiles and two strains of both sexes. Inert gas pressures in five compartments were estimated. Using ordinal logistic regression, model-fit of the calibration dataset was optimised by maximum log likelihood. Two validation datasets assessed model robustness.", "In the interpolation dataset the model predicted 10/15 cases of nDCS, 3/3 sDCS and 2/2 dDCS, totalling 15/20 (75% accuracy) and 18.5/20 (92.5%) were within 95% confidence intervals. Mean weight in the extrapolation dataset was more than 2", "\u00a0", "SD outside of the calibration dataset and the probability of each outcome was not predictable.", "This model is reliable for the prediction of DCS status providing the dive profile and rat characteristics are within the range of parameters used to optimise the model. The addition of data with a wider range of parameters should improve the applicability of the model."]},
{"title": "Interactive tooth partition of dental mesh base on tooth-target harmonic field", "highlights": ["A harmonic field (HF) is introduced to partition individual teeth from dental mesh.", "A modified weighting scheme of HF is used for mesh partition purpose.", "Constraints of HF are assigned based on prior knowledge of human teeth.", "A boundary refining strategy is introduced to handle complicated dental mesh.", "Smart user interfaces are proposed to improve flexibility and reduce interaction."], "abstract": ["The accurate tooth partition of dental mesh is a crucial step in computer-aided orthodontics. However, tooth boundary identification is not a trivial task for tooth partition, since different shapes and their arrangements vary substantially among common clinical cases. Though curvature field is traditionally used for identifying boundaries, it is normally not reliable enough. Other methods may improve the accuracy, but require intensive user interaction. Motivated by state-of-the-art general interactive mesh segmentation methods, this paper proposes a novel tooth-target partition framework that employs harmonic fields to partition teeth accurately and effectively. In addition, a refining strategy is introduced to successfully segment teeth from the complicated dental model with indistinctive tooth boundaries on its lingual side surface, addressing an issue that had not been solved properly before. To utilise high-level information provided by the user, smart and intuitive user interfaces are also proposed with minimum interaction. In fact, most published interactive methods specifically designed for tooth partition are lacking efficient user interfaces. Extensive experiments and quantitative analyses show that our tooth partition method outperforms the state-of-the-art approaches in terms of accuracy, robustness and efficiency."]},
{"title": "Neural network study for standardizing pulse-taking depth by the width of artery", "highlights": ["Pulse-taking depths could be expressed based on the width of the artery (WA).", "Two models for estimating WA were compared; the ANN model was better performance.", "The ANN model had two inputs (SD and CP) and one output, ", ". (i.e., estimated WA).", "Pulse signals at different depths could be acquired by ", " value for PDI application.", "The method made the description of pulse-taking depth modernized and scientized."], "abstract": ["To carry out a pulse diagnosis, a traditional Chinese medicine (TCM) physician presses the patient\u2019s wrist artery at three incremental depths, namely Fu (superficial), Zhong (medium), and Chen (deep). However, the definitions of the three depths are insufficiently clear for use with modern pulse diagnosis instruments (PDIs). In this paper, a quantitative method is proposed to express the pulse-taking depths based on the width of the artery (WA). Furthermore, an index, ", ", is developed for estimating WA for PDI application. The ", " value is obtained using an artificial neural network (ANN) model with contact pressure (CP) and sensor displacement (SD) as the inputs. The WA and SD data from an ultrasound instrument and CP and SD data from a PDI were analyzed. The results show that the mean prediction error and the standard deviation (STD) of the ANN model was 1.19% and 0.0467, respectively. Comparing the ANN model with the SD model by statistical method, it showed significant difference and the improvement in the mean prediction error and the STD was 71.62% and 29.78%, respectively. The ", " value can thus map WA with less individual variation than that of the values estimated directly using the SD model. Pulse signals at different depths thus can be acquired according to ", " value while using a PDI, providing TCM physicians with more reliable pulse information."]},
{"title": "Computational studies on strain transmission from a collagen gel construct to a cell and its internal cytoskeletal filaments", "highlights": ["A tissue model having a cell with cytoskeletal filaments (CSKs) is developed.", "Mechanical interactions between tissue, cell and CSKs are examined.", "The type of a tissue deformation is reflected in the distribution of CSK strain.", "The distribution of CSK strain varies with the Young\u05f3s modulus of the tissue."], "abstract": ["We developed a mechanical tissue model containing a cell with cytoskeletal filaments inside to investigate how tissue deformation is reflected in the deformation of a cell and its internal cytoskeletal filaments. Tissue that assumes a collagen gel construct was depicted as an isotropic linear elastic material, and the cell was modeled as an assembly of discrete elements including a cell membrane, nuclear envelope, and cytoskeletal filaments. Mechanical behaviors were calculated based on the minimum energy principle. The results demonstrated the effects of the type of tissue deformation on deformations of cytoskeletal filaments. The distribution of strains of cytoskeletal filaments was skewed toward compression when a tissue was stretched, toward stretch when the tissue was compressed, and almost normal when the tissue was sheared. The results also addressed the dependency of deformations of a cell and cytoskeletal filaments on the ratio of the Young\u2019s modulus of a tissue to that of a cell. Upon tissue stretching, cell strain increased and the distribution of strains of cytoskeletal filaments broadened on both stretch and compression sides with an increase in the Young\u2019s modulus ratio. This suggested that the manner of tissue deformation and the tissue/cell Young\u2019s modulus ratio are reflected in the distribution pattern of strains of cytoskeletal filaments. The present model is valuable to understanding the mechanisms of cellular responses in a tissue."]},
{"title": "A computational framework for cancer response assessment based on oncological PET-CT scans", "highlights": ["Computational cancer evolution assessment from a pair of oncological PET-CT scans.", "Automatic PET tumor segmentation and decision making system proposal.", "Supervised learning framework with a novel multi modal feature set.", "Introduction to computer aided diagnosis tools in a nuclear medicine scenario."], "abstract": ["In this work we present a comprehensive computational framework to help in the clinical assessment of cancer response from a pair of time consecutive oncological PET-CT scans. In this scenario, the design and implementation of a supervised machine learning system to predict and quantify cancer progression or response conditions by introducing a novel feature set that models the underlying clinical context is described. Performance results in 100 clinical cases (corresponding to 200 whole body PET-CT scans) in comparing expert-based visual analysis and classifier decision making show up to 70% accuracy within a completely automatic pipeline and 90% accuracy when providing the system with expert-guided PET tumor segmentation masks."]},
{"title": "Fast generation of ", "highlights": ["Fast generation of ", "2", " maps achieved by the weighted linear fitting of the logarithm of the signal (WLSL) method.", "Automatic truncation of signal decay curves allowed the extension of the WLSL method to the whole clinical range of ", "2", " values.", "Validation performed on synthetic images and on 60 thalassemia major patients with different levels of myocardial iron overload."], "abstract": ["2", " maps obtained by the processing of multiecho MR sequences can be useful in several clinical applications. ", "2", " map generation procedures should join a processing time compatible with on-line image analysis with a good precision in the entire ", "2", " range of clinical interest. Fast generation of ", "2", " maps can be achieved by the estimation of the ", "2", " values by the weighted linear fitting of the logarithm of the signal (WLSL) method. This approach fails if the signal decay diverges from a pure exponential decay, as happens at low ", "2", " values where the rapid decay in the signal intensity leads to a plateau in the later echo times (", "). The proposed method implements the automatic truncation of the signal decay curves to be fitted in order to compensate for the signal collapse at low ", "2", " values, allowing the extension of the WLSL method through the entire clinical range of ", "2", " values.", "Validation was performed on synthetic images and on 60 thalassemia major patients with different levels of myocardial iron overload. Phantom experiments showed that a 5% fitting error threshold represented the best compromise between ", "2", " value measurement precision and processing time. A good agreement was found between ", "2", " map pixel-wise measurements and ROI-based measurements performed by expert readers (CoV=1.84% in global heart ", "2", ", CoV=5.8% in segmental analysis). In conclusion, the developed procedure was effective in generating correct ", "2", " maps for the entire ", "2", " clinical range."]},
{"title": "Real time identification of active regions in muscles from high density surface electromyogram", "highlights": ["A new algorithm is proposed for the localization of the sources of muscle activity from high density surface EMG.", "The algorithm provides approximate estimates, but they are real time.", "The method is tested on simulated signals.", "Suggested potential applications: cross-talk removal, biofeedback, load sharing, MUNE, prosthesis, rehabilitation."], "abstract": ["Developing a real time method for the localization of muscle activity regions from high density surface electromyogram (EMG).", "The inverse problem of source localization is solved by a regularized technique applied to an over-determined problem searching for the least mean squares approximation of the recorded signal with a linear combination of a set of basis waveforms (subject specific).", "The method, tested on simulations, provides accurate estimates of the mean location of the sources (in ideal conditions, it has about 1", "\u00a0", "mm of mean error in locating the depth, negligible error in locating the transverse location of the active region). For reasonably small perturbations, it is stable to possible detection problems (e.g., misalignment between the electrodes and the fibres, noise), inaccurate knowledge of the anatomical and physical properties of the investigated tissues (e.g., tissue thickness, location of IZ, fibre length, tissue conductivity) with mean estimation errors of about 1.5\u20132.8", "\u00a0", "mm.", "An innovative algorithm is proposed for the non-invasive localization of the active regions of a muscle. It is real time and opens potential future applications for prosthesis control and biofeedback."]},
{"title": "Suggestions for a web based universal exchange and inference language for medicine. Continuity of patient care with PCAST disaggregation", "highlights": ["We show that the Q-UEL language can be extended to continuity of patient care.", "We believe that this follows the requirements of the 2010 PCAST report.", "Q-UEL tags have been constructed from patient records in CDA and VistA.", "Data elements in these tags can be disaggregated as PCAST requested.", "The patient data can be reaggregated rapidly by authorized persons."], "abstract": ["We describe here the applications of our recently proposed Q-UEL language to continuity of patient care between physicians, specialists and institutions as mediated via the Internet, giving examples derived from HL7 CDA and VistA of particular interest to workflow. Particular attention is given to the Universal Exchange Language for healthcare as requested by the US President\u05f3s Council of Advisors on Science and Technology (PCAST) released in December 2010, especially in regard to disaggregation of the patient record on the Internet. To illustrate many features and options, one of our most elaborate configurations combining them, for disaggregation and reaggregation, is described. The Q-UEL tags used do not physically join, but query each other from a random mix via the application. Despite the computationally demanding complexity of the configuration with two joining tags for each data tag and four independently evolving keys, plus a valuable but rate limiting isomorphism test, packets of essential clinical data for patient could be recovered and displayed every 2", "\u00a0", "s for a \u201cclub\u201d of 30,000\u201350,000 patients in the mix. All computation here is on a standard laptop, but for practical use of the Internet to display downloaded data, the above is adequate, so focus is primarily on increasing club size. In practice, it is not necessary that a club comprise an entire nation. Assuming that one does not use purely random assignments of patients to arbitrary clubs, there could for example be a club comprising all schoolchildren in Scotland, or a club comprising all military veterans in Illinois. In such cases, one is typically dealing with clubs each of the order of a mere million patients. Using such club sizes efficiently, and in principle even a club the size of a whole country, appears to be possible."]},
{"title": "Scalp EEG brain functional connectivity networks in pediatric epilepsy", "highlights": ["Introducing a new data driven graph theory-based methodology for constructing brain functional connectivity networks.", "Proposing a decision support system for pediatric epilepsy diagnosis.", "Developing a framework to assess the functional connectivity networks alterations using scalp EEG time series.", "Evaluation of graph theory measures of brain functional connectivity in pediatric epilepsy diagnosis."], "abstract": ["This study establishes a new data-driven approach to brain functional connectivity networks using scalp EEG recordings for classifying pediatric subjects with epilepsy from pediatric controls. Graph theory is explored on the functional connectivity networks of individuals where three different sets of topological features were defined and extracted for a thorough assessment of the two groups. The rater\u2019s opinion on the diagnosis could also be taken into consideration when deploying the general linear model (GLM) for feature selection in order to optimize classification. Results demonstrate the existence of statistically significant (", "<0.05) changes in the functional connectivity of patients with epilepsy compared to those of control subjects. Furthermore, clustering results demonstrate the ability to discriminate pediatric epilepsy patients from control subjects with an initial accuracy of 87.5%, prior to initiating the feature selection process and without taking into consideration the clinical rater\u2019s opinion. Otherwise, leave-one-out cross validation (LOOCV) showed a significant increase in the classification accuracy to 96.87% in epilepsy diagnosis."]},
{"title": "An efficient word typing P300-BCI system using a modified T9 interface and random forest classifier", "highlights": ["A Text on nine keys (T9) based efficient word typing P300-BCI system is proposed.", "We integrated a smart dictionary into the P300-BCI system for word suggestions.", "We propose a novel Random Forest (RF) classifier to improve accuracy.", "Words typing speed is improved by 51.87% in comparison to the conventional character speller.", "Accuracy of RF is significantly higher than the conventional SVM classifier."], "abstract": ["A typical P300-based spelling brain computer interface (BCI) system types a single character with a character presentation paradigm and a P300 classification system. Lately, a few attempts have been made to type a whole word with the help of a smart dictionary that suggests some candidate words with the input of a few initial characters.", "In this paper, we propose a novel paradigm utilizing initial character typing with word suggestions and a novel P300 classifier to increase word typing speed and accuracy. The novel paradigm involves modifying the Text on 9 keys (T9) interface, which is similar to the keypad of a mobile phone used for text messaging. Users can type the initial characters using a 3\u00d73 matrix interface and an integrated custom-built dictionary that suggests candidate words as the user types the initials. Then the user can select one of the given suggestions to complete word typing. We have adopted a random forest classifier, which significantly improves P300 classification accuracy by combining multiple decision trees.", "We conducted experiments with 10 subjects using the proposed BCI system. Our proposed paradigms significantly reduced word typing time and made word typing more convenient by outputting complete words with only a few initial character inputs. The conventional spelling system required an average time of 3.47", "\u00a0", "min per word while typing 10 random words, whereas our proposed system took an average time of 1.67", "\u00a0", "min per word, a 51.87% improvement, for the same words under the same conditions."]},
{"title": "Reconstruction of sparse-view X-ray computed tomography using adaptive iterative algorithms", "highlights": ["Two practical algorithms for limited-projection-view CT reconstruction are proposed.", "Reconstruction parameters of both algorithms are determined automatically.", "Computationally intense ART are saved if the data fidelity constraint is satisfied."], "abstract": ["In this paper, we propose two reconstruction algorithms for sparse-view X-ray computed tomography (CT). Treating the reconstruction problems as data fidelity constrained total variation (TV) minimization, both algorithms adapt the alternate two-stage strategy: projection onto convex sets (POCS) for data fidelity and non-negativity constraints and steepest descent for TV minimization. The novelty of this work is to determine iterative parameters automatically from data, thus avoiding tedious manual parameter tuning. In TV minimization, the step sizes of steepest descent are adaptively adjusted according to the difference from POCS update in either the projection domain or the image domain, while the step size of algebraic reconstruction technique (ART) in POCS is determined based on the data noise level. In addition, projection errors are used to compare with the error bound to decide whether to perform ART so as to reduce computational costs. The performance of the proposed methods is studied and evaluated using both simulated and physical phantom data. Our methods with automatic parameter tuning achieve similar, if not better, reconstruction performance compared to a representative two-stage algorithm."]},
{"title": "Genome-wide identification and structure-function studies of proteases and protease inhibitors in ", "highlights": ["Genes for a significant number of proteases and protease inhibitors have been identified in chickpea.", "Most of them have close orthologs in ", " and ", ".", "Although codon preference was seen for the catalytic residues but the relation is complex.", "A differential pattern of gene expression was observed in multiple plant tissues.", "The docking and molecular dynamics simulation showed chickpea protease-inhibitor molecular recognition pattern."], "abstract": ["Proteases are a family of enzymes present in almost all living organisms. In plants they are involved in many biological processes requiring stress response in situations such as water deficiency, pathogen attack, maintaining protein content of the cell, programmed cell death, senescence, reproduction and many more. Similarly, protease inhibitors (PIs) are involved in various important functions like suppression of invasion by pathogenic nematodes, inhibition of spores-germination and mycelium growth of ", " and response to wounding and fungal attack. As much as we know, no genome-wide study of proteases together with proteinaceous PIs is reported in any of the sequenced genomes till now.", "Phylogenetic studies and domain analysis of proteases were carried out to understand the molecular evolution as well as gene and protein features. Structural analysis was carried out to explore the binding mode and affinity of PIs for cognate proteases and prolyl oligopeptidase protease with inhibitor ligand.", "In the study reported here, a significant number of proteases and PIs were identified in chickpea genome. The gene expression profiles of proteases and PIs in five different plant tissues revealed a differential expression pattern in more than one plant tissue. Molecular dynamics studies revealed the formation of stable complex owing to increased number of protein\u2013ligand and inter and intramolecular protein\u2013protein hydrogen bonds.", "The genome-wide identification, characterization, evolutionary understanding, gene expression, and structural analysis of proteases and PIs provide a framework for future analysis when defining their roles in stress response and developing a more stress tolerant variety of chickpea."]},
{"title": "Adaptive robust control of cancer chemotherapy in the presence of parametric uncertainties: A comparison between three hypotheses", "highlights": ["Adaptive robust control of cancer chemotherapy in the presence of uncertainty.", "Control of three nonlinear cell-kill models: log-kill, Norton\u2013Simon and ", " hypotheses.", "Lyapunov stability theorem to investigate global stability and tracking convergence.", "Studying the effects of treatment period, initial value of tumor volume and uncertainty.", "For a wide range of uncertainties, controller guarantees the robust performance."], "abstract": ["In this paper, an adaptive robust control strategy is developed for the manipulation of drug usage and consequently the tumor volume in cancer chemotherapy. Three nonlinear mathematical cell-kill models including log-kill hypothesis, Norton\u2013Simon hypothesis and ", " hypothesis are considered in the presence of uncertainties. The Lyapunov stability theorem is used to investigate the global stability and tracking convergence of the process response. For the first time, performance of the uncertain process is investigated and compared for three nonlinear models. In addition, the effects of treatment period, initial value of tumor volume (carrying capacity) and the uncertainty amount on dynamic system behaviour are studied. Through a comprehensive evaluation, results are presented and compared for three cell-kill models. According to the results, for a wide range of model uncertainties, the adaptive controller guarantees the robust performance. However, for a given treatment period, more variation in drug usage is required as the amount of model uncertainty increases. Moreover, for both the nominal and uncertain models, less drug usage is required as the treatment period increases."]},
{"title": "Filtering multifocal VEP signals using Prony\u2019s method", "highlights": ["A new filter method based on Prony\u2019s method has been proposed.", "Main parameters of Prony\u2019s method have been adjusted to obtain maximum gain.", "Quality of signals filtered with three different methods have been compared.", "Prony\u2019s method improves the quality of mfVEP signals.", "A great number of analysable visual-field sectors has been obtained"], "abstract": ["This paper describes use of Prony\u2019s method as a filter applied to multifocal visual-evoked-potential (mfVEP) signals. Prony\u2019s method can be viewed as an extension of Fourier analysis that allows a signal to be decomposed into a linear combination of functions with different amplitudes, damping factors, frequencies and phase angles.", "By selecting Prony method parameters, a frequency filter has been developed which improves signal-to-noise ratio (SNR). Three different criteria were applied to data recorded from control subjects to produce three separate datasets: unfiltered raw data, data filtered using the traditional method (fast Fourier transform: FFT), and data filtered using Prony\u2019s method.", "Filtering using Prony\u2019s method improved the signals\u2019 original SNR by 44.52%, while the FFT filter improved the SNR by 33.56%. The extent to which signal can be separated from noise was analysed using receiver\u2013operating\u2013characteristic (ROC) curves. The area under the curve (AUC) was greater in the signals filtered using Prony\u2019s method than in the original signals or in those filtered using the FFT.", "filtering using Prony\u2019s method improves the quality of mfVEP signal pre-processing when compared with the original signals, or with those filtered using the FFT."]},
{"title": "Simulation of the microscopic process during initiation of stent thrombosis", "highlights": ["We computationally simulated the microscopic process of initial stent thrombosis.", "An increase in strut height increased the area of dysfunctional endothelium.", "Platelets preferentially deposited on endothelium outside recirculation regions.", "With intact endothelium, mural thrombus was largest with the highest strut.", "With denuded endothelium, mural thrombus was largest with the lowest strut."], "abstract": ["Coronary stenting is one of the most commonly used approaches to open coronary arteries blocked due to atherosclerosis. However, stent struts can induce stent thrombosis due to altered hemodynamics and endothelial dysfunction, and the microscopic process is poorly understood. The objective of this study was to determine the microscale processes during the initiation of stent thrombosis.", "We utilized a discrete element computational model to simulate the transport, collision, adhesion, and activation of thousands of individual platelets and red blood cells in thrombus formation around struts and dysfunctional endothelium.", "As strut height increased, the area of endothelium activated by low shear stress increased, which increased the number of platelets in mural thrombi. These thrombi were generally outside regions of recirculation for shorter struts. For the tallest strut, wall shear stress was sufficiently low to activate the entire endothelium. With the entire endothelium activated by injury or denudation, the number of platelets in mural thrombi was largest for the shortest strut. The type of platelet activation (by high shear stress or contact with activated endothelium) did not greatly affect results.", "During the initiation of stent thrombosis, platelets do not necessarily enter recirculation regions or deposit on endothelium near struts, as suggested by previous computational fluid dynamics simulations. Rather, platelets are more likely to deposit on activated endothelium outside recirculation regions and deposit directly on struts. Our study elucidated the effects of different mechanical factors on the roles of platelets and endothelium in stent thrombosis."]},
{"title": "Hypodensity extractor: A phantom study", "highlights": ["Standard CT low-contrast phantom was adjusted for hypodensity extraction purposes.", "Nonlinear approximations of signals were applied for hypodensity extraction.", "Objective, computational and repeatable criteria of assessment were proposed.", "Previously proposed and new hypodensity extractors were examined with procedures.", "Better visible small hypodensities and more correct shape assessment were obtained."], "abstract": ["We report on the extraction procedures of low-contrast symptomatic hypodensity optimized for a computed tomography-based diagnosis. The specific application is brain imaging with enhanced perception of hypodense areas which are direct symptoms of acute ischemia. A standard low-contrast phantom, as commonly employed in dosimetry and imaging quality evaluation, was used to derive numeric criteria for assessing the extraction effectiveness. Our proposed procedure is based on multiscale analysis of the image data expanded over the frames of wavelets, curvelets or complex wavelets, followed by nonlinear approximation of the symptom signatures. Apparent subtle density changes in the phantom were evaluated using computational metrics and subjective ratings. We discuss the advantages and disadvantages of our proposed optimized hypodensity extraction procedures."]},
{"title": "Computerized system for recognition of autism on the basis of gene expression microarray data", "highlights": ["We have developed ensemble of methods for efficient gene selection in autism.", "We have developed 2-stage ensemble system of automatic recognition of autism on the basis of gene microarray.", "Many gene selection methods with genetic algorithm and SVM classifiers increase the accuracy of autism recognition."], "abstract": ["The aim of this paper is to provide a means to recognize a case of autism using gene expression microarrays. The crucial task is to discover the most important genes which are strictly associated with autism. The paper presents an application of different methods of gene selection, to select the most representative input attributes for an ensemble of classifiers. The set of classifiers is responsible for distinguishing autism data from the reference class. Simultaneous application of a few gene selection methods enables analysis of the ill-conditioned gene expression matrix from different points of view. The results of selection combined with a genetic algorithm and SVM classifier have shown increased accuracy of autism recognition. Early recognition of autism is extremely important for treatment of children and increases the probability of their recovery and return to normal social communication. The results of this research can find practical application in early recognition of autism on the basis of gene expression microarray analysis."]},
{"title": "Identification of human drug targets using machine-learning algorithms", "highlights": ["Identification of candidate drug targets.", "Development of sequence based prediction method.", "SMOTE as a pre-processing step for balancing the training data.", "Complementary tool for novel drug target prediction."], "abstract": ["Identification of potential drug targets is a crucial task in the drug-discovery pipeline. Successful identification of candidate drug targets in entire genomes is very useful, and computational prediction methods can speed up this process. In the current work we have developed a sequence-based prediction method for the successful identification and discrimination of human drug target proteins, from human non-drug target proteins. The training features include sequence-based features, such as amino acid composition, amino acid property group composition, and dipeptide composition for generating predictive models. The classification of human drug target proteins presents a classic example of class imbalance. We have addressed this issue by using SMOTE (Synthetic Minority Over-sampling Technique) as a preprocessing step, for balancing the training data with a ratio of 1:1 between drug targets (minority samples) and non-drug targets (majority samples). Using ensemble classification learning method-Rotation Forest and ReliefF feature-selection technique for selecting the optimal subset of salient features, the best model with selected features can achieve 87.1% sensitivity, 83.6% specificity, and 85.3% accuracy, with 0.71 Matthews correlation coefficient (mcc) on a tenfold stratified cross-validation test. The subset of identified optimal features may help in assessing the compositional patterns in human drug targets. For further validation, using a rigorous leave-one-out cross-validation test, the model achieved 88.1% sensitivity, 83.0% specificity, 85.5% accuracy, and 0.712 mcc. The proposed method was tested on a second dataset, for which the current pipeline gave promising results. We suggest that the present approach can be applied successfully as a complementary tool to existing methods for novel drug target prediction."]},
{"title": "Real time estimation of generation, extinction and flow of muscle fibre action potentials in high density surface EMG", "highlights": ["New algorithm to estimate generation, propagation and extinction of muscle fibre action potentials from EMG.", "Application 1: investigation of muscle anatomy (orientation of fibres and position of innervation zone and tendons).", "Application 2: investigation of muscle activity (conduction velocity of the action potentials along the fibres).", "Application 3: selection of optimal locations of the channels from which to extract reliable EMG indexes.", "The method is real time, opening potential applications in prosthesis control and in biofeedback."], "abstract": ["Developing a real time method to estimate generation, extinction and propagation of muscle fibre action potentials from bi-dimensional and high density surface electromyogram (EMG).", "A multi-frame generalization of an optical flow technique including a source term is considered. A model describing generation, extinction and propagation of action potentials is fit to epochs of surface EMG.", "The algorithm is tested on simulations of high density surface EMG (inter-electrode distance equal to 5", "\u00a0", "mm) from finite length fibres generated using a multi-layer volume conductor model. The flow and source term estimated from interference EMG reflect the anatomy of the muscle, i.e. the direction of the fibres (2\u00b0 of average estimation error) and the positions of innervation zone and tendons under the electrode grid (mean errors of about 1 and 2", "\u00a0", "mm, respectively). The global conduction velocity of the action potentials from motor units under the detection system is also obtained from the estimated flow. The processing time is about 1", "\u00a0", "ms per channel for an epoch of EMG of duration 150", "\u00a0", "ms.", "A new real time image processing algorithm is proposed to investigate muscle anatomy and activity. Potential applications are proposed in prosthesis control, automatic detection of optimal channels for EMG index extraction and biofeedback."]},
{"title": "VAAPA: A web platform for visualization and analysis of alternative polyadenylation", "highlights": ["A web platform for visualization and analysis of alternative polyadenylation (VAAPA) was developed.", "It was designed in a multi-tier architecture and developed based on Smart Google Web Toolkit using Java language.", "It can visualize the distribution of poly(A) sites and poly(A) clusters of a gene or a section of a chromosome.", "It can highlight genes with switched APA sites among different conditions and display the respective sequence and poly(A) signals."], "abstract": ["Polyadenylation [poly(A)] is an essential process during the maturation of most mRNAs in eukaryotes. Alternative polyadenylation (APA) as an important layer of gene expression regulation has been increasingly recognized in various species. Here, a web platform for visualization and analysis of alternative polyadenylation (VAAPA) was developed. This platform can visualize the distribution of poly(A) sites and poly(A) clusters of a gene or a section of a chromosome. It can also highlight genes with switched APA sites among different conditions. VAAPA is an easy-to-use web-based tool that provides functions of poly(A) site query, data uploading, downloading, and APA sites visualization. It was designed in a multi-tier architecture and developed based on Smart GWT (Google Web Toolkit) using Java as the development language. VAAPA will be a valuable addition to the community for the comprehensive study of APA, not only by making the high quality poly(A) site data more accessible, but also by providing users with numerous valuable functions for poly(A) site analysis and visualization."]},
{"title": "Pre-operative prediction of surgical morbidity in children: Comparison of five statistical models", "highlights": ["We aimed to predict pediatric surgical morbidity using preoperative characteristics.", "We compared logistic regression models to data mining algorithms.", "The data mining algorithms performed as well as a simple logistic regression model.", "A flexible logistic regression model performed best on most model fit criteria."], "abstract": ["The accurate prediction of surgical risk is important to patients and physicians. Logistic regression (LR) models are typically used to estimate these risks. However, in the fields of data mining and machine-learning, many alternative classification and prediction algorithms have been developed. This study aimed to compare the performance of LR to several data mining algorithms for predicting 30-day surgical morbidity in children.", "We used the 2012 National Surgical Quality Improvement Program-Pediatric dataset to compare the performance of (1) a LR model that assumed linearity and additivity (simple LR model) (2) a LR model incorporating restricted cubic splines and interactions (flexible LR model) (3) a support vector machine, (4) a random forest and (5) boosted classification trees for predicting surgical morbidity.", "The ensemble-based methods showed significantly higher accuracy, sensitivity, specificity, PPV, and NPV than the simple LR model. However, none of the models performed better than the flexible LR model in terms of the aforementioned measures or in model calibration or discrimination.", "Support vector machines, random forests, and boosted classification trees do not show better performance than LR for predicting pediatric surgical morbidity. After further validation, the flexible LR model derived in this study could be used to assist with clinical decision-making based on patient-specific surgical risks."]},
{"title": "Classification of breast regions as mass and non-mass based on digital mammograms using taxonomic indexes and SVM", "highlights": ["This work proposes a methodology for classification of regions of mass and non-mass.", "Taxonomic diversity and distinctness are used to describe the texture.", "The tests were carried out over a sample of 3404 regions of mass and non-mass.", "The proposed work achieved good results for the classification with accuracy of 98.8%."], "abstract": ["Breast cancer is the second most common type of cancer in the world. Several computer-aided detection and diagnosis systems have been used to assist health experts identify suspicious areas that are difficult to perceive with the human eye, thus aiding in the detection and diagnosis of cancer. This work proposes a methodology for the discrimination and classification of regions extracted from mammograms as mass and non-mass. The Digital Database for Screening Mammography (DDSM) was used in this work for the acquisition of mammograms. The taxonomic diversity index (", ") and the taxonomic distinctness (", "), which were originally used in ecology, were used to describe the texture of the regions of interest. These indexes were computed based on phylogenetic trees, which were applied to describe the patterns in regions of breast images. Two approaches were used for the analysis of texture: internal and external masks. A support vector machine was used to classify the regions as mass and non-mass. The proposed methodology successfully classified the masses and non-masses, with an average accuracy of 98.88%."]},
{"title": "Multiple texture mapping of alveolar bone area for implant treatment in prosthetic dentistry", "highlights": ["We propose intuitive texture mapping on the alveolar bone area for implant treatment.", "After segment alveolar bone, texture patterns are mapped using graph-cut algorithm.", "We make result applying a texture image corresponding to the bone structure.", "It is helpful for facilitating communication and understanding of implant treatment."], "abstract": ["Treatment using implants is frequently employed in prosthetic dentistry. In this method, determining the bone density of the upper and lower jaws is important. Generally, a dentist can recognize the condition of the alveolar bone to be manipulated using a cone-beam computed tomography (CBCT) image. However, communicating the data to the patient is a challenge because it is difficult for the nonprofessional person to interpret the image, which contains a distribution of pixels with similar density. We present an intuitive texture mapping method of the alveolar bone area for application in implant treatment. Our method aims to help patients better understand the treatment process by using a textured image that includes several different texture patterns that reflect the density of the alveolar bone area. We segment the area in accordance with the density of corresponding parts in the alveolar bone and the gingiva. By simplifying the boundary of each segmented region, the distribution of pixels with similar density on the alveolar bone area can be easily recognized. Next, the texture patterns for several segmented regions are mapped onto the alveolar bone area using the graph-cut algorithm, which is used for smooth texture mapping at the boundary of the segmented region. The result is an applied texture on the alveolar bone area that corresponds to the bone structure. Our method is helpful for facilitating communication and understanding of treatment using dental implants."]},
{"title": "Permutation entropy analysis of vital signs data for outcome prediction of patients with severe traumatic brain injury", "highlights": ["Permutation entropy can measure complexity of clinical time series.", "Features derived from early vital signs have prediction power for long-term outcomes.", "Ordinal patterns from vital signs could provide clinical meaningful interpretation."], "abstract": ["Permutation entropy is computationally efficient, robust to outliers, and effective to measure complexity of time series. We used this technique to quantify the complexity of continuous vital signs recorded from patients with traumatic brain injury (TBI). Using permutation entropy calculated from early vital signs (initial 10\u201320% of patient hospital stay time), we built classifiers to predict in-hospital mortality and mobility, measured by 3-month Extended Glasgow Outcome Score (GOSE). Sixty patients with severe TBI produced a skewed dataset that we evaluated for accuracy, sensitivity and specificity. The overall prediction accuracy achieved 91.67% for mortality, and 76.67% for 3-month GOSE in testing datasets, using the leave-one-out cross validation. We also applied Receiver Operating Characteristic analysis to compare classifiers built from different learning methods. Those results support the applicability of permutation entropy in analyzing the dynamic behavior of TBI vital signs for early prediction of mortality and long-term patient outcomes."]},
{"title": "Automatic detection of epileptic seizures in long-term EEG records", "highlights": ["It is developed to reduce the visual inspection time spent by physicians.", "It is based on simple and low computational costs methods.", "It uses thresholds instead of complex classifier. So, no training step is required.", "It can be applied to new records that have not been presented to the algorithm.", "It is simpler than other algorithms, similar or superior performances were achieved."], "abstract": ["Epilepsy is a neurological disorder which affects nearly 1.5% of the world\u05f3s total population. Trained physicians and neurologists visually scan the long-term electroencephalographic (EEG) records to identify epileptic seizures. It generally requires many hours to interpret the data. Therefore, tools for quick detection of seizures in long-term EEG records are very useful. This study proposes an algorithm to help detect seizures in long-term iEEG based on low computational costs methods using Spectral Power and Wavelet analysis. The detector was tested on 21 invasive intracranial EEG (iEEG) records. A sensitivity of 85.39% was achieved. The results indicate that the proposed method detects epileptic seizures in long-term iEEG records successfully. Moreover, the algorithm does not require long processing time due to its simplicity. This feature will allow significant time reduction of the visual inspection of iEEG records performed by the specialists."]},
{"title": "Numerical study of stenotic side branch hemodynamics in true bifurcation lesions", "highlights": ["Numerical models were generated to investigate stenotic side branch hemodynamics.", "We studied the effect of stenosis severity and location on WSS and blood perfusion.", "Low blood perfusion was found in type (1,0,1), corresponding to branch steal.", "Critically low WSS developed along the outer wall in the SB of lesion type (1,0,1).", "This is counterintuitive to the Medina classification where (1,1,1) is most severe."], "abstract": ["Coronary bifurcation lesions are complex. Whether a critical condition exists in the stenotic side branch (SB) of bifurcation lesions, according to the Medina classification, is unassessed. Computational models of coronary bifurcations were developed with different stenosis severities and locations, in order to study the flow distribution and wall shear stress (WSS) in the SB. It was found that bifurcation lesion type (1,0,1) had a flow ratio of 0.25, much less than the corresponding normal ratio of 0.47, and the 0.46 and 0.39 ratios computed for lesion types (0,1,1) and (1,1,1), respectively. Bifurcation type (1,0,1) was associated with a 47% reduction from normal coronary flow, resulting in coronary branch steal. Blood flow to the SB decreased as the stenosis severity increased and approached the carina, in the proximal, distal, and side branches. Similarly, WSS values decreased with increasing stenosis severity. Bifurcation type (1,0,1) had the lowest WSS values in the SB, and were below the 0.5", "\u00a0", "Pa threshold for atherosclerotic growth. In conclusion, the results suggest that lesion type (1,0,1) is at the greatest risk of coronary branch steal, leading to potential ischemia, as well as further atherosclerotic growth. This is counterintuitive to the Medina classification, where bifurcation type (1,1,1) is usually considered the most severe."]},
{"title": "A protein mapping method based on physicochemical properties and dimension reduction", "highlights": ["We propose a simple and intuitive 2D graphical map of protein sequence.", "Graphical map is based on 12 major physicochemical properties of amino acids.", "Two separate applications illustrate the utility of the method."], "abstract": ["The graphical mapping of a protein sequence is more difficult than the graphical mapping of a DNA sequence because of the twenty amino acids and their complicated physicochemical properties. However, the graphical mapping for protein sequences attracts many researchers to develop different mapping methods. Currently, researchers have proposed their mapping methods based on several physicochemical properties. In this article, a new mapping method for protein sequences is developed by considering additional physicochemical properties, which is a simple and effective approach.", "Based on the 12 major physicochemical properties of amino acids and the PCA method, we propose a simple and intuitive 2D graphical mapping method for protein sequences. Next, we extract a 20D vector from the graphical mapping which is used to characterize a protein sequence.", "The proposed graphical mapping consists of three important properties, one-to-one, no circuit, and good visualization. This mapping contains more physicochemical information. Next, this proposed method is applied to two separate applications. The results illustrate the utility of the proposed method.", "To validate the proposed method, we first give a comparison of protein sequences, which consists of nine ND6 proteins. The similarity/dissimilarity matrix for the ssnine ND6 proteins correctly reveals their evolutionary relationship. Next, we give another application for the cluster analysis of HA genes of influenza A (H1N1) isolates. The results are consistent with the known evolution fact of the H1N1 virus. The separate applications further illustrate the utility of the proposed method."]},
{"title": "A leave-one-out cross-validation SAS macro for the identification of markers associated with survival", "highlights": ["A cross-validated model selection macro is proposed for prognostic survival data using Cox Proportional Hazards (PH) model.", "User-friendly with controls included for maximum size of model and prevalence of variable.", "Checks proportional PH assumption graphically and statistically."], "abstract": ["A proper internal validation is necessary for the development of a reliable and reproducible prognostic model for external validation. Variable selection is an important step for building prognostic models. However, not many existing approaches couple the ability to specify the number of covariates in the model with a cross-validation algorithm. We describe a user-friendly SAS macro that implements a score selection method and a leave-one-out cross-validation approach. We discuss the method and applications behind this algorithm, as well as details of the SAS macro."]},
{"title": "Comparison of an analog and digital quantitative and qualitative analysis for the fit of dental copings", "highlights": ["The Ez-data (deviation in height) are particularly interesting for clinicians.", "The analog 2D replica technique revealed a loss of information due to the cutting.", "The digital method can be done automatically and objectively over the complete surface without data loss.", "Modified data density and the enlargement both significantly influenced the fit.", "The digital computer-based method provided a 3D quantitative and qualitative analysis."], "abstract": ["Precision in fit is crucial for dental crowns and bridges. Most analyses of fit are based on analog 2D techniques. Aim of this in-vitro study was to compare an analog and a digital quantitative and qualitative analysis for the fit of CAD/CAM fabricated dental copings.", "A prepared steel canine served as master die. CAD surface models, varying in data density, were purposely enlarged in height (Ez), circumference (Exy) and both of these aspects at once (Exyz). Two titanium copings for each variation were produced. The silicone-replica-technique was applied to analyze the fit by means of a 2D analog light microscope measurement (LMM) and a 3D computer-assisted measurement using an optical digitizing system (ODKM97), respectively.", "In most cases, restorations based on the low data density showed a better fit than those based on high data density. Original size low density data showed the lowest marginal and axial values in the quantitative 2D analyses (LMM and ODKM97). The 3D measurements (ODKM97) revealed best fit of the low density original size specimens, whereas the Ez specimens showed the highest values. Noticeable variations in fit were detected marginally and axially depending on the specific measurement point (mesial, distal, oral, or buccal) for both measurement systems.", "The analog 2D replica technique revealed a loss of information due to the necessary cutting process. By contrast, the digital computer-based method provided 3D quantitative and qualitative results without data loss over the complete surface."]},
{"title": "Towards a social and context-aware multi-sensor fall detection and risk assessment platform", "highlights": ["We propose a new extensible and adaptable cloud-based fall detection framework.", "An ontology-based approach is used to model the situation of patient and caregivers.", "The captured information from the sensors is combined with contextual information.", "Existing fall detection systems can easily be integrated to improve accuracy.", "Different combinations of fall detection systems and sensors can be used."], "abstract": ["For elderly people fall incidents are life-changing events that lead to degradation or even loss of autonomy. Current fall detection systems are not integrated and often associated with undetected falls and/or false alarms.", "In this paper, a social- and context-aware multi-sensor platform is presented, which integrates information gathered by a plethora of fall detection systems and sensors at the home of the elderly, by using a cloud-based solution, making use of an ontology. Within the ontology, both static and dynamic information is captured to model the situation of a specific patient and his/her (in)formal caregivers. This integrated contextual information allows to automatically and continuously assess the fall risk of the elderly, to more accurately detect falls and identify false alarms and to automatically notify the appropriate caregiver, e.g., based on location or their current task.", "The main advantage of the proposed platform is that multiple fall detection systems and sensors can be integrated, as they can be easily plugged in, this can be done based on the specific needs of the patient. The combination of several systems and sensors leads to a more reliable system, with better accuracy. The proof of concept was tested with the use of the visualizer, which enables a better way to analyze the data flow within the back-end and with the use of the portable testbed, which is equipped with several different sensors."]},
{"title": "Integrative epigenetic profiling analysis identifies DNA methylation changes associated with chronic alcohol consumption", "highlights": ["We presented a longitudinal study on alcohol use disorder in Taiwanese aborigines.", "Genome-wide methylation profiles uncovered genes affected by alcohol misuse.", "Bioinformatics analysis revealed the pathways affected by chronic alcohol exposure.", "Some of these genes and pathways have been implicated in alcohol-related diseases.", "We provided additional support for the role methylation plays in alcohol damage."], "abstract": ["Alcoholism has always been a major public health concern in Taiwan, especially in the aboriginal communities. Emerging evidence supports the association between DNA methylation and alcoholism, though very few studies have examined the effect of chronic alcohol consumption on the epignome. Since 1986, we have been following up on the mental health conditions of four major aboriginal peoples of Taiwan. The 993 aboriginal people who underwent the phase 1 (1986) clinical interviews were followed up through phase 2 (1990\u20131992), and phase 3 (2003\u20132009). Selected individuals for the current study included 10 males from the phase 1 normal cohort who remained normal at phase 2 and became dependent on alcohol by phase 3 and 10 control subjects who have not had any drinking problems throughout the study. We profiled the DNA methylation changes in the blood samples collected at phases 2 and 3. Enrichment analyses have identified several biological processes related to immune system responses and aging in the control group. In contrast, differentially methylated genes in the case group were mostly associated with susceptibility to infections, as well as pathways related to muscular contraction and neural degeneration. The methylation levels of six genes were found to correlate with alcohol consumption. These include genes involved in neurogenesis (", ") and inflammation (", "), as well as alcoholism-associated genes ", ", ", ", and ", ". Given the limited sample size, our approach uncovered genes and disease pathways associated with chronic alcohol consumption at the epigenetic level. The results offer a preliminary methylome map that enhances our understanding of alcohol-induced damages and offers new targets for alcohol injury research."]},
{"title": "Automatic characterization of thoracic aortic aneurysms from CT images", "highlights": ["We have developed an automatic method for measuring the diameter at any point along the length of the thoracic aorta.", "The proposed method uses principal component analysis for determining the maximum diameter of the aorta.", "The method developed clearly reduces the time spent by any semi-automatic commercial method available in the market.", "The proposed method obtains reproducible results of the aortic diameters, contrary to what happens with semi-automatic methods."], "abstract": ["Accurate determination of the diameter is an important step for diagnosis and follow-up of aortic abnormalities such as aneurysms, caused by dilation of the vessel lumen. In this work we focus on the development of an automatic method for measuring the calibre of the thoracic aorta. The method is based on the application of principal component analysis on normal planes extracted from the aorta to establish the main axis of each section of the vessel.", "Two experiments were performed in order to test the accuracy and the rotational invariance of the developed method. Accuracy was determined by using a database of 15 clinical cases, where our method and a commercial software, which was considered as the gold standard, were compared. For the rotational invariance check, phantom images in different orientations were obtained and the diameter was measured with the proposed method.", "For clinical cases, a good agreement was observed between our method and the gold standard. The Bland Altman plots indicated that all of the values were within the acceptable limits of agreement with a bias of 0.2", "\u00a0", "mm between both methods. For phantom cases, an ANOVA test revealed that the results achieved for the data sets acquired for the different orientations were not statistically different (", "=1.88, ", "=0.153), which demonstrates the robustness of the method for rotations.", "The proposed method is applicable for measuring the diameter in all tested cases, and the results achieved underscored the capability of our approach for automatic characterization of thoracic aortic aneurysms."]},
{"title": "The association of CD28 polymorphism, rs3116496, with Cancer: A meta-analysis", "highlights": ["We determined the relationship between CD28 polymorphisms, rs3116496, and cancer.", "Carriers of a T allele had a lower incidence of cancer than carriers of a C allele.", "A lower cancer incidence was observed among Asians carrying a T allele."], "abstract": ["To determine the relationship between CD28 polymorphisms, rs3116496, and cancer.", "Meta-analysis.", "PubMed, EMBASE, Web of Science, and Cochrane library databases were searched to identify studies reporting the association between CD28 polymorphism and cancer. Two authors selected identified studies, extracted, and analyzed the data independently.", "Individuals carrying a T allele (TT homozygotes and TT+TC heterozygotes) at rs3116496 had a lower incidence of cancer than carriers of a C allele. Subgroup analysis showed that this association held true for Asians, but not Europeans.", "CD28 polymorphism, rs3116496, contributes to cancer susceptibility in the case of multiple cancers."]},
{"title": "Noninvasive identification of two lesions with local repolarization changes using two dipoles in inverse solution simulation study", "highlights": ["We modeled one or two lesions with repolarization changes in heart ventricle.", "We computed difference integral body surface potential maps on the torso surface.", "We suggested the inverse solution for identification of two lesions by two dipoles.", "Characteristics of solutions were specified for classification of correct solutions.", "For suggested method no apriori information about the number of lesions is needed."], "abstract": ["The method for inverse localization and identification of two distinct simultaneous lesions with changed repolarization in the ventricular myocardium (two-vessel disease) is proposed and its robustness to errors in input data is tested in this simulation study.", "The inverse solution was obtained from the difference between STT integral body surface potential map computed with repolarization changes and the STT integral map from normal activation. In a numerical model of ventricles 48 cases of two simultaneous lesions and 48 cases of a single lesion were modeled. The effect of the lesions was taken to be represented by two dipoles. The input data were disturbed by three types of added noise. Twenty three characteristics of every obtained inverse solution were defined and four of them were used as the features in discriminant analysis task distinguishing the correct inverse solutions identifying two lesions.", "The mean localization error for identified two lesions was 1.1\u00b10.7", "\u00a0", "cm. The sensitivity and specificity of quadratic discriminant analysis with cross-validation and feature selection was higher than 90%.", "The combination of the inverse solution with two dipoles and discriminant analysis allows the identification of two simultaneous lesions without a priori information about the number of lesions."]},
{"title": "Changes in aerodynamics during vocal cord dysfunction", "highlights": ["At different inhalation rate, mass flow rate in the normal cavity more than doubles flow in the VCD cavity", "Visualization results showed a much slower flow above the glottal region in the VCD cavity than in the normal cavity", "During episodes of VCD trying to increase inspiratory pressure so as to inhale more air may result in greater glottal resistance."], "abstract": ["Changes in laryngeal airflow dynamics during episodes of vocal cord dysfunction (VCD) have not been well described. Very little is known about how inspiratory airflow is impacted when the vocal cords transition from normal inhalation state to a paradoxical adducted state; and how much change in laryngeal airflow and resistance occur before symptoms of stridor and air hunger emerge. This study provides new insight on the effects of VCD on respiratory airflow using computational fluid dynamics (CFD) techniques.", "Computed tomography images of a subject with normal vocal cords opening at the time of scanning were digitally modified to mimic an episode of VCD. To quantify and compare changes in inspiratory flow during VCD attack and normal inhalation, steady-state, laminar simulations were performed for three different breathing rates.", "Pressure-flow analysis during VCD revealed that increasing inspiratory effort is not as efficient as in normal inhalation. Airflow resistance at the epiglottis was higher in the normal state (0.04", "\u00a0", "Pa.s/mL versus 0.02", "\u00a0", "Pa.s/mL) than in VCD; while resistance at the glottis and trachea remained roughly the same (0.04", "\u00a0", "Pa.s/mL) during normal inhalation, it escalated during VCD (0.11", "\u00a0", "Pa.s/mL and 0.13", "\u00a0", "Pa.s/mL at the glottis and trachea, respectively). Peak airflow velocity and vorticity occurred around the glottis during VCD, and at the epiglottis during normal inhalation.", "This pilot study demonstrates that attempting to force more inspired air will yield greater glottal resistance during VCD. Furthermore, there were evidence of abrupt laryngeal pressure gradient, chaotic airflow and high concentration of shear stresses in the glottal region."]},
{"title": "Mathematical modelling of thermoregulation processes for premature infants in closed convectively heated incubators", "highlights": ["We propose a model for heat transfer between a premature infant and incubator.", "The model incorporates the main mechanisms of generation and heat exchange.", "We formulate a functional to minimize the metabolic cost of newborn.", "Minimum functional generates the baby\u05f3s temperature is in neutral temperature ranges."], "abstract": ["The low-weight newborns and especially the premature infants have difficulty in maintaining their temperature in the range considered to be normal. Several studies revealed the importance of thermal environment and moisture to increase the survival rate of newborns. This work models the process of heat exchange and energy balance in premature newborns during the first hours of life in a closed incubator. In addition, a control problem was proposed and solved in order to maintain thermal stability of premature newborns to increase their rate of survival and weight. For this purpose, we propose an algorithm to control the temperature inside the incubator. It takes into account the measurements of the body temperature of a premature newborn which are recorded continuously. We show that using this model the temperature of a premature newborn inside the incubator can be kept in a thermal stability range."]},
{"title": "Indicators of hypertriglyceridemia from anthropometric measures based on data mining", "highlights": ["Waist-to-height ratio was the best predictor of hypertriglyceridemia in Korean women.", "Rib-to-forehead ratio was the strongest indicator of hypertriglyceridemia in men.", "The best indicator of hypertriglyceridemia may differ according to age and gender.", "Combined measures provide better predictive power compared with individual measures."], "abstract": ["The best indicator for the prediction of hypertriglyceridemia derived from anthropometric measures of body shape remains a matter of debate. The objectives are to determine the strongest predictor of hypertriglyceridemia from anthropometric measures and to investigate whether a combination of measures can improve the prediction accuracy compared with individual measures.", "A total of 5517 subjects aged 20\u201390 years participated in this study. The numbers of normal and hypertriglyceridemia subjects were 3022 and 653 females, respectively, and 1306 and 536 males, respectively. We evaluated 33 anthropometric measures for the prediction of hypertriglyceridemia using statistical analysis and data mining.", "In the 20\u201390-year-old groups, age in women was the variable that exhibited the highest predictive power; however, this was not the case in men in all age groups. Of the anthropometric measures, the waist-to-height ratio (WHtR) was the best predictor of hypertriglyceridemia in women. In men, the rib-to-forehead circumference ratio (RFcR) was the strongest indicator. The use of a combination of measures provides better predictive power compared with individual measures in both women and men. However, in the subgroups of ages 20\u201350 and 51\u201390 years, the strongest indicators for hypertriglyceridemia were rib circumference in the 20\u201350-year-old group and WHtR in the 51\u201390-year-old group in women and RFcR in the 20\u201350-year-old group and BMI in the 51\u201390-year-old group in men.", "Our results demonstrated that the best predictor of hypertriglyceridemia may differ according to gender and age."]},
{"title": "An automated lung segmentation approach using bidirectional chain codes to improve nodule detection accuracy", "highlights": ["A novel lung segmentation algorithm is proposed, focusing on juxtapleural nodules.", "A bidirectional chain coding method is proposed to detect border inflection points.", "A support vector machine classifier is used to selectively smooth the lung border.", "The method is evaluated on 233 CT studies with 403 juxtapleural nodules."], "abstract": ["Computer-aided detection and diagnosis (CAD) has been widely investigated to improve radiologists\u05f3 diagnostic accuracy in detecting and characterizing lung disease, as well as to assist with the processing of increasingly sizable volumes of imaging. Lung segmentation is a requisite preprocessing step for most CAD schemes. This paper proposes a parameter-free lung segmentation algorithm with the aim of improving lung nodule detection accuracy, focusing on juxtapleural nodules. A bidirectional chain coding method combined with a support vector machine (SVM) classifier is used to selectively smooth the lung border while minimizing the over-segmentation of adjacent regions. This automated method was tested on 233 computed tomography (CT) studies from the lung imaging database consortium (LIDC), representing 403 juxtapleural nodules. The approach obtained a 92.6% re-inclusion rate. Segmentation accuracy was further validated on 10 randomly selected CT series, finding a 0.3% average over-segmentation ratio and 2.4% under-segmentation rate when compared to manually segmented reference standards done by an expert."]},
{"title": "A bilateral analysis scheme for false positive reduction in mammogram mass detection", "highlights": ["The proposed region matching method using shape context is robust to segmentation error.", "Matching cost is defined to quantify the matching credibility, which is discarded by existing bilateral analysis schemes.", "A hierarchical similarity is designed for FP reduction in bilateral mass detection.", "Promising FP reduction is obtained using bilateral analysis in mass detection."], "abstract": ["In this paper, a bilateral image analysis scheme is developed for the purpose of reducing false positives (FPs) in the detection of masses in dense mammograms. It consists of two steps: a region matching step for determining the correspondence between a pair of mammograms, and a bilateral similarity analysis step for discarding FPs in the detection. For the first step, a matching cost is defined to quantify the credibility of the corresponding region in a pair of bilateral mammograms. For the second step, a similarity measurement is introduced to discriminate between mass and normal for a pair of bilateral regions based on both global and local image appearances. The proposed scheme is tested on a set of 332 mammograms. The results show that the proposed scheme could obtain better performance when compared with several existing bilateral analysis schemes. With detection sensitivity at 85%, the proposed bilateral scheme could reduce the FP rate of a unilateral scheme from 3.64 to 2.39 per image, a 34% reduction."]},
{"title": "A virtual reality based simulator for learning nasogastric tube placement", "highlights": ["A virtual training simulator is proposed for nasogastric tube (NGT) placement.", "Hybrid force model simulates the interactions between NGT and nasogastric passage.", "Insertion forces are computed analytically or by interpolating pre-computed FEM data.", "Quantitative performance metrics are available for objective assessment.", "The haptic feeling is similar to nurses\u2019 experience in real NGT placement."], "abstract": ["Nasogastric tube (NGT) placement is a common clinical procedure where a plastic tube is inserted into the stomach through the nostril for feeding or drainage. However, the placement is a blind process in which the tube may be mistakenly inserted into other locations, leading to unexpected complications or fatal incidents. The placement techniques are conventionally acquired by practising on unrealistic rubber mannequins or on humans. In this paper, a virtual reality based training simulation system is proposed to facilitate the training of NGT placement. It focuses on the simulation of tube insertion and the rendering of the feedback forces with a haptic device. A hybrid force model is developed to compute the forces analytically or numerically under different conditions, including the situations when the patient is swallowing or when the tube is buckled at the nostril. To ensure real-time interactive simulations, an offline simulation approach is adopted to obtain the relationship between the insertion depth and insertion force using a non-linear finite element method. The offline dataset is then used to generate real-time feedback forces by interpolation. The virtual training process is logged quantitatively with metrics that can be used for assessing objective performance and tracking progress. The system has been evaluated by nursing professionals. They found that the haptic feeling produced by the simulated forces is similar to their experience during real NGT insertion. The proposed system provides a new educational tool to enhance conventional training in NGT placement."]},
{"title": "Mathematical model in left ventricle segmentation", "highlights": ["A parametric model of the left ventricle is presented.", "B\u00e9zier curves are used to model the left ventricle shape.", "The model improves the results obtained by other segmentation algorithms.", "The presented algorithm is fully automatic."], "abstract": ["In this paper a parametric model of the left ventricle is presented. Its task is to estimate the myocardium shape on those slices, on which the segmentation algorithm has outlined the structure incorrectly. The aim of using the model on improperly segmented slices is to improve the accuracy of computing cardiac hemodynamic parameters and the heart mass. The proposed model works with any segmentation algorithm. The usefulness of the model is the largest while determining the myocardium at end-systole and calculating the heart mass. In case of the segmentation algorithm applied in this study, the error decreased from clinically unacceptable to acceptable after using the presented model."]},
{"title": "Non-invasive estimation of skin thickness from hyperspectral imaging and validation using echography", "highlights": ["This is the first study to validate skin thickness estimation to a gold standard.", "We found skin thickness errors on the order of the gold standard resolution.", "Proof-of-concept that skin can be robustly modeled using hyperspectral imaging.", "Proof-of-concept that skin thickness can be estimated using machine learning."], "abstract": [": The skin is the largest organ and is subject to the greatest exposure to outside elements throughout one\u05f3s lifetime. Current data by the American Academy of Dermatology suggests that approximately ten people die each hour worldwide due to skin related conditions. Cancers such as melanoma are growths that originate in the epidermis. Therefore, an accurate and non-invasive method to estimate skin constitutive elements can play an important clinical role in detecting the early onset of skin tumors. It can also serve as a valuable tool for research and development in cosmetics and pharmaceuticals in general.", ": In our prior work, we developed a method that combined a physics-based model of human skin with machine learning and Hyperspectral imaging to non-invasively estimate physiological skin parameters, including melanosomes, collagen, oxygen saturation, and blood volume. In this work, we extend that model to also estimate skin thickness. Moreover, for the first time, we develop a protocol to test our methodology for skin thickness estimation using Ultrasound to acquire a gold standard dataset.", ": We tested our methodology for skin thickness estimation on a dataset of 48 Hyperspectral signatures obtained ", " from six patients under IRB at Johns Hopkins Hospital. We found mean absolute errors on the order of the Ultrasound resolution (i.e., our gold standard).", ": This is the first study of its kind to validate skin thickness estimates using a gold standard. Our preliminary results constitute a proof-of-concept that hyperspectral-based methods can accurately and non-invasively estimate skin thickness in clinical settings."]},
{"title": "Discontinuous Galerkin finite element method for solving population density functions of cortical pyramidal and thalamic neuronal populations", "highlights": ["Discontinuous Galerkin FEM resolves stability problem in simulation of neural systems.", "Discontinuous Galerkin FEM excels Monte Carlo simulation for LIF and LIFB models.", "Discontinuous Galerkin FEM can simulate EEG which involves three populations."], "abstract": ["Compared with the Monte Carlo method, the population density method is efficient for modeling collective dynamics of neuronal populations in human brain. In this method, a population density function describes the probabilistic distribution of states of all neurons in the population and it is governed by a hyperbolic partial differential equation. In the past, the problem was mainly solved by using the finite difference method. In a previous study, a continuous Galerkin finite element method was found better than the finite difference method for solving the hyperbolic partial differential equation; however, the population density function often has discontinuity and both methods suffer from a numerical stability problem. The goal of this study is to improve the numerical stability of the solution using discontinuous Galerkin finite element method. To test the performance of the new approach, interaction of a population of cortical pyramidal neurons and a population of thalamic neurons was simulated. The numerical results showed good agreement between results of discontinuous Galerkin finite element and Monte Carlo methods. The convergence and accuracy of the solutions are excellent. The numerical stability problem could be resolved using the discontinuous Galerkin finite element method which has total-variation-diminishing property. The efficient approach will be employed to simulate the electroencephalogram or dynamics of thalamocortical network which involves three populations, namely, thalamic reticular neurons, thalamocortical neurons and cortical pyramidal neurons."]},
{"title": "Cladograms with Path to Event (ClaPTE): A novel algorithm to detect associations between genotypes or phenotypes using phylogenies", "highlights": ["Novel method identifies associated mutations, accounting for common ancestry.", "Controls false positive rates when a multiple hypothesis correction is applied.", "Computationally efficient method, suitable for large data sets."], "abstract": ["Associations between genotype and phenotype provide insight into the evolution of pathogenesis, drug resistance, and the spread of pathogens between hosts. However, common ancestry can lead to apparent associations between biologically unrelated features. The novel method Cladograms with Path to Event (ClaPTE) detects associations between character-pairs (either a pair of mutations or a mutation paired with a phenotype) while adjusting for common ancestry, using phylogenetic trees.", "ClaPTE tests for character-pairs changing close together on the phylogenetic tree, consistent with an associated character-pair. ClaPTE is compared to three existing methods (independent contrasts, mixed model, and likelihood ratio) to detect character-pair associations adjusted for common ancestry. Comparisons utilize simulations on gene trees for: HIV Env, HIV promoter, and bacterial DnaJ and GuaB; and case studies for Oseltamavir resistance in Influenza, and for DnaJ and GuaB. Simulated data include both true-positive/associated character-pairs, and true-negative/not-associated character-pairs, used to assess type I (frequency of ", "-values in true-negatives) and type II (sensitivity to true-positives) error control.", "ClaPTE has competitive sensitivity and better type I error control than existing methods. In the Influenza/Oseltamavir case study, ClaPTE reports no new permissive mutations but detects associations between adjacent (in primary sequence) amino acid positions which other methods miss. In the DnaJ and GuaB case study, ClaPTE reports more frequent associations between positions both from the same protein family than between positions from different families, in contrast to other methods. In both case studies, the results from ClaPTE are biologically plausible."]},
{"title": "Modeling of high sodium intake effects on left ventricular hypertrophy", "highlights": ["A system-level computer simulation of chronic high sodium intake effects is conducted.", "The current model couples a cardiovascular hemodynamics model and a renal system model.", "Left ventricular (LV) wall stress is modeled to compute the changes of the LV wall thickness.", "Modeling results suggest high sodium intake alters the body-fluid homeostasis and increases the LV work load and relative wall thickness."], "abstract": ["Many clinical studies suggest that chronic high sodium intake contributes to the development of essential hypertension and left ventricular (LV) hypertrophy. In the present study, a system-level computer model has been developed to simulate the long-term effects of increased sodium intake on the LV mechanical functions and the body-fluid homeostasis. The new model couples a cardiovascular hemodynamics function model with an explicit account of the LV wall thickness variation and a long-term renal system model. The present model is validated with published results of clinical studies. The results suggest that, with increased sodium intake, the renal system function, the plasma hormone concentrations, and the blood pressure adapt to new levels of equilibrium. The LV work output and the relative wall thickness increase due to the increase of sodium intake. The results of the present model match well with the patient data."]},
{"title": "A study of the sink effect by blood vessels in radiofrequency ablation", "highlights": ["The impact of blood vessel in radiofrequency ablation (RFA) therapy.", "Guideline to assess maximum temperature and lesion volume during RFA.", "Initial treatment planning in thermal therapy."], "abstract": ["The objective of the current work was to study the sink effect in radiofrequency ablation (RFA) caused by a blood vessel located close to an electrode in a two-compartment numerical model, consisting of a spherical tumor embedded in healthy liver tissue. Several blood vessels of different sizes were studied at different distances from the electrode. It was found that when a straight blood vessel, cylindrical in shape, is located parallel to the electrode, the minimum distance for a drop of only 10% in the isothermal treatment volume above 50", "\u00a0", "\u00b0C, compared to the model without the blood vessel, varies from 4.49", "\u00a0", "mm (for a vessel of 2", "\u00a0", "mm in diameter) to 20.02", "\u00a0", "mm (for a vessel 20", "\u00a0", "mm in diameter). The results can be used as a guideline to clinical practitioners, in order to quickly assess the potential impact of existing blood vessels on the resulting treatment volume."]},
{"title": "Retinal vessel extraction using Lattice Neural Networks with dendritic processing", "highlights": ["First implementation of a Lattice Neural Network with Dendritic Processing to solve classify retinal images.", "The performance is competitive compared with common approaches like Support Vector Machines and Multilayer Perceptrons.", "The Lattice Neural Network with Dendritic Processing does not require the adjustment of parameters by the user."], "abstract": ["Retinal images can be used to detect and follow up several important chronic diseases. The classification of retinal images requires an experienced ophthalmologist. This has been a bottleneck to implement routine screenings performed by general physicians. It has been proposed to create automated systems that can perform such task with little intervention from humans, with partial success. In this work, we report advances in such endeavor, by using a Lattice Neural Network with Dendritic Processing (LNNDP). We report results using several metrics, and compare against well known methods such as Support Vector Machines (SVM) and Multilayer Perceptrons (MLP). Our proposal shows better performance than other approaches reported in the literature. An additional advantage is that unlike those other tools, LNNDP requires no parameters, and it automatically constructs its structure to solve a particular problem. The proposed methodology requires four steps: (1) Pre-processing, (2) Feature computation, (3) Classification and (4) Post-processing. The Hotelling ", " control chart was used to reduce the dimensionality of the feature vector, from 7 that were used before to 5 in this work. The experiments were run on images of DRIVE and STARE databases. The results show that on average, F1-Score is better in LNNDP, compared with SVM and MLP implementations. Same improvement is observed for MCC and the accuracy."]},
{"title": "The qualitative detection of decreases in cardiac output", "highlights": ["We collected routinely monitored data from patients during anaesthesia.", "We determined when significant changes in variables occurred using runs analysis.", "Significant changes in multiple variables were used to suggest possible diagnoses.", "End-tidal carbon dioxide decreased when a fall in cardiac output diagnosed.", "End-tidal carbon dioxide did not decrease significantly without this diagnosis."], "abstract": ["Cardiac output is a major factor in the maintenance of physiological homeostasis and is difficult to measure with accuracy. This study describes an evidence-based technique, based on physiological changes, which may indicate small changes in cardiac output that cannot be measured by current techniques.", "Synchronous changes in blood pressure, heart rate, pulse amplitude and end-tidal carbon dioxide are analysed using runs analysis and a normalisation technique. An evidence-based algorithm was used to detect possible changes in cardiac output and data extracts from 31 consenting patients are presented as examples.", "The decrease in end-tidal carbon dioxide, during steady state ventilation, was greater in those events notified as hypovolaemia associated with a fall in cardiac output than those events notified as hypovolaemia alone. The difference in end-tidal carbon dioxide between the two groups was \u22120.25", "\u00a0", "kPa (CI \u22120.42 to \u22120.09) ", "<0.003.", "Runs analysis can detect trends in EtCO", " that during steady state ventilation may indicate a decrease in cardiac output. It is a safe technique; no additional hardware is required and the generated alerts only notify the clinician of the possibility of an adverse change. Determination of the rate of clinically significant false positives and negatives requires further work."]},
{"title": "Effects of blood in veins of dragonfly wing on the vibration characteristics", "highlights": ["The microstructures of dragonfly wing are observed by the SEM.", "Accurate three-dimensional FE model of the dragonfly forewing is developed.", "The blood in veins is considered in the analysis of the mass, moments of inertia and natural frequency/mode.", "We report the influence of the blood on the vibration characteristics of the wing for the first time."], "abstract": ["How the blood in veins of dragonfly wing affects its vibration characteristics is investigated. Based on the experimental results of the wing\u05f3s morphology and microstructures, including the veins, the membranes and the pterostigma, accurate three-dimensional finite element models of the dragonfly forewing are developed. Considering the blood in veins, the total mass, mass distribution and the moments of inertia of the wing are studied. The natural frequencies/modal shapes are analyzed when the veins are filled with and without blood, respectively. The based natural frequency of the model with blood (189", "\u00a0", "Hz) is much closer to the experimental result. Relative to bending modal shapes, the torsional ones are affected more significantly by the blood. The results in this article reveal the multi-functions of the blood in dragonfly wings and have important implications for the bionic design of flapping-wing micro air vehicles."]},
{"title": "Integration of a prognostic gene module with a drug sensitivity module to identify drugs that could be repurposed for breast cancer therapy", "highlights": ["New approach to identify suitable drugs for breast cancer therapy is proposed.", "Etoposide is found to have strong association with breast cancer outcome.", "Exemestane may be more specific to ER+ breast cancer.", "Our method can find drugs that could be repurposed for breast cancer therapy."], "abstract": ["Efficiently discovering low risk drugs is important for drug development. However, the heterogeneity in patient population complicates the prediction of the therapeutic efficiency. Drug repositioning aiming to discover new indications of known drugs provides a possible gateway.", "We introduce a novel computational method to identify suitable drugs by using prognosis information of patients. First, we identify prognostic related gene modules, Prognostic Gene Ontology Module (PGOMs), by incorporating multiple functional annotations. Then, we build the drug sensitivity modules based on gene expressions and drug activity patterns. Finally, we analyze the potential effects of drugs on prognostic gene modules and establish the links between PGOMs and drugs.", "With PGOMs generated based on the patient outcome, FDA approved drugs for breast cancer treatment have been successfully identified on one hand; several drugs that have not been approved by FDA, such as Etoposide, have found to strongly associate with the outcome on the other hand. With PGOMs generated based on the patient ER status, Tamoxifen and Exemestane rank at the top of the drug list, suggesting that they may be more specific to ER status of breast cancer. Especially, the rank difference of Exemestane in ER+ group and ER\u2212 group is very large, demonstrating that Exemestane may be more specific to ER+ breast cancer and would cause side-effect to ER\u2212 breast cancer patients. Our method can not only identify the drugs that could be repurposed for breast cancer therapy, but also can reveal their effective pharmacological mechanisms."]},
{"title": "Virtual screening for the identification of novel inhibitors of ", "highlights": ["3D structure of ", " RmlB (", "RmlB) was built by homology modeling.", "Pharmacophore models of RmlB inhibitors and RmlC inhibitors were built based on the ligand\u2013receptor complexes.", "A hybrid virtual screening approach based on drug-likeness prediction, pharmacophore and molecular docking was performed for retrieving novel inhibitors targeting RmlB and RmlC simultaneously."], "abstract": ["Tuberculosis remains one of the deadliest infectious diseases in humans. It has caused more than 100 million deaths since its discovery in 1882. Currently, more than 5 million people are infected with TB bacterium each year. The cell wall of ", " plays an important role in maintaining the ability of mycobacteria to survive in a hostile environment. Therefore, we report a virtual screening (VS) study aiming to identify novel inhibitors that simultaneously target RmlB and RmlC, which are two essential enzymes for the synthesis of the cell wall of ", "A hybrid VS method that combines drug-likeness prediction, pharmacophore modeling and molecular docking studies was used to indentify inhibitors targeting RmlB and RmlC.", "The pharmacophore models HypoB and HypoC of RmlB inhibitors and RmlC inhibitors, respectively, were developed based on ligands complexing with their corresponding receptors. In total, 20 compounds with good absorption, distribution, metabolism, excretion, and toxicity properties were carefully selected using the hybird VS method.", "We have established a hybrid VS method to discover novel inhibitors with new scaffolds. The molecular interactions of the selected potential inhibitors with the active-site residues are discussed in detail. These compounds will be further evaluated using biological activity assays and deserve consideration for further structure-activity relationship studies."]},
{"title": "Modeling and simulation of a low-grade urinary bladder carcinoma", "highlights": ["We present a model of a low-grade urinary bladder carcinoma.", "We simulate oxygen diffusion, carcinogen penetration and angiogenesis.", "We combine cellular automata with nonlinear diffusion\u2013absorption equations.", "A view is on therapy personalization by simulation of cancer progression."], "abstract": ["In this work, we present a mathematical model of the initiation and progression of a low-grade urinary bladder carcinoma. We simulate the crucial processes affecting tumor growth, such as oxygen diffusion, carcinogen penetration, and angiogenesis, within the framework of the urothelial cell dynamics. The cell dynamics are modeled using the discrete technique of cellular automata, while the continuous processes of carcinogen penetration and oxygen diffusion are described by nonlinear diffusion\u2013absorption equations. As the availability of oxygen is necessary for tumor progression, processes of oxygen transport to the tumor growth site seem most important. Our model yields a theoretical insight into the main stages of development and growth of urinary bladder carcinoma with emphasis on the two most common types: bladder polyps and carcinoma ", ". Analysis of histological structure of bladder tumor is important to avoid misdiagnosis and wrong treatment. We expect our model to be a valuable tool in the study of bladder cancer progression due to the exposure to carcinogens and the oxygen dependent expression of genes promoting tumor growth. Our numerical simulations have good qualitative agreement with ", " results reported in the corresponding medical literature."]},
{"title": "MR image super-resolution reconstruction using sparse representation, nonlocal similarity and sparse derivative prior", "highlights": ["Do MR image SR by jointly using sparsity prior, nonlocal similarity, and sparse derivative prior.", "Use multi-scale first- and second-order derivative to estimate high-frequency information.", "Use sparse derivative prior based post-processing to suppress blurring effects in MR images."], "abstract": ["In magnetic resonance (MR) imaging, image spatial resolution is determined by various instrumental limitations and physical considerations. This paper presents a new algorithm for producing a high-resolution version of a low-resolution MR image. The proposed method consists of two consecutive steps: (1) reconstructs a high-resolution MR image from a given low-resolution observation via solving a joint sparse representation and nonlocal similarity ", "1-norm minimization problem; and (2) applies a sparse derivative prior based post-processing to suppress blurring effects. Extensive experiments on simulated brain MR images and two real clinical MR image datasets validate that the proposed method achieves much better results than many state-of-the-art algorithms in terms of both quantitative measures and visual perception."]},
{"title": "Representing and extracting lung cancer study metadata: Study objective and study design", "highlights": ["We propose to improve retrieval by representing and extracting study metadata.", "Multiple expert readers produced a gold standard of 430 abstracts on lung cancer.", "Automatic classification performed better than or comparable to PubMed\u05f3s filters.", "Study design classification was robust to differences in vocabulary across corpora.", "Top-ranked features were not domain-specific and could generalize to other domains."], "abstract": ["This paper describes the information retrieval step in Casama (Contextualized Semantic Maps), a project that summarizes and contextualizes current research papers on driver mutations in non-small cell lung cancer. Casama\u05f3s representation of lung cancer studies aims to capture elements that will assist an end-user in retrieving studies and, importantly, judging their strength. This paper focuses on two types of study metadata: study objective and study design. 430 abstracts on EGFR and ALK mutations in lung cancer were annotated manually. Casama\u05f3s support vector machine (SVM) automatically classified the abstracts by study objective with as much as 129% higher ", "-scores compared to PubMed\u05f3s built-in filters. A second SVM classified the abstracts by epidemiological study design, suggesting strength of evidence at a more granular level than in previous work. The classification results and the top features determined by the classifiers suggest that this scheme would be generalizable to other mutations in lung cancer, as well as studies on driver mutations in other cancer domains."]},
{"title": "Predicting conversion from MCI to AD with FDG-PET brain images at different prodromal stages", "highlights": ["We study the prediction of MCI to AD conversion using FDG-PET and several classifiers.", "We study how the disease stage impacts diagnostic performance.", "Results show a decrease in performance with the temporal distance to conversion.", "We achieve an accuracy of 85.1% at the time of conversion and 75% 24 months before it."], "abstract": ["Early diagnosis of Alzheimer disease (AD), while still at the stage known as mild cognitive impairment (MCI), is important for the development of new treatments. However, brain degeneration in MCI evolves with time and differs from patient to patient, making early diagnosis a very challenging task. Despite these difficulties, many machine learning techniques have already been used for the diagnosis of MCI and for predicting MCI to AD conversion, but the MCI group used in previous works is usually very heterogeneous containing subjects at different stages. The goal of this paper is to investigate how the disease stage impacts on the ability of machine learning methodologies to predict conversion. After identifying the converters and estimating the time of conversion (TC) (using neuropsychological test scores), we devised 5 subgroups of MCI converters (MCI-C) based on their temporal distance to the conversion instant (0, 6, 12, 18 and 24 months before conversion). Next, we used the FDG-PET images of these subgroups and trained classifiers to distinguish between the MCI-C at different stages and stable non-converters (MCI-NC). Our results show that MCI to AD conversion can be predicted as early as 24 months prior to conversion and that the discriminative power of the machine learning methods decreases with the increasing temporal distance to the TC, as expected. These findings were consistent for all the tested classifiers. Our results also show that this decrease arises from a reduction in the information contained in the regions used for classification and by a decrease in the stability of the automatic selection procedure."]},
{"title": "Theoretical estimation of retinal oxygenation in chronic diabetic retinopathy", "highlights": ["A model for retinal oxygenation estimates ischemia associated with diabetes.", "The model employs diffusion, saturable consumption, and a nonlinear transport.", "The model\u2019s results reinforces the need for tighter glycemic control.", "A new extension includes vasodilation as a feedback response to ischemia."], "abstract": ["This paper uses computer modeling to estimate the progressive decline in oxygenation that occurs in the human diabetic retina after years of slowly progressive ischemic insult. An established model combines diffusion, saturable consumption, and blood capillary sources to determine the oxygen distribution across the retina. Incorporating long-term degradation of blood supply from the retinal capillaries into the model yields insight into the effects of progressive ischemia associated with prolonged hyperglycemia, suggesting time-scales over which therapeutic mitigation could have beneficial effect. A new extension of the model for oxygen distribution introduces a feedback mechanism for vasodilation and its potential to prolong healthy retinal function."]},
{"title": "The role of real-time in biomedical science: A meta-analysis on computational complexity, delay and speedup", "highlights": ["Real-time: The promise of performance in real world scenarios.", "In this review we have discussed 120 real-time processing papers in the biomedical engineering field.", "The majority of papers lacks appropriate measures to support the real-time claim.", "An appropriate measure for theoretical claims on real-time is computational complexity.", "An appropriate measure for practical claims on real-time is speedup."], "abstract": [" The concept of real-time is very important, as it deals with the realizability of computer based health care systems.", " In this paper we review biomedical real-time systems with a meta-analysis on computational complexity (CC), delay (", ") and speedup (", ").", " During the review we found that, in the majority of papers, the term real-time is part of the thesis indicating that a proposed system or algorithm is practical. However, these papers were not considered for detailed scrutiny. Our detailed analysis focused on papers which support their claim of achieving real-time, with a discussion on CC or ", ". These papers were analyzed in terms of processing system used, application area (AA), CC, ", ", ", ", implementation/algorithm (I/A) and competition.", " The results show that the ideas of parallel processing and algorithm delay were only recently introduced and journal papers focus more on Algorithm (A) development than on implementation (I). Most authors compete on big ", " notation (", ") and processing time (", "). Based on these results, we adopt the position that the concept of real-time will continue to play an important role in biomedical systems design. We predict that parallel processing considerations, such as ", " and algorithm scaling, will become more important."]},
{"title": "Influence of computer work under time pressure on cardiac activity", "highlights": ["Computer work stress under time pressure and its effect on the cardiac activity were investigated.", "Heightened vagal tone was noted in the relaxation computer work without time pressure.", "Stressful computer tasks caused decreased HRV and activated sympathetic activity.", "Poincar\u00e9 plot analysis outperforms power spectral analysis."], "abstract": ["Computer users are often under stress when required to complete computer work within a required time. Work stress has repeatedly been associated with an increased risk for cardiovascular disease. The present study examined the effects of time pressure workload during computer tasks on cardiac activity in 20 healthy subjects. Heart rate, time domain and frequency domain indices of heart rate variability (HRV) and Poincar\u00e9 plot parameters were compared among five computer tasks and two rest periods. Faster heart rate and decreased standard deviation of ", "\u2013", " interval were noted in response to computer tasks under time pressure. The Poincar\u00e9 plot parameters showed significant differences between different levels of time pressure workload during computer tasks, and between computer tasks and the rest periods. In contrast, no significant differences were identified for the frequency domain indices of HRV. The results suggest that the quantitative Poincar\u00e9 plot analysis used in this study was able to reveal the intrinsic nonlinear nature of the autonomically regulated cardiac rhythm. Specifically, heightened vagal tone occurred during the relaxation computer tasks without time pressure. In contrast, the stressful computer tasks with added time pressure stimulated cardiac sympathetic activity."]},
{"title": "Quantitative assessment of responses of the eyeball based on data from the Corvis tonometer", "highlights": ["The proposed algorithm enables to determine responses of the eyeball to an air puff.", "Responses of the eyeball can be linked to some features of corneal deformation.", "Using profiled algorithm enables to measure additional parameters from the Corvis."], "abstract": ["The \u201cair-puff\u201d tonometers, include the Corvis, are a type of device for measuring intraocular pressure and biomechanics parameters. The paper attempts to analyse this response and its relationship with other parameters measured in the Corvis tonometer.", "A number of 13,400 2D images were acquired from the Corvis device and analysed (32 healthy and 16 ill people). A new method has been proposed for the analysis of responses of the eyeball based on morphological transformations and contextual operations.", "The proposed algorithm enables to determine responses of the eyeball to an air puff coming from the Corvis tonometer. Additionally, responses of the eyeball have been linked to some selected features of corneal deformation. The results include, among others: (1) distinguishability between the left and right eye with an error of 7%; (2) the correlation between the area under the curve in corneal deformation and the response of the eyeball \u22120.26; (3) the correlation between the highest concavity time and the maximum deformation amplitude of 0.4. All these features are obtained fully automatically and repetitively at a time of 3.8", "\u00a0", "s per patient (Core i7 10", "\u00a0", "GB RAM).", "It is possible to measure additional parameters of the eye deformation which are not available in the original software of the Corvis tonometer. The use of the proposed methods of image analysis and processing provides results directly from the eye response measurement when measuring intraocular pressure."]},
{"title": "Development and validation of an open source quantification tool for DSC-MRI studies", "highlights": ["We have developed an open source tool for DSC-MRI quantification.", "The software runs as a plugin for the multiplatform public domain software ImageJ.", "It contains an open API that allows external users to implement new methods.", "It was validated, with good results, against a clinical tool, Philips IntelliSpace."], "abstract": ["This work presents the development of an open source tool for the quantification of dynamic susceptibility-weighted contrast-enhanced (DSC) perfusion studies. The development of this tool is motivated by the lack of open source tools implemented on open platforms to allow external developers to implement their own quantification methods easily and without the need of paying for a development license.", "This quantification tool was developed as a plugin for the ImageJ image analysis platform using the Java programming language. A modular approach was used in the implementation of the components, in such a way that the addition of new methods can be done without breaking any of the existing functionalities. For the validation process, images from seven patients with brain tumors were acquired and quantified with the presented tool and with a widely used clinical software package. The resulting perfusion parameters were then compared.", "Perfusion parameters and the corresponding parametric images were obtained. When no gamma-fitting is used, an excellent agreement with the tool used as a gold-standard was obtained (", ">0.8 and values are within 95% CI limits in Bland\u2013Altman plots).", "An open source tool that performs quantification of perfusion studies using magnetic resonance imaging has been developed and validated using a clinical software package. It works as an ImageJ plugin and the source code has been published with an open source license."]},
{"title": "Maximizing clinical cohort size using free text queries", "highlights": ["We demonstrate the value of free text queries in cohort identification.", "Incremental value is added compared to structured data queries alone.", "We determine the value of free text using 3 disparate use cases.", "Use case specific values and limitations are identified in large data sets.", "Exploratory value of a direct search tool in contrast with heavier NLP systems."], "abstract": ["Cohort identification is important in both population health management and research. In this project we sought to assess the use of text queries for cohort identification. Specifically we sought to determine the incremental value of unstructured data queries when added to structured queries for the purpose of patient cohort identification.", "Three cohort identification tasks were evaluated: identification of individuals taking gingko biloba and warfarin simultaneously (Gingko/Warfarin), individuals who were overweight, and individuals with uncontrolled diabetes (UCD). We assessed the increase in cohort size when unstructured data queries were added to structured data queries. The positive predictive value of unstructured data queries was assessed by manual chart review of a random sample of 500 patients.", "For Gingko/Warfarin, text query increased the cohort size from 9 to 28,924 over the cohort identified by query of pharmacy data only. For the weight-related tasks, text search increased the cohort by 5\u201329% compared to the cohort identified by query of the vitals table. For the UCD task, text query increased the cohort size by 2\u201343% compared to the cohort identified by query of laboratory results or ICD codes. The positive predictive values for text searches were 52% for Gingko/Warfarin, 19\u201394% for the weight cohort and 44% for UCD.", "This project demonstrates the value and limitation of free text queries in patient cohort identification from large data sets. The clinical domain and prevalence of the inclusion and exclusion criteria in the patient population influence the utility and yield of this approach."]},
{"title": "Monitoring eating habits using a piezoelectric sensor-based necklace", "highlights": ["Piezoelectric sensors in the lower trachea are viable for detection of food intake.", "Accelerometers for activity recognition can reduce the false positive rate of detected swallows.", "Classification can be performed between several food types using Bayes classifiers."], "abstract": ["Maintaining appropriate levels of food intake and developing regularity in eating habits is crucial to weight loss and the preservation of a healthy lifestyle. Moreover, awareness of eating habits is an important step towards portion control and weight loss. In this paper, we introduce a novel food-intake monitoring system based around a wearable wireless-enabled necklace.", "The proposed necklace includes an embedded piezoelectric sensor, small Arduino-compatible microcontroller, Bluetooth LE transceiver, and Lithium-Polymer battery. Motion in the throat is captured and transmitted to a mobile application for processing and user guidance. Results from data collected from 30 subjects indicate that it is possible to detect solid and liquid foods, with an ", "-measure of 0.837 and 0.864, respectively, using a naive Bayes classifier. Furthermore, identification of extraneous motions such as head turns and walking are shown to significantly reduce the false positive rate of swallow detection."]},
{"title": "A comparative analysis of DBSCAN, K-means, and quadratic variation algorithms for automatic identification of swallows from swallowing accelerometry signals", "highlights": ["Three swallowing segmentation algorithms were compared.", "The new DBSCAN algorithm can be used to segment swallowing vibrations.", "The DBSCAN algorithm is outright superior to the k-means algorithm.", "The DBSCAN algorithm has similar sensitivity to the quadratic variation algorithm.", "The DBSCAN algorithm is the fastest and most easily applied to general data."], "abstract": [": Cervical auscultation with high resolution sensors is currently under consideration as a method of automatically screening for specific swallowing abnormalities. To be clinically useful without human involvement, any devices based on cervical auscultation should be able to detect specified swallowing events in an automatic manner.", ": In this paper, we comparatively analyze the density-based spatial clustering of applications with noise algorithm (DBSCAN), a k-means based algorithm, and an algorithm based on quadratic variation as methods of differentiating periods of swallowing activity from periods of time without swallows. These algorithms utilized swallowing vibration data exclusively and compared the results to a gold standard measure of swallowing duration. Data was collected from 23 subjects that were actively suffering from swallowing difficulties.", ": Comparing the performance of the DBSCAN algorithm with a proven segmentation algorithm that utilizes k-means clustering demonstrated that the DBSCAN algorithm had a higher sensitivity and correctly segmented more swallows. Comparing its performance with a threshold-based algorithm that utilized the quadratic variation of the signal showed that the DBSCAN algorithm offered no direct increase in performance. However, it offered several other benefits including a faster run time and more consistent performance between patients. All algorithms showed noticeable differentiation from the endpoints provided by a videofluoroscopy examination as well as reduced sensitivity.", ": In summary, we showed that the DBSCAN algorithm is a viable method for detecting the occurrence of a swallowing event using cervical auscultation signals, but significant work must be done to improve its performance before it can be implemented in an unsupervised manner."]},
{"title": "BtoxDB: A comprehensive database of protein structural data on toxin\u2013antitoxin systems", "highlights": ["We developed a web-based system named BtoxDB.", "BtoxDB provides an interface for all available protein structural data on TA systems.", "BtoxDB provides three modules: \u201cSearch\u201d, \u201cBrowse\u201d and \u201cStatistics\u201d.", "BtoxDB allows refined search and friendly acquisition of contents."], "abstract": ["Toxin\u2013antitoxin (TA) systems are diverse and abundant genetic modules in prokaryotic cells that are typically formed by two genes encoding a stable toxin and a labile antitoxin. Because TA systems are able to repress growth or kill cells and are considered to be important actors in cell persistence (multidrug resistance without genetic change), these modules are considered potential targets for alternative drug design. In this scenario, structural information for the proteins in these systems is highly valuable. In this report, we describe the development of a web-based system, named BtoxDB, that stores all protein structural data on TA systems.", "The BtoxDB database was implemented as a MySQL relational database using PHP scripting language. Web interfaces were developed using HTML, CSS and JavaScript. The data were collected from the PDB, UniProt and Entrez databases. These data were appropriately filtered using specialized literature and our previous knowledge about toxin\u2013antitoxin systems.", "The database provides three modules (\u201cSearch\u201d, \u201cBrowse\u201d and \u201cStatistics\u201d) that enable searches, acquisition of contents and access to statistical data. Direct links to matching external databases are also available.", "The compilation of all protein structural data on TA systems in one platform is highly useful for researchers interested in this content. BtoxDB is publicly available at ", "."]},
{"title": "Influence of sperm impact angle on successful fertilization through mZP oscillatory spherical net model", "highlights": ["An oscillatory model of spermatozoa\u2013oocyte interaction is proposed.", "By using generalized Lussajous curves, frequency analysis of mZP oscillations was done.", "Oscillations of mZP are influenced by sperm impact angles.", "Very small/very large impact angles are unfavorable for successful fertilization.", "Oblique angles are more fertilization favorable and their values have been predicted."], "abstract": ["According to the available literature, penetrating sperm creates an oblique path trough Zona pellucida (ZP) \u2013 the most outer surface of oocytes. Considering fertilization process as an oscillatory phenomenon, the influence of sperm impact angle relative to the oscillatory behavior of mouse ZP is described by using the discrete continuum mechanical model in the form of a spherical net model.", "A parametric frequency analysis of oscillatory behavior of knot material particles in the mouse ZP (mZP) spherical net model is conducted by using generalized Lussajous curves. The influence of impact angles of sperm cells on the corresponding knot mass particles\u2019 resultant trajectory is discussed. Favorable sperm impact angles for successful fertilization are identified."]},
{"title": "Quasi-supervised scoring of human sleep in polysomnograms using augmented input variables", "highlights": ["Three common problems associated with clinical sleep scoring are identified.", "A quasi-supervised classification framework is proposed to address these problems.", "The proposed framework compensates for rater uncertainty with reasonable accuracy."], "abstract": ["The limitations of manual sleep scoring make computerized methods highly desirable. Scoring errors can arise from human rater uncertainty or inter-rater variability. Sleep scoring algorithms either come as supervised classifiers that need scored samples of each state to be trained, or as unsupervised classifiers that use heuristics or structural clues in unscored data to define states. We propose a quasi-supervised classifier that models observations in an unsupervised manner but mimics a human rater wherever training scores are available. EEG, EMG, and EOG features were extracted in 30", "\u00a0", "s epochs from human-scored polysomnograms recorded from 42 healthy human subjects (18\u201379 years) and archived in an anonymized, publicly accessible database. Hypnograms were modified so that: 1. Some states are scored but not others; 2. Samples of all states are scored but not for transitional epochs; and 3. Two raters with 67% agreement are simulated. A framework for quasi-supervised classification was devised in which unsupervised statistical models\u2014specifically Gaussian mixtures and hidden Markov models\u2014are estimated from unlabeled training data, but the training samples are augmented with variables whose values depend on available scores. Classifiers were fitted to signal features incorporating partial scores, and used to predict scores for complete recordings. Performance was assessed using Cohen\u05f3s ", " statistic. The quasi-supervised classifier performed significantly better than an unsupervised model and sometimes as well as a completely supervised model despite receiving only partial scores. The quasi-supervised algorithm addresses the need for classifiers that mimic scoring patterns of human raters while compensating for their limitations."]},
{"title": "Assessing diagnostic complexity: An image feature-based strategy to reduce annotation costs", "highlights": ["Defined a metric for measuring the consensus difficulty in CT scan diagnosis.", "Developed a difficulty-predictive model, based solely on low-level image features.", "Reviewed most significant low-level image features for predicting case difficulty.", "Presented a radiologist assignment approach for reducing classification cost."], "abstract": ["Computer-aided diagnosis systems can play an important role in lowering the workload of clinical radiologists and reducing costs by automatically analyzing vast amounts of image data and providing meaningful and timely insights during the decision making process. In this paper, we present strategies on how to better manage the limited time of clinical radiologists in conjunction with predictive model diagnosis. We first introduce a metric for discriminating between the different categories of diagnostic complexity (such as easy versus hard) encountered when interpreting CT scans. Second, we propose to learn the diagnostic complexity using a classification approach based on low-level image features automatically extracted from pixel data. We then show how this classification can be used to decide how to best allocate additional radiologists to interpret a case based on its diagnosis category. Using a lung nodule image dataset, we determined that, by a simple division of cases into hard and easy to diagnose, the number of interpretations can be distributed to significantly lower the cost with limited loss in prediction accuracy. Furthermore, we show that with just a few low-level image features (18% of the original set) we are able to determine the easy from hard cases for a significant subset (66%) of the lung nodule image data."]},
{"title": "New real-time heartbeat detection method using the angle of a single-lead electrocardiogram", "highlights": ["This study presents a new real-time heartbeat detection algorithm using the geometric angle.", "The angle was adopted as a new index representing the slope of ECG signal.", "The proposed method shows superior or comparable performance than previous works.", "It could be effectively applied in real-time healthcare and medical devices."], "abstract": ["This study presents a new real-time heartbeat detection algorithm using the geometric angle between two consecutive samples of single-lead electrocardiogram (ECG) signals. The angle was adopted as a new index representing the slope of ECG signal. The method consists of three steps: elimination of high-frequency noise, calculation of the angle of ECG signal, and detection of R-waves using a simple adaptive thresholding technique. The MIT-BIH arrhythmia database, QT database, European ST-T database, T-wave alternans database and synthesized ECG signals were used to evaluate the performance of the proposed algorithm and compare with the results of other methods suggested in literature. The proposed method shows a high detection rate\u201499.95% of the sensitivity, 99.95% of the positive predictivity, and 0.10% of the fail detection rate on the four databases. The result shows that the proposed method can yield better or comparable performance than other literature despite the relatively simple process. The proposed algorithm needs only a single-lead ECG, and involves a simple and quick calculation. Moreover, it does not require post-processing to enhance the detection. Thus, it can be effectively applied to various real-time healthcare and medical devices."]},
{"title": "Active surface model improvement by energy function optimization for 3D segmentation", "highlights": ["We improve DAS in different parts to enhanced overall results of algorithms.", "Two different external energies are presented.", "As the first external energy, we use gradient of the wavelet edge map.", "We use phase coherence of wavelet image near each pixel as second external energy.", "We apply our method on CT images of different organs to validate their performance."], "abstract": ["This paper proposes an optimized and efficient active surface model by improving the energy functions, searching method, neighborhood definition and resampling criterion. Extracting an accurate surface of the desired object from a number of 3D images using active surface and deformable models plays an important role in computer vision especially medical image processing. Different powerful segmentation algorithms have been suggested to address the limitations associated with the model initialization, poor convergence to surface concavities and slow convergence rate. This paper proposes a method to improve one of the strongest and recent segmentation algorithms, namely the Decoupled Active Surface (DAS) method. We consider a gradient of wavelet edge extracted image and local phase coherence as external energy to extract more information from images and we use curvature integral as internal energy to focus on high curvature region extraction. Similarly, we use resampling of points and a line search for point selection to improve the accuracy of the algorithm. We further employ an estimation of the desired object as an initialization for the active surface model. A number of tests and experiments have been done and the results show the improvements with regards to the extracted surface accuracy and computational time of the presented algorithm compared with the best and recent active surface models."]},
{"title": "Empirical mode decomposition analysis of near-infrared spectroscopy muscular signals to assess the effect of physical activity in type 2 diabetic patients", "highlights": ["Empirical mode decomposition applied to near-infrared spectroscopy signals.", "Adapted physical therapy and fit walking as therapy for diabetic patients.", "Physical therapy normalizes the metabolic muscular pattern of diabetic patients."], "abstract": ["Type 2 diabetes is a metabolic disorder that may cause major problems to several physiological systems. Exercise has proven to be very effective in the prevention, management and improvement of this pathology in patients. Muscle metabolism is often studied with near-infrared spectroscopy (NIRS), a noninvasive technique that can measure changes in the concentration of oxygenated (O2Hb) and reduced hemoglobin (HHb) of tissues. These NIRS signals are highly non-stationary, non-Gaussian and nonlinear in nature.", "The empirical mode decomposition (EMD) is used as a nonlinear adaptive model to extract information present in the NIRS signals. NIRS signals acquired from the tibialis anterior muscle of controls and type 2 diabetic patients are processed by EMD to yield three intrinsic mode functions (IMF). The sample entropy (SE), fractal dimension (FD), and Hurst exponent (HE) are computed from these IMFs. Subjects are monitored at the beginning of the study and after one year of a physical training programme.", "Following the exercise programme, we observed an increase in the SE and FD and a decrease in the HE in all diabetic subjects. Our results show the influence of physical exercise program in improving muscle performance and muscle drive by the central nervous system in the patients. A multivariate analysis of variance performed at the end of the training programme also indicated that the NIRS metabolic patterns of controls and diabetic subjects are more similar than at the beginning of the study.", "Hence, the proposed EMD technique applied to NIRS signals may be very useful to gain a non-invasive understanding of the neuromuscular and vascular impairment in diabetic subjects."]},
{"title": "A two-step automatic sleep stage classification method with dubious range detection", "highlights": ["A two-step classifier for automatic sleep staging is proposed.", "The system provides two outputs: non-dubious and dubious classification.", "The dubious epochs are tagged and re-assigned according to a post-processing step.", "The system indicates to an expert physician which results need revision.", "The accuracy of non-dubious classification for wake and REM is around 97%."], "abstract": ["The limitations of the current systems of automatic sleep stage classification (ASSC) are essentially related to the similarities between epochs from different sleep stages and the subjects\u05f3 variability. Several studies have already identified the situations with the highest likelihood of misclassification in sleep scoring. Here, we took advantage of such information to develop an ASSC system based on knowledge of subjects\u05f3 variability of some indicators that characterize sleep stages and on the American Academy of Sleep Medicine (AASM) rules.", "An ASSC system consisting of a two-step classifier is proposed. In the first step, epochs are classified using support vector machines (SVMs) spread into different nodes of a decision tree. In the post-processing step, the epochs suspected of misclassification (dubious classification) are tagged, and a new classification is suggested. Identification and correction are based on the AASM rules, and on misclassifications most commonly found/reported in automatic sleep staging. Six electroencephalographic and two electrooculographic channels were used to classify wake, non-rapid eye movement (NREM) sleep \u2013 N1, N2 and N3, and rapid eye movement (REM) sleep.", "The proposed system was tested in a dataset of 14 clinical polysomnographic records of subjects suspected of apnea disorders. Wake and REM epochs not falling in the dubious range, are classified with accuracy levels compatible with the requirements for clinical applications. The suggested correction assigned to the epochs that are tagged as dubious enhances the global results of all sleep stages.", "This approach provides reliable sleep staging results for non-dubious epochs."]},
{"title": "Detecting hypovolemia in postoperative patients using a discrete Fourier transform", "highlights": ["Arterial blood pressure graphs of hypovolemic patients were analysed using a discrete Fourier transform.", "The amplitude of the first harmonic was significantly higher in normovolemic cases than in hypovolemic ones.", "Using this variable, additional patients were categorized into hypovolemic and normovolemic groups using logistic regression.", "The prediction was correct in 80% for hypovolemic patients and wrong in 20%."], "abstract": ["In the present paper, an attempt was made to find waveform-derived variables that would be useful for a more precise diagnosis of hypovolemia. In attempting this, arterial blood pressure graphs of 18 hypovolemic postoperative patients were analysed using a discrete Fourier transform. Using a paired samples t-test, the amplitude of the first harmonic (A1) is shown to be significantly higher in normovolemic cases than in hypovolemic ones (p<0.001). Based on the values of A1, a preliminary study was performed in which an additional group of 14 hypovolemic and 14 normovolemic patients were categorized into hypovolemic and normovolemic groups using logistic regression. The method proved to be successful in identifying hypovolemic patients: the prediction was correct in 80% and wrong only in 20%, indicating that A1 is potentially a useful parameter in detecting hypovolemia."]},
{"title": "Modeling of human artery tissue with probabilistic approach", "highlights": ["Probabilistic approach is used to model the inhomogeneity of human artery tissue.", "Tissue properties are represented by a statistical function with normal distribution.", "Mean value of the material parameters are identified using genetic algorithm.", "Empirical 3-sigma rule is used for reliability study of the statistical model.", "The statistical model represents the human artery properties accurately."], "abstract": ["Accurate modeling of biological soft tissue properties is vital for realistic medical simulation. Mechanical response of biological soft tissue always exhibits a strong variability due to the complex microstructure and different loading conditions. The inhomogeneity in human artery tissue is modeled with a computational probabilistic approach by assuming that the instantaneous stress at a specific strain varies according to normal distribution. Material parameters of the artery tissue which are modeled with a combined logarithmic and polynomial energy equation are represented by a statistical function with normal distribution. Mean and standard deviation of the material parameters are determined using genetic algorithm (GA) and inverse mean-value first-order second-moment (IMVFOSM) method, respectively. This nondeterministic approach was verified using computer simulation based on the Monte-Carlo (MC) method. Cumulative distribution function (CDF) of the MC simulation corresponds well with that of the experimental stress\u2013strain data and the probabilistic approach is further validated using data from other studies. By taking into account the inhomogeneous mechanical properties of human biological tissue, the proposed method is suitable for realistic virtual simulation as well as an accurate computational approach for medical device validation."]},
{"title": "Reducing dimensionality in remote homology detection using predicted contact maps", "highlights": ["Predicted contact maps can be used to detect remote homology.", "The dimensionality of the protein representation can be reduced.", "A new discriminative strategy to detect remote homology is proposed.", "We model every protein family using diverse classification techniques."], "abstract": ["In this paper, a new method for remote protein homology detection is presented. Most discriminative methods concatenate the values extracted from physicochemical properties to build a model that separates homolog and non-homolog examples. Each discriminative method uses a specific strategy to represent the information extracted from the protein sequence and a different number of indices. After the vector representation is achieved, support vector machines (SVM) are usually used. Most classification techniques are not suitable in remote homology detection because they do not address high dimensional datasets. In this paper, we propose a method that reduces the high dimensionality of the vector representation using models that are defined at the 3D level. Next, the models are mapped from the protein primary sequence. The new method, called remote-C3D, is presented and tested on the SCOP 1.53 and SCOP 1.55 datasets. The remote-C3D method achieves a higher accuracy than the composition-based methods and a comparable performance with profile-based methods."]},
{"title": "Finite element analysis of uncommonly large renal arteriovenous malformation\u2014Adjacent renal cyst complex", "highlights": ["This study investigate hemodynamics of RAVM and corresponding clinical implications.", "A case of a 50-year-old patient with a large RAVM and adjacent renal cyst was studied.", "Calculated physical quantities were velocity, pressure, shear stress and drag forces.", "Unstable hemodynamics of RAVM made attempts for embolization risky and unsuccessful.", "The computer methods could be used to overcome current clinical diagnostic limits."], "abstract": ["Renal arteriovenous malformation (RAVM) represents abnormal communication between the intrarenal arterial and venous system. The purpose of this study was to investigate hemodynamics and biomechanics quantities which may influence the instability of RAVM and imply clinical complications.", "A detailed 3D reconstruction of RAVM was obtained from the patient CT scans, aortic inlet flow was measured by color-flow Doppler ultrasound, while material characteristics were adopted from the literature. A numerical finite element analysis (FEA) of the blood flow was performed by solving the governing equations for the viscous incompressible flow. The physical quantities calculated at the systolic and diastolic peak moment were velocity, pressure, shear stress and drag forces.", "We reported a case of a 50-year-old patient with a large RAVM and adjacent renal cyst, who unsuccessfully underwent two attempts of embolization that resulted in the consequent nephrectomy. FEA showed that the cyst had a very low pressure intensity and velocity field (with unstable flow in diastolic peak). For both systolic and diastolic moments, increased values of wall shear stress were found on the places with intensive wall calcification. Unusually high values of drag force which would likely explain the presence of pressure in the cystic formation were found on the infero-medial side where the cyst wall was the thinnest and where the flow streamlines converged.", "FEA showed that the hemodynamics of the cyst-RAVM complex was unstable making it prone to rupture. Clinically established diagnosis of imminent rupture together with unfavorable hemodynamics of the lesion consequently made additional attempts of embolization risky and unsuccessful leading to total nephrectomy."]},
{"title": "New feature selection for gene expression classification based on degree of class overlap in principal dimensions", "highlights": ["A feature selection based on degree of class overlap is proposed.", "Sorting the degrees of overlap for selecting the candidate dimensions.", "The minimum degree of overlap must be used as the relevant features.", "Using forward feature selection with SVM classification.", "The PMDO method selected fewer candidate features."], "abstract": ["Micro-array data are typically characterized by high dimensional features with a small number of samples. Several problems in identifying genes causing diseases from micro-array data can be transformed into the problem of classifying the features extracted from gene expression in micro-array data. However, too many features can cause low prediction accuracy as well as high computational complexity. Dimensional reduction is a method to eliminate irrelevant features to improve the prediction accuracy. Typically, the eigenvalues or dimensional data variance from principal component analysis are used as criteria to select relevant features. This approach is simple but not efficient since it does not concern the degree of data overlap in each dimension in the feature space. A new method to select relevant features based on degree of dimensional data overlap with proper feature selection was introduced. Furthermore, our study concentrated on small sized data sets which usually occur in reality. The experimental results signified that this new approach can achieve substantially higher prediction accuracy when compared with other methods."]},
{"title": "Low-complexity detection of atrial fibrillation in continuous long-term monitoring", "highlights": ["This study describes an atrial fibrillation detector for implementation in a battery-powered device.", "The detector is suitable for detection of brief atrial fibrillation episodes.", "The detector performs better on the MIT\u2013BIH Atrial Fibrillation database than do existing detectors.", "The detector can be implemented with just a few arithmetical operations."], "abstract": ["This study describes an atrial fibrillation (AF) detector whose structure is well-adapted both for detection of subclinical AF episodes and for implementation in a battery-powered device for use in continuous long-term monitoring applications. A key aspect for achieving these two properties is the use of an 8-beat sliding window, which thus is much shorter than the 128-beat window used in most existing AF detectors. The building blocks of the proposed detector include ectopic beat filtering, bigeminal suppression, characterization of RR interval irregularity, and signal fusion. With one design parameter, the performance can be tuned to put more emphasis on avoiding false alarms due to non-AF arrhythmias or more emphasis on detecting brief AF episodes. Despite its very simple structure, the results show that the detector performs better on the MIT\u2013BIH Atrial Fibrillation database than do existing detectors, with high sensitivity and specificity (97.1% and 98.3%, respectively). The detector can be implemented with just a few arithmetical operations and does not require a large memory buffer thanks to the short window."]},
{"title": "Respiratory rate extraction from single-lead ECG using homomorphic filtering", "highlights": ["A new technique for the EDR extraction using homomorphic filtering is proposed.", "The performance of proposed EDR technique depends on selection of filter type.", "Results of proposed scheme are significantly better than R Peak amplitude algorithm."], "abstract": ["In this paper a new technique for the extraction of respiratory signal from the single-lead ECG using generalized homomorphic filtering is presented. It is proposed to perform band pass filtering on the cepstrum of the ECG signal to extract the respiratory signal. For this study, transforms used in generalized homomorphic filtering are the discrete Fourier transform (DFT) and the discrete cosine transform (DCT). The performance of the ECG-derived respiration (EDR) signal obtained using the proposed method is compared with the reference respiratory signal in terms of the correlation, magnitude squared coherence coefficients and breath rate accuracy. It is observed from the comparisons that the EDR technique based on generalized homomorphic filtering using DFT performs better than the homomorphic filtering using DCT. The proposed EDR technique is also compared with the two well-known EDR techniques: principal component analysis and R peak amplitude algorithm. It is seen from the results that the proposed EDR technique (", ") performs significantly better than the R peak amplitude algorithm, but significant improvements are not observed when compared with the PCA based EDR technique."]},
{"title": "No-reference hair occlusion assessment for dermoscopy images based on distribution feature", "highlights": ["A novel local adaptive hair detection method is presented.", "The proposed hair detection method works well on both sparse hair and dense hair.", "Hair distribution features based on quantity, position and dispersion are extracted.", "An objective assessment metric for the degree of hair occlusion is designed.", "Our assessment method can effectively evaluate the degree of hair occlusion."], "abstract": ["The presence of hair is a common quality problem for dermoscopy images, which may influence the accuracy of lesion analysis. In this paper, a novel no-reference hair occlusion assessment method is proposed according to the distribution feature of hairs in the dermoscopy image. Firstly, the image is adaptively enhanced by simple linear iterative clustering (SLIC) combined with isotropic nonlinear filtering (INF). Then, hairs are extracted from the image by an automatic threshold and meanwhile the postprocessing is used to refine the hair through re-extracting omissive hairs and filtering false hairs. Finally, the degree of hair occlusion is evaluated by an objective metric based on the hair distribution. A series of experiments was carried out on both simulated images and real images. The result shows that the proposed local adaptive hair detection method can work well on both sparse hair and dense hair, and the designed metric can effectively evaluate the degree of hair occlusion."]},
{"title": "Evaluation of bile reflux in HIDA images based on fluid mechanics", "highlights": ["We used optical flow to ascertain the direction and velocity of bile flow to help physicians diagnose bile reflux exact.", "We combined image processing technology and hydrodynamic models to quantify bile reflux.", "We reconstructed 3D images from SPECT and CT to help physicians diagnosis."], "abstract": ["We propose a new method to help physicians assess, using a hepatobiliary iminodiacetic acid scan image, whether or not there is bile reflux into the stomach. The degree of bile reflux is an important index for clinical diagnosis of stomach diseases. The proposed method applies image-processing technology combined with a hydrodynamic model to determine the extent of bile reflux or whether the duodenum is also folded above the stomach. This condition in 2D dynamic images suggests that bile refluxes into the stomach, when endoscopy shows no bile reflux. In this study, we used optical flow to analyze images from Tc99m-diisopropyl iminodiacetic acid cholescintigraphy (Tc99m-DISIDA) to ascertain the direction and velocity of bile passing through the pylorus. In clinical diagnoses, single photon emission computed tomography (SPECT) is the main clinical tool for evaluating functional images of hepatobiliary metabolism. Computed tomography (CT) shows anatomical images of the external contours of the stomach, liver, and biliary extent. By exploiting the functional fusion of the two kinds of medical image, physicians can obtain a more accurate diagnosis. We accordingly reconstructed 3D images from SPECT and CT to help physicians choose which cross sections to fuse with software and to help them more accurately diagnose the extent and quantity of bile reflux."]},
{"title": "Classification of laryngeal disorders based on shape and vascular defects of vocal folds", "highlights": ["Vocal folds on videolaryngostroboscopy images are detected by HOG for examination.", "Vocal fold images are classified into five laryngeal disorder types.", "We exploit shape and vascular features of vocal folds for classification.", "An average classification success rate of 81% is achived.", "We demonstrate that visible vessels of vocal folds can act as a prognostic marker."], "abstract": ["Vocal fold disorders such as laryngitis, vocal nodules, and vocal polyps may cause hoarseness, breathing and swallowing difficulties due to vocal fold malfunction. Despite the fact that state of the art medical imaging techniques help physicians to obtain more detailed information, difficulty in differentiating minor anomalies of vocal folds encourages physicians to research new strategies and technologies to aid the diagnostic process. Recent studies on vocal fold disorders note the potential role of the vascular structure of vocal folds in differential diagnosis of anomalies. However, standards of clinical usage of the blood vessels have not been well established yet due to the lack of objective and comprehensive evaluation of the vascular structure.", "In this paper, we present a novel approach that categorizes vocal folds into healthy, nodule, polyp, sulcus vocalis, and laryngitis classes exploiting visible blood vessels on the superior surface of vocal folds and shapes of vocal fold edges by using image processing techniques and machine learning methods. We first detected the vocal folds on videolaryngostroboscopy images by using Histogram of Oriented Gradients (HOG) descriptors. Then we examined the shape of vocal fold edges in order to provide features such as size and splay portion of mass lesions. We developed a new vessel centerline extraction procedure that is specialized to the vascular structure of vocal folds. Extracted vessel centerlines were evaluated in order to get vascular features of vocal folds, such as the amount of vessels in the longitudinal and transverse form. During the last step, categorization of vocal folds was performed by a novel binary decision tree architecture, which evaluates features of the vocal fold edge shape and vascular structure.", "The performance of the proposed system was evaluated by using laryngeal images of 70 patients. Sensitivity of 86%, 94%, 80%, 73%, and 76% were obtained for healthy, polyp, nodule, laryngitis, and sulcus vocalis classes, respectively. These results indicate that visible vessels of vocal folds can act as a prognostic marker for vocal fold pathologies, as well as the vocal fold shape features, and may play a critical role in more effective diagnosis."]},
{"title": "An efficient machine learning approach for diagnosis of paraquat-poisoned patients", "highlights": ["A novel method is presented for diagnosis of paraquat (PQ) poisoning.", "The key parameters involved in ELM have been investigated in detail.", "The effectiveness of the proposed method has been rigorously evaluated.", "The approach has detected the most influential feature account for PQ poisoning."], "abstract": ["Numerous people die of paraquat (PQ) poisoning because they were not diagnosed and treated promptly at an early stage. Till now, determination of PQ levels in blood or urine is still the only way to confirm the PQ poisoning. In order to develop a new diagnostic method, the potential of machine learning technique was explored in this study. A newly developed classification technique, extreme learning machine (ELM), was taken to discriminate the PQ-poisoned patients from the healthy controls. 15 PQ-poisoned patients recruited from The First Affiliated Hospital of Wenzhou Medical University who had a history of direct contact with PQ and 16 healthy volunteers were involved in the study. The ELM method is examined based on the metabolites of blood samples determined by gas chromatography coupled with mass spectrometry in terms of classification accuracy, sensitivity, specificity and AUC (area under the receiver operating characteristic (ROC) curve) criterion, respectively. Additionally, the feature selection was also investigated to further boost the performance of ELM and the most influential feature was detected. The experimental results demonstrate that the proposed approach can be regarded as a success with the excellent classification accuracy, AUC, sensitivity and specificity of 91.64%, 0.9156%, 91.33% and 91.78%, respectively. Promisingly, the proposed method might serve as a new candidate of powerful tools for diagnosis of PQ-poisoned patients with excellent performance."]},
{"title": "Software for analysing multifocal visual evoked potential signal latency progression", "highlights": ["New software application with which to process mfVEP signal latency.", "Several important parameters can be configured before starting analysis.", "Signal display comparing different tests.", "Detects which zone of the eye is most affected.", "Achieves high latency measurement accuracy."], "abstract": ["This paper describes a new non-commercial software application (mfVEP", ") developed to process multifocal visual-evoked-potential (mfVEP) signals in latency (monocular and interocular) progression studies.", "The software performs analysis by cross-correlating signals from the same patients. The criteria applied by the software include best channels, signal window, cross-correlation limits and signal-to-noise ratio (SNR). Software features include signal display comparing different tests and groups of sectors (quadrants, rings and hemispheres).", "The software\u05f3s performance and capabilities are demonstrated on the results obtained from a patient with acute optic neuritis who underwent 9 follow-up mfVEP tests. Numerical values and graphics are presented and discussed for this case.", "The authors present a software application used to study progression in mfVEP signals. It is also useful in research projects designed to improve mfVEP techniques. This software makes it easier for users to manage the signals and allows them to choose various ways of selecting signals and representing results."]},
{"title": "Survey on computer aided decision support for diagnosis of celiac disease", "highlights": ["The state-of-the-art research in automated diagnosis of celiac disease is presented.", "A systematic review of methods and techniques used in this field is given.", "Specific issues and challenges in the field are identified and discussed."], "abstract": ["Celiac disease (CD) is a complex autoimmune disorder in genetically predisposed individuals of all age groups triggered by the ingestion of food containing gluten. A reliable diagnosis is of high interest in view of embarking on a strict gluten-free diet, which is the CD treatment modality of first choice. The gold standard for diagnosis of CD is currently based on a histological confirmation of serology, using biopsies performed during upper endoscopy. Computer aided decision support is an emerging option in medicine and endoscopy in particular. Such systems could potentially save costs and manpower while simultaneously increasing the safety of the procedure. Research focused on computer-assisted systems in the context of automated diagnosis of CD has started in 2008. Since then, over 40 publications on the topic have appeared. In this context, data from classical flexible endoscopy as well as wireless capsule endoscopy (WCE) and confocal laser endomicrosopy (CLE) has been used. In this survey paper, we try to give a comprehensive overview of the research focused on computer-assisted diagnosis of CD."]},
{"title": "Missing data imputation on the 5-year survival prediction of breast cancer patients with unknown discrete values", "highlights": ["A 5-year survival model is constructed in a real breast cancer context.", "The dataset is a challenge in terms of complexity due to its high missing data ratio.", "Several representative imputation and decision methods are analyzed.", "Obtained results are very interesting and accurate for this complex clinical dataset."], "abstract": ["Breast cancer is the most frequently diagnosed cancer in women. Using historical patient information stored in clinical datasets, data mining and machine learning approaches can be applied to predict the survival of breast cancer patients. A common drawback is the absence of information, i.e., missing data, in certain clinical trials. However, most standard prediction methods are not able to handle incomplete samples and, then, missing data imputation is a widely applied approach for solving this inconvenience. Therefore, and taking into account the characteristics of each breast cancer dataset, it is required to perform a detailed analysis to determine the most appropriate imputation and prediction methods in each clinical environment. This research work analyzes a real breast cancer dataset from Institute Portuguese of Oncology of Porto with a high percentage of unknown categorical information (most clinical data of the patients are incomplete), which is a challenge in terms of complexity. Four scenarios are evaluated: (I) 5-year survival prediction without imputation and 5-year survival prediction from cleaned dataset with (II) Mode imputation, (III) Expectation-Maximization imputation and (IV) ", "-Nearest Neighbors imputation. Prediction models for breast cancer survivability are constructed using four different methods: ", "-Nearest Neighbors, Classification Trees, Logistic Regression and Support Vector Machines. Experiments are performed in a nested ten-fold cross-validation procedure and, according to the obtained results, the best results are provided by the ", "-Nearest Neighbors algorithm: more than 81% of accuracy and more than 0.78 of area under the Receiver Operator Characteristic curve, which constitutes very good results in this complex scenario."]},
{"title": "FPGA-based design and implementation of arterial pulse wave generator using piecewise Gaussian\u2013cosine fitting", "highlights": ["An arterial pulse wave generator has been designed and implemented based on FPGA.", "Proposed a method to reconstruct pulse wave from the feature points set by users.", "The function for adding baseline drift and three types of noises is integrated.", "It has good performance such as efficiency, portability, and re-configurability."], "abstract": ["Because arterial pulse waves contain vital information related to the condition of the cardiovascular system, considerable attention has been devoted to the study of pulse waves in recent years. Accurate acquisition is essential to investigate arterial pulse waves. However, at the stage of developing equipment for acquiring and analyzing arterial pulse waves, specific pulse signals may be unavailable for debugging and evaluating the system under development. To produce test signals that reflect specific physiological conditions, in this paper, an arterial pulse wave generator has been designed and implemented using a field programmable gate array (FPGA), which can produce the desired pulse waves according to the feature points set by users. To reconstruct a periodic pulse wave from the given feature points, a method known as piecewise Gaussian\u2013cosine fitting is also proposed in this paper. Using a test database that contains four types of typical pulse waves with each type containing 25 pulse wave signals, the maximum residual error of each sampling point of the fitted pulse wave in comparison with the real pulse wave is within 8%. In addition, the function for adding baseline drift and three types of noises is integrated into the developed system because the baseline occasionally wanders, and noise needs to be added for testing the performance of the designed circuits and the analysis algorithms. The proposed arterial pulse wave generator can be considered as a special signal generator with a simple structure, low cost and compact size, which can also provide flexible solutions for many other related research purposes."]},
{"title": "Computer-Aided Detection and diagnosis for prostate cancer based on mono and multi-parametric MRI: A review", "highlights": ["Techniques used in mono and multi-parametric MRI CADe and CADx for CaP are reviewed.", "A comparison between the different studies is given.", "The contribution of multi-parametric CAD compared with mono-parametric is discussed.", "Potential avenues for future research are discussed.", "A public multi-parametric MRI database is brought to the community\u05f3s knowledge."], "abstract": ["Prostate cancer is the second most diagnosed cancer of men all over the world. In the last few decades, new imaging techniques based on Magnetic Resonance Imaging (MRI) have been developed to improve diagnosis. In practise, diagnosis can be affected by multiple factors such as observer variability and visibility and complexity of the lesions. In this regard, computer-aided detection and computer-aided diagnosis systems have been designed to help radiologists in their clinical practice. Research on computer-aided systems specifically focused for prostate cancer is a young technology and has been part of a dynamic field of research for the last 10", "\u00a0", "years. This survey aims to provide a comprehensive review of the state-of-the-art in this lapse of time, focusing on the different stages composing the work-flow of a computer-aided system. We also provide a comparison between studies and a discussion about the potential avenues for future research. In addition, this paper presents a new public online dataset which is made available to the research community with the aim of providing a common evaluation framework to overcome some of the current limitations identified in this survey."]},
{"title": "Region-based snake with edge constraint for segmentation of lymph nodes on CT images", "highlights": ["Edge- and region-based snakes were complementary for lymph nodes segmentation.", "We proposed a segmentation method which integrates both edge and region information.", "The method does not suffer from the trade-off between edge and region terms.", "This is a clinical application-based implementation of the ", "-spline snake.", "It allows automatic delineation of targeted lymph nodes in CT images."], "abstract": ["Lymph nodes segmentation is a tedious process with large inter-user variability when performed manually. To facilitate lymph nodes assessment for lung cancer patient, we present an automatic and improved snake segmentation method for thoracic lymph nodes on CT images in this paper. We first investigated the performance of both edge-based and region-based snake algorithms for the segmentation task, using a ", "-spline contour parameterization. The effect of the number of ", "-spline control points on the snake performance was also examined. Both edge-based and region-based snakes were found to have their own advantages and disadvantages for lymph nodes segmentation. We further developed a method of region-based snake with edge constraint, which utilizes a self-adjusting mechanism to integrate both edge and region information in a constructive manner. The average Dice Similarity Coefficient obtained was 0.853\u00b10.059 and 0.841\u00b10.108 for the baseline and follow-up lymph nodes respectively using the proposed method. The method was found to be an effective lymph node segmentation method and would potentially be useful to help with treatment response evaluations in the clinical practice."]},
{"title": "Structure constrained semi-nonnegative matrix factorization for EEG-based motor imagery classification", "highlights": ["Structure constrained semi-NMF method for motor imagery classification.", "The mean envelopes of ERP are extracted as structural constraints.", "Enables the analysis of temporal patterns of general EEG time series by semi-NMF.", "The temporal patterns extracted by SCS-NMF can be combined with other features."], "abstract": ["Electroencephalogram (EEG) provides a non-invasive approach to measure the electrical activities of brain neurons and has long been employed for the development of brain-computer interface (BCI). For this purpose, various patterns/features of EEG data need to be extracted and associated with specific events like cue-paced motor imagery. However, this is a challenging task since EEG data are usually non-stationary time series with a low signal-to-noise ratio.", "In this study, we propose a novel method, called structure constrained semi-nonnegative matrix factorization (SCS-NMF), to extract the key patterns of EEG data in time domain by imposing the mean envelopes of event-related potentials (ERPs) as constraints on the semi-NMF procedure. The proposed method is applicable to general EEG time series, and the extracted temporal features by SCS-NMF can also be combined with other features in frequency domain to improve the performance of motor imagery classification.", "Real data experiments have been performed using the SCS-NMF approach for motor imagery classification, and the results clearly suggest the superiority of the proposed method.", "Comparison experiments have also been conducted. The compared methods include ICA, PCA, Semi-NMF, Wavelets, EMD and CSP, which further verified the effectivity of SCS-NMF.", "The SCS-NMF method could obtain better or competitive performance over the state of the art methods, which provides a novel solution for brain pattern analysis from the perspective of structure constraint."]},
{"title": "Mobilizing clinical decision support to facilitate knowledge translation: A case study in China", "highlights": ["A pilot effort in China to build a computer aided knowledge translation platform.", "A technical infrastructure for evidence based clinical decision support.", "An ontology that incorporates, coordinates and synergizes various knowledge types.", "Integrate and manage hardcoded knowledge by an application store service."], "abstract": ["A wide gulf remains between knowledge and clinical practice. Clinical decision support has been demonstrated to be an effective knowledge tool that healthcare organizations can employ to deliver the \u201cright knowledge to the right people in the right form at the right time\u201d. How to adopt various clinical decision support (CDS) systems efficiently to facilitate evidence-based practice is one challenge faced by knowledge translation research.", "A computer-aided knowledge translation method that mobilizes evidence-based decision supports is proposed. The foundation of the method is a knowledge representation model that is able to cover, coordinate and synergize various types of medical knowledge to achieve centralized and effective knowledge management. Next, web-based knowledge-authoring and natural language processing based knowledge acquisition tools are designed to accelerate the transformation of the latest clinical evidence into computerized knowledge content. Finally, a batch of fundamental services, such as data acquisition and inference engine, are designed to actuate the acquired knowledge content. These services can be used as building blocks for various evidence-based decision support applications.", "Based on the above method, a computer-aided knowledge translation platform was constructed as a CDS infrastructure. Based on this platform, typical CDS applications were developed. A case study of drug use check demonstrates that the CDS intervention delivered by the platform has produced observable behavior changes (89.7% of alerted medical orders were revised by physicians).", "Computer-aided knowledge translation via a CDS infrastructure can be effective in facilitating knowledge translation in clinical settings."]},
{"title": "Multifractality in heartbeat dynamics in patients undergoing beating-heart myocardial revascularization", "highlights": ["Multifractality of heartbeat dynamics was evaluated before and after off-pump CABG.", "A decay of multifractal complexity into monofractal is observed after off-pump CABG.", "Circadian pattern of multifractality is partially regained after off-pump CABG."], "abstract": ["The multifractal approach of HRV analysis offers new insight into the mechanisms of autonomic modulation of the diseased hearts and has a potential to depict subtle changes in cardiac autonomic nervous control not revealed by conventional linear and non-linear analyses in various conditions like heart failure or stable angina pectoris. The aim of this study was to employ the multifractality approach in cardiac surgery patients and evaluate the multifractality before and after beating-heart myocardial revascularization (off-pump CABG).", "Twenty-four hour Holter recordings were performed pre- and postoperatively in 60 patients undergoing off-pump CABG. Selected conventional time- and frequency-domain linear HRV indices were calculated from the 24", "\u00a0", "h and 5", "\u00a0", "min ECG segments, and preselected multifractal parameters ", "(", "), ", "(", "), ", "_top and \u0394", " were determined for daytime (12:00\u201318:00) and nighttime (00:00\u201306:00) periods of the ECG recordings using Ivanov\u05f3s method. Mean differences over time were tested using paired-samples ", "-test and exact Wilcoxon matched-pairs test. The results are reported as mean\u00b1SD and median with interquartile range. A ", " value of <0.05 was considered statistically significant.", "All selected conventional linear HRV parameters decreased significantly after off pump CABG (", " from <0.001\u20130.015). Preoperatively, multifractal parameter ", "(", ") was \u22120.60\u00b10.12 and \u22120.54\u00b10.12, ", "(", ") \u22120.52\u00b10.18 and \u22120.49\u00b10.17, ", "_top 0.20\u00b10.07 and 0.15\u00b10.07 and \u0394", " 0.31\u00b10.14 and 0.17\u00b10.14 for daytime and nighttime periods, respectively. Postoperatively, ", "(", ") and ", "(", ") were significantly higher for daytime (\u22120.49\u00b10.15, ", "<0.001 and \u22120.43\u00b10.23, ", "=0.015), whereas ", "_top and \u0394", " were significanly higher for both daytime and nighttime (0.25\u00b10.07, ", "<0.001 and 0.19\u00b10.06, ", "=0.002 for ", "_top and 0.41\u00b10.20, ", "=0.003 and 0.31\u00b10.19, ", "<0.001 for \u0394", ", respectively). All pre- and postoperative parameters, except ", "(", ") and ", "(", ") preoperatively, were significantly lower for nighttime as compared to daytime periods.", "A significant breakdown of multifractal complexity and anti-correlation behavior with a significant sympathetic overdrive and a concomitant parasympathetic withdrawal occurs after off-pump CABG. The circadian pattern of multifractality regains its day\u2013night variation in the first week after the surgical procedure."]},
{"title": "Reconstruction of optical scanned images of inhomogeneities in biological tissues by Monte Carlo simulation", "highlights": ["Simulation of multiport optical scanning system.", "Capable of differentiating the embedded inhomogeneities.", "Characterization of their optical properties."], "abstract": ["The optical imaging of inhomogeneities located in phantoms of biological tissues, prepared from goat\u05f3s isolated heart as control tissue and embedded with spleen and adipose tissues representing tumors, by Monte Carlo simulation, is carried out. The proposed scanning probe consists of nine units. Each unit is equipped with one photon injection port and three ports arranged in a straight line to collect backscattered photons emerging from various depths, and one port, placed coaxially to the source on the opposite side of the phantom, to collect the transmitted component. At each position of the grid, superposed on the tissue phantom, photons are introduced through source port into the phantoms and backscattered and transmitted photons are collected by respective ports. Based on the data collected from the entire grid surface the respective gray-level images are reconstructed. The inhomogeneity located at certain depth (2, 4, 6", "\u00a0", "mm) is visualized in three images formed by the backscattered data collected by three ports. Increase or decrease in normalized backscattered intensity (NBI) observed in their scans corresponds to that of high scattering (adipose) or absorbing (spleen) inhomogeneity compared to that of control tissue and also their location as determined by NBI variation as received at various ports. The images constructed from the transmitted data are associated with decrease in intensity. The scans of these images through their centers show that normalized transmitted intensity (NTI) attains its maximum value when the inhomogeneities are at depth 6", "\u00a0", "mm. These scans are of higher amplitude for spleen compared to that of adipose tissues. Thus the data received by backscattering and transmission complement each other in identifying the location and type of inhomogeneities."]},
{"title": "Theoretical distribution of load in the radius and ulna carpal joint", "highlights": ["We analyze the load distribution through the wrist joint.", "We used the Rigid Body Spring Model method on a 3D model of the wrist joint.", "We found the load distributions on each carpal articular surface of radius.", "Obtained results are comparable with previous outcomes reported on former studies.", "The proposed methodology is suitable to predict changes in load in the wrist."], "abstract": ["The purpose of this study is to validate a model for the analysis of the load distribution through the wrist joint, subjected to forces on the axes of the metacarpals from distal to proximal for two different mesh densities.", "To this end, the Rigid Body Spring Model (RBSM) method was used on a three-dimensional model of the wrist joint, simulating the conditions when making a grip handle. The cartilage and ligaments were simulated as springs acting under compression and tension, respectively, while the bones were considered as rigid bodies. At the proximal end of the ulna the movement was completely restricted, and the radius was allowed to move only in the lateral/medial direction.", "With these models, we found the load distributions on each carpal articular surface of radius. Additionally, the results show that the percentage of the applied load transmitted through the radius was about 86% for one mesh and 88% for the coarser one; for the ulna it was 21% for one mesh and 18% for the coarser.", "The obtained results are comparable with previous outcomes reported in prior studies. The latter allows concluding that, in theory, the methodology can be used to describe the changes in load distribution in the wrist."]},
{"title": "Computerized methodology for micro-CT and histological data inflation using an IVUS based translation map", "highlights": ["A framework for inflating micro-CT/histological images is proposed.", "The micro-CT/histological images are transformed and validated based on IVUS.", "A realistic matching between IVUS and micro-CT/histological images is achieved.", "The overlapping plaque areas are significantly after inflation.", "The reliability of validation procedure is enhanced in IVUS plaque detection."], "abstract": ["A framework for the inflation of micro-CT and histology data using intravascular ultrasound (IVUS) images, is presented. The proposed methodology consists of three steps. In the first step the micro-CT/histological images are manually co-registered with IVUS by experts using fiducial points as landmarks. In the second step the lumen of both the micro-CT/histological images and IVUS images are automatically segmented. Finally, in the third step the micro-CT/histological images are inflated by applying a transformation method on each image. The transformation method is based on the IVUS and micro-CT/histological contour difference. In order to validate the proposed image inflation methodology, plaque areas in the inflated micro-CT and histological images are compared with the ones in the IVUS images. The proposed methodology for inflating micro-CT/histological images increases the sensitivity of plaque area matching between the inflated and the IVUS images (7% and 22% in histological and micro-CT images, respectively)."]},
{"title": "Computerized segmentation and measurement of chronic wound images", "highlights": ["We proposed a method to segment chronic wounds based on 4D probability map.", "The algorithm characterizes wounds into granulation, slough and eschar tissues.", "Experiment on 80 wound images gives an average accuracy of 75.1%.", "We also analyze the agreement between experts on wound and tissue boundaries.", "For wound boundary, mean agreement between experts is between 67.4% and 84.3%."], "abstract": ["An estimated 6.5 million patients in the United States are affected by chronic wounds, with more than US$25 billion and countless hours spent annually for all aspects of chronic wound care. There is a need for an intelligent software tool to analyze wound images, characterize wound tissue composition, measure wound size, and monitor changes in wound in between visits. Performed manually, this process is very time-consuming and subject to intra- and inter-reader variability. In this work, our objective is to develop methods to segment, measure and characterize clinically presented chronic wounds from photographic images. The first step of our method is to generate a Red-Yellow-Black-White (RYKW) probability map, which then guides the segmentation process using either optimal thresholding or region growing. The red, yellow and black probability maps are designed to handle the granulation, slough and eschar tissues, respectively; while the white probability map is to detect the white label card for measurement calibration purposes. The innovative aspects of this work include defining a four-dimensional probability map specific to wound characteristics, a computationally efficient method to segment wound images utilizing the probability map, and auto-calibration of wound measurements using the content of the image. These methods were applied to 80 wound images, captured in a clinical setting at the Ohio State University Comprehensive Wound Center, with the ground truth independently generated by the consensus of at least two clinicians. While the mean inter-reader agreement between the readers varied between 67.4% and 84.3%, the computer achieved an average accuracy of 75.1%."]},
{"title": "Analysis of speed, curvature, planarity and frequency characteristics of heart vector movement to evaluate the electrophysiological substrate associated with ventricular tachycardia", "highlights": ["Novel method to study ventricular conduction on surface ECG is presented.", "Speed, curvature, orbital frequency, planarity of filtered VCG loops was measured.", "Electrophysiology of post-infarct ventricular tachycardia was characterized."], "abstract": ["We developed a novel method of assessing ventricular conduction using the surface ECG.", "Orthogonal ECGs of 81 healthy controls (age 39.0\u00b114.2 y; 51.8% males; 94% white), were compared with iDower-transformed 12-lead ECGs (both 1000", "\u00a0", "Hz), recorded in 8 patients with infarct-cardiomyopathy and sustained monomorphic ventricular tachycardia (VT) (age 68.0\u00b17.8", "\u00a0", "y, 37.5% male, mean LVEF 29\u00b112%). Normalized speed at 10 QRS segments was calculated as the distance traveled by the heart vector along the QRS loop in three-dimensional space, divided by 1/10th of the QRS duration. Curvature was calculated as the magnitude of the derivative of the QRS loop tangent vector divided by speed. Planarity was calculated as the mean of the dihedral angles between 2 consecutive planes for all planes generated for the median beat. Orbital frequency (a scalar measure of rotation rate of the QRS vector) was calculated as a product of speed and curvature.", "Mixed regression analysis showed that speed was slower [6.6 (95%CI 4.4\u20138.9) vs. 24.6 (95%CI 11.5\u201337.7)", "\u00a0", "\u00b5V/ms; ", "<0.0001]; orbital frequency was smaller [1.4 (95%CI 1.2\u20131.6) vs. 6.8 (95%CI 5.4\u20138.1)", "\u00a0", "ms", "; ", "<0.0001], and planarity was larger by 3.6\u00b0 (95%CI 1.4\u00b0\u20135.8; ", "=0.002) in VT cases than in healthy controls. ROC AUC for orbital frequency was 0.940 (95%CI 0.935\u20130.944) across all frequencies and QRS segments. ROC AUC for planarity at 70\u2013249", "\u00a0", "Hz was 0.995 (95%CI 0.985\u20131.00). ROC AUC for speed at 70\u201379", "\u00a0", "Hz was 0.979 (95%CI 0.969\u20130.989).", "This novel method reveals characteristic features of an abnormal electrophysiological substrate associated with VT."]},
{"title": "Number of distinct sequence alignments with k-match and match sections", "highlights": ["Count sequence alignments with ", "-match.", "Count sequence alignments with match sections of size ", ".", "Count sequence alignments with match sections of size at least ", "."], "abstract": ["Background: Recent developments in sequence alignment have led to significant advances in our understanding of the functional, structural or evolutionary relationships among biological sequences. Great efforts have been made to count the total number of sequence alignments, but little attention has been paid to specific alignments associated with conserved patterns.", "Methods: We propose a new combinatorial method to count specific alignments. First, we represent a sequence alignment as a system of cells. Using combinatorial techniques and Stirling\u05f3s formula, we then count the numbers of specific alignments with a ", "-match or match section of size ", ".", "Results: We developed three theorems related to different types of specific alignments. We found that the number of the alignments with match sections of at least ", " was less than that of ", "-match sections and the number of specific alignments was significantly lower than the results reported by Covington.", "Discussion: The presence of a large number of alignments makes a direct search for the optimal alignment unfeasible for long sequences, whereas our proposed method based on specific alignments decreases the search space by many times. This facilitates the development of a faster algorithm for performing sequence comparisons."]},
{"title": "Median prior constrained TV algorithm for sparse view low-dose CT reconstruction", "highlights": ["A Median prior constrained TV algorithm is proposed for sparse view low-dose CT reconstruction.", "The median gradient proves to be sparse as well gradient image.", "The proposed algorithm can not only ensure a higher SNR but also a higher resolution."], "abstract": ["It is known that lowering the X-ray tube current (mAs) or tube voltage (kVp) and simultaneously reducing the total number of X-ray views (sparse view) is an effective means to achieve low-dose in computed tomography (CT) scan. However, the associated image quality by the conventional filtered back-projection (FBP) usually degrades due to the excessive quantum noise. Although sparse-view CT reconstruction algorithm via total variation (TV), in the scanning protocol of reducing X-ray tube current, has been demonstrated to be able to result in significant radiation dose reduction while maintain image quality, noticeable patchy artifacts still exist in reconstructed images. In this study, to address the problem of patchy artifacts, we proposed a median prior constrained TV regularization to retain the image quality by introducing an auxiliary vector ", " in register with the object. Specifically, the approximate action of ", " is to draw, in each iteration, an object voxel toward its own local median, aiming to improve low-dose image quality with sparse-view projection measurements. Subsequently, an alternating optimization algorithm is adopted to optimize the associative objective function. We refer to the median prior constrained TV regularization as \u201cTV_MP\u201d for simplicity. Experimental results on digital phantoms and clinical phantom demonstrated that the proposed TV_MP with appropriate control parameters can not only ensure a higher signal to noise ratio (SNR) of the reconstructed image, but also its resolution compared with the original TV method."]},
{"title": "Automatic identification of oculomotor behavior using pattern recognition techniques", "highlights": ["Classification of three types of eye movements: saccades, blinks and fixations.", "Statistical features distinguish the three types of eye movements.", "Classification by a cascade of three classifiers with overall accuracy 95.9%."], "abstract": ["In this paper, a methodological scheme for identifying distinct patterns of oculomotor behavior such as saccades, microsaccades, blinks and fixations from time series of eye\u05f3s angular displacement is presented. The first step of the proposed methodology involves signal detrending for artifacts removal and estimation of eye\u05f3s angular velocity. Then, feature vectors from fourteen first-order statistical features are formed from each angular displacement and velocity signal using sliding, fixed-length time windows. The obtained feature vectors are used for training and testing three artificial neural network classifiers, connected in cascade. The three classifiers discriminate between blinks and non-blinks, fixations and non-fixations and saccades and microsaccades, respectively. The proposed methodology was tested on a dataset from 1392 subjects, each performing three oculomotor fixation conditions.", "The average overall accuracy of the three classifiers, with respect to the manual identification of eye movements by experts, was 95.9%. The proposed methodological scheme provided better results than the well-known Velocity Threshold algorithm, which was used for comparison. The findings of the present study indicate that the utilization of pattern recognition techniques in the task of identifying the various eye movements may provide accurate and robust results."]},
{"title": "Automated colon cancer detection using hybrid of novel geometric features and some traditional features", "highlights": ["A novel colon cancer detection system has been proposed.", "The proposed system utilizes a hybrid of traditional and novel features for classification.", "Novel geometric features, which mathematically quantify the variation in the structure of normal and malignant colon tissues, have been proposed.", "The proposed geometric features show superior classification performance compared to traditional features.", "RBF kernel of SVM shows promising results for classification of colon cancer data."], "abstract": ["Automatic classification of colon into normal and malignant classes is complex due to numerous factors including similar colors in different biological constituents of histopathological imagery. Therefore, such techniques, which exploit the textural and geometric properties of constituents of colon tissues, are desired. In this paper, a novel feature extraction strategy that mathematically models the geometric characteristics of constituents of colon tissues is proposed. In this study, we also show that the hybrid feature space encompassing diverse knowledge about the tissues\u05f3 characteristics is quite promising for classification of colon biopsy images. This paper thus presents a hybrid feature space based colon classification (HFS-CC) technique, which utilizes hybrid features for differentiating normal and malignant colon samples. The hybrid feature space is formed to provide the classifier different types of discriminative features such as features having rich information about geometric structure and image texture. Along with the proposed geometric features, a few conventional features such as morphological, texture, scale invariant feature transform (SIFT), and elliptic Fourier descriptors (EFDs) are also used to develop a hybrid feature set. The SIFT features are reduced using minimum redundancy and maximum relevancy (mRMR). Various kernels of support vector machines (SVM) are employed as classifiers, and their performance is analyzed on 174 colon biopsy images. The proposed geometric features have achieved an accuracy of 92.62%, thereby showing their effectiveness. Moreover, the proposed HFS-CC technique achieves 98.07% testing and 99.18% training accuracy. The better performance of HFS-CC is largely due to the discerning ability of the proposed geometric features and the developed hybrid feature space."]},
{"title": "Automatic detection of atrial fibrillation using stationary wavelet transform and support vector machine", "highlights": ["A novel method for automatic detection of Atrial fibrillation (AF) is proposed.", "This method eliminates the need for P and/or R peak detection.", "Sensitivity and specificity of the method is 97.0% and 97.1%, respectively.", "It holds several interesting properties suitable for practical applications."], "abstract": ["Atrial fibrillation (AF) is the most common cardiac arrhythmia, and a major public health burden associated with significant morbidity and mortality. Automatic detection of AF could substantially help in early diagnosis, management and consequently prevention of the complications associated with chronic AF. In this paper, we propose a novel method for automatic AF detection.", "Stationary wavelet transform and support vector machine have been employed to detect AF episodes. The proposed method eliminates the need for P-peak or R-Peak detection (a pre-processing step required by many existing algorithms), and hence its performance (sensitivity, specificity) does not depend on the performance of beat detection. The proposed method has been compared with those of the existing methods in terms of various measures including performance, transition time (detection delay associated with transitioning from a non-AF to AF episode), and computation time (using MIT-BIH Atrial Fibrillation database).", "Results of a stratified 2-fold cross-validation reveals that the area under the Receiver Operative Characteristics (ROC) curve of the proposed method is 99.5%. Moreover, the method maintains its high accuracy regardless of the choice of the parameters\u05f3 values and even for data segments as short as 10", "\u00a0", "s. Using the optimal values of the parameters, the method achieves sensitivity and specificity of 97.0% and 97.1%, respectively.", "The proposed AF detection method has high sensitivity and specificity, and holds several interesting properties which make it a suitable choice for practical applications."]},
{"title": "Determining the bistability parameter ranges of artificially induced lac operon using the root locus method", "highlights": ["We use root locus method to determine bistability parameter ranges for lac operon.", "The method captures parameter ranges ensuring triple roots to equilibrium equation.", "Analyzing gene regulatory networks under parameter uncertainties is achieved.", "Bistability ranges for TMG- and IPTG-induced lac operon models are obtained.", "The method is applicable to any biological model based on mass action kinetics."], "abstract": ["This paper employs the root locus method to conduct a detailed investigation of the parameter regions that ensure bistability in a well-studied gene regulatory network namely, lac operon of ", " (", "). In contrast to previous works, the parametric bistability conditions observed in this study constitute a complete set of necessary and sufficient conditions. These conditions were derived by applying the root locus method to the polynomial equilibrium equation of the lac operon model to determine the parameter values yielding the multiple real roots necessary for bistability. The lac operon model used was defined as an ordinary differential equation system in a state equation form with a rational right hand side, and it was compatible with the Hill and Michaelis\u2013Menten approaches of enzyme kinetics used to describe biochemical reactions that govern lactose metabolism. The developed root locus method can be used to study the steady-state behavior of any type of convergent biological system model based on mass action kinetics. This method provides a solution to the problem of analyzing gene regulatory networks under parameter uncertainties because the root locus method considers the model parameters as variable, rather than fixed. The obtained bistability ranges for the lac operon model parameters have the potential to elucidate the appearance of bistability for ", " cells in ", " experiments, and they could also be used to design robust hysteretic switches in synthetic biology."]},
{"title": "-Order and maximum fuzzy similarity entropy for discrimination of signals of different complexity: Application to fetal heart rate signals", "highlights": ["Solving the problem of setting entropy descriptors by varying the pattern size instead of the tolerance.", "Improving the discrimination of signals of different complexity such as fBm and fetal heart rate signal (normal-IUGR).", "Linking intrinsic features of time series with the optimal pattern size that maximizes the similarity entropy.", "Improving the discrimination by developing a new paradigm as the ", "-order fuzzy similarity entropy."], "abstract": ["This paper presents two new concepts for discrimination of signals of different complexity. The first focused initially on solving the problem of setting entropy descriptors by varying the pattern size instead of the tolerance. This led to the search for the optimal pattern size that maximized the similarity entropy. The second paradigm was based on the ", "-order similarity entropy that encompasses the 1-order similarity entropy. To improve the statistical stability, ", "-order fuzzy similarity entropy was proposed. Fractional Brownian motion was simulated to validate the different methods proposed, and fetal heart rate signals were used to discriminate normal from abnormal fetuses. In all cases, it was found that it was possible to discriminate time series of different complexity such as fractional Brownian motion and fetal heart rate signals. The best levels of performance in terms of sensitivity (90%) and specificity (90%) were obtained with the ", "-order fuzzy similarity entropy. However, it was shown that the optimal pattern size and the maximum similarity measurement were related to intrinsic features of the time series."]},
{"title": "Effects of game-based virtual reality on health-related quality of life in chronic stroke patients: A randomized, controlled study", "highlights": ["Rehabilitation can improve physical function and quality of life after a stroke.", "Virtual reality (VR) is an emerging modality for performing rehabilitation.", "We evaluated the effects of VR and conventional therapy among stroke patients.", "VR plus therapy improved specific health-related quality of life items.", "VR plus therapy also improved depression and physical function measures."], "abstract": ["In the present study, we aimed to determine whether game-based virtual reality (VR) rehabilitation, combined with occupational therapy (OT), could improve health-related quality of life, depression, and upper extremity function. We recruited 35 patients with chronic hemiparetic stroke, and these participants were randomized into groups that underwent VR rehabilitation plus conventional OT, or the same amount of conventional OT alone, for 20 sessions over 4 weeks. Compared to baseline, the VR rehabilitation plus OT group exhibited significantly improved role limitation due to emotional problems (", "=0.047). Compared to baseline, both groups also exhibited significantly improved depression (", "=0.017) and upper extremity function (", "=0.001), although the inter-group differences were not significant. However, a significant inter-group difference was observed for role limitation due to physical problems (", "=0.031). Our results indicate that game-based VR rehabilitation has specific effects on health-related quality of life, depression, and upper extremity function among patients with chronic hemiparetic stroke."]},
{"title": "Data-driven modeling of pharmacological systems using endpoint information fusion", "highlights": ["We propose a novel method to derive data-driven model of pharmacological systems.", "The method is built upon information fusion of endpoint responses.", "System\u05f3s identifiability is shown by analyzing a relation between endpoint responses.", "System is fully identifiable in case all the responses involve effect compartments.", "The efficacy of the method is shown by benchmark pharmacological modeling problems."], "abstract": ["This study investigated the feasibility of deriving data-driven model of a class of pharmacological systems using the information fusion of endpoint responses. For a class of pharmacological systems subsuming conventional steady-state dose-response models, compartmental pharmacokinetic\u2013pharmacodynamic models and indirect response models, a relation between multiple endpoint responses was formalized and analyzed to elucidate if this class of systems is identifiable, i.e., if the data-driven model of this class of systems can be derived from the endpoint responses alone. It was shown that this class of systems is fully identifiable in case all the responses involve effect compartments. However, it was also observed that persistently exciting dose profiles may be required in accurately deriving reliable data-driven model with low variance. The findings from the identifiability analysis were demonstrated using benchmark pharmacological system examples."]},
{"title": "A systematic study of temperature sensitive liposomal delivery of doxorubicin using a mathematical model", "highlights": ["Multi-compartment model is used to study TSL-mediated drug delivery to solid tumour.", "Drug release rate has a saturable effect on peak intracellular drug concentration.", "A similar saturable effect has also been found for heating duration.", "Longer heating duration than 1", "\u00a0", "h may not be effective for drug resistant tumour."], "abstract": ["Temperature-sensitive liposomes (TSL) in combination with hyperthermia (HT) exposure have emerged as a potentially attractive option to achieve therapeutic drug concentration at targeted tumour site while reducing adverse side effects associated with systemic administration of anticancer drugs. The aim of this study is to elucidate the interplay among different kinetic steps by means of computational modelling.", "A multi-compartment model for TSL-mediated delivery of doxorubicin (DOX) is developed, which incorporates descriptions of the pharmacokinetics of TSL and DOX, and their accumulation in tumour tissue following intravascular triggered release. By examining dynamic interactions among TSL properties, tumour physiological properties and treatment regimen, peak intracellular DOX concentration is predicted for continuous and pulse HT exposures.", "Drug release rate from TSL has a saturable effect on peak intracellular drug concentration, and no further gain could be achieved for release rates greater than 0.1018", "\u00a0", "s", ". A similar effect has also been found for heating duration, such that for a given bolus injection, peak intracellular drug concentration reaches its maximum and then levels off after HT duration of 2", "\u00a0", "h. These results suggest that both TSL release rate and HT duration can be optimised in accordance with other parameters, e.g. clearance rate of TSL and administration mode, in order to achieve a desirable level of intracellular drug concentration. However, prolonged heating is not effective for resistant tumour cells with overexpression of ABC (ATP-binding cassette) transporter proteins.", "The results obtained in this study can be used to guide the design and optimisation of TSL parameters and treatment regimens."]},
{"title": "Effect of prosthodontic planning on intercuspal occlusal contacts: Comparison of digital and conventional planning", "highlights": ["Wax-ups can improve the occlusal contacts.", "The two wax-up had provided a comparable outcome.", "The practicality of digital wax-up is yet to be investigated."], "abstract": ["Adequate occlusal contacts are critical for masticatory function. The aim of this study is to evaluate the intercuspal occlusal contacts following conventional and digital wax-ups.", "Stone casts of 15 patients undergoing prosthodontic treatment were gathered. Each cast was duplicated twice, so that conventional and digital wax-ups could be performed. To assess the occlusion, the following variables were evaluated: contact number per tooth (CNT), contact area per tooth (CAT) and contact accuracy. Further, the impact of tooth location in the arch was assessed.", "The CNT and CAT after the wax-ups increased significantly following each wax-up, and this increase was more prominent for the posterior teeth than the anterior teeth. The conventional wax-up was associated with lower CNT than the digital wax-up, especially for the posterior teeth. On the other hand, the CAT was greater for the conventional wax-up than the digital wax-up for the anterior and posterior teeth. In terms of accuracy, the two wax-ups showed greater discrepancies than the pre-treatment casts, however, the magnitude of discrepancy was greater for the digital wax-up.", "The two wax-ups improved the contact number and area. Despite the statistical variation between the wax-ups, the actual difference was minimal. Therefore, it could be speculated that the two wax-ups produced a similar outcome."]},
{"title": "Application of magnetic rods for fixation in orthopedic treatments", "highlights": ["An innovative magnetic fixation method is proposed for orthopedics.", "Magnetic rods can be used for long bone and scaffold fixation.", "Magnetic configurations, compatible with real patient cases, were analyzed.", "Magnetic fixation forces can achieve values as high as 75", "\u00a0", "N.", "Magnetic fixation can reduce the interface micromotions and provide bone stimulation."], "abstract": ["Achieving an efficient fixation for complicated fractures and scaffold application treatments is a challenging surgery problem. Although many fixation approaches have been advanced and actively pursued, the optimal solution for long bone defects has not yet been defined. This paper promotes an innovative fixation method based on application of magnetic forces.", "The efficiency of this approach was investigated on the basis of finite element modeling for scaffold application and analytical calculations for diaphyseal fractures. Three different configurations have been analyzed including combinations of small cylindrical permanent magnets or stainless steel rods, inserted rigidly in the bone intramedullary canals and in the scaffold. It was shown that attractive forces as high as 75", "\u00a0", "N can be achieved. While these forces do not reach the strength of mechanical forces in traditional fixators, the employment of magnetic rods is expected to be beneficial by reducing considerably the interface micromotions. It can additionally support magneto-mechanical stimulations as well as enabling a magnetically assisted targeted delivery of drugs and other bio-agents."]},
{"title": "Evolutionary growth of certain metabolic pathways involved in the functioning of GAD and INS genes in Type 1 ", "highlights": ["Algorithm for evolutionary growth of 4 pathways in Type 1 Diabetes Mellitus.", "Integration from 36 databases for constructing the metabolic pathways.", "Finds patterns for community, interaction, growth using Hamming, cuts and payoff.", "Pathways grow in a manner when densest metabolite evolves followed by its links.", "This model of evolution has been termed as \u2018divergent root-to-leaf\u2019 growth model."], "abstract": [": Studying biochemical pathway evolution for diseases is a flourishing area of Systems Biology. Here, we study Type 1 ", " (T1D), focusing on growth of ", ", ", ", ", ", and ", " metabolisms involved in onset of ", " and ", " genes in ", " with comparative analysis in non-obese diabetic ", ", biobreeding Diabetes-prone ", ", ", ", ", ", ", " and ", " respectively.", ": We propose an algorithm for growth analysis for four metabolic pathways involved in T1D. It has three modules, ", " finding, ", " identification and ", " detection. The first module identifies patterns using ", " using ", " and the ", ". We have performed functional analysis by representing patterns using ODEs, and identified ", ", ", " and ", " matrices. The second module identifies interactions among patterns using cut-sets and network-partitioning by \u2018Divide-and-conquer\u2019. The third module identifies functions of patterns using interactions, thereby highlighting their nature of growth.", ": We observed that metabolites that are genetically robust and resist alterations against stable state during evolution, account for emergence of a scale-free network.", ": New modules get acquired to the fundamental cluster in a preferential manner, an instance of ", ". For instance, ", " acts as a fundamental cluster in ", " metabolism. Moreover, the interactions among metabolites are divergent in nature."]},
{"title": "Signal analysis and classification methods for the calcium transient data of stem cell-derived cardiomyocytes", "highlights": ["Calcium cycling is important in cardiomyocytes and in cardiac functionality.", "Data analysis techniques for stem cell derived cardiomyocytes were developed.", "The techniques were based on peak detection and classification.", "Signals were classified into normal or abnormal classes with machine learning.", "It is crucial to detect abnormal signals corresponding to abnormal cells of culture."], "abstract": ["Calcium cycling is crucial in the excitation\u2013contraction coupling of cardiomyocytes, and therefore has a key role in cardiac functionality. Cardiac disorders and different drugs alter the calcium transients of cardiomyocytes and can cause serious dysfunction of the heart. New insights into this biochemical phenomena can be achieved by studying and analyzing calcium transients. Calcium transients of spontaneously beating human induced pluripotent stem cell-derived cardiomyocytes were recorded for a data set of 280 signals. Our objective was to develop and program procedures: (1) to automatically detect cycling peaks from signals and to classify the peaks of signals as either normal or abnormal, and (2) on the basis of the preceding peak detection results, to classify the entire signals into either a normal class or an abnormal class. We obtained a classification accuracy of approximately 80% compared to class decisions made separately by an experienced researcher, which is promising for the further development of an automatic classification approach. Automated classification software would be beneficial in the future for analyzing cardiomyocyte functionality on a large scale when screening for the adverse cardiac effects of new potential compounds, and also in future clinical applications."]},
{"title": "Numerical simulation of airflow and micro-particle deposition in human nasal airway pre- and post-virtual sphenoidotomy surgery", "highlights": ["Transient airflow pattern during a full breathing cycle were simulated numerically.", "Unsteady particle tracking during the inhalation phase was performed.", "Virtual ethmoidotomy and sphenoidotomy surgery was performed.", "Sphenoidotomy operation increases the airflow into the sphenoid sinus.", "After sphenoidotomy deposition of micro-particles in the sphenoid sinus increases."], "abstract": ["In the present study, the effects of endoscopic sphenoidotomy surgery on the flow patterns and deposition of micro-particles in the human nasal airway and sphenoid sinus were investigated. A realistic model of a human nasal passage including nasal cavity and paranasal sinuses was constructed using a series of CT scan images of a healthy subject. Then, a virtual sphenoidotomy by endoscopic sinus surgery was performed in the left nasal passage and sphenoid sinus. Transient airflow patterns pre- and post-surgery during a full breathing cycle (inhalation and exhalation) were simulated numerically under cyclic flow condition. The Lagrangian approach was used for evaluating the transport and deposition of inhaled micro-particles. An unsteady particle tracking was performed for the inhalation phase of the breathing cycle for the case that particles were continuously entering into the nasal airway. The total deposition pattern and sphenoid deposition fraction of micro-particles were evaluated and compared for pre- and post-surgery cases. The presented results show that sphenoidotomy increased the airflow into the sphenoid sinus, which led to increased deposition of micro-particles in this region. Particles up to 25", "\u00a0", "\u03bcm were able to penetrate into the sphenoid in the post-operation case, and the highest deposition in the sphenoid for the resting breathing rate occurred for 10", "\u00a0", "\u03bcm particles at about 1.5%."]},
{"title": "A software platform for phase contrast x-ray breast imaging research", "highlights": ["A simulation platform dedicated for PhC x-ray breast imaging research is developed.", "Various in complexity phantoms for PhC (from simple cuboids to anthropomorphic breasts) are modelled.", "Generation of PhC images from thick objects (breast models) is now feasible.", "Simulation results are validated against measurements performed at ESRF.", "Experimental studies on PhC may be optimized in advance."], "abstract": ["To present and validate a computer-based simulation platform dedicated for phase contrast x-ray breast imaging research.", "The software platform, developed at the Technical University of Varna on the basis of a previously validated x-ray imaging software simulator, comprises modules for object creation and for x-ray image formation. These modules were updated to take into account the refractive index for phase contrast imaging as well as implementation of the Fresnel\u2013Kirchhoff diffraction theory of the propagating x-ray waves. Projection images are generated in an in-line acquisition geometry. To test and validate the platform, several phantoms differing in their complexity were constructed and imaged at 25", "\u00a0", "keV and 60", "\u00a0", "keV at the beamline ID17 of the European Synchrotron Radiation Facility. The software platform was used to design computational phantoms that mimic those used in the experimental study and to generate x-ray images in absorption and phase contrast modes.", "The visual and quantitative results of the validation process showed an overall good correlation between simulated and experimental images and show the potential of this platform for research in phase contrast x-ray imaging of the breast. The application of the platform is demonstrated in a feasibility study for phase contrast images of complex inhomogeneous and anthropomorphic breast phantoms, compared to x-ray images generated in absorption mode.", "The improved visibility of mammographic structures suggests further investigation and optimisation of phase contrast x-ray breast imaging, especially when abnormalities are present. The software platform can be exploited also for educational purposes."]},
{"title": "A model-based approach to stability analysis of autonomic-cardiac regulation", "highlights": ["We present a model-based method to analyze autonomic-cardiac regulation.", "Our method provides a quantitative measure of autonomic-cardiac stability.", "Our method can assess subject-specific autonomic-cardiac stability.", "Our method may translate to therapies for preventing autonomic-cardiac instability."], "abstract": ["This paper presents a model-based approach to analyze the stability of autonomic-cardiac regulation. In the proposed approach, a low-order lumped parameter model of autonomic-cardiac regulation is used to derive the system equilibria based on the measurements of heart rate and blood pressure, and then the stability margin associated with the equilibria is quantified via the Lyapunov\u05f3s stability analysis method. A unique strength of the proposed approach is that it provides a quantitative measure of autonomic-cardiac stability via a computationally efficient analysis. Therefore, by integrating it with system identification techniques to derive autonomic-cardiac regulation model tuned to each individual, the proposed approach is able to assess subject-specific autonomic-cardiac stability. Indeed, our initial in-silico investigation showed that the proposed approach could estimate the system equilibria accurately, and the associated stability margin behaved consistently with widely accepted physiologic knowledge. The proposed approach may be useful in identifying physiological conditions that can lead to instability in autonomic-cardiac regulation, quantifying the margin of stability and distance to instability related to autonomic-cardiac regulation, and developing interventions to prevent autonomic-cardiac instability."]},
{"title": "Allele frequency calibration for SNP based genotyping of DNA pools: A regression based local\u2013global error fusion method", "highlights": ["A novel allele frequency estimation method.", "Machine learning based approach to estimation.", "A local\u2013global error fusion method."], "abstract": ["The costs associated with developing high density microarray technologies are prohibitive for genotyping animals when there is low economic value associated with a single animal (e.g. prawns). DNA pooling is an attempt to address this issue by combining multiple DNA samples prior to genotyping. Instead of genotyping the DNA samples of the individuals, a mixture of DNA samples (i.e. the pool) from the individuals is genotyped only once. This greatly reduces the cost of genotyping. Pooled samples are subject to greater genotyping inaccuracies than individual samples. Wrong genotyping will lead to wrong biological conclusions. It is thus required to calibrate the resulting genotypes (allele frequencies).", "We present a regression based approach to translate raw array output to allele frequency. During training, few pools and the individuals that constitute the pools are genotyped. Given the genotypes of individuals that constitute the pool, we compute the true allele frequency. We then train a regression algorithm to produce a mapping between the raw array outputs to the true allele frequency. We test the algorithm using pool samples withheld from the training set. During prediction, we use this map to genotype pools with no prior knowledge of the individuals constituting the pools.", "After data quality control we have available a dataset comprised of 912 pools. We estimate allele frequency using three approaches: the raw data, a commonly used piecewise linear transformation, and the proposed local\u2013global learner fusion method. The resulting RMS errors for the three approaches are 0.135, 0.120, and 0.080 respectively."]},
{"title": "Multi-class biological tissue classification based on a multi-classifier: Preliminary study of an automatic output power control for ultrasonic surgical units", "highlights": ["We applied a machine learning algorithm-based classification method.", "We developed a device for measuring the impedance characteristics of ", " tissues.", "Novel multi-classifier could automatically detect 6 classes of tissue effectively.", "The multi-classifier with an electrode of 3", "\u00a0", "mm invasive type showed the best performance.", "Introduced model could be applied in ischemia monitoring and tumor detection."], "abstract": ["Ultrasonic surgical units (USUs) have the advantage of minimizing tissue damage during surgeries that require tissue dissection by reducing problems such as coagulation and unwanted carbonization, but the disadvantage of requiring manual adjustment of power output according to the target tissue. In order to overcome this limitation, it is necessary to determine the properties of ", " tissues automatically. We propose a multi-classifier that can accurately classify tissues based on the unique impedance of each tissue. For this purpose, a multi-classifier was built based on single classifiers with high classification rates, and the classification accuracy of the proposed model was compared with that of single classifiers for various electrode types (Type-I: 6", "\u00a0", "mm invasive; Type-II: 3", "\u00a0", "mm invasive; Type-III: surface). The sensitivity and positive predictive value (PPV) of the multi-classifier by cross checks were determined. According to the 10-fold cross validation results, the classification accuracy of the proposed model was significantly higher (", "<0.05 or <0.01) than that of existing single classifiers for all electrode types. In particular, the classification accuracy of the proposed model was highest when the 3", "\u00a0", "mm invasive electrode (Type-II) was used (sensitivity=97.33\u2013100.00%; PPV=96.71\u2013100.00%). The results of this study are an important contribution to achieving automatic optimal output power adjustment of USUs according to the properties of individual tissues."]},
{"title": "Prediction of hot regions in protein\u2013protein interaction by combining density-based incremental clustering with feature-based classification", "highlights": ["Proposed method significantly improves the prediction performance for hot regions.", "We combine density-based incremental clustering with feature-based classification.", "Feature selection is used to get the best features for classification."], "abstract": ["Discovering hot regions in protein\u2013protein interaction is important for drug and protein design, while experimental identification of hot regions is a time-consuming and labor-intensive effort; thus, the development of predictive models can be very helpful. In hot region prediction research, some models are based on structure information, and others are based on a protein interaction network. However, the prediction accuracy of these methods can still be improved. In this paper, a new method is proposed for hot region prediction, which combines density-based incremental clustering with feature-based classification. The method uses density-based incremental clustering to obtain rough hot regions, and uses feature-based classification to remove the non-hot spot residues from the rough hot regions. Experimental results show that the proposed method significantly improves the prediction performance of hot regions."]},
{"title": "Improving brain\u2013computer interface classification using adaptive common spatial patterns", "highlights": ["An adaptive common spatial patterns method is proposed for EEG spatial filtering.", "The method is evaluated for intra- and inter-subject classifications.", "The method is compared to existing techniques and shows superior performance.", "The effects of adding misclassified trials to training data are investigated.", "The method is potentially applicable to various real-time BCI tasks."], "abstract": ["Common Spatial Patterns (CSP) is a widely used spatial filtering technique for electroencephalography (EEG)-based brain\u2013computer interface (BCI). It is a two-class supervised technique that needs subject-specific training data. Due to EEG nonstationarity, EEG signal may exhibit significant intra- and inter-subject variation. As a result, spatial filters learned from a subject may not perform well for data acquired from the same subject at a different time or from other subjects performing the same task. Studies have been performed to improve CSP\u05f3s performance by adding regularization terms into the training. Most of them require target subjects\u05f3 training data with known class labels. In this work, an adaptive CSP (ACSP) method is proposed to analyze single trial EEG data from single and multiple subjects. The method does not estimate target data\u05f3s class labels during the adaptive learning and updates spatial filters for both classes simultaneously. The proposed method was evaluated based on a comparison study with the classic CSP and several CSP-based adaptive methods using motor imagery EEG data from BCI competitions. Experimental results indicate that the proposed method can improve the classification performance as compared to the other methods. For circumstances where true class labels of target data are not instantly available, it was examined if adding classified target data to training data would improve the ACSP learning. Experimental results show that it would be better to exclude them from the training data. The proposed ACSP method can be performed in real-time and is potentially applicable to various EEG-based BCI applications."]},
{"title": "A new machine learning approach for predicting the response to anemia treatment in a large cohort of End Stage Renal Disease patients undergoing dialysis", "highlights": ["Prediction algorithm trained and tested on a large sample of real clinical data.", "Prediction improvement based on red blood cell dynamics and drug kinetics.", "There is still room for improvement of anemia management in dialysis.", "The model presented is suitable for the application in a daily clinical practice."], "abstract": ["Chronic Kidney Disease (CKD) anemia is one of the main common comorbidities in patients undergoing End Stage Renal Disease (ESRD). Iron supplement and especially Erythropoiesis Stimulating Agents (ESA) have become the treatment of choice for that anemia. However, it is very complicated to find an adequate treatment for every patient in each particular situation since dosage guidelines are based on average behaviors, and thus, they do not take into account the particular response to those drugs by different patients, although that response may vary enormously from one patient to another and even for the same patient in different stages of the anemia. This work proposes an advance with respect to previous works that have faced this problem using different methodologies (Machine Learning (ML), among others), since the diversity of the CKD population has been explicitly taken into account in order to produce a general and reliable model for the prediction of ESA/Iron therapy response. Furthermore, the ML model makes use of both human physiology and drug pharmacology to produce a model that outperforms previous approaches, yielding Mean Absolute Errors (MAE) of the Hemoglobin (Hb) prediction around or lower than 0.6", "\u00a0", "g/dl in the three countries analyzed in the study, namely, Spain, Italy and Portugal."]},
{"title": "Development of a novel imaging informatics-based system with an intelligent workflow engine (IWEIS) to support imaging-based clinical trials", "highlights": ["A system with an integrated workflow engine to support imaging-based clinical trials.", "Deployment and modification of a data management system without extensive programming.", "Medical Imaging features are integrated together with informatics data management.", "Evaluated by a phase I rehabilitation clinical trial with imaging studies."], "abstract": ["Imaging based clinical trials can benefit from a solution to efficiently collect, analyze, and distribute multimedia data at various stages within the workflow. Currently, the data management needs of these trials are typically addressed with custom-built systems. However, software development of the custom-built systems for versatile workflows can be resource-consuming. To address these challenges, we present a system with a workflow engine for imaging based clinical trials. The system enables a project coordinator to build a data collection and management system specifically related to study protocol workflow without programming. Web Access to DICOM Objects (WADO) module with novel features is integrated to further facilitate imaging related study. The system was initially evaluated by an imaging based rehabilitation clinical trial. The evaluation shows that the cost of the development of system can be much reduced compared to the custom-built system. By providing a solution to customize a system and automate the workflow, the system will save on development time and reduce errors especially for imaging clinical trials."]},
{"title": "Detecting tympanostomy tubes from otoscopic images via offline and online training", "highlights": ["A new system is developed to detect tympanostomy tubes in otoscopic images.", "Image features are derived to reflect the characteristics of tympanostomy tubes.", "A 3-layer cascaded classifier is trained in an offline training process.", "A real-time refinement process is designed to improve the classifier at the point of patient care.", "The proposed system achieves high detection accuracy in an empirical study."], "abstract": ["Tympanostomy tube placement has been commonly used nowadays as a surgical treatment for otitis media. Following the placement, regular scheduled follow-ups for checking the status of the tympanostomy tubes are important during the treatment. The complexity of performing the follow up care mainly lies on identifying the presence and patency of the tympanostomy tube. An automated tube detection program will largely reduce the care costs and enhance the clinical efficiency of the ear nose and throat specialists and general practitioners. In this paper, we develop a computer vision system that is able to automatically detect a tympanostomy tube in an otoscopic image of the ear drum. The system comprises an offline classifier training process followed by a real-time refinement stage performed at the point of care. The offline training process constructs a three-layer cascaded classifier with each layer reflecting specific characteristics of the tube. The real-time refinement process enables the end users to interact and adjust the system over time based on their otoscopic images and patient care. The support vector machine (SVM) algorithm has been applied to train all of the classifiers. Empirical evaluation of the proposed system on both high quality hospital images and low quality internet images demonstrates the effectiveness of the system. The offline classifier trained using 215 images could achieve a 90% accuracy in terms of classifying otoscopic images with and without a tympanostomy tube, and then the real-time refinement process could improve the classification accuracy by 3\u20135% based on additional 20 images."]},
{"title": "Understanding the effects of pre-processing on extracted signal features from gait accelerometry signals", "highlights": ["Gait accelerometry is a popular approach for an instrumented walking analysis.", "Acquired recording typically need to be preprocessed to remove artifacts.", "The effects of preprocessing need to be understood.", "Wavelet denoising has no effects.", "Tilt correction can potentially enhance group discriminations."], "abstract": ["Gait accelerometry is an important approach for gait assessment. Previous contributions have adopted various pre-processing approaches for gait accelerometry signals, but none have thoroughly investigated the effects of such pre-processing operations on the obtained results. Therefore, this paper investigated the influence of pre-processing operations on signal features extracted from gait accelerometry signals. These signals were collected from 35 participants aged over 65", "\u00a0", "years: 14 of them were healthy controls (HC), 10 had Parkinson\u05f3s disease (PD) and 11 had peripheral neuropathy (PN). The participants walked on a treadmill at preferred speed. Signal features in time, frequency and time\u2013frequency domains were computed for both raw and pre-processed signals. The pre-processing stage consisted of applying tilt correction and denoising operations to acquired signals. We first examined the effects of these operations separately, followed by the investigation of their joint effects. Several important observations were made based on the obtained results. First, the denoising operation alone had almost no effects in comparison to the trends observed in the raw data. Second, the tilt correction affected the reported results to a certain degree, which could lead to a better discrimination between groups. Third, the combination of the two pre-processing operations yielded similar trends as the tilt correction alone. These results indicated that while gait accelerometry is a valuable approach for the gait assessment, one has to carefully adopt any pre-processing steps as they alter the observed findings."]},
{"title": "A human-phantom coupling experiment and a dispersive simulation model for investigating the variation of dielectric properties of biological tissues", "highlights": ["The effect of tissue dispersion was studied using current and voltage stimulations.", "A novel method was proposed to systematically change the dispersion of the tissues.", "Current needs to be increased to generate activation under decreased impedance.", "Voltage needs to be decreased to generate activation under decreased impedance.", "Optimal pulse duration also depends on the pulse stimulation."], "abstract": ["Variation of the dielectric properties of tissues could happen due to aging, moisture of the skin, muscle denervation, and variation of blood flow by temperature. Several studies used burst-modulated alternating stimulation to improve activation and comfort by reducing tissue impedance as a possible mechanism to generate muscle activation with less energy. The study of the effect of dielectric properties of biological tissues in nerve activation presents a fundamental problem, which is the difficulty of systematically changing the morphological factors and dielectric properties of the subjects under study. We tackle this problem by using a simulation and an experimental study. The experimental study is a novel method that combines a fat tissue-equivalent phantom, with known and adjustable dielectric properties, with the human thigh. In this way, the dispersion of the tissue under study could be modified to observe its effects systematically in muscle activation. We observed that, to generate a given amount of muscle or nerve activation under conditions of decreased impedance, the magnitude of the current needs to be increased while the magnitude of the voltage needs to be decreased."]},
{"title": "WHATIF: An open-source desktop application for extraction and management of the incidental findings from next-generation sequencing variant data", "highlights": ["WHATIF is self-explanatory and accesses the most updated variation information.", "WHATIF offers user-friendly interface and efficient ways to investigate variation.", "WHATIF\u05f3s design and implementation is highly flexible and allows for customization."], "abstract": ["Identification and evaluation of incidental findings in patients following whole exome (WGS) or whole genome sequencing (WGS) is challenging for both practicing physicians and researchers. The American College of Medical Genetics and Genomics (ACMG) recently recommended a list of reportable incidental genetic findings. However, no informatics tools are currently available to support evaluation of incidental findings in next-generation sequencing data.", "The Wisconsin Hierarchical Analysis Tool for Incidental Findings (WHATIF), was developed as a stand-alone Windows-based desktop executable, to support the interactive analysis of incidental findings in the context of the ACMG recommendations. WHATIF integrates the European Bioinformatics Institute Variant Effect Predictor (VEP) tool for biological interpretation and the National Center for Biotechnology Information ClinVar tool for clinical interpretation.", "An open-source desktop program was created to annotate incidental findings and present the results with a user-friendly interface. Further, a meaningful index (WHATIF Index) was devised for each gene to facilitate ranking of the relative importance of the variants and estimate the potential workload associated with further evaluation of the variants. Our WHATIF application is available at: ", "The WHATIF application offers a user-friendly interface and allows users to investigate the extracted variant information efficiently and intuitively while always accessing the up to date information on variants via application programming interfaces (API) connections. WHATIF\u05f3s highly flexible design and straightforward implementation aids users in customizing the source code to meet their own special needs."]},
{"title": "Heat treatment modelling using strongly continuous semigroups", "highlights": ["Two methods for solving thermal wave model are derived based on semigroups theory.", "We examine changes in tissue temperature by applying different heat sources.", "Closed form analytical solution is given.", "This simulation benefits to medical therapy to find the best heating strategy."], "abstract": ["In this paper, mathematical simulation of bioheat transfer phenomenon within the living tissue is studied using the thermal wave model. Three different sources that have therapeutic applications in laser surgery, cornea laser heating and cancer hyperthermia are used. Spatial and transient heating source, on the skin surface and inside biological body, are considered by using step heating, sinusoidal and constant heating. Mathematical simulations describe a non-Fourier process. Exact solution for the corresponding non-Fourier bioheat transfer model that has time lag in its heat flux is proposed using strongly continuous semigroup theory in conjunction with variational methods. The abstract differential equation, infinitesimal generator and corresponding strongly continuous semigroup are proposed. It is proved that related semigroup is a contraction semigroup and is exponentially stable. Mathematical simulations are done for skin burning and thermal therapy in 10 different models and the related solutions are depicted. Unlike numerical solutions, which suffer from uncertain physical results, proposed analytical solutions do not have unwanted numerical oscillations."]},
{"title": "Evaluation of monoscopic and stereoscopic displays for visual\u2013spatial tasks in medical contexts", "highlights": ["Controlled visual experiments were used to examine performance on 3D spatial tasks.", "We examine the effects of stereopsis in medical spatial tasks.", "Stereopsis significantly effects spatial task performance in specific ranges.", "Color did not affect the performance of participants.", "Orientation of the 3D representation influenced the effect of steropsis."], "abstract": ["In the medical field, digital images are present in diagnosis, pre-operative planning, minimally invasive surgery, instruction, and training. The use of medical digital imaging has afforded new ways to interact with a patient, such as seeing fine details inside a body. This increased usage also raises many basic research questions on human perception and performance when utilizing these images. The work presented here attempts to answer the question: How would adding the stereopsis depth cue affect relative position tasks in a medical context compared to a monoscopic view? By designing and conducting a study to isolate the benefits between monoscopic 3D and stereoscopic 3D displays in a relative position task, the following hypothesis was tested: stereoscopic 3D displays are beneficial over monoscopic 3D displays for relative position judgment tasks in a medical visualization setting. 44 medical students completed a series of relative position judgments tasks. The results show that stereoscopic condition yielded a higher score than the monoscopic condition with regard to the hypothesis."]},
{"title": "Computer-aided diagnosis of Myocardial Infarction using ultrasound images with DWT, GLCM and HOS methods: A comparative study", "highlights": ["Classification of normal and MI subjects using ultrasound images.", "Features are extracted using DWT, GLCM and HOS methods.", "Ranked features are subjected to SVM classifier.", "Achieved classification accuracy of 99.5% using the DWT method."], "abstract": ["Myocardial Infarction (MI) or acute MI (AMI) is one of the leading causes of death worldwide. Precise and timely identification of MI and extent of muscle damage helps in early treatment and reduction in the time taken for further tests. MI diagnosis using 2D echocardiography is prone to inter-/intra-observer variability in the assessment. Therefore, a computerised scheme based on image processing and artificial intelligent techniques can reduce the workload of clinicians and improve the diagnosis accuracy. A Computer-Aided Diagnosis (CAD) of infarcted and normal ultrasound images will be useful for clinicians. In this study, the performance of CAD approach using Discrete Wavelet Transform (DWT), second order statistics calculated from Gray-Level Co-Occurrence Matrix (GLCM) and Higher-Order Spectra (HOS) texture descriptors are compared. The proposed system is validated using 400 MI and 400 normal ultrasound images, obtained from 80 patients with MI and 80 normal subjects. The extracted features are ranked based on ", "-value and fed to the Support Vector Machine (SVM) classifier to obtain the best performance using minimum number of features. The features extracted from DWT coefficients obtained an accuracy of 99.5%, sensitivity of 99.75% and specificity of 99.25%; GLCM have achieved an accuracy of 85.75%, sensitivity of 90.25% and specificity of 81.25%; and HOS obtained an accuracy of 93.0%, sensitivity of 94.75% and specificity of 91.25%. Among the three techniques presented DWT yielded the highest classification accuracy. Thus, the proposed CAD approach may be used as a complementary tool to assist cardiologists in making a more accurate diagnosis for the presence of MI."]},
{"title": "BioBankWarden: A web-based system to support translational cancer research by managing clinical and biomaterial data", "highlights": ["A web-based computer system\u2014BioBankWarden\u2014is proposed and deployed.", "The system allows researchers to integrate and analyze clinical and biomolecular data.", "BiobankWarden concerns the control of patient records and biomaterial storage.", "BiobankWarden allows the control of disease research groups and research projects.", "The system allows integrated studies under translational research."], "abstract": ["Researchers of translational medicine face numerous challenges in attempting to bring research results to the bedside. This field of research covers a wide range of resources, including blood and tissue samples, which are processed for isolation of RNA and DNA to study cancer omics data (genomics, proteomics and metabolomics). Clinical information about patients\u05f3 habits, family history, physical examinations, remissions, etc., is also important to underpin studies aimed at identifying patterns that lead to the development of cancer and to its successful treatment.", "Development of a web-based computer system\u2014BioBankWarden\u2014to manage, consolidate and integrate these diversified data, enabling cancer research groups to retrieve and analyze clinical and biomolecular data within an integrative environment. The system has a three-tier architecture comprising database, logic and user-interface layers.", "The system\u05f3s integrated database and user-friendly interface allow for the control of patient records, biomaterial storage, research groups, research projects, users and biomaterial exchange.", "BioBankWarden can be used to store and retrieve specific information from different clinical fields linked to biomaterials collected from patients, providing the functionalities required to support translational research in the field of cancer."]},
{"title": "Clustering high throughput biological data with B-MST, a minimum spanning tree based heuristic", "highlights": ["A novel minimum spanning tree based heuristic (B-MST) for clustering is introduced.", "The heuristic has multi-objective of obtaining both tight and separate clusters.", "The objective function of the heuristic can be used as an internal validation index.", "It is applied on 13 different data sets and compared with diverse algorithms.", "B-MST performs better than the compared algorithms on most of the data sets."], "abstract": ["To address important challenges in bioinformatics, high throughput data technologies are needed to interpret biological data efficiently and reliably. Clustering is widely used as a first step to interpreting high dimensional biological data, such as the gene expression data measured by microarrays. A good clustering algorithm should be efficient, reliable, and effective, as demonstrated by its capability of determining biologically relevant clusters. This paper proposes a new minimum spanning tree based heuristic B-MST, that is guided by an innovative objective function: the tightness and separation index (TSI). The TSI presented here obtains biologically meaningful clusters, making use of co-expression network topology, and this paper develops a local search procedure to minimize the TSI value. The proposed B-MST is tested by comparing results to: (1) adjusted rand index (ARI), for microarray data sets with known object classes, and (2) gene ontology (GO) annotations for data sets without documented object classes."]},
{"title": "Design and implementation of a multiband digital filter using FPGA to extract the ECG signal in the presence of different interference signals", "highlights": ["In this research, the ECG signal simulator, and the multiband digital filter is successfully designed.", "The designed filter has a linear phase response, so this reduces the filtered signal distortions and makes the diagnosis operation close to the real case.", "The practical experiments ensure that the possibility of using this type of filters in the ECG devices."], "abstract": ["In this paper, we propose a practical way to synthesize and filter an ECG signal in the presence of four types of interference signals: (1) those arising from power networks with a fundamental frequency of 50", "\u00a0", "Hz, (2) those arising from respiration, having a frequency range from 0.05 to 0.5", "\u00a0", "Hz, (3) muscle signals with a frequency of 25", "\u00a0", "Hz, and (4) white noise present within the ECG signal band. This was done by implementing a multiband digital filter (seven bands) of type FIR Multiband Least Squares using a digital programmable device (Cyclone II EP2C70F896C6 FPGA, Altera), which was placed on an education and development board (DE2-70, Terasic). This filter was designed using the VHDL language in the Quartus II 9.1 design environment. The proposed method depends on Direct Digital Frequency Synthesizers (DDFS) designed to synthesize the ECG signal and various interference signals. So that the synthetic ECG specifications would be closer to actual ECG signals after filtering, we designed in a single multiband digital filter instead of using three separate digital filters LPF, HPF, BSF. Thus all interference signals were removed with a single digital filter. The multiband digital filter results were studied using a digital oscilloscope to characterize input and output signals in the presence of differing sinusoidal interference signals and white noise."]},
{"title": "Mathematical model of a heterogeneous pulmonary acinus structure", "highlights": ["We propose a mathematical model of a heterogeneous acinus structure.", "The model uses Voronoi and Delaunay tessellations and simulated annealing.", "The model presents the alveolar and ductal structure distributions.", "Applying the modeling technique to rat acinus demonstrates the availability.", "The approach provides a platform for investigating heterogeneous nature of acinus."], "abstract": ["The pulmonary acinus is a gas exchange unit distal to the terminal bronchioles. A model of its structure is important for the computational investigation of mechanical phenomena at the acinus level. We propose a mathematical model of a heterogeneous acinus structure composed of alveoli of irregular sizes, shapes, and locations. The alveoli coalesce into an intricately branched ductal tree, which meets the space-filling requirement of the acinus structure. Our model uses Voronoi tessellation to generate an assemblage of the alveolar or ductal airspace, and Delaunay tessellation and simulated annealing for the ductal tree structure. The modeling condition is based on average acinar and alveolar volume characteristics from published experimental information. By applying this modeling technique to the acinus of healthy mature rats, we demonstrate that the proposed acinus structure model reproduces the available experimental information. In the model, the shape and size of alveoli and the length, generation, tortuosity, and branching angle of the ductal paths are distributed in several ranges. This approach provides a platform for investigating the heterogeneous nature of the acinus structure and its relationship with mechanical phenomena at the acinus level."]},
{"title": "Automated breast-region segmentation in the axial breast MR images", "highlights": ["The method for the breast-region segmentation in the axial MR images is proposed.", "It uses the shortest-path search incorporating information from the adjacent slice.", "The cost function incorporates the edges obtained using a tunable Gabor filter.", "The method is applicable for cases with no visible contrast at breast-region border.", "The method requires no training procedure."], "abstract": ["The purpose of this study was to develop a robust breast-region segmentation method independent from the visible contrast between the breast region and surrounding chest wall and skin.", "A fully-automated method for segmentation of the breast region in the axial MR images is presented relying on the edge map (EM) obtained by applying a tunable Gabor filter which sets its parameters according to the local MR image characteristics to detect non-visible transitions between different tissues having a similar MRI signal intensity. The method applies the shortest-path search technique by incorporating a novel cost function using the EM information within the border-search area obtained based on the border information from the adjacent slice. It is validated on 52 MRI scans covering the full American College of Radiology Breast Imaging-Reporting and Data System (BI-RADS) breast-density range.", "The obtained results indicate that the method is robust and applicable for the challenging cases where a part of the fibroglandular tissue is connected to the chest wall and/or skin with no visible contrast, i.e. no fat presence, between them compared to the literature methods proposed for the axial MR images. The overall agreement between automatically- and manually-obtained breast-region segmentations is 96.1% in terms of the Dice Similarity Coefficient, and for the breast-chest wall and breast-skin border delineations it is 1.9", "\u00a0", "mm and 1.2", "\u00a0", "mm, respectively, in terms of the Mean-Deviation Distance.", "The accuracy, robustness and applicability for the challenging cases of the proposed method show its potential to be incorporated into computer-aided analysis systems to support physicians in their decision making."]},
{"title": "An extended Cellular Potts Model analyzing a wound healing assay", "highlights": ["A wound healing assay is modeled by a suitable version of the Cellular Potts Model.", "Cells are represented as compartmentalized elements.", "Phenotypic differentiations occur among cells sited in distinct areas of the mass.", "The invasiveness of the culture depends on cell elasticity and adhesiveness.", "The healing process depends on topological properties of matrix fibrous component."], "abstract": ["A suitable Cellular Potts Model is developed to reproduce and analyze an ", " wound-healing assay. The proposed approach is able both to quantify the invasive capacity of the overall cell population and to evaluate selected determinants of single cell movement (velocity, directional movement, and final displacement). In this respect, the present CPM allows us to capture differences and correlations in the migratory behavior of cells initially located at different distances from the wound edge. In the case of an undifferentiated extracellular matrix, the model then predicts that a maximal healing can be obtained by a chemically induced increment of cell elasticity and not by a chemically induced downregulation of intercellular adhesive contacts. Moreover, in the case of two-component substrates (formed by a mesh of collagenous-like threads and by a homogeneous medium), CPM simulations show that both fiber number and cell\u2013fiber adhesiveness influence cell speed and wound closure rate in a biphasic fashion. On the contrary, the topology of the fibrous network affects the healing process by mediating the productive directional cell movement. The paper, also equipped with comments on the computational cost of the CPM algorithm, ends with a throughout discussion of the pertinent experimental and theoretical literature."]},
{"title": "Continuous Positive Airway Pressure treatment of premature infants; application of a computerized decision support system", "highlights": ["A computerized decision support system could give estimations of blood gases of infants on CPAP.", "A computerized decision support system could accurately predict improvement in oxygenation by supplemental oxygen if the infant under CPAP therapy was hypoxemic.", "Computerized decision support systems can be used as helpful tools to analyze infants\u2019 ventilation data."], "abstract": ["The predictions of a computerized decision-support system (CDSS) are compared to clinical data obtained from a group of premature infants. The infants were suffering from respiratory distress syndrome (RDS) and were treated by the Continuous Positive Airway Pressure (CPAP) therapy. The predictions of the CDSS are found to be in general agreement with clinical measurements. The CDSS is also used to determine the effect of low level oxygen treatment on arterial oxygen pressure if the infant\u05f3s oxygenation is low despite CPAP therapy. Based on the computational results, application of low levels of supplemental inspired fraction of oxygen (", ") would result in significant improvement in oxygenation of premature infants provided such treatment is carefully controlled to avoid hyperoxemia."]},
{"title": "Assessing the performance characteristics and clinical forces in simulated shape memory bone staple surgical procedure: The significance of SMA material model", "highlights": ["A powder metallurgically processed NiTi material was calibrated for the bone staple.", "Pre/post-surgical performance is studied for a body-temperature-activated SMA staple.", "Generated staple forces tended to increase under sustained thermal loading.", "The simulation results correlated well with the experimental measurements.", "The thermal activation scheme of the SMA staple was proved to be clinically viable."], "abstract": ["This work is focused on the detailed computer simulation of the key stages involved in a shape memory alloy (SMA) osteosynthesis bone stapling procedure. To this end, a recently developed three-dimensional constitutive SMA material model was characterized from test data of three simple uniaxial-isothermal-tension experiments for powder metallurgically processed nickel-rich NiTi (PM/NiTi-P) material. The calibrated model was subsequently used under the complex, thermomechanical loading conditions involved in the surgical procedure using the body-temperature-activated PM/NiTi-P bone staple. Our aim here is to assess the immediate and post-surgical performance characteristics of the stapling operation using the material model. From this study: (1) it was found that adequate compressive forces were developed by the PM/NiTi-P bone staple, with the tendency of this force to even increase under sustained thermal loading due to the ", " \u201cinverse relaxation phenomena\u201d in the SMA material, (2) the simulation results correlated well with those from experimental measurements, (3) the body-temperature-activated PM/NiTi-P staple was proved to be clinically viable, providing a stable clamping force needed for speedy coaptation of the fractured bones, and (4) these realistic assessments crucially depend on the use of suitable and comprehensive SMA material models."]},
{"title": "Improving PLS\u2013RFE based gene selection for microarray data classification", "highlights": ["We propose to accelerate PLS\u2013RFE based gene selection to classify microarray data.", "Two dynamic feature elimination schemes are combined with PLS\u2013RFE.", "The two proposed approaches can select similar gene subsets to PLS\u2013RFE.", "Experimental results demonstrate their effectiveness and efficiency in actual use."], "abstract": ["Gene selection plays a crucial role in constructing efficient classifiers for microarray data classification, since microarray data is characterized by high dimensionality and small sample sizes and contains irrelevant and redundant genes. In practical use, partial least squares-based gene selection approaches can obtain gene subsets of good qualities, but are considerably time-consuming. In this paper, we propose to integrate partial least squares based recursive feature elimination (PLS\u2013RFE) with two feature elimination schemes: ", " and ", ", respectively, to speed up the feature selection process. Inspired from the strategy of annealing schedule, the two proposed approaches eliminate a number of features rather than one least informative feature during each iteration and the number of removed features decreases as the iteration proceeds. To verify the effectiveness and efficiency of the proposed approaches, we perform extensive experiments on six publicly available microarray data with three typical classifiers, including Na\u00efve Bayes, K-Nearest-Neighbor and Support Vector Machine, and compare our approaches with ReliefF, PLS and PLS\u2013RFE feature selectors in terms of classification accuracy and running time. Experimental results demonstrate that the two proposed approaches accelerate the feature selection process impressively without degrading the classification accuracy and obtain more compact feature subsets for both two-category and multi-category problems. Further experimental comparisons in feature subset consistency show that the proposed approach with ", " scheme not only has better time performance, but also obtains slightly better feature subset consistency than the one with ", " scheme."]},
{"title": "Estimated confidence interval from single blood pressure measurement based on algorithmic fusion", "highlights": ["The CI of blood pressure estimates from a single measurement is derived.", "The parametric bootstrap technique with multiple regression is used.", "The exactness of SBP and DBP estimation is improved with tighter confidence intervals."], "abstract": [": Current oscillometric blood pressure measurement devices generally provide only single-point estimates for systolic and diastolic blood pressures and rarely provide confidence ranges for these estimates. A novel methodology to obtain confidence intervals (CIs) for systolic blood pressure (SBP) and diastolic blood pressure (DBP) estimates from a single oscillometric blood pressure measurement is presented.", ": The proposed methodology utilizes the multiple regression technique to fuse optimally a set of SBP and DBP estimates obtained through different algorithms. However, the set of SBP and DBP estimates is a small number to determine the CI of each individual subject. To address this issue, the weighted bootstrap approach based on the multiple regression technique was used to generate a pseudo sample set for the SBP and the DBP. In this paper, the multiple regression technique can estimate the best-fitting surface of an efficient function that relates the input sample set as an independent vector to the auscultatory nurse measurement as a dependent vector to estimate regression coefficients. Consequently, the coefficients are assigned to an eight-sample set obtained from the fusion of different algorithms as optimally weighted parameters. CIs are also estimated using the conventional methods on the set of fused SBP and DBP estimates for comparison purposes.", ": The proposed method was applied to an experimental dataset of 85 patients. The results indicated that the proposed approach provides better blood pressure estimates than the existing algorithms and, in addition, is able to provide CIs for a single measurement.", ": The CIs derived from the proposed scheme are much smaller than those calculated by conventional methods except for the pseudo maximum amplitude-envelope algorithm for both the SBP and the DBP, probably because of the decrease in the standard deviation through the increase in the pseudo measurements using the weighted bootstrap method for each subject. The proposed methodology is likely the only one currently available that can provide CIs for single-sample blood pressure measurements."]},
{"title": "Motility bar: A new tool for motility analysis of endoluminal videos", "highlights": ["We propose computer vision tools for the analysis of the motility bar image ", ".", "A method for contraction analysis is proposed.", "A method for lumen perimeter change characterization is introduced.", "An algorithm for stable motility segments detection is described.", "Quick analysis time with good qualitative and quantitative results."], "abstract": ["Wireless Capsule Endoscopy (WCE) provides a new perspective of the small intestine, since it enables, for the first time, visualization of the entire organ. However, the long visual video analysis time, due to the large number of data in a single WCE study, was an important factor impeding the widespread use of the capsule as a tool for intestinal abnormalities detection. Therefore, the introduction of WCE triggered a new field for the application of computational methods, and in particular, of computer vision. In this paper, we follow the computational approach and come up with a new perspective on the small intestine motility problem. Our approach consists of three steps: first, we review a tool for the visualization of the motility information contained in WCE video; second, we propose algorithms for the characterization of two motility building-blocks: contraction detector and lumen size estimation; finally, we introduce an approach to detect segments of stable motility behavior. Our claims are supported by an evaluation performed with 10 WCE videos, suggesting that our methods ably capture the intestinal motility information."]},
{"title": "A computer-aided automated methodology for the detection and classification of occlusal caries from photographic color images", "highlights": ["An automated methodology for the diagnosis of occlusal caries is presented.", "Methodology can be a support to the dentists for making the final decision.", "Methodology is based on digital photography analysis.", "Methodology can incorporate new knowledge on caries diagnosis.", "Detection and classification (7 ICDAS classes) accuracy is 80% and 83%, respectively."], "abstract": ["The aim of this work is to present a computer-aided automated methodology for the assessment of carious lesions, according to the International Caries Detection and Assessment System (ICDAS II), which are located on the occlusal surfaces of posterior permanent teeth from photographic color tooth images. The proposed methodology consists of two stages: (a) the detection of regions of interest and (b) the classification of the detected regions according to ICDAS \u0399\u0399. In the first stage, pre-processing, segmentation and post-processing mechanisms were employed. For each pixel of the detected regions, a 15\u00d715 neighborhood is used and a set of intensity-based and texture-based features were extracted. A correlation based technique was applied to select a subset of 36 features which were given as input into the classification stage, where five classifiers (J48, Random Tree, Random Forests, Support Vector Machines and Na\u00efve Bayes) were compared to conclude to the best one, in our case, to Random Forests. The methodology was evaluated on a set of 103 digital color images where 425 regions of interest from occlusal surfaces of extracted permanent teeth were manually segmented and classified, based on visual assessments by two experts. The methodology correctly detected 337 out of 340 regions in the detection stage with accuracy of detection 80%. For the classification stage an overall accuracy 83% is achieved. The proposed methodology provides an objective and fully automated caries diagnostic system for occlusal carious lesions with similar or better performance of a trained dentist taking into consideration the available medical knowledge."]},
{"title": "Automated high-content morphological analysis of muscle fiber histology", "highlights": ["We developed an image processing pipeline to automatically analyze muscle fibers.", "The method can quickly quantify muscle fiber images for high-content analysis.", "Validation showed that the method achieved high objectivity.", "The method can help researchers make discovery in muscle-related experiments."], "abstract": ["In the search for a cure for many muscular disorders it is often necessary to analyze muscle fibers under a microscope. For this morphological analysis, we developed an image processing approach to automatically analyze and quantify muscle fiber images so as to replace today\u2019s less accurate and time-consuming manual method. Muscular disorders, that include cardiomyopathy, muscular dystrophies, and diseases of nerves that affect muscles such as neuropathy and myasthenia gravis, affect a large percentage of the population and, therefore, are an area of active research for new treatments. In research, the morphological features of muscle fibers play an important role as they are often used as biomarkers to evaluate the progress of underlying diseases and the effects of potential treatments. Such analysis involves assessing histopathological changes of muscle fibers as indicators for disease severity and also as a criterion in evaluating whether or not potential treatments work. However, quantifying morphological features is time-consuming, as it is usually performed manually, and error-prone. To replace this standard method, we developed an image processing approach to automatically detect and measure the cross-sections of muscle fibers observed under microscopy that produces faster and more objective results. As such, it is well-suited to processing the large number of muscle fiber images acquired in typical experiments, such as those from studies with pre-clinical models that often create many images. Tests on real images showed that the approach can segment and detect muscle fiber membranes and extract morphological features from highly complex images to generate quantitative results that are readily available for statistical analysis."]},
{"title": "Suggestions for automatic quantitation of endoscopic image analysis to improve detection of small intestinal pathology in celiac disease patients", "highlights": ["Endoscopic imaging is utilized to diagnose celiac disease and other small bowel disorders.", "Currently, detection of abnormality in the images is mostly done by visual inspection.", "If classification could be quantitated and automated, it would be useful for unbiased and rapid analysis.", "A review of the literature, and pathways to proceed in quantitation, are suggested in the article.", "Use of a variety of features, weighted in order of importance, may provide the best paradigm for quantitation."], "abstract": ["Although many groups have attempted to develop an automated computerized method to detect pathology of the small intestinal mucosa caused by celiac disease, the efforts have thus far failed. This is due in part to the occult presence of the disease. When pathological evidence of celiac disease exists in the small bowel it is visually often patchy and subtle. Due to presence of extraneous substances such as air bubbles and opaque fluids, the use of computerized automation methods have only been partially successful in detecting the hallmarks of the disease in the small intestine\u2014villous atrophy, fissuring, and a mottled appearance. By using a variety of computerized techniques and assigning a weight or vote to each technique, it is possible to improve the detection of abnormal regions which are indicative of celiac disease, and of treatment progress in diagnosed patients. Herein a paradigm is suggested for improving the efficacy of automated methods for measuring celiac disease manifestation in the small intestinal mucosa. The suggestions are applicable to both standard and videocapsule endoscopic imaging, since both methods could potentially benefit from computerized quantitation to improve celiac disease diagnosis."]},
{"title": "Image-based computational simulation of sub-endothelial LDL accumulation in a human right coronary artery", "highlights": ["Branch points are the main regions of disturbed flow with low values of ", ".", "Elevated levels of LDL accumulation are located at the branch points.", "Hypertension increases risk of atherosclerosis at the branch points.", "Non-Newtonian behavior of blood flow reduces sub-endothelial accumulation of LDL."], "abstract": ["Accumulation of low density lipoproteins (LDL) in the vessel wall is suggested as the initiator of atherosclerosis and coronary stenosis. This process is associated with the performance of endothelium layer that regulates entering of macromolecules to the vessel wall. Therefore, the present study aims to investigate sub-endothelial accumulation of LDL molecules in a coronary tree and predict atherosclerosis prone sites. Non-Newtonian blood flow is simulated for normal and hypertensive conditions through the lumen of a right coronary artery reconstructed from computed tomography (CT) images. A three-pore model is implemented as the endothelium boundary condition and hence, plasma flow and LDL transport are simulated within the arterial wall. Based on the pore model, endothelium pathways divide into normal junctions, vesicles and leaky junctions. Most of LDL molecules pass through the leaky junctions that arise at locations with low wall shear stress (", "). Results indicate that increase in the number of leaky junctions at branch points with low ", " can lead to both elevated levels of sub-endothelial LDL accumulation and atherosclerosis risk. Findings reveal that at the branch points with disturbed flow, sub-endothelial concentration of LDL for the hypertensive condition is higher than the normal condition, however for the rest of regions with uniform geometry and unidirectional flow, this is reversed. Comparisons of non-Newtonian and Newtonian flows show mean increases of 34% and 13% in the sub-endothelial concentrations of Newtonian flows during the normal and hypertensive conditions, respectively."]},
{"title": "Evaluation of vermillion border descriptors and relevance vector machines discrimination model for making probabilistic predictions of solar cheilosis on digital lip photographs", "highlights": ["Robust descriptors of lip vermillion border.", "Probabilistic model for lip abnormality assessment.", "Reliability and reproducibility evaluation."], "abstract": ["Solar cheilosis (SC), a common precancer of the lower lip with a high potential to progress to invasive squamous cell carcinoma, presents with characteristic morphological vermillion-skin border alterations, like the border retraction.", "To determine robust macro-morphological descriptors of the vermillion border from non-standardized digital photographs and to exploit a probabilistic model for SC recognition in real clinical environments.", "Lip borders of 150 individuals (75 SC patients, 75 non-SC controls) were quantified on the basis of the extent of vermillion retraction and the degree of border irregularity employing fractal features and type-P Fourier descriptors. Eight lip border quantifiers were evaluated in terms of their reliability and reproducibility. The probabilistic \u2018diagnostic\u2019 model was implemented using the relevance vector machine (RVM) algorithm.", "Picture acquisition contributes substantially to overall variability of lip border images; however, for the different lip descriptors 33% to 65% of border morphological variability is due to differences among individuals. Different camera operators or the use of cameras with different specifications did not affect significantly the extracted lip features. The proposed RVM probabilistic model yielded a high sensitivity and specificity of 94.6% and 96%, respectively.", "We explored the use of digital photography within the clinical routine setting to validate a probabilistic model for the assessment of lip conditions like SC. The proposed method opens new perspectives toward a cost effective, non-invasive monitoring of SC to support large scale epidemiological and interventional studies in different clinical environments."]},
{"title": "Discrimination of retinal images containing bright lesions using sparse coded features and SVM", "highlights": ["Automatic feature extraction from retinal images.", "Discrimination between retinal images containing bright lesions.", "Retinal images classification using sparse coding.", "Better performance than bag-of-word approach."], "abstract": ["Diabetic Retinopathy (DR) is a chronic progressive disease of the retinal microvasculature which is among the major causes of vision loss in the world. The diagnosis of DR is based on the detection of retinal lesions such as microaneurysms, exudates and drusen in retinal images acquired by a fundus camera. However, bright lesions such as exudates and drusen share similar appearances while being signs of different diseases. Therefore, discriminating between different types of lesions is of interest for improving screening performances. In this paper, we propose to use sparse coding techniques for retinal images classification. In particular, we are interested in discriminating between retinal images containing either exudates or drusen, and normal images free of lesions. Extensive experiments show that dictionary learning techniques can capture strong structures of retinal images and produce discriminant descriptors for classification. In particular, using a linear SVM with the obtained sparse coded features, the proposed method achieves superior performance as compared with the popular Bag-of-Visual-Word approach for image classification. Experiments with a dataset of 828 retinal images collected from various sources show that the proposed approach provides excellent discrimination results for normal, drusen and exudates images. It achieves a sensitivity and a specificity of 96.50% and 97.70% for the normal class; 99.10% and 100% for the drusen class; and 97.40% and 98.20% for the exudates class with a medium size dictionary of 100 atoms."]},
{"title": "Rapid 3-D delineation of cell nuclei for high-content screening platforms", "highlights": ["We developed a 3D-RSD algorithm to detect nuclei in 3-D image space.", "3D-RSD can effectively deal with highly confluent specimens.", "3D-RSD is fast and more accurate than three other reference methods, and can be used in high-content screening tasks to evaluate drug efficacy in human cancer cell lines."], "abstract": ["High-resolution three-dimensional (3-D) microscopy combined with multiplexing of fluorescent labels allows high-content analysis of large numbers of cell nuclei. The full automation of 3-D screening platforms necessitates image processing algorithms that can accurately and robustly delineate nuclei in images with little to no human intervention. Imaging-based high-content screening was originally developed as a powerful tool for drug discovery. However, cell confluency, complexity of nuclear staining as well as poor contrast between nuclei and background result in slow and unreliable 3-D image processing and therefore negatively affect the performance of studying a drug response.", "Here, we propose a new method, 3D-RSD, to delineate nuclei by means of 3-D radial symmetries and test it on high-resolution image data of human cancer cells treated by drugs. The nuclei detection performance was evaluated by means of manually generated ground truth from 2351 nuclei (27 confocal stacks). When compared to three other nuclei segmentation methods, 3D-RSD possessed a better true positive rate of 83.3% and ", "-score of 0.895\u00b10.045 (", "-value=0.047). Altogether, 3D-RSD is a method with a very good overall segmentation performance. Furthermore, implementation of radial symmetries offers good processing speed, and makes 3D-RSD less sensitive to staining patterns. In particular, the 3D-RSD method performs well in cell lines, which are often used in imaging-based HCS platforms and are afflicted by nuclear crowding and overlaps that hinder feature extraction."]},
{"title": "Multiclassifier system with hybrid learning applied to the control of bioprosthetic hand", "highlights": ["Application of multiclassifier system for bioprosthetic hand control is proposed.", "Three multiclassifier systems using feedback information are developed.", "Three multiclassifier systems are compared in the static mode.", "Three multiclassifier systems are compared in the dynamic mode."], "abstract": ["In this paper the problem of recognition of the intended hand movements for the control of bio-prosthetic hand is addressed. The proposed method is based on recognition of electromiographic (EMG) and mechanomiographic (MMG) biosignals using a multiclassifier system (MCS) working in a two-level structure with a dynamic ensemble selection (DES) scheme and original concepts of competence function. Additionally, feedback information coming from bioprosthesis sensors on the correct/incorrect classification is applied to the adjustment of the combining mechanism during MCS operation through adaptive tuning competences of base classifiers depending on their decisions. Three MCS systems operating in decision tree structure and with different tuning algorithms are developed. In the MCS1 system, competence is uniformly allocated to each class belonging to the group indicated by the feedback signal. In the MCS2 system, the modification of competence depends on the node of decision tree at which a correct/incorrect classification is made. In the MCS3 system, the randomized model of classifier and the concept of cross-competence are used in the tuning procedure.", "Experimental investigations on the real data and computer-simulated procedure of generating feedback signals are performed. In these investigations classification accuracy of the MCS systems developed is compared and furthermore, the MCS systems are evaluated with respect to the effectiveness of the procedure of tuning competence. The results obtained indicate that modification of competence of base classifiers during the working phase essentially improves performance of the MCS system and that this improvement depends on the MCS system and tuning method used."]},
{"title": "Use of whole genome DNA spectrograms in bacterial classification", "highlights": ["We developed the SpectCMP method, which is suitable for the classification of bacteria at different taxonomic levels.", "The SpectCMP method is an alignment-free method that is based on DNA spectrogram comparisons.", "The SpectCMP method works with whole genome sequences; it does not require retrieving protein coding regions from the genome."], "abstract": ["A spectrogram reflects the arrangement of nucleotides through the whole chromosome or genome. Our previous study suggested that the spectrogram of whole genome DNA sequences is a suitable tool for the determination of relationships among bacteria. Related bacteria have similar spectrograms, and similarity in spectrograms was measured using a color layout descriptor. Several parameters, such as the mapping of four bases into a spectrogram, the number of considered elements in the color layout descriptor, the color model of the image and the building tree method, can be changed. This study addresses the use of parameter selection to ensure the best classification results. The quality of the classification was measured by Matthew\u2019s correlation coefficient (MCC). The proposed method with optimal parameters (called SpectCMP\u2014Spectrogram CoMParison method) achieved an average MCC of 0.73 at the phylum level. The SpectCMP method was also tested at the order level; the average MCC in the classification of class ", " was 0.76. The success of a classification with respect to the correct phyla was compared to three methods that are used in bacterial phylogeny: the CVTree method, OGTree method and moment vector method. The results show that the SpectCMP method can be used in bacterial classification at various taxonomic levels."]},
{"title": "Medical data sheet in safe havens \u2013 A tri-layer cryptic solution", "highlights": ["Tri-layer cryptic approach on Digital Imaging and Communications in Medicine (", ") images", "Involves Latin square image cipher (", "), Discrete Gould Transform (", ") and ", " encryption algorithms", "Encrypted data was tested by trans-received using Universal Software Radio Peripheral (", ") with ", "Confusion, permutation, randomness and authentication have been fused in single platform", ", ", ", Information entropy and key sensitivity was analysed and compared with the existing literature"], "abstract": ["Secured sharing of the diagnostic reports and scan images of patients among doctors with complementary expertise for collaborative treatment will help to provide maximum care through faster and decisive decisions. In this context, a tri-layer cryptic solution has been proposed and implemented on Digital Imaging and Communications in Medicine (DICOM) images to establish a secured communication for effective referrals among peers without compromising the privacy of patients. In this approach, a blend of three cryptic schemes, namely Latin square image cipher (LSIC), discrete Gould transform (DGT) and Rubik\u05f3s encryption, has been adopted. Among them, LSIC provides better substitution, confusion and shuffling of the image blocks; DGT incorporates tamper proofing with authentication; and Rubik renders a permutation of DICOM image pixels. The developed algorithm has been successfully implemented and tested in both the software (MATLAB 7) and hardware Universal Software Radio Peripheral (USRP) environments. Specifically, the encrypted data were tested by transmitting them through an additive white Gaussian noise (AWGN) channel model. Furthermore, the sternness of the implemented algorithm was validated by employing standard metrics such as the unified average changing intensity (UACI), number of pixels change rate (NPCR), correlation values and histograms. The estimated metrics have also been compared with the existing methods and dominate in terms of large key space to defy brute force attack, cropping attack, strong key sensitivity and uniform pixel value distribution on encryption."]},
{"title": "An efficient detection of epileptic seizure by differentiation and spectral analysis of electroencephalograms", "highlights": ["A new epileptic seizure detection algorithm with less complexity is proposed.", "The algorithm was based on Hjorth\u05f3s mobility and spectral features.", "We also employed quadratic discrimination analysis to reduce computing loads.", "The proposed algorithm produced seizure detection performance above 99%."], "abstract": ["Epilepsy is a critical neurological disorder resulting from abnormal hyper-excitability of neurons in the brain. Studies have shown that epilepsy can be detected in electroencephalography (EEG) recordings of patients suffering from seizures. The performance of EEG-based epileptic seizure detection relies largely on how well one can extract features from an EEG that characterize seizure activity. Conventional feature extraction methods using time-series analysis, spectral analysis and nonlinear dynamic analysis have advanced in recent years to improve detection. The computational complexity has also increased to obtain a higher detection rate. This study aimed to develop an efficient feature extraction method based on Hjorth\u05f3s mobility to reduce computational complexity while maintaining high detection accuracy. A new feature extraction method was proposed by computing the spectral power of Hjorth\u05f3s mobility components, which were effectively estimated by differentiating EEG signals in real-time. Using EEG data in five epileptic patients, this method resulted in a detection rate of 99.46% between interictal and epileptic EEG signals and 99.78% between normal and epileptic EEG signals, which is comparable to most advanced nonlinear methods. These results suggest that the spectral features of Hjorth\u05f3s mobility components in EEG signals can represent seizure activity and may pave the way for developing a fast and reliable epileptic seizure detection method."]},
{"title": "Model of unidirectional block formation leading to reentrant ventricular tachycardia in the infarct border zone of postinfarction canine hearts", "highlights": ["Ventricular tachycardia is common after myocardial infarction and it is a serious health problem worldwide.", "The source of reentrant ventricular tachycardia is often located in the infarct border zone, the layer of thin surviving myocardium adjacent to the infarct.", "Although there is electrical activity occurring in the infarct border zone, the conduction velocity of the leading edge of the propagating wavefront is not constant.", "Where the infarct border zone dimension changes from thin-to-thick in the travel direction, very slow conduction or even functional block occur at locations where the wavefront becomes critically convex.", "In this study, the activation rate and IBZ geometric structure necessary for functional block to form during premature stimulation and during reentrant ventricular tachycardia, are determined."], "abstract": ["When the infarct border zone is stimulated prematurely, a unidirectional block line (UBL) can form and lead to double-loop (figure-of-eight) reentrant ventricular tachycardia (VT) with a central isthmus. The isthmus is composed of an entrance, center, and exit. It was hypothesized that for certain stimulus site locations and coupling intervals, the UBL would coincide with the isthmus entrance boundary, where infarct border zone thickness changes from thin-to-thick in the travel direction of the premature stimulus wavefront.", "A quantitative model was developed to describe how thin-to-thick changes in the border zone result in critically convex wavefront curvature leading to conduction block, which is dependent upon coupling interval. The model was tested in 12 retrospectively analyzed postinfarction canine experiments. Electrical activation was mapped for premature stimulation and for the first reentrant VT cycle. The relationship of functional conduction block forming during premature stimulation to functional block during reentrant VT was quantified.", "For an appropriately placed stimulus, in accord with model predictions: (1) The UBL and reentrant VT isthmus lateral boundaries overlapped (error: 4.8\u00b15.7", "\u00a0", "mm). (2) The UBL leading edge coincided with the distal isthmus where the center-entrance boundary would be expected to occur. (3) The mean coupling interval was 164.6\u00b111.0", "\u00a0", "ms during premature stimulation and 190.7\u00b120.4", "\u00a0", "ms during the first reentrant VT cycle, in accord with model calculations, which resulted in critically convex wavefront curvature with functional conduction block, respectively, at the location of the isthmus entrance boundary and at the lateral isthmus edges.", "Reentrant VT onset following premature stimulation can be explained by the presence of critically convex wavefront curvature and unidirectional block at the isthmus entrance boundary when the premature stimulation interval is sufficiently short. The double-loop reentrant circuit pattern is a consequence of wavefront bifurcation around this UBL followed by coalescence, and then impulse propagation through the isthmus. The wavefront is blocked from propagating laterally away from the isthmus by sharp increases in border zone thickness, which results in critically convex wavefront curvature at VT cycle lengths."]},
{"title": "Proposition, identification, and experimental evaluation of an inverse dynamic neuromusculoskeletal model for the human finger", "highlights": ["We propose an inverse dynamic neuromusculosketal model for the middle finger during motion.", "The optimization problem for estimating the model unknowns is complex.", "We propose a multistep robust estimation procedure to solve this problem.", "We test it by simulation and propose an attempt to experimentally evaluate the proposed model.", "The proposed model should be useful for rehabilitation applications."], "abstract": ["The purpose of this study is to develop an inverse dynamic model of the human middle finger in order to identify the muscle activation, muscle force, and neural activation of the muscles involved during motion. Its originality comes from the coupling of biomechanical and physiological models and the proposition of a dedicated optimization procedure and cost function for identifying the model unknowns.", "Three sub-models work in interaction: the first is the biomechanical model, primarily consisting of the dynamic equations of the middle finger system; the second is the muscle model, which helps to identify the muscle force from muscle activation and dynamic deformation for six involved muscles. The third model allows one to link muscle activation to neural intent from the Central Nervous System (CNS). This modeling procedure leads to a complex analytical nonlinear system identified using multi-step energy minimization procedure and a specific cost function.", "Numerical simulations with different articulation velocities are presented and discussed. Then, experimental evaluation of the proposed model is performed following a protocol combining electromyography and motion capture during a hand opening\u2013closing paradigm. After comparison, several results from the simulation and experiments were found to be in accordance. The difficulty in evaluating such complex dynamic models is also demonstrated.", "Despite the model simplifications, the obtained preliminary results are promising. Indeed, the proposed model, once correctly validated in future works, should be a relevant tool to simulate and predict deficiencies of the middle finger system for rehabilitation purposes."]},
{"title": "Automatic segmentation of vessels from angiogram sequences using adaptive feature transformation", "highlights": ["We developed an efficient method for automatically segmenting vessels from angiogram sequences.", "The proposed method can extract high-contrast angiograms and segment vessels.", "The proposed method uses two modification factors to improve vesselness response.", "The proposed method is easier to control, and requires less computational time."], "abstract": ["This paper proposes an efficient method for automatically segmenting vessels from angiogram sequences. The method includes two steps: extracting high-contrast angiograms and segmenting vessels. First, we select high-contrast angiograms automatically using vessel intensity distribution. Based on multiscale Hessian-based filtering, we propose an adaptive feature transformation function to improve the vesselness response. This method overcomes numerous problems, which exist in the X-ray angiograms by using the scale factors and transformed intensities. Various scales are established to mitigate variations of the intensity distribution. The transformed intensities are applied to coping with lower contrast and nonuniform intensity distribution. Finally, the connected component labeling method is used to extract the vessels. The proposed method can distinguish between the vessel and the background in a complex background. In our experiments, 20 angiogram sequences are used to evaluate the accuracy of the selected high-contrast angiogram. The accuracy of extracting high-contrast angiograms is 98%. For evaluating the accuracy of the segmentation results, 72 angiograms were selected. The accuracy of the proposed segmentation method is 96.3%. The Kappa value is 81.8%. After inspection by a cardiologist, the experimental results show that the proposed method can automatically and accurately segment vessels."]},
{"title": "A fully automatic 2D segmentation method for uterine fibroid in MRgFUS treatment evaluation", "highlights": ["The proposed method performs a fully automatic uterus and fibroids segmentation.", "We provide the boundary and volume of ablated fibroid regions and uterus.", "Our approach is based on FCM and iterative optimal threshold selection algorithms.", "This solution could optimize the current operative MRgFUS methodology.", "Our approach can improve the follow-up of patients undergone MRgFUS treatments."], "abstract": ["Magnetic Resonance guided Focused UltraSound (MRgFUS) represents a non-invasive surgical approach that uses thermal ablation to treat uterine fibroids. After the MRgFUS treatment, an operator must manually segment the treated fibroid areas to evaluate the NonPerfused Volume (NPV). This manual approach is operator-dependent, introducing issues of result reproducibility, which could lead to errors in the subsequent follow-up phase. Moreover, manual segmentation is time-consuming, and can have a negative impact on the optimization of both machine-time and operator-time.", "To address these issues, in this paper a novel fully automatic method based on the unsupervised ", " and ", " algorithms for uterus and fibroid segmentation is proposed. The developed method could be used to enhance the current manual methodology performed by healthcare operators for post-operative NPV evaluation in uterine fibroid MRgFUS treatments.", "The proposed method was tested on 15 MR datasets of 15 different patients with uterine fibroids and evaluated using area-based and distance-based metrics. A comparison of extracted volume was also performed. Average values for fibroid (ROT) segmentation are SDI=88.67%, JI=80.70%, SE=89.79%, SP=88.73%, MAD=2.200 [pixels], MAXD=6.233 [pixels] and HD=2.988 [pixels]. Moreover, to make a quantitative evaluation of this method, our experimental results were compared with similar literature approaches.", "The proposed method provides a practical approach for the automatic evaluation of the boundary and volume of ablated fibroid regions, without any external user input. The achieved segmentation results show the validity and the effectiveness of the proposed solution."]},
{"title": "Age-series based link prediction in evolving disease networks", "highlights": ["A weighted disease network which indicates the relationships between diseases.", "A novel age-series based link prediction method for the disease network.", "A large number of correlations are hinted by the topology of a disease network.", "Medical histories of a set of similar patients are important for prediction.", "Good performance values for the case-based link prediction."], "abstract": ["Recently, several research efforts based on social network analysis and methods have been made for medical care information. One of these efforts is to extract the relationships between diseases by using social network modeling. However, all of previous works used the relationships in a simple way in a network consisting of diseases regardless of time or age factors. In this paper, we predict the onset of future diseases on the basis of the current health status of patients by considering age factor. The problem of predicting the relations between diseases is a really difficult and, at the same time, an important task. For this purpose, this paper first constructs a weighted disease network and then, it proposes a novel link prediction method, to identify the connections between diseases, building the evolving structure of the disease network with respect to patients\u2019 ages. To the best of our knowledge, this is the first attempt in predicting the connections between diseases according to patients\u2019 ages. Experiments on a real network demonstrate that the proposed approach can reveal disease correlations accurately and perform well at capturing future disease risks."]},
{"title": "Application of dual tree complex wavelet transform in tandem mass spectrometry", "highlights": ["Application of dual tree complex wavelet transform in tandem mass spectroscopy.", "New class of almost symmetric wavelet used to denoise MS/MS data.", "Results in a better detection of peptides in biological samples."], "abstract": ["Mass Spectrometry (MS) is a widely used technique in molecular biology for high throughput identification and sequencing of peptides (and proteins). Tandem mass spectrometry (MS/MS) is a specialised mass spectrometry technique whereby the sequence of peptides can be determined. Preprocessing of the MS/MS data is indispensable before performing any statistical analysis on the data. In this work, preprocessing of MS/MS data is proposed based on the Dual Tree Complex Wavelet Transform (DTCWT) using almost symmetric Hilbert pair of wavelets. After the preprocessing step, the identification of peptides is done using the database search approach. The performance of the proposed preprocessing technique is evaluated by comparing its performance against Discrete Wavelet Transform (DWT) and Stationary Wavelet Transform (SWT). The preprocessing performed using DTCWT identified more peptides compared to DWT and SWT."]},
{"title": "Computational modeling of cardiac optogenetics: Methodology overview & review of findings from simulations", "highlights": ["In Part 1, we discuss state-of-the-art methods used to model cardiac optogenetics.", "Required techniques at the protein, cell, tissue, and organ scales are summarized.", "A novel exploration of optogenetic cardiac action potential silencing is included.", "In Part 2, we review findings that have emerged from cardiac optogenetics modeling.", "Simulations will help assess the efficacy of potential clinical applications."], "abstract": ["Cardiac optogenetics is emerging as an exciting new potential avenue to enable spatiotemporally precise control of excitable cells and tissue in the heart with low-energy optical stimuli. This approach involves the expression of exogenous light-sensitive proteins (opsins) in target heart tissue via viral gene or cell delivery. Preliminary experiments in optogenetically-modified cells, tissue, and organisms have made great strides towards demonstrating the feasibility of basic applications, including the use of light stimuli to pace or disrupt reentrant activity. However, it remains unknown whether techniques based on this intriguing technology could be scaled up and used in humans for novel clinical applications, such as pain-free optical defibrillation or dynamic modulation of action potential shape. A key step towards answering such questions is to explore potential optogenetics-based therapies using sophisticated computer simulation tools capable of realistically representing opsin delivery and light stimulation in biophysically detailed, patient-specific models of the human heart. This review provides (1) a detailed overview of the methodological developments necessary to represent optogenetics-based solutions in existing virtual heart platforms and (2) a survey of findings that have been derived from such simulations and a critical assessment of their significance with respect to the progress of the field."]},
{"title": "Using Petri nets for experimental design in a multi-organ elimination pathway", "highlights": ["We developed a Petri net to assist in the design of experiments.", "We modelled the human multi-organ genistein elimination pathway.", "Gut epithelium profiles and additional constraints improve the model parameterization."], "abstract": ["Genistein is a soy metabolite with estrogenic activity that may result in (un)favorable effects on human health. Elucidation of the mechanisms through which food additives such as genistein exert their beneficiary effects is a major challenge for the food industry. A better understanding of the genistein elimination pathway could shed light on such mechanisms. We developed a Petri net model that represents this multi-organ elimination pathway and which assists in the design of future experiments. Using this model we show that metabolic profiles solely measured in venous blood are not sufficient to uniquely parameterize the model. Based on simulations we suggest two solutions that provide better results: parameterize the model using gut epithelium profiles or add additional biological constrains in the model."]},
{"title": "First review on psoriasis severity risk stratification: An engineering perspective", "highlights": ["First review on psoriasis severity risk stratification in engineering perspective.", "In-depth examination of the existing literature for all parameters of PASI.", "pCAD system for psoriasis severity assessment in large population.", "Variety of psoriasis features based on tissue characteristics.", "Validation of system in terms of reliability and stability."], "abstract": ["Computer-aided diagnosis (CAD) systems have been used for characterization of several dermatologic diseases in the last few years. Psoriasis is a potentially life-threatening skin disease which affects 125 million people worldwide. The paper presents the first state-of-the-art review of technology solicitation in psoriasis along with its current practices, challenges and assessment techniques. The paper also conducts in-depth examination of the existing literature for all clinical parameters of Psoriasis Area and Severity Index (PASI) ", " area, erythema, scaliness and thickness. We suggest a role of risk assessment using a decision support system for stratification of psoriasis in large populations. A balanced insight has been presented in all the components of the design, namely: feature extraction, feature selection, disease stratification and overall CAD performance evaluation. We conclude that CAD systems are promising for risk stratification and assessment of psoriasis."]},
{"title": "Practical use of medical terminology in curriculum mapping", "highlights": ["We review the existing and widely-accepted medical terminologies and coding systems.", "We design and implement a web-based system for medical curriculum management.", "A practical use of the MeSH thesaurus in a curriculum mapping application is showed.", "Two data-analytical reports based on the MeSH thesaurus are described in detail."], "abstract": ["Various information systems for medical curriculum mapping and harmonization have been developed and successfully applied to date. However, the methods for exploiting the datasets captured inside the systems are rather lacking.", "We reviewed the existing medical terminologies, nomenclatures, coding and classification systems in order to select the most suitable one and apply it in delivering visual analytic tools and reports for the benefit of medical curriculum designers and innovators.", "A formal description of a particular curriculum of general medicine is based on 1347 learning units covering 7075 learning outcomes. Two data-analytical reports have been developed and discussed, showing how the curriculum is consistent with the MeSH thesaurus and how the MeSH thesaurus can be used to demonstrate interconnectivity of the curriculum through association analysis.", "Although the MeSH thesaurus is designed mainly to index medical literature and support searching through bibliographic databases, we have proved its use in medical curriculum mapping as being beneficial for curriculum designers and innovators. The presented approach can be followed wherever needed to identify all the mandatory components used for transparent and comprehensive overview of medical curriculum data."]},
{"title": "Evidence for the relationship between the regulatory effects of microRNAs and attack robustness of biological networks", "highlights": ["We introduce miRNA-based measures of importance of nodes in a network.", "The concept of \u201cattack robustness\u201d is used to study the relevance of these measures.", "We show that these measures can locate the important network components.", "Our results suggest that miRNA regulation and network robustness are related."], "abstract": ["It has been previously suggested that microRNAs (miRNAs) have a tendency to regulate the important components of biological networks. The goal of the present study was to systematically test if one can establish a relationship between miRNA targets and the important components of biological networks (including human protein-protein interaction network, signaling network and metabolic network). For this analysis, we have studied the attack robustness of these networks. It has been previously shown that deletion of network vertices in descending order of their importance (e.g., in decreasing order of vertex degrees) can affect the network structure much more considerably. In the current study, we introduced three miRNA-based measures of importance: \u201cmiRNA count\u201d (i.e., the number of miRNAs that regulate a given network component); average adjacent miRNA count, \u201cAAmiC\u201d (i.e., the average number of miRNAs regulating the targeted components adjacent to a given component); and total adjacent miRNA count, \u201cTAmiC\u201d (i.e., the total number of miRNAs regulating the targeted components adjacent to a given component). Our results suggest that \u201cmiRNA count\u201d is only marginally capable of locating the important components of the networks, while TAmiC was the most relevant measure. By comparing TAmiC with the classical centrality measures (which are solely based on the network structure) when simultaneously removing vertices, we show that this measure is correlated to degree and betweenness centrality measures, while its performance is generally better than that of closeness and eigenvector centrality measures. The results of this study suggest that TAmiC which represents a measure based on both network structure and biological knowledge, can successfully determine the important network components indicating that miRNA regulation and network robustness are related."]},
{"title": "Combining conceptual graphs and argumentation for aiding in the teleexpertise", "highlights": ["Improving the reasoning in conceptual graphs for better knowledge representation.", "Incorporation of argumentation algorithms for medical decision making processes.", "Structuring of reasoning procedures to guarantee traceability in the telemedicine.", "Elucidation of the mechanisms underlying some ethical problems in telemedicine.", "Effective benefits for both medical professionals and patients in the remote sites."], "abstract": ["Current medical information systems are too complex to be meaningfully exploited. Hence there is a need to develop new strategies for maximising the exploitation of medical data to the benefit of medical professionals. It is against this backdrop that we want to propose a tangible contribution by providing a tool which combines conceptual graphs and Dung\u05f3s argumentation system in order to assist medical professionals in their decision making process. The proposed tool allows medical professionals to easily manipulate and visualise queries and answers for making decisions during the practice of teleexpertise. The knowledge modelling is made using an open application programming interface (API) called CoGui, which offers the means for building structured knowledge bases with the dedicated functionalities of graph-based reasoning via retrieved data from different institutions (hospitals, national security centre, and nursing homes). The tool that we have described in this study supports a formal traceable structure of the reasoning with acceptable arguments to elucidate some ethical problems that occur very often in the telemedicine domain."]},
{"title": "Adaptive image inversion of contrast 3D echocardiography for enabling automated analysis", "highlights": ["Contrast 3d echocardiography suffers from high speckle.", "The automated analysis from contrast 3d echocardiography is a difficult task.", "Contrast image inversion is proposed to match appearance with non-contrast image.", "An automatic threshold estimation method is proposed to enable image inversion.", "The contrast image inversion enables LV segmentation using non-contrast methods."], "abstract": ["Contrast 3D echocardiography (C3DE) is commonly used to enhance the visual quality of ultrasound images in comparison with non-contrast 3D echocardiography (3DE). Although the image quality in C3DE is perceived to be improved for visual analysis, however it actually deteriorates for the purpose of automatic or semi-automatic analysis due to higher speckle noise and intensity inhomogeneity. Therefore, the LV endocardial feature extraction and segmentation from the C3DE images remains a challenging problem.", "To address this challenge, this work proposes an adaptive pre-processing method to invert the appearance of C3DE image. The image inversion is based on an image intensity threshold value which is automatically estimated through image histogram analysis.", "In the inverted appearance, the LV cavity appears dark while the myocardium appears bright thus making it similar in appearance to a 3DE image. Moreover, the resulting inverted image has high contrast and low noise appearance, yielding strong LV endocardium boundary and facilitating feature extraction for segmentation.", "Our results demonstrate that the inverse appearance of contrast image enables the subsequent LV segmentation."]},
{"title": "Control of cardiac alternans in an electromechanical model of cardiac tissue", "highlights": ["This study presents a novel mechanical perturbation algorithm to control alternans.", "The control algorithm manipulates the cardiac tissue mechanics to suppress alternans.", "The proposed algorithm suppresses alternans in relevantly sized cardiac tissues.", "We alter the action potential duration by perturbing cardiac tissue mechanics."], "abstract": ["Electrical alternations in cardiac action potential duration have been shown to be a precursor to arrhythmias and sudden cardiac death. Through the mechanism of excitation\u2013contraction coupling, the presence of electrical alternans induces alternations in the heart muscle contractile activity. Also, contraction of cardiac tissue affects the process of cardiac electric wave propagation through the mechanism of the so-called mechanoelectrical feedback. Electrical excitation and contraction of cardiac tissue can be linked by an electromechanical model such as the Nash\u2013Panfilov model. In this work, we explore the feasibility of suppressing cardiac alternans in the Nash-Panfilov model which is employed for small and large deformations. Several electrical pacing and mechanical perturbation feedback strategies are considered to demonstrate successful suppression of alternans on a one-dimensional cable. This is the first attempt to combine electrophysiologically relevant cardiac models of electrical wave propagation and contractility of cardiac tissue in a synergistic effort to suppress cardiac alternans. Numerical examples are provided to illustrate the feasibility and the effects of the proposed algorithms to suppress cardiac alternans."]},
{"title": "Multimodal predictor of neurodevelopmental outcome in newborns with hypoxic-ischaemic encephalopathy", "highlights": ["We investigate the means to assist a clinician in prediction of neurodevelopmental outcome in newborns.", "We combine information from three different modalities \u2013 clinical, EEG and ECG.", "We identify a set of consistently informative features.", "Automated analysis of multimodal data complements clinical prediction."], "abstract": ["Automated multimodal prediction of outcome in newborns with hypoxic-ischaemic encephalopathy is investigated in this work. Routine clinical measures and 1", "\u00a0", "h EEG and ECG recordings 24", "\u00a0", "h after birth were obtained from 38 newborns with different grades of HIE. Each newborn was reassessed at 24 months to establish their neurodevelopmental outcome. A set of multimodal features is extracted from the clinical, heart rate and EEG measures and is fed into a support vector machine classifier. The performance is reported with the statistically most unbiased leave-one-patient-out performance assessment routine. A subset of informative features, whose rankings are consistent across all patients, is identified. The best performance is obtained using a subset of 9 EEG, 2", "\u00a0", "h and 1 clinical feature, leading to an area under the ROC curve of 87% and accuracy of 84% which compares favourably to the EEG-based clinical outcome prediction, previously reported on the same data. The work presents a promising step towards the use of multimodal data in building an objective decision support tool for clinical prediction of neurodevelopmental outcome in newborns with hypoxic-ischaemic encephalopathy."]},
{"title": "Prediction of mortality after radical cystectomy for bladder cancer by machine learning techniques", "highlights": ["Machine learning methods are used to predict the mortality after radical cystectomy.", "Extreme learning machine (ELM) based algorithms outperform in speed and accuracy.", "ELM and regularized ELM can identify the predictors of mortality after the surgery."], "abstract": ["Bladder cancer is a common cancer in genitourinary malignancy. For muscle invasive bladder cancer, surgical removal of the bladder, i.e. radical cystectomy, is in general the definitive treatment which, unfortunately, carries significant morbidities and mortalities. Accurate prediction of the mortality of radical cystectomy is therefore needed. Statistical methods have conventionally been used for this purpose, despite the complex interactions of high-dimensional medical data. Machine learning has emerged as a promising technique for handling high-dimensional data, with increasing application in clinical decision support, e.g. cancer prediction and prognosis. Its ability to reveal the hidden nonlinear interactions and interpretable rules between dependent and independent variables is favorable for constructing models of effective generalization performance. In this paper, seven machine learning methods are utilized to predict the 5-year mortality of radical cystectomy, including back-propagation neural network (BPN), radial basis function (RBFN), extreme learning machine (ELM), regularized ELM (RELM), support vector machine (SVM), naive Bayes (NB) classifier and k-nearest neighbour (KNN), on a clinicopathological dataset of 117 patients of the urology unit of a hospital in Hong Kong. The experimental results indicate that RELM achieved the highest average prediction accuracy of 0.8 at a fast learning speed. The research findings demonstrate the potential of applying machine learning techniques to support clinical decision making."]},
{"title": "Comparative assessment of feature extraction methods for visual odometry in wireless capsule endoscopy", "highlights": ["Wireless capsule endoscopes can be localized based solely on visual features.", "State-of-the-art visual localization strategies are compared.", "Several visual feature extraction methods are applied.", "A significantly lower localization error than the state of the art is reported."], "abstract": ["Wireless capsule endoscopy (WCE) enables the non-invasive examination of the gastrointestinal (GI) tract by a swallowable device equipped with a miniature camera. Accurate localization of the capsule in the GI tract enables accurate localization of abnormalities for medical interventions such as biopsy and polyp resection; therefore, the optimization of the localization outcome is important. Current approaches to endoscopic capsule localization are mainly based on external sensors and transit time estimations. Recently, we demonstrated the feasibility of capsule localization based\u2014entirely\u2014on visual features, without the use of external sensors. This technique relies on a motion estimation algorithm that enables measurements of the distance and the rotation of the capsule from the acquired video frames. Towards the determination of an optimal visual feature extraction technique for capsule motion estimation, an extensive comparative assessment of several state-of-the-art techniques, using a publicly available dataset, is presented. The results show that the minimization of the localization error is possible at the cost of computational efficiency. A localization error of approximately one order of magnitude higher than the minimal one can be considered as compromise for the use of current computationally efficient feature extraction techniques."]},
{"title": "Piperine derivatives as potential inhibitors of Survivin: An in silico molecular docking", "highlights": ["Molecular docking shows strong inhibition of some Piperine derivatives to Survivin.", "Hydrogen bonding and van der Waals interactions have role in the binding process.", "Piperine derivatives may interfere with the function of Survivin in mitosis."], "abstract": ["Targeting Survivin, as an inhibitor of apoptosis and a regulator of cell division, has become a worldwide controversial issue. Piperine as a pungent alkaloid has been identified as the most potent adjuvant at enhancing the efficacy of tumor necrosis factor-related apoptosis-inducing ligand (TRAIL)-based therapies in triple-negative breast cancer (TNBC) cells ", " and ", ", which might be mediated through inhibition of Survivin. In this work, the binding energies, inhibition constants and binding modes of a group of previously synthesized Piperine derivatives at the binding site of Survivin have been studied using molecular docking tools and the best compounds with minimum binding energies are proposed as potential drugs for the inhibition of Survivin. A comprehensive SAR analysis has been done on the results that can be used for designing new Piperine analogs with higher efficacy. Molecular docking computations also show that the studied compounds can bind to BIR domain of Survivin in the same binding site as that of Smac/DIABLO with a suitable binding energy. This binding may result in the segregation of Smac/DIABLO in the cytosol and subsequently free Smac/DIABLO molecules could be available for binding with inhibitors of apoptosis to initiate caspase mediated apoptosis."]},
{"title": "Set of rules for genomic signal downsampling", "highlights": ["We set the criteria for genomic signal downsampling.", "We performed detailed analysis on large dataset of real sequences.", "We quantified the possible saving of computation time."], "abstract": ["Comparison and classification of organisms based on molecular data is an important task of computational biology, since at least parts of DNA sequences for many organisms are available. Unfortunately, methods for comparison are computationally very demanding, suitable only for short sequences. In this paper, we focus on the redundancy of genetic information stored in DNA sequences. We proposed rules for downsampling of DNA signals of cumulated phase. According to the length of an original sequence, we are able to significantly reduce the amount of data with only slight loss of original information. Dyadic wavelet transform was chosen for fast downsampling with minimum influence on signal shape carrying the biological information. We proved the usability of such new short signals by measuring percentage deviation of pairs of original and downsampled signals while maintaining spectral power of signals. Minimal loss of biological information was proved by measuring the Robinson\u2013Foulds distance between pairs of phylogenetic trees reconstructed from the original and downsampled signals. The preservation of inter-species and intra-species information makes these signals suitable for fast sequence identification as well as for more detailed phylogeny reconstruction."]},
{"title": "A cellular Potts model analyzing differentiated cell behavior during ", "highlights": ["An in vivo angiogenic process is modeled by a suitable version of the Cellular Potts Model.", "Vascular cells undergo phenotypic differentiations (i.e., from quiescent to stalk or tip).", "A fundamental mechanism is the chemotactic movement of tip cells.", "Stalk cells are characterized by a directionally preferential mitosis.", "The size of the hypoxic tissue only affects the duration of the process."], "abstract": ["Angiogenesis, the formation of new blood vessel networks from existing capillary or post-capillary venules, is an intrinsically multiscale process occurring in several physio-pathological conditions. In particular, hypoxic tissue cells activate downstream cascades culminating in the secretion of a wide range of angiogenic factors, including VEGF isoforms. Such diffusive chemicals activate the endothelial cells (ECs) forming the external walls of the nearby vessels that chemotactically migrate toward the hypoxic areas of the tissue as multicellular sprouts. A functional network eventually emerges by further branching and anastomosis processes. We here propose a CPM-based approach reproducing selected features of the angiogenic progression necessary for the reoxygenation of a hypoxic tissue. Our model is able to span the different scale involved in the angiogenic progression as it incorporates reaction\u2013diffusion equations for the description of the evolution of microenvironmental variables in a discrete mesoscopic cellular Potts model (CPM) that reproduces the dynamics of the vascular cells. A key feature of this work is the explicit phenotypic differentiation of the ECs themselves, distinguished in quiescent, stalk and tip. The simulation results allow identifying a set of key mechanisms underlying tissue vascularization. Further, we provide evidence that the nascent pattern is characterized by precise topological properties. Finally, we link abnormal sprouting angiogenesis with alteration in selected cell behavior."]},
{"title": "Micro-intestinal robot with wireless power transmission: design, analysis and experiment", "highlights": ["A robot with spiral legs is designed and tested in the in vitro intestines.", "The mechanism of robot movement for optimization is analyzed.", "Wireless energy is supplied to the robot, instead of batteries or wires.", "Ring-like receiving coil is designed for saving space and ensuring enough energy."], "abstract": ["Video capsule endoscopy is a useful tool for noninvasive intestinal detection, but it is not capable of active movement; wireless power is an effective solution to this problem.", "The research in this paper consists of two parts: the mechanical structure which enables the robot to move smoothly inside the intestinal tract, and the wireless power supply which ensures efficiency. First, an intestinal robot with leg architectures was developed based on the Archimedes spiral, which mimics the movement of an inchworm. The spiral legs were capable of unfolding to an angle of approximately 155\u00b0, which guaranteed stability of clamping, consistency of surface pressure, and avoided the risk of puncturing the intestinal tract. Secondly, the necessary power to operate the robot was far beyond the capacity of button batteries, so a wireless power transmission (WPT) platform was developed. The design of the platform focused on power transfer efficiency and frequency stability. In addition, the safety of human tissue in the alternating electromagnetic field was also taken into consideration. Finally, the assembled robot was tested and verified with the use of the WPT platform.", "In the isolated intestine, the robot system successfully traveled along the intestine with an average speed of 23", "\u00a0", "mm per minute. The obtained videos displayed a resolution of 320\u00d7240 and a transmission rate of 30 frames per second. The WPT platform supplied up to 500", "\u00a0", "mW of energy to the robot, and achieved a power transfer efficiency of 12%.", "It has been experimentally verified that the intestinal robot is safe and effective as an endoscopy tool, for which wireless power is feasible. Proposals for further improving the robot and wireless power supply are provided later in this paper."]},
{"title": "Local configuration pattern features for age-related macular degeneration characterization and classification", "highlights": ["Automated detection of age-related macular degeneration (AMD) using fundus images.", "Features are extracted using local configuration pattern (LCP) method.", "Ranked features are subjected to various classifiers.", "Proposed method classifies two classes with 97.80% accuracy"], "abstract": ["Age-related Macular Degeneration (AMD) is an irreversible and chronic medical condition characterized by drusen, Choroidal Neovascularization (CNV) and Geographic Atrophy (GA). AMD is one of the major causes of visual loss among elderly people. It is caused by the degeneration of cells in the macula which is responsible for central vision. AMD can be dry or wet type, however dry AMD is most common. It is classified into early, intermediate and late AMD. The early detection and treatment may help one to stop the progression of the disease. Automated AMD diagnosis may reduce the screening time of the clinicians. In this work, we have introduced LCP to characterize normal and AMD classes using fundus images. Linear Configuration Coefficients (CC) and Pattern Occurrence (PO) features are extracted from fundus images. These extracted features are ranked using ", "-value of the ", "-test and fed to various supervised classifiers viz. Decision Tree (DT), Nearest Neighbour (", "-NN), Naive Bayes (NB), Probabilistic Neural Network (PNN) and Support Vector Machine (SVM) to classify normal and AMD classes. The performance of the system is evaluated using both private (Kasturba Medical Hospital, Manipal, India) and public domain datasets viz. Automated Retinal Image Analysis (ARIA) and STructured Analysis of the Retina (STARE) using ", "-fold cross validation. The proposed approach yielded best performance with a highest average accuracy of 97.78%, sensitivity of 98.00% and specificity of 97.50% for STARE dataset using 22 significant features. Hence, this system can be used as an aiding tool to the clinicians during mass eye screening programs to diagnose AMD."]},
{"title": "Trends in celiac disease research", "highlights": ["Celiac disease is present in 1% of the population worldwide.", "Currently there is no cure for celiac disease.", "To determine celiac disease research focus, a MEDLINE search paradigm was developed.", "By detecting associations with celiac disease, research trends can be discovered.", "Certain areas of research are hot topics while others are less researched.", "Trends in research tend to follow the evolving symptomology of the disease."], "abstract": ["To improve diagnosis and treatment of celiac disease (CD), research efforts are being made in many different areas. However, the focus, trend, and direction of such efforts require clarity, so that future efforts and directions can be appropriately planned.", "In this study, MEDLINE was used to search for trends in CD research. The keyword \u2018celiac disease\u2019 and its variants were searched in tandem with keywords commonly associated with CD. This search was done for each year from 1960 to 2013. Year of first instance of the associated keyword, linear regression coefficient, and trend in terms of the slope of the regression line were tabulated. For perspective, the same keywords were searched in tandem with \u2018inflammatory bowel disease\u2019 (IBD).", "CD appeared in the medical literature prior to 1960, and IBD first appeared in 1964. However, IBD overtook CD in terms of the number of research papers published per year, beginning in 1988. Keywords with strong positive trends (", ">0.7) in association with CD were: \u2018diagnosis\u2019, \u2018gluten\u2019, \u2018serology\u2019, \u2018autoimmune\u2019, \u2018treatment\u2019, \u2018gluten-free diet\u2019, \u2018endoscopy\u2019, \u2018villous atrophy\u2019, \u2018wasting\u2019, \u2018inflammation\u2019, and \u2018microbiome\u2019. The keyword \u2018malabsorption\u2019 had the sole strong negative trend in association with CD. Keywords with strong positive trends (", ">0.7) in association with IBD also had strong positive association with CD: \u2018autoimmune\u2019, \u2018treatment, \u2018inflammation\u2019, and \u2018microbiome\u2019.", "The MEDLINE search approach is helpful to show first instance, association, and trend of keywords that are affiliated with CD in published biomedical research, and to compare CD research trends with those of other diseases."]},
{"title": "Prediction of feature genes in trauma patients with the TNF rs1800629 A allele using support vector machine", "highlights": ["A total of 133 feature genes were screened out in training set.", "SVM classifier peaked at 100% and 86.2% correct rate in training and validation sets.", "Feature genes were mainly enriched in cell proliferation and ribosome pathway."], "abstract": ["Tumor necrosis factor (TNF)-\u03b1 variant is closely linked to sepsis syndrome and mortality after severe trauma. We aimed to identify feature genes associated with the TNF rs1800629 A allele in trauma patients and help to direct them toward alternative successful treatment.", "In this study, we used 58 sets of gene expression data from Gene Expression Omnibus to predict the feature genes associated with the TNF rs1800629 A allele in trauma patients. We applied support vector machine (SVM) classifier model for classification prediction combining with leave-one-out cross validation method. Functional annotation of feature genes was carried out to study the biological function using database for annotation, visualization, and integrated discovery (DAVID).", "A total of 133 feature genes were screened out and was well differentiated in the training set (14 patients with variant, 15 with wild type). Moreover, SVM classifier peaked in predictive accuracy with 100% correct rate in training set and 86.2% in testing set. Interestingly, functional annotation showed that feature genes, such as ", " (heme oxygenase (decycling) 1) and ", " (ribosomal protein S7) were mainly enriched in terms of cell proliferation and ribosome.", " and ", " may be key feature genes associated with the TNF rs1800629 A allele and may play a crucial role in the inflammatory response in trauma patients. Moreover, the cell proliferation and ribosome pathway may contribute to the progression of severe trauma."]},
{"title": "Maximum sphere method for shell patency measurements in viviparous land snails based on X-ray microcomputed tomography imaging", "highlights": ["A new method of measuring cavities is proposed.", "Described algorithm shows its unique behavior steering by selected settings.", "The method proves usability of solution in measuring snail\u2019s shell patency.", "The definition of snail\u2019s shell can be successfully carry on based on XMT images.", "Results of have been shown using land snail samples and test models."], "abstract": ["This article presents the working principle of an algorithm designed for the purpose of examining a section of the snail shell canal. The procedure of scanning the specimens is described as well as the tests performed using the proposed algorithm. Also, the digital models used for testing the algorithm are described. The article contains a description of the initial processing of the data, including segmentation and detection of the edges of the image. A flowchart of the algorithm is presented together with its implementation. The data obtained in the course of the microtomographic scanning of one of the snails and a digital model of a canal created for this purpose were subjected to the application of the measurement algorithm. This algorithm was aimed at conducting a spatial analysis of the varying dimensions in the canal section. The process of applying the algorithm and the measurement errors are presented and discussed on the basis of the results."]},
{"title": "Multimodal data and machine learning for surgery outcome prediction in complicated cases of mesial temporal lobe epilepsy", "highlights": ["Machine learning with multimodal data can accurately predict postsurgical outcome in patients with drug resistant mesial temporal lobe epilepsy.", "Features resulting from quantitative analysis of structural MRI and intracranial EEG are informative predictors of postsurgical outcome.", "Least-square support vector machine with radial basis function kernel resulted in optimal prediction.", "Clinical factors such as family history of epilepsy and duration of epilepsy significantly affect the chance of seizure freedom post epilepsy surgery."], "abstract": ["This study sought to predict postsurgical seizure freedom from pre-operative diagnostic test results and clinical information using a rapid automated approach, based on supervised learning methods in patients with drug-resistant focal seizures suspected to begin in temporal lobe.", "We applied machine learning, specifically a combination of mutual information-based feature selection and supervised learning classifiers on multimodal data, to predict surgery outcome retrospectively in 20 presurgical patients (13 female; mean age\u00b1SD, in years 33\u00b19.7 for females, and 35.3\u00b19.4 for males) who were diagnosed with mesial temporal lobe epilepsy (MTLE) and subsequently underwent standard anteromesial temporal lobectomy. The main advantage of the present work over previous studies is the inclusion of the extent of ipsilateral neocortical gray matter atrophy and spatiotemporal properties of depth electrode-recorded seizures as training features for individual patient surgery planning.", "A maximum relevance minimum redundancy (mRMR) feature selector identified the following features as the most informative predictors of postsurgical seizure freedom in this study\u2019s sample of patients: family history of epilepsy, ictal EEG onset pattern (positive correlation with seizure freedom), MRI-based gray matter thickness reduction in the hemisphere ipsilateral to seizure onset, proportion of seizures that first appeared in ipsilateral amygdala to total seizures, age, epilepsy duration, delay in the spread of ipsilateral ictal discharges from site of onset, gender, and number of electrode contacts at seizure onset (negative correlation with seizure freedom). Using these features in combination with a least square support vector machine (LS-SVM) classifier compared to other commonly used classifiers resulted in very high surgical outcome prediction accuracy (95%).", "Supervised machine learning using multimodal compared to unimodal data accurately predicted postsurgical outcome in patients with atypical MTLE."]},
{"title": "Surgical planning for horizontal strabismus using Support Vector Regression", "highlights": ["This work investigates computational methods for horizontal strabismus surgeries.", "Our approach used Support Vector Regression to estimate the surgical plan.", "We used data from the records of 114 patients with horizontal/vertical strabismus.", "For the medial rectus muscles, the mean error was 0.5", "\u00a0", "mm for recoil and 0.7 for resection.", "For the lateral rectus muscles, the mean error was 0.6 for recoil and 0.8 for resection."], "abstract": ["Strabismus is a pathology which affects about 4% of the population, causing esthetic problems (reversible at any age) and irreversible sensory disorders, altering the vision mechanism. Many techniques can be applied to settle the muscular balance, thus eliminating strabismus. However, when the conservative treatment is not enough, the surgical treatment is adopted, applying recoils or resections to the ocular muscles affected. The factors involved in the surgical strategy in cases of strabismus are complex, demanding both theoretical knowledge and experience from the surgeon. So, the present work proposes a methodology based on Support Vector Regression to help the physician with decision related to horizontal strabismus surgeries. The efficiency of the method at the indication of the surgical plan was evaluated through the average difference between the values that it provided and the values indicated by the specialists. In the planning of medial rectus muscles surgeries, the average error was 0.5", "\u00a0", "mm for recoil and 0.7 for resection. For lateral rectus muscles, the mean error was 0.6 for recoil and 0.8 for resection. The results are promising and prove the feasibility of the use of Support Vector Regression in the indication of strabismus surgeries."]},
{"title": "Topology adaptive vessel network skeleton extraction with novel medialness measuring function", "highlights": ["A robust, topology adaptive tree-like structure skeleton extraction framework is proposed.", "A novel medialness measuring function is proposed to reduce the adjacent interferences.", "A wave propagation procedure is built to identify important topological nodes.", "The extracted curve skeletons are modeled by active contour models."], "abstract": ["Vessel tree skeleton extraction is widely applied in vascular structure segmentation, however, conventional approaches often suffer from the adjacent interferences and poor topological adaptability. To avoid these problems, a robust, topology adaptive tree-like structure skeleton extraction framework is proposed in this paper. Specifically, to avoid the adjacent interferences, a local message passing procedure called Gaussian affinity voting (GAV) is proposed to realize adaptive scale-growing of vessel voxels. Then the medialness measuring function (MMF) based on GAV, namely GAV\u2013MMF, is constructed to extract medialness patterns robustly. In order to improve topological adaptability, a level-set graph embedded with GAV\u2013MMF is employed to build initial curve skeletons without any user interaction. Furthermore, the GAV\u2013MMF is embedded in stretching open active contours (SOAC) to drive the initial curves to the expected location, maintaining smoothness and continuity. In addition, to provide an accurate and smooth final skeleton tree topology, topological checks and skeleton network reconfiguration is proposed. The continuity and scalability of this method is validated experimentally on synthetic and clinical images for multi-scale vessels. Experimental results show that the proposed method achieves acceptable topological adaptability for skeleton extraction of vessel trees."]},
{"title": "Combined heart rate variability and dynamic measures for quantitatively characterizing the cardiac stress status during cycling exercise", "highlights": ["We seek for different ways of measuring cardiac stress that occurs when exercising.", "Both the SDNN and fractal exponent ", " produced a decrease during the cycling exercise.", "Both the SDNN and fractal exponent ", " were negatively correlated with the Borg index.", "A time-varying index CSM was devised for quantitatively measuring the cardiac stress.", "The study may be used in sports medicine or stress tests for heart stress monitoring."], "abstract": ["In this study, we aimed to seek for different ways of measuring cardiac stress in terms of heart rate variability (HRV) and heart rate (HR) dynamics, and to develop a novel index that can effectively summarize the information reflected by these measures to continuously and quantitatively characterize the cardiac stress status during physical exercise. Standard deviation, spectral measure of HRV as well as a nonlinear detrended fluctuation analysis (DFA) based fractal-like behavior measure of HR dynamics were all evaluated on the RR time series derived from windowed electrocardiogram (ECG) data for the subjects undergoing cycling exercise. We recruited eleven young healthy subjects in our tests. Each subject was asked to maintain a fixed speed under a constant load during the pedaling test. We obtained the running estimates of the standard deviation of the normal-to-normal interval (SDNN), the high-fidelity power spectral density (PSD) of HRV, and the DFA scaling exponent ", ", respectively. A trend analysis and a multivariate linear regression analysis of these measures were then performed. Numerical experimental results produced by our analyses showed that a decrease in both SDNN and ", " was seen during the cycling exercise, while there was no significant correlation between the standard lower frequency to higher frequency (LF-to-HF) spectral power ratio of HRV and the exercise intensity. In addition, while the SDNN and ", " were both negatively correlated with the Borg rating of perceived exertion (RPE) scale value, it seemed that the LF-to-HF power ratio might not have substantial impact on the Borg value, suggesting that the SDNN and ", " may be further used as features to detect the cardiac stress status during the physical exercise. We further approached this detection problem by applying a linear discriminant analysis (LDA) to both feature candidates for the task of cardiac stress stratification. As a result, a time-varying parameter, referred to as the cardiac stress measure (CSM), is developed for quantitatively on-line measuring and stratifying cardiac stress status."]},
{"title": "Evaluation of the continuous detection of mental calculation episodes as a BCI control input", "highlights": ["The mental calculation task proposed produces EEG dynamics similar to MI.", "The mental task realization is always verifiable.", "Performance of the detection of task episodes is above the standard for MI.", "Classification is done using only eight input features, both spectral and non linear."], "abstract": ["This paper presents an evaluation of the continuous detection of mental calculation episodes, which may be useful for users who strive to operate current BCI paradigms or even for augmenting degrees of freedom. The experimentation consisted in the alternated realization of basic arithmetic mental calculations and resting periods. EEG data were analyzed using sliding windows of 2s length. The experimental population was comprised of fifteen healthy subjects who participated in three sessions on different days. The features used for the classification process were the power spectral density over the beta band (", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", ", " Hz) and the scaling exponent obtained via detrended fluctuation analysis. Both indices were estimated over four channels, specifically selected for each subject. The performance was evaluated using the Area Under the ROC Curve (AUC) by measuring the overall classification performance of each experimental session with a cross-validation procedure, and by transferring the model obtained from one session to the others called inter Session Validation (iSV). The best AUC values computed in each cross-validation session were: 0.87\u00b10.067, 0.89\u00b10.056 and 0.88\u00b10.040 respectively; and the iSV provided a value of 0.67\u00b10.122. These high values indicate that a mental calculation paradigm and a combination of features can efficiently control a BCI system. Notwithstanding that several days passed between sessions, the AUC mean value estimated for the iSV is similar to the performance of a motor imagery-BCI calibrated on the same day."]},
{"title": "Hybrid facial image feature extraction and recognition for non-invasive chronic fatigue syndrome diagnosis", "highlights": ["We proposed a hybrid facial feature based on clinical observations of TCM experts.", "Feature components are extracted from different facial regions with proper methods.", "Feature reduction and fusion are performed before prediction of CFS.", "The feature got an error rate less than 17.31% in cross-validation experiments.", "The whole method achieves an average accuracy of 88.32% for CFS prediction."], "abstract": ["Due to an absence of reliable biochemical markers, the diagnosis of chronic fatigue syndrome (CFS) mainly relies on the clinical symptoms, and the experience and skill of the doctors currently. To improve objectivity and reduce work intensity, a hybrid facial feature is proposed. First, several kinds of appearance features are identified in different facial regions according to clinical observations of traditional Chinese medicine experts, including vertical striped wrinkles on the forehead, puffiness of the lower eyelid, the skin colour of the cheeks, nose and lips, and the shape of the mouth corner. Afterwards, such features are extracted and systematically combined to form a hybrid feature. We divide the face into several regions based on twelve active appearance model (AAM) feature points, and ten straight lines across them. Then, Gabor wavelet filtering, CIELab color components, threshold-based segmentation and curve fitting are applied to extract features, and Gabor features are reduced by a manifold preserving projection method. Finally, an AdaBoost based score level fusion of multi-modal features is performed after classification of each feature. Despite that the subjects involved in this trial are exclusively Chinese, the method achieves an average accuracy of 89.04% on the training set and 88.32% on the testing set based on the ", "-fold cross-validation. In addition, the method also possesses desirable sensitivity and specificity on CFS prediction."]},
{"title": "Breast cancer diagnosis in digitized mammograms using curvelet moments", "highlights": ["Mammogram analysis based on curvelet moments, with particular attention to the choice of curvelet subband and moment order.", "A feature selection step to keep the most discriminative moments.", "An empirical comparison against state-of-the-art curvelet-based methods on two mammogram databases.", "The effectiveness and the superiority of curvelet moment for abnormality and malignancy detection."], "abstract": [": Feature extraction is a key issue in designing a computer aided diagnosis system. Recent researches on breast cancer diagnosis have reported the effectiveness of multiscale transforms (wavelets and curvelets) for mammogram analysis and have shown the superiority of curvelet transform. However, the curse of dimensionality problem arises when using the curvelet coefficients and therefore a reduction method is required to extract a reduced set of discriminative features.", ": This paper deals with this problem and proposes a feature extraction method based on curvelet transform and moment theory for mammogram description. First, we performed discrete curvelet transform and we computed the four first-order moments from curvelet coefficients distribution. Hence, two feature sets can be obtained: moments from each band and moments from each level. In this work, both sets are studied. Then, the ", "-test ranking technique was applied to select the best features from each set. Finally, a ", "-nearest neighbor classifier was used to distinguish between normal and abnormal breast tissues and to classify tumors as malignant or benign. Experiments were performed on 252 mammograms from the Mammographic Image Analysis Society (mini-MIAS) database using the leave-one-out cross validation as well as on 11553 mammograms from the Digital Database for Screening Mammography (DDSM) database using 2\u00d75-fold cross validation.", ": Experimental results prove the effectiveness and the superiority of curvelet moments for mammogram analysis. Indeed, results on the mini-MIAS database show that curvelet moments yield an accuracy of 91.27% (resp. 81.35 %) with 10 (resp. 8) features for abnormality (resp. malignancy) detection. In addition, empirical comparisons of the proposed method against state-of-the-art curvelet-based methods on the DDSM database show that the suggested method does not only lead to a more reduced feature set, but it also statistically outperforms all the compared methods in terms of accuracy.", " In summary, curvelet moments are an efficient and effective way to extract a reduced set of discriminative features for breast cancer diagnosis."]},
{"title": "Comparison of the effect of different sampling modes on computer analysis of cardiotocograms", "highlights": ["FHR sampling mode affects usual computer analysis of CTGs.", "Effect of FHR sampling mode on computer analysis depends on parameters definition.", "Alerts in computer analysis of CTGs should take into account the FHR sampling mode."], "abstract": ["Cardiotocographic (CTG) monitors may provide fetal heart rate (FHR) signals as beat-to-beat (BTB) or alternatively at a fixed sampling rate. The aim of this study was to assess the effect of different sampling modes on the evaluation provided by a commercially available system for computer analysis of CTGs.", "Internal FHR signals were acquired during the last hour of labor in 27 singleton term cephalic pregnancies, using the STAN S31", " fetal monitor (Neoventa, Gothemburg, Sweden). BTB and 4", "\u00a0", "Hz sampling outputs of the monitor were compared using the Omniview-SisPorto", " system for computer analysis of CTGs (Speculum, Lisbon, Portugal). The following parameters were analyzed: signal loss, signal quality, baseline, accelerations, decelerations, percentage of abnormal short-term variability (%aSTV), abnormal long-term variability (%aLTV), average short-term variability (avSTV) and the system\u05f3s clinical alerts. Statistical inference was performed using the Spearman correlation coefficient, 95% nonparametric confidence intervals, Wilcoxon and McNemar statistical tests, setting significance at 0.05, and a non-parametric measure of disagreement (valued 0\u20131 from lowest to highest disagreement).", "Comparing BTB with 4", "\u00a0", "Hz sampling, the median values for signal quality (95% versus 96%), number of accelerations (5 versus 7) and %aSTV (31 versus 39) were significantly lower in the former. On the other hand, with BTB signals the median value of avSTV was significantly higher (3.1 versus 2.3). Nevertheless, BTB and 4", "\u00a0", "Hz parameters were highly correlated (", "=0.89\u20130.97), and there were no significant differences in the quantification of the number of decelerations or in the clinical alerts elicited by the system.", "In conclusion, different sampling modes have a significant effect on the parameters provided by computer analysis of CTGs that are related with the quantification of STV, with a small impact on baseline estimation and on the subsequent quantification of accelerations. However, there does not seem to be significant impact on the quantification of decelerations or on the alerts provided by the system."]},
{"title": "Quantitative breast mass classification based on the integration of B-mode features and strain features in elastography", "highlights": ["Quantitative strain features wereextracted fromelastographic images to express tissue elasticity.", "A computer-aided diagnosis system based on the quantitative strain features was developed to classify breast masses.", "Combining the strain features with the B-mode features obtained a significantly better performance in malignancy evaluation."], "abstract": ["Elastography is a new sonographic imaging technique to acquire the strain information of tissues and transform the information into images. Radiologists have to observe the gray-scale distribution of tissues on the elastographic image interpreted as the reciprocal of Young\u05f3s modulus to evaluate the pathological changes such as scirrhous carcinoma. In this study, a computer-aided diagnosis (CAD) system was developed to extract quantitative strain features from elastographic images to reduce operator-dependence and provide an automatic procedure for breast mass classification.", "The collected image database was composed of 45 malignant and 45 benign breast masses. For each case, tumor segmentation was performed on the B-mode image to obtain tumor contour which was then mapped to the elastographic images to define the corresponding tumor area. The gray-scale pixels around tumor area were classified into white, gray, and black by fuzzy c-means clustering to highlight stiff tissues with darker values. Quantitative strain features were then extracted from the black cluster and compared with the B-mode features in the classification of breast masses.", "The performance of the proposed strain features achieved an accuracy of 80% (72/90), a sensitivity of 80% (36/45), a specificity of 80% (36/45), and a normalized area under the receiver operating characteristic curve, Az=0.84. Combining the strain features with the B-mode features obtained a significantly better Az=0.93, ", "-value<0.05.", "Summarily, the quantified strain features can be combined with the B-mode features to provide a promising suggestion in distinguishing malignant from benign tumors."]},
{"title": "Modeling left and right atrial contributions to the ECG: A dipole-current source approach", "highlights": ["We used a model of the atria and torso to compute ECG from dipole-current sources.", "We simulated the separate contributions of the left and right atria to the ECG.", "The approach helped relate the P wave morphology to atrial anatomy.", "Atrial fibrillation simulations emphasized the role of each atrium on ECG waveforms."], "abstract": ["This paper presents the mathematical formulation, the numerical validation and several illustrations of a forward-modeling approach based on dipole-current sources to compute the contribution of a part of the heart to the electrocardiogram (ECG). Clinically relevant applications include identifying in the ECG the contributions from the right and the left atrium. In a Courtemanche-based monodomain computer model of the atria and torso, 1000 dipoles distributed throughout the atrial mid-myocardium are found to be sufficient to reproduce body surface potential maps with a relative error ", " during both sinus rhythm and atrial fibrillation. When the boundary element method is applied to solve the forward problem, this approach enables fast offline computation of the ECG contribution of any anatomical part of the atria by applying the principle of superposition to the dipole sources. In the presence of a right\u2013left activation delay (sinus rhythm), pulmonary vein isolation (sinus rhythm) or left\u2013right differences in refractory period (atrial fibrillation), the decomposition of the ECG is shown to help interpret ECG morphology in relation to the atrial substrate. These tools provide a theoretical basis for a deeper understanding of the genesis of the P wave or fibrillatory waves in normal and pathological cases."]},
{"title": "A wrapper-based approach for feature selection and classification of major depressive disorder\u2013bipolar disorders", "highlights": ["We used the combination of support vector machine (SVM) and improved Ant Colony Optimization (IACO).", "We used an improved version of standard Ant Colony Optimization.", "We used coherence as a biomarker.", "We reduced the feature set from 48 to 22 using IACO.", "We modeled an SVM model using QEEG coherence values.", "We increased overall accuracy from 62.37% to 80.19% and AUC from 0.631 to 0.793 while decreasing the computational complexity and the number of features."], "abstract": ["Feature selection (FS) and classification are consecutive artificial intelligence (AI) methods used in data analysis, pattern classification, data mining and medical informatics. Beside promising studies in the application of AI methods to health informatics, working with more informative features is crucial in order to contribute to early diagnosis. Being one of the prevalent psychiatric disorders, depressive episodes of bipolar disorder (BD) is often misdiagnosed as major depressive disorder (MDD), leading to suboptimal therapy and poor outcomes. Therefore discriminating MDD and BD at earlier stages of illness could help to facilitate efficient and specific treatment. In this study, a nature inspired and novel FS algorithm based on standard Ant Colony Optimization (ACO), called improved ACO (IACO), was used to reduce the number of features by removing irrelevant and redundant data. The selected features were then fed into support vector machine (SVM), a powerful mathematical tool for data classification, regression, function estimation and modeling processes, in order to classify MDD and BD subjects. Proposed method used coherence, a promising quantitative electroencephalography (EEG) biomarker, values calculated from alpha, theta and delta frequency bands. The noteworthy performance of novel IACO\u2013SVM approach stated that it is possible to discriminate 46 BD and 55 MDD subjects using 22 of 48 features with 80.19% overall classification accuracy. The performance of IACO algorithm was also compared to the performance of standard ACO, genetic algorithm (GA) and particle swarm optimization (PSO) algorithms in terms of their classification accuracy and number of selected features. In order to provide an almost unbiased estimate of classification error, the validation process was performed using nested cross-validation (CV) procedure."]},
{"title": "Neonatal brain MRI segmentation: A review", "highlights": ["We present a review of state-of-the-art techniques in neonatal brain MRI segmentation.", "Both intracranial and brain tissue segmentation algorithms are included.", "Brain tissue segmentation methods are classified based on their level of atlas use.", "Research gaps in the existing literature are highlighted."], "abstract": ["This review paper focuses on the neonatal brain segmentation algorithms in the literature. It provides an overview of clinical magnetic resonance imaging (MRI) of the newborn brain and the challenges in automated tissue classification of neonatal brain MRI. It presents a complete survey of the existing segmentation methods and their salient features. The different approaches are categorized into intracranial and brain tissue segmentation algorithms based on their level of tissue classification. Further, the brain tissue segmentation techniques are grouped based on their atlas usage into atlas-based, augmented atlas-based and atlas-free methods. In addition, the research gaps and lacunae in literature are also identified."]},
{"title": "Analysis of shared miRNAs of different species using ensemble CCA and genetic distance", "highlights": ["We analyze miRNA precursor sequences of several species.", "We examine changes of common miRNAs among species under different classes.", "Genetic relationship of species can be analyzed by miRNA sequences.", "CCA/ECCA can identify the genetic relationships among species.", "RI for CCA/ECCA has high correlation with genetic distance of species."], "abstract": ["MicroRNA is a type of single stranded RNA molecule and has an important role for gene expression. Although there have been a number of computational methodologies in bioinformatics research for miRNA classification and target prediction tasks, analysis of shared miRNAs among different species has not yet been addressed. In this article, we analyzed miRNAs that have the same name and function but have different sequences and belong to different (but closely related) species which are constructed from the online miRBase database. We used sequence-driven features and performed the standard and the ensemble versions of Canonical Correlation Analysis (CCA). However, due to its sensitivity to noise and outliers, we extended it using an ensemble approach. Using linear combinations of dimer features, the proposed Ensemble CCA (ECCA) method has identified higher test-set-correlations than CCA. Moreover, our analysis reveals that the Redundancy Index of ECCA applied to a pair of species has correlation with their genetic distance."]},
{"title": "Referral system for hard exudates in eye fundus", "highlights": ["A referral system for diabetic retinopathy can decrease the load on ophthalmologists.", "A referral system is developed by using combination of mathematical techniques.", "Referral system makes treatment of patient cost and time effective.", "The referral system has been tested on four fundus databases.", "We have examined the referral system using in two different scenarios."], "abstract": ["Hard exudates are one of the most common anomalies/artifacts found in the eye fundus of patients suffering from diabetic retinopathy. These exudates are the major cause of loss of sight or blindness in people having diabetic retinopathy. Diagnosis of hard exudates requires considerable time and effort of an ophthalmologist. The ophthalmologists have become overloaded, so that there is a need for an automated diagnostic/referral system. In this paper a referral system for the hard exudates in the eye-fundus images has been presented. The proposed referral system works by combining different techniques like Scale Invariant Feature Transform (SIFT), K-means Clustering, Visual Dictionaries and Support Vector Machine (SVM). The system was also tested with Back Propagation Neural Network as a classifier. To test the performance of the system four fundus image databases were used. One publicly available image database was used to compare the performance of the system to the existing systems. To test the general performance of the system when the images are taken under different conditions and come from different sources, three other fundus image databases were mixed. The evaluation of the system was also performed on different sizes of the visual dictionaries. When using only one fundus image database the area under the curve (AUC) of maximum 0.9702 (97.02%) was achieved with accuracy of 95.02%. In case of mixed image databases an AUC of 0.9349 (93.49%) was recorded having accuracy of 87.23%. The results were compared to the existing systems and were found better/comparable."]},
{"title": "Detection of the optic disc in fundus images by combining probability models", "highlights": ["We propose an ensemble framework of detectors for OD localization.", "We use probability maps for the detection.", "We investigate some combination approach.", "Our method outperforms several state-of-the-art approaches."], "abstract": ["In this paper, we propose a combination method for the automatic detection of the optic disc (OD) in fundus images based on ensembles of individual algorithms. We have studied and adapted some of the state-of-the-art OD detectors and finally organized them into a complex framework in order to maximize the accuracy of the localization of the OD. The detection of the OD can be considered as a single-object detection problem. This object can be localized with high accuracy by several algorithms extracting single candidates for the center of the OD and the final location can be defined using a single majority voting rule. To include more information to support the final decision, we can use member algorithms providing more candidates which can be ranked based on the confidence ordered by the algorithms. In this case, a spatial weighted graph is defined where the candidates are considered as its nodes, and the final OD position is determined in terms of finding a maximum-weighted clique. Now, we examine how to apply in our ensemble-based framework all the accessible information supplied by the member algorithms by making them return confidence values for each image pixel. These confidence values inform us about the probability that a given pixel is the center point of the object. We apply axiomatic and Bayesian approaches, as in the case of aggregation of judgments of experts in decision and risk analysis, to combine these confidence values. According to our experimental study, the accuracy of the localization of OD increases further. Besides single localization, this approach can be adapted for the precise detection of the boundary of the OD. Comparative experimental results are also given for several publicly available datasets."]},
{"title": "Wavelet analysis of cardiac optical mapping data", "highlights": ["Optical mapping (OM) technology is important to study cardiac electrophysiology.", "Wavelet analysis offers simultaneous localization in time and frequency domains.", "Wavelet analysis improved the signal quality compared to Fourier-based filtering.", "Spatial wavelet filtering produced more accurate conduction velocity estimates.", "Wavelet analysis is a promising tool for OM electrophysiological characterization."], "abstract": ["Optical mapping technology is an important tool to study cardiac electrophysiology. Transmembrane fluorescence signals from voltage-dependent dyes need to be preprocessed before analysis to improve the signal-to-noise ratio. Fourier analysis, based on spectral properties of stationary signals, cannot directly provide information on the spectrum changes with respect to time. Fourier filtering has the disadvantage of causing degradation of abrupt waveform changes such as those in action potential signals. Wavelet analysis has the ability to offer simultaneous localization in time and frequency domains, suitable for the analysis and reconstruction of irregular, non-stationary signals like the fast action-potential upstroke, and better than conventional filters for denoising.", "We applied discrete wavelet transformation for temporal processing of optical mapping signals and wavelet packet analysis approaches to process activation maps from simulated and experimental optical mapping data from canine right atrium. We compared the results obtained with the wavelet approach to a variety of other methods (Fast Fourier Transformation (FFT) with finite or infinite response filtering, and Gaussian filters).", " Temporal wavelet analysis improved signal-to-noise ratio (", ") better than FFT filtering for 5\u201310", "\u00a0", "dB ", ", and caused less distortion of the action potential waveform over the full range of simulated noise (5\u201320", "\u00a0", "dB). Spatial wavelet filtering produced more efficient denoising and/or more accurate conduction velocity estimates than Gaussian filtering. Propagation patterns were also best revealed by wavelet filtering.", " Wavelet analysis is a promising tool, facilitating accurate action potential characterization, activation map formation, and conduction velocity estimation."]},
{"title": "Stabilizing 3D in vivo intravital microscopy images with an iteratively refined soft-tissue model for immunology experiments", "highlights": ["Software for stabilizing in vivo two-photon microscopy images with tissue movements.", "Pixel weighted registration algorithm that explicitly treats inter and intrastack motion errors.", "A nonlinear soft-tissue deformation alignment correction called the poor man\u05f3s diffeomorphic map.", "A method for detecting and removing multiple exposure errors caused by undersampling.", "A globally stabilization method using a constrained optimization method."], "abstract": ["We describe a set of new algorithms and a software tool, ", ", for stabilizing in vivo intravital microscopy images that suffer from soft-tissue background movement. Because these images lack predetermined anchors and are dominated by noise, we use a pixel weighted image alignment together with a correction for nonlinear tissue deformations. We call this correction a ", "\u05f3", " since it ascertains the nonlinear regions of the image without resorting to a full integral equation method. To determine the quality of the image stabilization, we developed an ensemble sampling method that quantifies the coincidence between image pairs from randomly distributed image regions. We obtain global stabilization alignment through an iterative constrained simulated annealing optimization procedure. To show the accuracy of our algorithm with existing software, we measured the misalignment error rate in datasets taken from two different organs and compared the results to a similar and popular open-source solution. Present open-source stabilization software tools perform poorly because they do not treat the specific needs of the IV-2pM datasets with soft-tissue deformation, speckle noise, full 5D inter- and intra-stack motion error correction, and undefined anchors. In contrast, the results of our tests demonstrate that our method is more immune to noise and provides better performance for datasets\u2019 possessing nonlinear tissue deformations. As a practical application of our software, we show how our stabilization improves cell tracking, where the presence of background movement would degrade track information. We also provide a qualitative comparison of our software with other open-source libraries/applications. Our software is freely available at the open source repository ", "."]},
{"title": "Monte Carlo method based QSAR modeling of maleimide derivatives as glycogen synthase kinase-3\u03b2 inhibitors", "highlights": ["QSAR models for glycogen synthase kinase-3\u03b2 inhibitors were built.", "SMILES notation based optimal descriptors and Monte Carlo method were used.", "The predictability of proposed QSPR models is very good.", "Structural alerts with the influence on studied inhibitors are defined."], "abstract": ["The Monte Carlo method was used for QSAR modeling of maleimide derivatives as glycogen synthase kinase-3\u03b2 inhibitors. The first QSAR model was developed for a series of 74 3-anilino-4-arylmaleimide derivatives. The second QSAR model was developed for a series of 177 maleimide derivatives. QSAR models were calculated with the representation of the molecular structure by the simplified molecular input-line entry system. Two splits have been examined: one split into the training and test set for the first QSAR model, and one split into the training, test and validation set for the second. The statistical quality of the developed model is very good. The calculated model for 3-anilino-4-arylmaleimide derivatives had following statistical parameters: ", "=0.8617 for the training set; ", "=0.8659, and ", "=0.7361 for the test set. The calculated model for maleimide derivatives had following statistical parameters: ", "=0.9435, for the training, ", "=0.9262 and ", "=0.8199 for the test and ", "=0.8418, ", "=0.7469 and \u2206", "=0.1476 for the validation set. Structural indicators considered as molecular fragments responsible for the increase and decrease in the inhibition activity have been defined. The computer-aided design of new potential glycogen synthase kinase-3\u03b2 inhibitors has been presented by using defined structural alerts."]},
{"title": "Probability distribution function-based classification of structural MRI for the detection of Alzheimer\u2019s disease", "highlights": ["PDF based feature extraction is introduced for the representation of sMRI.", "Voxels within statistically obtained 3D masks are used to generate localized PDFs.", "Dimensionality reduction of PDF is provided by determining optimum number of bins.", "Fisher criterion is used to determine the optimal dimension of PDF vector."], "abstract": ["High-dimensional classification methods have been a major target of machine learning for the automatic classification of patients who suffer from Alzheimer\u2019s disease (AD). One major issue of automatic classification is the feature-selection method from high-dimensional data. In this paper, a novel approach for statistical feature reduction and selection in high-dimensional magnetic resonance imaging (MRI) data based on the probability distribution function (PDF) is introduced. To develop an automatic computer-aided diagnosis (CAD) technique, this research explores the statistical patterns extracted from structural MRI (sMRI) data on four systematic levels. First, global and local differences of gray matter in patients with AD compared to healthy controls (HCs) using the voxel-based morphometric (VBM) technique with 3-Tesla 3D T1-weighted MRI are investigated. Second, feature extraction based on the voxel clusters detected by VBM on sMRI and voxel values as volume of interest (VOI) is used. Third, a novel statistical feature-selection process is employed, utilizing the PDF of the VOI to represent statistical patterns of the respective high-dimensional sMRI sample. Finally, the proposed feature-selection method for early detection of AD with support vector machine (SVM) classifiers compared to other standard feature selection methods, such as partial least squares (PLS) techniques, is assessed. The performance of the proposed technique is evaluated using 130 AD and 130 HC MRI data from the ADNI dataset with 10-fold cross validation", ". The results show that the PDF-based feature selection approach is a reliable technique that is highly competitive with respect to the state-of-the-art techniques in classifying AD from high-dimensional sMRI samples."]},
{"title": "A novel multi-scale Hessian based spot enhancement filter for two dimensional gel electrophoresis images", "highlights": ["We introduce a novel spot filter for separating the closely overlapping protein spots.", "We compute the eigen components of the image Hessian of two overlapping spots.", "We evaluate the performance of the proposed filter on the synthetic and real images.", "Comparison with other methods and a commercial software package reveals the superior performance of the proposed filter."], "abstract": ["Two dimensional gel electrophoresis (2DGE) is a useful method for studying proteins in a wide variety of applications including identifying post-translation modification (PTM), biomarker discovery, and protein purification. Computerized segmentation and detection of the proteins are the two main processes that are carried out on the scanned image of the gel. Due to the complexities of 2DGE images and the presence of artifacts, the segmentation and detection of protein spots in these images are non-trivial, and involve supervised and time consuming processes. This paper introduces a new spot filter for enhancing, and separating the closely overlapping spots of protein in 2DGE images based on the multi-scale eigenvalue analysis of the image Hessian. Using a Gaussian spot model, we have derived closed form equations to compute the eigen components of the image Hessian of two overlapping spots in a multi-scale fashion. Based on this analysis, we have proposed a novel filter that suppresses the overlapping area and results in a better spot separation. The performance of the proposed filter has been evaluated on the synthetic and real 2DGE images. The comparison with three conventional techniques and a commercial software package reveals the superiority and effectiveness of the proposed filter."]},
{"title": "Fast automated segmentation of wrist bones in magnetic resonance images", "highlights": ["Magnetic resonance of wrist is used in the diagnosis of rheumatoid arthritis (RA).", "Segmentation of wrist bones is necessary for automated evaluation of RA lesions.", "A framework for automated segmentation of wrist bones was developed.", "An excellent agreement with gold truth data was reported."], "abstract": ["According to current recommendations in diagnostics of rheumatoid arthritis (RA), Magnetic resonance (MR) images of wrist joints are used to evaluate three main signs of RA: synovitis, bone edema and bone erosions. In this paper we present an efficient method for segmentation of 15 bones present on MR images of the wrist which is inevitable for future computer-assisted diagnosis system for RA lesions.", "The segmentation procedure consists of two stages. The first stage is evaluation of markers (parts of bones working as seeds for the watershed algorithm) for bones in every joint: the distal parts of ulna and radius, the proximal parts of metacarpal bones and carpal bones. In the second stage the watershed from markers algorithm is applied based on the markers determined in the previous stage and the wrist bones are segmented. The markers were found using Multi Otsu algorithm along with custom method for filtering bones from other tissues.", "We analyzed 34 MR images. The automated segmentations were compared with manual segmentations using metrics: accuracy ACC derived from area under ROC curve AUC, Dice coefficient and mean absolute distance MAD. The mean (standard deviation) values of ACC, Dice and MAD were 0.99 (0.02), 0.98 (0.04) and 1.21 (0.39), respectively.", "The results of this study prove that our method is efficient and gives satisfactory results for segmentation of bones on low-field MR images of the wrist"]},
{"title": "Contribution of ", "highlights": ["CD24 polymorphism, rs8734/rs52812045 is associated with autoimmune disease.", "T allele carriers have a higher risk of autoimmune diseases than C allele carriers.", "CD24 polymorphism, rs3838646 is not associated with autoimmune diseases."], "abstract": ["To determine the relationship between two ", " polymorphisms, rs8734/rs52812045 and rs3838646, and autoimmune disease.", "Meta-analysis.", "The Medline, EMBASE, Web of Science, and Cochrane Library databases were searched for studies reporting the association between ", " polymorphisms and autoimmune disease. Two of the authors selected eligible studies and extracted and analyzed the data independently.", "Compared with carriers of the C allele (CC, CT, CT+CC), individuals homozygous for the T allele (TT) and heterozygous (CT+TT) at rs8734/rs52812045 have a higher incidence of autoimmune disease, whereas rs3838646 is not associated with autoimmune disease. Subgroup analysis found an increased risk of multiple sclerosis with the TT vs. CC, TT vs. CT, and TT vs. CC+CT alleles.", "The ", " polymorphism rs8734/rs52812045 contributes to the development of autoimmune disease."]},
{"title": "Audio-based detection and evaluation of eating behavior using the smartwatch platform", "highlights": ["We evaluate the potential of a smartwatch for monitoring eating.", "We use the microphone of the Samsung Galaxy Gear to record audio samples.", "We use the RandomForest classifier to classify between several foods.", "We are able to discriminate between eating and talking."], "abstract": ["In recent years, smartwatches have emerged as a viable platform for a variety of medical and health-related applications. In addition to the benefits of a stable hardware platform, these devices have a significant advantage over other wrist-worn devices, in that user acceptance of watches is higher than other custom hardware solutions. In this paper, we describe signal-processing techniques for identification of chews and swallows using a smartwatch device\u05f3s built-in microphone. Moreover, we conduct a survey to evaluate the potential of the smartwatch as a platform for monitoring nutrition. The focus of this paper is to analyze the overall applicability of a smartwatch-based system for food-intake monitoring. Evaluation results confirm the efficacy of our technique; classification was performed between apple and potato chip bites, water swallows, talking, and ambient noise, with an ", "-measure of 94.5% based on 250 collected samples."]},
{"title": "Similarity-balanced discriminant neighbor embedding and its application to cancer classification based on gene expression data", "highlights": ["A similarity-balanced discriminant neighborhood embedding (SBDNE) is proposed.", "SBDNE is applied to cancer classification using gene expression data.", "SBDNE constructs two adjacent graphs using a new similarity function.", "Experimental results on six microarray datasets show that SBDNE is promising."], "abstract": ["The family of discriminant neighborhood embedding (DNE) methods is typical graph-based methods for dimension reduction, and has been successfully applied to face recognition. This paper proposes a new variant of DNE, called similarity-balanced discriminant neighborhood embedding (SBDNE) and applies it to cancer classification using gene expression data. By introducing a novel similarity function, SBDNE deals with two data points in the same class and the different classes with different ways. The homogeneous and heterogeneous neighbors are selected according to the new similarity function instead of the Euclidean distance. SBDNE constructs two adjacent graphs, or between-class adjacent graph and within-class adjacent graph, using the new similarity function. According to these two adjacent graphs, we can generate the local between-class scatter and the local within-class scatter, respectively. Thus, SBDNE can maximize the between-class scatter and simultaneously minimize the within-class scatter to find the optimal projection matrix. Experimental results on six microarray datasets show that SBDNE is a promising method for cancer classification."]},
{"title": "Quantitative measurements in capsule endoscopy", "highlights": ["Image processing may shorten video capsule reading and free physicians\u05f3 time.", "Wireless measurement of esophageal pH detects acid gastro-esophageal reflux.", "Pressure, pH and temperature are measured by a wireless motility capsule."], "abstract": ["This review summarizes several approaches for quantitative measurement in capsule endoscopy. Video capsule endoscopy (VCE) typically provides wireless imaging of small bowel. Currently, a variety of quantitative measurements are implemented in commercially available hardware/software. The majority is proprietary and hence undisclosed algorithms. Measurement of amount of luminal contamination allows calculating scores from whole VCE studies. Other scores express the severity of small bowel lesions in Crohn\u05f3s disease or the degree of villous atrophy in celiac disease. Image processing with numerous algorithms of textural and color feature extraction is further in the research focuses for automated image analysis. These tools aim to select single images with relevant lesions as blood, ulcers, polyps and tumors or to omit images showing only luminal contamination. Analysis of motility pattern, size measurement and determination of capsule localization are additional topics.", "Non-visual wireless capsules transmitting data acquired with specific sensors from the gastrointestinal (GI) tract are available for clinical routine. This includes pH measurement in the esophagus for the diagnosis of acid gastro-esophageal reflux. A wireless motility capsule provides GI motility analysis on the basis of pH, pressure, and temperature measurement. Electromagnetically tracking of another motility capsule allows visualization of motility. However, measurement of substances by GI capsules is of great interest but still at an early stage of development."]},
{"title": "Performance comparison of multi-label learning algorithms on clinical data for chronic diseases", "highlights": ["We evaluate multi-label learning algorithms for the analysis of clinical data.", "We focus on patients affected by multiple chronic diseases.", "We use a summary statistics approach to extract features on medical time series."], "abstract": ["We are motivated by the issue of classifying diseases of chronically ill patients to assist physicians in their everyday work. Our goal is to provide a performance comparison of state-of-the-art multi-label learning algorithms for the analysis of multivariate sequential clinical data from medical records of patients affected by chronic diseases. As a matter of fact, the multi-label learning approach appears to be a good candidate for modeling overlapped medical conditions, specific to chronically ill patients. With the availability of such comparison study, the evaluation of new algorithms should be enhanced.", "According to the method, we choose a summary statistics approach for the processing of the sequential clinical data, so that the extracted features maintain an interpretable link to their corresponding medical records. The publicly available MIMIC-II dataset, which contains more than 19,000 patients with chronic diseases, is used in this study. For the comparison we selected the following multi-label algorithms: ML-", "NN, AdaBoostMH, binary relevance, classifier chains, HOMER and RA", "EL.", "Regarding the results, binary relevance approaches, despite their elementary design and their independence assumption concerning the chronic illnesses, perform optimally in most scenarios, in particular for the detection of relevant diseases. In addition, binary relevance approaches scale up to large dataset and are easy to learn. However, the RA", "EL algorithm, despite its scalability problems when it is confronted to large dataset, performs well in the scenario which consists of the ranking of the labels according to the dominant disease of the patient."]},
{"title": "Implementation of a web based universal exchange and inference language for medicine: Sparse data, probabilities and inference in data mining of clinical data repositories", "highlights": ["The Q-UEL language has been extended to traditional public health analysis.", "Traditional and inference probabilities are compared and co-presented.", "Enforcing coherence as Bayes Rule and marginal summation is described.", "Simple worked examples of clinical decision support are given.", "Methods of testing predictive capability are discussed."], "abstract": ["We extend Q-UEL, our universal exchange language for interoperability and inference in healthcare and biomedicine, to the more traditional fields of public health surveys. These are the type associated with screening, epidemiological and cross-sectional studies, and cohort studies in some cases similar to clinical trials. There is the challenge that there is some degree of split between frequentist notions of probability as (a) classical measures based only on the idea of counting and proportion and on classical biostatistics as used in the above conservative disciplines, and (b) more subjectivist notions of uncertainty, belief, reliability, or confidence often used in automated inference and decision support systems. Samples in the above kind of public health survey are typically small compared with our earlier \u201cBig Data\u201d mining efforts. An issue addressed here is how much impact on decisions should sparse data have. We describe a new Q-UEL compatible toolkit including a data analytics application DiracMiner that also delivers more standard biostatistical results, DiracBuilder that uses its output to build Hyperbolic Dirac Nets (HDN) for decision support, and HDNcoherer that ensures that probabilities are mutually consistent. Use is exemplified by participating in a real word health-screening project, and also by deployment in a industrial platform called the BioIngine, a cognitive computing platform for health management."]},
{"title": "Effect of home reliner on occlusal relationships and oral mucosa: viscoelastic analyses by smoothed particle hydrodynamics simulation", "highlights": ["An excessive amount of home reliner (HR) would increase the vertical dimension.", "Denture is set in an inclined position by HR, stress on the mucosa greatly increase.", "Proper amount of HR decentralizes occlusal pressure.", "Proper amount of HR suppresses the depression of ill-fitting denture.", "A well-fitting denture is the most effective for distributing occlusal pressure."], "abstract": ["For decades, many reports have expressed negative opinions about home reliner (HR), because it may result in residual ridge resorption. Recently, some clinical studies evaluated HR. However, the effect of HR on occlusal relationships and the oral mucosa remains unclear. Here, we dynamically analyzed the situation in which a patient applies HR to an upper complete denture.", "We numerically simulated the effect of HR on occlusal relationships and the oral mucosa. In the simulation, the thickness of HR was set to 2", "\u00a0", "mm as a proper amount and 4", "\u00a0", "mm as an excessive amount. The loading points were set at the center of the right and left occlusal surfaces of the denture.", "Compared with the case without using HR, at proper amounts (2", "\u00a0", "mm on the right and left sides), HR suppressed the depression of the ill-fitting denture, and stress on the oral mucosa was decreased. In the excessive HR model (4", "\u00a0", "mm on the right and left sides), the vertical occlusal dimension was increased, and stress on the oral mucosa originally fitted with the denture base was increased. When the denture was modeled in an inclined position (2", "\u00a0", "mm on the left side and 4", "\u00a0", "mm on the right side), stress on the oral mucosa on the left buccal side was markedly increased.", "It was found that when an improper amount of HR was applied, the occlusal vertical dimension increased and the oral mucosa was pressured more than that under non-HR conditions."]},
{"title": "Clinic expert information extraction based on domain model and block importance model", "highlights": ["Clinic expert information provides references for residents who need hospital care.", "A domain model was defined to identify Web Query Interfaces.", "A virtual cluster was established for improving the performance of the domain model.", "A block importance model was proposed to filter noisy information in a Web page."], "abstract": ["To extract expert clinic information from the Deep Web, there are two challenges to face. The first one is to make a judgment on forms. A novel method based on a domain model, which is a tree structure constructed by the attributes of query interfaces is proposed. With this model, query interfaces can be classified to a domain and filled in with domain keywords. Another challenge is to extract information from response Web pages indexed by query interfaces. To filter the noisy information on a Web page, a block importance model is proposed, both content and spatial features are taken into account in this model. The experimental results indicate that the domain model yields a precision 4.89% higher than that of the rule-based method, whereas the block importance model yields an F1 measure 10.5% higher than that of the XPath method."]},
{"title": "Searching for \u201corder\u201d in atrial fibrillation using electrogram morphology recurrence plots", "highlights": ["Recurrence plots show patterns within atrial fibrillation electrograms.", "Recurrence plot diagonal and vertical lines indicate determinism and stationarity.", "Wide range of morphology recurrence rates seen in canine atrial fibrillation model.", "Shuffling electrogram order simulates random timing of electrogram recurrence."], "abstract": ["Bipolar electrograms recorded during atrial fibrillation (AF) can have an appearance of chaotic/random behavior. The aim of this study was to use a novel electrogram morphology recurrence (EMR) analysis to quantify the level of order in the morphology patterns in AF.", "Rapid atrial pacing was performed in seven dogs at 600", "\u00a0", "bpm for 3 weeks leading to sustained AF. Open chest high density electrical recordings were made in multiple atrial sites. EMR plots of bipolar electrograms at each site were created by cross-correlating morphologies of each detected activations with morphologies of every other activation. The following features of the EMR plots were quantified: recurrence rate (RR), determinism (DET), laminarity (LAM), average diagonal line length (", "), trapping time (TT), divergence (DIV), and Shannon\u05f3s entropy (ENTR). For each recording site, these measures were calculated for the normal sequence of morphologies and also after random shuffling of the electrogram orders.", "Electrograms recordings from a total of 3961 sites had average cycle lengths of 104\u00b122", "\u00a0", "ms resulting in an average of 100\u00b119 activations detected per 10-s recording and an average RR of 0.38\u00b10.28 (range 0.02\u20131.00). Shuffling the order of the activation morphologies resulted in significant decreases in DET, LAM, ", ", TT, and ENTR and significant increases in DIV.", "EMR plots of AF electrograms show varying rates of recurrence with patterns that suggest an underlying deterministic structure to the activation sequences. A better understanding of AF dynamics could lead to improved methods in mapping and treating AF."]},
{"title": "The use of compressive sensing and peak detection in the reconstruction of microtubules length time series in the process of dynamic instability", "highlights": ["We use the Compressed Sensing (CS) technique for the reconstruction of the microtubule (MT) filament length and estimation of the MT characteristic parameters based on minimum sampling measurements.", "CS method along with the wavelet transform significantly reduces the recovery errors.", "We apply a peak detection technique to the wavelet coefficients to approximate the growth and shrinkage rate of MTs for computing the essential dynamic instability parameters."], "abstract": ["Microtubules (MTs) are intra-cellular cylindrical protein filaments. They exhibit a unique phenomenon of stochastic growth and shrinkage, called dynamic instability. In this paper, we introduce a theoretical framework for applying Compressive Sensing (CS) to the sampled data of the microtubule length in the process of dynamic instability. To reduce data density and reconstruct the original signal with relatively low sampling rates, we have applied CS to experimental MT lament length time series modeled as a Dichotomous Markov Noise (DMN). The results show that using CS along with the wavelet transform significantly reduces the recovery errors comparing in the absence of wavelet transform, especially in the low and the medium sampling rates. In a sampling rate ranging from 0.2 to 0.5, the Root-Mean-Squared Error (RMSE) decreases by approximately 3 times and between 0.5 and 1, RMSE is small. We also apply a peak detection technique to the wavelet coefficients to detect and closely approximate the growth and shrinkage of MTs for computing the essential dynamic instability parameters, i.e., transition frequencies and specially growth and shrinkage rates. The results show that using compressed sensing along with the peak detection technique and wavelet transform in sampling rates reduces the recovery errors for the parameters."]},
{"title": "A clinical decision support system for diagnosis of Allergic Rhinitis based on intradermal skin tests", "highlights": ["Allergic Rhinitis is a symptomatic disorder of the nose due to allergen exposure.", "A clinical decision support system (CDSS) for diagnosis of Rhinitis is proposed.", "Skin test results of 872 patients were obtained from an allergy testing centre.", "The clinical relevance of the CDSS was validated and compared with existing methods.", "In expert\u05f3s absence, the system assists junior clinicians in decision making."], "abstract": ["Allergic Rhinitis is a universal common disease, especially in populated cities and urban areas. Diagnosis and treatment of Allergic Rhinitis will improve the quality of life of allergic patients. Though skin tests remain the gold standard test for diagnosis of allergic disorders, clinical experts are required for accurate interpretation of test outcomes. This work presents a clinical decision support system (CDSS) to assist junior clinicians in the diagnosis of Allergic Rhinitis.", "Intradermal Skin tests were performed on patients who had plausible allergic symptoms. Based on patient\u05f3s history, 40 clinically relevant allergens were tested. 872 patients who had allergic symptoms were considered for this study. The rule based classification approach and the clinical test results were used to develop and validate the CDSS. Clinical relevance of the CDSS was compared with the Score for Allergic Rhinitis (SFAR). Tests were conducted for junior clinicians to assess their diagnostic capability in the absence of an expert.", "The class based Association rule generation approach provides a concise set of rules that is further validated by clinical experts. The interpretations of the experts are considered as the gold standard. The CDSS diagnoses the presence or absence of rhinitis with an accuracy of 88.31%. The allergy specialist and the junior clinicians prefer the rule based approach for its comprehendible knowledge model.", "The Clinical Decision Support Systems with rule based classification approach assists junior doctors and clinicians in the diagnosis of Allergic Rhinitis to make reliable decisions based on the reports of intradermal skin tests."]},
{"title": "Improving point correspondence in cephalograms by using a two-stage rectified point transform", "highlights": ["A two-stage rectified point correspondence for cephalograms is proposed.", "Correspondence is improved by constraining the variance of point transformations.", "Dental landmarks are classified into three categories to improve correspondence.", "Users can flexibly add and remove landmarks without intensive image pretraining."], "abstract": ["An improved point correspondence method was developed for automatically detecting two-dimensional cephalometric landmarks. The proposed method uses a two-stage rectified point transform: the global correspondence of interest points between two images and the local correspondence of landmarks.", "In the first stage, point-to-point matching pairs were established using local corner point features. The matched points on an input image were treated as a set of transformations, with varying directions and magnitudes, from the template image. Similarity of the transformation vectors was achieved through rectification to exclude vectors that deviated widely from the statistical mean. Rectification attempted to remove noise and irrelevant matched points. In the second stage, the point correspondences were fine-tuned within the regional centers of the landmarks, which were classified into three categories\u2014corners, edges, and structural points\u2014and each category was fine-tuned using a different strategy. Correspondence was performed by evaluating the shortest Euclidean distance between the point descriptors of the template and test images.", "The correspondence results of 20 orthodontic landmarks were compared with those identified by dental professionals on 80 digital cephalograms collected from a dental clinic. The proposed method detected both hard and soft tissue landmarks with mean error distances of 1.63", "\u00a0", "mm, compared with the 2-mm standard reported by previous studies.", "This study enhanced the point correspondence technique for cephalometric landmarking. Using the proposed method, users can preferentially and flexibly add and remove landmarks on a template before correspondence without intensive image pretraining."]},
{"title": "Differential cryptanalysis of a medical image cryptosystem with multiple rounds", "highlights": ["Differential cryptanalysis proves that the security of Fu\u05f3s cryptosystem only depends on permutation key.", "Propose double differential cryptanalysis comparison method to analyze multiple round Fu\u05f3s cryptosystem.", "Simulation results demonstrate the effectiveness of our differential attacks.", "Point out the other defects of the cryptosystem."], "abstract": ["Recently, Fu et al. proposed a chaos-based medical image encryption scheme that has permutation\u2013substitution architecture. The authors believe that the scheme with bit-level cat map shuffling can be achieved at high level of security even if it is only applied with a few encryption rounds. However, we find that the scheme cannot resist differential cryptanalysis. The differential cryptanalysis shows that the security of the original scheme depends only on permutation key instead of on all of the keys. Moreover, 17 chosen plain-images can reveal equivalent permutation key for 1-round and 2-round encryption. We propose a novel analysis method called double differential cryptanalysis comparison (DDCC) that is valid to break multi-round encryption with ", " chosen plain-images, where ", " is the size of the image. We also point out several weaknesses of the cryptosystem. The theoretical analysis and simulation results indicate that the encryption scheme is insecure."]},
{"title": "Voice data mining for laryngeal pathology assessment", "highlights": ["We examined pathological changes in the speech signal of vowels /a/, /i/ and /u/.", "Various methods of voice signal analysis were examined to detect voice pathologies.", "Selected features have the influence on detection accuracies of pathological voice.", "100% voice pathology detection is achievable using Random Forest algorithm."], "abstract": ["The aim of this study was to evaluate the usefulness of different methods of speech signal analysis in the detection of voice pathologies. Firstly, an initial vector was created consisting of 28 parameters extracted from time, frequency and cepstral domain describing the human voice signal based on the analysis of sustained vowels /a/, /i/ and /u/ all at high, low and normal pitch. Afterwards we used a linear feature extraction technique (principal component analysis), which enabled a reduction in the number of parameters and choose the most effective acoustic features describing the speech signal. We have also performed non-linear data transformation which was calculated using kernel principal components. The results of the presented methods for normal and pathological cases will be revealed and discussed in this paper. The initial and extracted feature vectors were classified using the ", "-means clustering and the random forest classifier. We found that reasonably good classification accuracies could be achieved by selecting appropriate features. We obtained accuracies of up to 100% for classification of healthy versus pathology voice using random forest classification for female and male recordings. These results may assist in the feature development of automated detection systems for diagnosis of patients with symptoms of pathological voice."]},
{"title": "Enteroscopic findings of Celiac Disease and their correlation with mucosal histopathologic changes", "highlights": ["Celiac Disease is the most common inflammatory disease of the small bowel.", "Variety of endoscopic changes can be found in Celiac Disease patients.", "According to endoscopic and histopathologic data: Celiac Disease is a patchy mucosal disease and every patient with Celiac Disease exhibited at least one or more endoscopic changes through the proximal jejunum."], "abstract": ["Single Balloon Enteroscopy enables us to examine the small bowel for various diseases. It provides a view of the intestinal mucosa with biopsy capability, which may be helpful in search of a mucosal disease such as Celiac Disease. Celiac Disease is a proximal enteropathy developed in genetically susceptible individuals to wheat protein gluten. Examination of the duodenum and proximal jejunum are mostly diagnostic. We aimed to review enteroscopic findings of the patients with Celiac Disease.", "Consecutive adult patients (>18", "\u00a0", "y) who needed intestinal or duodenal biopsy for the diagnosis of the Celiac Disease were included. Single Balloon Enteroscopy system was used to enter the proximal jejunum. All of the patients had biopsies in order to diagnose Celiac Disease.", "Single Balloon Enteroscopy was performed in 33 patients. Twenty two (66.7%) subjects were diagnosed as Celiac Disease. The most common endoscopic abnormality in Celiac Disease was mucosal atrophy in 20 patients (90.9%), continuous involvement was the most common presentation (36.4%). All of the patients with Celiac Disease exhibited at least one endoscopic change.", "This study confirmed the patchy nature of the disease with mostly diffuse involvement of the small bowel. However, any endoscopic abnormality can be found in every patient with Celiac Disease. Analysis of images from either conventional upper endoscopy or capsule endoscopy may aid the diagnosis."]},
{"title": "Exploring the color feature power for psoriasis risk stratification and classification: A data mining paradigm", "highlights": ["Exploration of 14 color spaces (86 color features).", "Exploration of 8 grayscale feature extraction techniques (60 grayscale features).", "PCA-based dominant feature selection without altering their original feature values.", "Introducing feature power for three spaces: color, grayscale and combined.", "Introducing a reliability factor for psoriasis based CADx system."], "abstract": ["A large percentage of dermatologist\u05f3s decision in psoriasis disease assessment is based on color. The current computer-aided diagnosis systems for psoriasis risk stratification and classification lack the vigor of color paradigm. The paper presents an automated psoriasis computer-aided diagnosis (pCAD) system for classification of psoriasis skin images into psoriatic lesion and healthy skin, which solves the two major challenges: (i) fulfills the color feature requirements and (ii) selects the powerful dominant color features while retaining high classification accuracy.", "Fourteen color spaces are discovered for psoriasis disease analysis leading to 86 color features. The pCAD system is implemented in a support vector-based machine learning framework where the offline image data set is used for computing machine learning offline color machine learning parameters. These are then used for transformation of the online color features to predict the class labels for healthy vs. diseased cases. The above paradigm uses principal component analysis for color feature selection of dominant features, keeping the original color feature unaltered. Using the cross-validation protocol, the above machine learning protocol is compared against the standalone grayscale features with 60 features and against the combined grayscale and color feature set of 146.", "Using a fixed data size of 540 images with equal number of healthy and diseased, 10 fold cross-validation protocol, and SVM of polynomial kernel of type two, pCAD system shows an accuracy of 99.94% with sensitivity and specificity of 99.93% and 99.96%. Using a varying data size protocol, the mean classification accuracies for color, grayscale, and combined scenarios are: 92.85%, 93.83% and 93.99%, respectively. The reliability of the system in these three scenarios are: 94.42%, 97.39% and 96.00%, respectively. We conclude that pCAD system using color space alone is compatible to grayscale space or combined color and grayscale spaces. We validated our pCAD system against facial color databases and the results are consistent in accuracy and reliability."]},
{"title": "A probabilistic approach for automated discovery of perturbed genes using expression data from microarray or RNA-Seq", "highlights": ["Automated analysis of high throughput data for disease gene and biomarker discovery.", "Network based analysis using probability of change for gene expression.", "Analysis of coherent path level changes in regulation in response to complex disease.", "Biomarkers with high precision and accuracy of classification."], "abstract": ["In complex diseases, alterations of multiple molecular and cellular components in response to perturbations are indicative of disease physiology. While expression level of genes from high-throughput analysis can vary among patients, the common path among disease progression suggests that the underlying cellular sub-processes involving associated genes follow similar fates. Motivated by the interconnected nature of sub-processes, we have developed an automated methodology that combines ideas from biological networks, statistical models, and game theory, to probe connected cellular processes. The core concept in our approach uses probability of change (POC) to indicate the probability that a gene\u2019s expression level has changed between two conditions. POC facilitates the definition of change at the neighborhood, pathway, and network levels and enables evaluation of the influence of diseases on the expression. The \u2018connected\u2019 disease-related genes (DRG) identified display coherent and concomitant differential expression levels along paths.", "RNA-Seq and microarray breast cancer subtyping expression data sets were used to identify DRG between subtypes. A machine-learning algorithm was trained for subtype discrimination using the DRG, and the training yielded a set of biomarkers. The discriminative power of the biomarkers was tested using an unseen data set. Biomarkers identified overlaps with disease-specific identified genes, and we were able to classify disease subtypes with 100% and 80% agreement with PAM50, for microarray and RNA-Seq data set respectively.", "We present an automated probabilistic approach that offers unbiased and reproducible results, thus complementing existing methods in DRG and biomarker discovery for complex diseases."]},
{"title": "Quantitative assessment of myocardial fibrosis in an age-related rat model by ex vivo late gadolinium enhancement magnetic resonance imaging with histopathological correlation", "highlights": ["High-resolution LGE CMR imaging can detect age-related myocardial fibrosis.", "There was a significant increase of collagen in the aged vs. the young rat hearts.", "Quantitative LGE and texture analysis correlated well with histology measurements.", "Texture analysis may improve the myocardial fibrosis assessment in LGE CMR images."], "abstract": ["Late gadolinium enhanced (LGE) cardiac magnetic resonance (CMR) imaging can detect the presence of myocardial infarction from ischemic cardiomyopathies (ICM). However, it is more challenging to detect diffuse myocardial fibrosis from non-ischemic cardiomyopathy (NICM) with this technique due to more subtle and heterogeneous enhancement of the myocardium. This study investigates whether high-resolution LGE CMR can detect age-related myocardial fibrosis using quantitative texture analysis with histological validation. LGE CMR of twenty-four rat hearts (twelve 6-week-old and twelve 2-year-old) was performed using a 7", "\u00a0", "T MRI scanner. Picrosirius red was used as the histopathology reference for collagen staining. Fibrosis in the myocardium was quantified with standard deviation (SD) threshold methods from the LGE CMR images and 3D ", " texture maps that were computed from gray level co-occurrence matrix of the CMR images. There was a significant increase of collagen fibers in the aged compared to the young rat histology slices (2.60\u00b10.27 %LV vs. 1.24\u00b10.29 %LV, ", "<0.01). Both LGE CMR and texture images showed a significant increase of myocardial fibrosis in the elderly compared to the young rats. Fibrosis in the LGE CMR images correlated strongly with histology with the 3 SD threshold (", "=0.84, ", "=0.99", "+0.00). Similarly, fibrosis in the ", " texture maps correlated with the histology using the 4 SD threshold (", "=0.89, ", "=1.01", "+0.00). High resolution ex-vivo LGE CMR can detect the presence of diffuse fibrosis that naturally developed in elderly rat hearts. Our results suggest that texture analysis may improve the assessment of myocardial fibrosis in LGE CMR images."]},
{"title": "Multidimensional extended spatial evolutionary games", "highlights": ["Introducing time-varying payoff matrices in mean-field games.", "New possibilities for more accurate and complex game theoretic simulation.", "Spatial lattice with simulation of interaction between cells and environment.", "Heterogeneity of individuals (poly-phenotypic players) in spatial games (MSEG).", "More precisely biological phenomena, especially for cancer cell populations."], "abstract": ["The goal of this paper is to study the classical hawk\u2013dove model using mixed spatial evolutionary games (MSEG). In these games, played on a lattice, an additional spatial layer is introduced for dependence on more complex parameters and simulation of changes in the environment. Furthermore, diverse polymorphic equilibrium points dependent on cell reproduction, model parameters, and their simulation are discussed. Our analysis demonstrates the sensitivity properties of MSEGs and possibilities for further development. We discuss applications of MSEGs, particularly algorithms for modelling cell interactions during the development of tumours."]},
{"title": "Mid-level image representations for real-time heart view plane classification of echocardiograms", "highlights": ["Proposal of new mid-level representations for real-time heart view plane classification of 2D echocardiograms.", "Approach relies on bags of visual words with image sampling using large regions.", "Extensive set of experiments comparing the proposed method with existing descriptors.", "Evaluation considering real-time constraints, noise filtering, and different machine learning classifiers.", "Proposed approach is very fast to compute and consistently achieves accuracy above 90%."], "abstract": ["In this paper, we explore mid-level image representations for real-time heart view plane classification of 2D echocardiogram ultrasound images. The proposed representations rely on bags of visual words, successfully used by the computer vision community in visual recognition problems. An important element of the proposed representations is the image sampling with large regions, drastically reducing the execution time of the image characterization procedure. Throughout an extensive set of experiments, we evaluate the proposed approach against different image descriptors for classifying four heart view planes. The results show that our approach is effective and efficient for the target problem, making it suitable for use in real-time setups. The proposed representations are also robust to different image transformations, e.g., downsampling, noise filtering, and different machine learning classifiers, keeping classification accuracy above 90%. Feature extraction can be performed in 30", "\u00a0", "fps or 60", "\u00a0", "fps in some cases. This paper also includes an in-depth review of the literature in the area of automatic echocardiogram view classification giving the reader a through comprehension of this field of study."]},
{"title": "Reprint of 'Model of unidirectional block formation leading to reentrant ventricular tachycardia in the infarct border zone of postinfarction canine hearts'", "highlights": ["Ventricular tachycardia is common after myocardial infarction.", "Reentrant ventricular tachycardia is often located in the infarct border zone.", "Conduction velocity of the leading edge of the propagating wavefront is not constant.", "Functional block occurs at locations where the wavefront becomes critically convex.", "Activation rate and IBZ geometric structure during reentrant ventricular tachycardia are determined."], "abstract": ["When the infarct border zone is stimulated prematurely, a unidirectional block line (UBL) can form and lead to double-loop (figure-of-eight) reentrant ventricular tachycardia (VT) with a central isthmus. The isthmus is composed of an entrance, center, and exit. It was hypothesized that for certain stimulus site locations and coupling intervals, the UBL would coincide with the isthmus entrance boundary, where infarct border zone thickness changes from thin-to-thick in the travel direction of the premature stimulus wavefront.", "A quantitative model was developed to describe how thin-to-thick changes in the border zone result in critically convex wavefront curvature leading to conduction block, which is dependent upon coupling interval. The model was tested in 12 retrospectively analyzed postinfarction canine experiments. Electrical activation was mapped for premature stimulation and for the first reentrant VT cycle. The relationship of functional conduction block forming during premature stimulation to functional block during reentrant VT was quantified.", "For an appropriately placed stimulus, in accord with model predictions: 1. The UBL and reentrant VT isthmus lateral boundaries overlapped (error: 4.8\u00b15.7", "\u00a0", "mm). 2. The UBL leading edge coincided with the distal isthmus where the center-entrance boundary would be expected to occur. 3. The mean coupling interval was 164.6\u00b111.0", "\u00a0", "ms during premature stimulation and 190.7\u00b120.4", "\u00a0", "ms during the first reentrant VT cycle, in accord with model calculations, which resulted in critically convex wavefront curvature and functional conduction block, respectively, at the location of the isthmus entrance boundary and at the lateral isthmus edges.", "Reentrant VT onset following premature stimulation can be explained by the presence of critically convex wavefront curvature and unidirectional block at the isthmus entrance boundary when the premature stimulation interval is sufficiently short. The double-loop reentrant circuit pattern is a consequence of wavefront bifurcation around this UBL followed by coalescence, and then impulse propagation through the isthmus. The wavefront is blocked from propagating laterally away from the isthmus by sharp increases in border zone thickness, which results in critically convex wavefront curvature at VT cycle lengths."]},
{"title": "Analysis of flow and LDL concentration polarization in siphon of internal carotid artery: Non-Newtonian effects", "highlights": ["Non-Newtonian effects are considerable in U-type model of ICA siphon.", "Extend of the high LDL concentration regions are larger for the V type model.", "Local highest maximum concentrations occur for the C type model.", "C and V type models are more prone to atherosclerosis comparing with the U type.", "Averaged WSS can predict sites with more LDL accumulation."], "abstract": ["Carotid siphon is known as one of the risky sites among the human intracranial arteries, which is prone to formation of atherosclerotic lesions. Indeed, scientists believe that accumulation of low density lipoprotein (LDL) inside the lumen is the major cause of atherosclerosis. To this aim, three types of internal carotid artery (ICA) siphon have been constructed to examine variations of hemodynamic parameters in different regions of the arteries. Providing real physiological conditions, blood considered as non-Newtonian fluid and real velocity and pressure waveforms have been employed as flow boundary conditions. Moreover, to have a better estimation of risky sites, the accumulation of LDL particles has been considered, which has been usually ignored in previous relevant studies. Governing equations have been discretized and solved via open source OpenFOAM software. A new solver has been built to meet essential parameters related to the flow and mass transfer phenomena. In contrast to the common belief regarding negligible effect of blood non-Newtonian behavior inside large arteries, current study suggests that the non-Newtonian blood behavior is notable, especially on the velocity field of the U-type model. In addition, it is concluded that neglecting non-Newtonian effects underestimates the LDL accumulation up to 3% in the U-type model at the inner side of both its bends. However, in the V and C type models, non-Newtonian effects become relatively small. Results also emphasize that the outer part of the second bend at the downstream is also at risk similar to the inner part of the carotid bends. Furthermore, from findings it can be implied that the risky sites strongly depend on the ICA shape since the extension of the risky sites are relatively larger for the V-type model, while the LDL concentrations are higher for the C-type model."]},
{"title": "Investigations on the role of CH\u2026O interactions and its impact on stability and specificity of penicillin binding proteins", "highlights": ["Importance of CH\u2026O bonds in the structural stability of PBPs are analyzed.", "Significant number of interacting residues shows higher conservation score.", "Interestingly residues with higher conservation score have more stabilizing regions.", "Our study provides detailed structural information about PBPs.", "Results obtained from this study will be useful in designing effective antibiotics."], "abstract": ["Penicillin binding proteins are recognized as important antibacterial targets because of their crucial role in the cell wall synthesis of bacteria. Alteration in the binding site of penicillin binding proteins is one of the major problems for beta lactam antibiotics to exert its effect. In the present study the influence of CH\u2026O interactions in the conformational stability of penicillin binding proteins were analyzed in both Gram positive and Gram negative bacteria. CH\u2026O interactions constitute about 20 to 25% of total hydrogen bonds and act as an important driving force in ligand selectivity. From our analysis we observed a total of 13,398 CH\u2026O interactions in Gram positive bacteria and 10,855 CH\u2026O interactions in Gram negative bacteria. It was interesting to observe that CH\u2026O interactions were higher in Gram positive bacteria than in Gram negative bacteria, which augurs well for the discrepancy in cell wall of the bacteria. CH\u2026O interactions are classified into four types depending on the interaction of acceptor residues with the back bone or side chain of CH groups. From our results we observed that major contribution to penicillin binding proteins was observed from side chain atoms of donor residues and back bone atoms of acceptor residues [SM CH\u2026O] in both Gram positive and Gram negative bacteria. Conformational preference of Gram positive bacteria indicated that amino acids lacking side chain and the cyclized amino acids preferred to be in turn regions, whereas aromatic amino acids dominated in Gram negative bacteria. Our analysis gives detailed information about the principles involved in the conformational stability of penicillin binding proteins and the results will be useful for researchers exploring penicillin binding proteins."]},
{"title": "An experimental comparison of feature selection methods on two-class biomedical datasets", "highlights": ["Ten feature selection methods are compared using stability and similarity measures.", "Univariate FS perform better than multivariate FS for high dimensional datasets.", "Multivariate FS slightly outperform univariate FS for complex and smaller datasets.", "Most stable appears to be entropy based FS.", "FS yielding the highest prediction performance are MRMR and Bhattacharyya distance."], "abstract": ["Feature selection is a significant part of many machine learning applications dealing with small-sample and high-dimensional data. Choosing the most important features is an essential step for knowledge discovery in many areas of biomedical informatics. The increased popularity of feature selection methods and their frequent utilisation raise challenging new questions about the interpretability and stability of feature selection techniques. In this study, we compared the behaviour of ten state-of-the-art filter methods for feature selection in terms of their stability, similarity, and influence on prediction performance. All of the experiments were conducted on eight two-class datasets from biomedical areas. While entropy-based feature selection appears to be the most stable, the feature selection techniques yielding the highest prediction performance are minimum redundance maximum relevance method and feature selection based on Bhattacharyya distance. In general, univariate feature selection techniques perform similarly to or even better than more complex multivariate feature selection techniques with high-dimensional datasets. However, with more complex and smaller datasets multivariate methods slightly outperform univariate techniques."]},
{"title": "Automated lesion detectors in retinal fundus images", "highlights": ["A novel computer-aided decision support system for diabetic retinopathy using retinal fundus images is proposed.", "Microaneurysm, hemorrhage and bright lesion binary classifier detectors are devised.", "Validation of the system on large datasets using standard measures is performed."], "abstract": ["Diabetic retinopathy (DR) is a sight-threatening condition occurring in persons with diabetes, which causes progressive damage to the retina. The early detection and diagnosis of DR is vital for saving the vision of diabetic persons. The early signs of DR which appear on the surface of the retina are the dark lesions such as microaneurysms (MAs) and hemorrhages (HEMs), and bright lesions (BLs) such as exudates. In this paper, we propose a novel automated system for the detection and diagnosis of these retinal lesions by processing retinal fundus images. We devise appropriate binary classifiers for these three different types of lesions. Some novel contextual/numerical features are derived, for each lesion type, depending on its inherent properties. This is performed by analysing several wavelet bands (resulting from the isotropic undecimated wavelet transform decomposition of the retinal image green channel) and by using an appropriate combination of Hessian multiscale analysis, variational segmentation and cartoon+texture decomposition. The proposed methodology has been validated on several medical datasets, with a total of 45,770 images, using standard performance measures such as sensitivity and specificity. The individual performance, per frame, of the MA detector is 93% sensitivity and 89% specificity, of the HEM detector is 86% sensitivity and 90% specificity, and of the BL detector is 90% sensitivity and 97% specificity. Regarding the collective performance of these binary detectors, as an automated screening system for DR (meaning that a patient is considered to have DR if it is a positive patient for at least one of the detectors) it achieves an average 95\u2013100% of sensitivity and 70% of specificity at a per patient basis. Furthermore, evaluation conducted on publicly available datasets, for comparison with other existing techniques, shows the promising potential of the proposed detectors."]},
{"title": "Changeability of tissue\u2019s magnetic remanence after galvanic-magnetostimulation in upper-back pain treatment", "highlights": ["GMT 2.0 treatment, improves the mobility range of the cervical and thoracic spine.", "Decrease in the levels of bilirubin, creatinine, Fe, an increase in the levels of Mg, K.", "Magnetic remanence in Group A underwent partial reduction before the 10th treatment.", "Marker points\u2019 magnetic susceptibility reached lower values during the 10th treatment.", "Correlation between the value of magnetic remanence and HR, RR, VAS, BDI, BIL,CREA, F."], "abstract": ["Research was conducted on parametric profiles of healthy subjects and patients with cervico-brachial pain syndrome resulting from C4/5 and/or C5/6 discopathy, including magnetic remanence of tissues in marker points 1-12 (L+R) and functional parameters, and their subsequent change after treatment in group A, using method of push-pull galvanic magnetostimulation (GMT 2.0).", "GMT 2.0 device, comprised of one air solenoid and three galvanic solenoids in electrolytic tubs, was designed for push-pull magnetostimulation of the head, coupled with simultaneous stimulation of the limbs.", "Clinical trial was conducted in Outpatient Private Clinic \u201cVIS\u201d under the auspices of Silesian Higher Medical School in Katowice, Poland.", "55 subjects participated in the study: control group K consisted of 23 healthy individuals, whereas 33 patients in group A were treated using GMT 2.0.", "Only patients in group A were treated with GMT 2.0 during 40-min sessions over a period of 10 days.", "Parametric profile of the patients was defined using various measurements: electronic SFTR test (C-Th-shoulders), HR, RR, BDI and VAS tests, magnetic remanence in marker points 1-12 (L+R) and blood parameters: HB, ER, CREA, BIL, K", ", Na", ", Cl", " Fe", ", Ca", " and Mg", ".", "There was a significant reduction in pain (VAS), increase in the range of motion (SFTR), lower depression symptoms (BDI), slower heart rate (HR), lower blood pressure (RR), greater concentration of Mg", ", K", ", Ca", "ions and reduction in the concentration of BIL, CREA Fe", " after GMT 2.0 treatment in group A. Evaluation of magnetic remanence in marker points M1-12 (L+R) initially showed higher values in group K, which after treatment were normalized to values similar to those in group K.", "GMT 2.0 treatment in group A resulted in normalization of magnetic remanence, synergically with increased range of motion (SFTR test), decreased HR and RR parameters, smaller depressive trends (BDI test), as well as increased ion levels (K", ", Mg", ", Ca", ") and better functional parameters of kidneys and liver."]},
{"title": "A novel Morse code-inspired method for multiclass motor imagery brain\u2013computer interface (BCI) design", "highlights": ["A reliable multiclass BCI is constructed with limited motor imagery tasks using the Morse code-inspired approach.", "The proposed BCI system is used to control a real three joints robot arm to grasp a target object.", "The controllability of our BCI system is improved with the feedback mechanism embedded in the robot arm."], "abstract": ["Motor imagery (MI)-based brain\u2013computer interfaces (BCIs) allow disabled individuals to control external devices voluntarily, helping us to restore lost motor functions. However, the number of control commands available in MI-based BCIs remains limited, limiting the usability of BCI systems in control applications involving multiple degrees of freedom (DOF), such as control of a robot arm. To address this problem, we developed a novel Morse code-inspired method for MI-based BCI design to increase the number of output commands. Using this method, brain activities are modulated by sequences of MI (sMI) tasks, which are constructed by alternately imagining movements of the left or right hand or no motion. The codes of the sMI task was detected from EEG signals and mapped to special commands. According to permutation theory, an sMI task with ", "-length allows ", " possible commands with the left and right MI tasks under self-paced conditions. To verify its feasibility, the new method was used to construct a six-class BCI system to control the arm of a humanoid robot. Four subjects participated in our experiment and the averaged accuracy of the six-class sMI tasks was 89.4%. The Cohen's kappa coefficient and the throughput of our BCI paradigm are ", " and 23.5", "\u00a0", "bits per minute (bpm), respectively. Furthermore, all of the subjects could operate an actual three-joint robot arm to grasp an object in around 49.1", "\u00a0", "s using our approach. These promising results suggest that the Morse code-inspired method could be used in the design of BCIs for multi-DOF control."]},
{"title": "Non-uniform central airways ventilation model based on vascular segmentation", "highlights": ["A new lung volume sectioning technique based on vascular segmentation is presented.", "It is used to partition 4DCT based lung volumes, sampling a full breathing cycle.", "Volume change of lung sections over time is used to estimate non-uniform ventilation.", "The technique was shown to predict lobar volume estimates within a tolerance of 2%.", "Enables subject-specific boundary conditions for modelling the central airway flow."], "abstract": ["Improvements in the understanding of the physiology of the central airways require an appropriate representation of the non-uniform ventilation at its terminal branches. This paper proposes a new technique for estimating the non-uniform ventilation at the terminal branches by modelling the volume change of their distal peripheral airways, based on vascular segmentation. The vascular tree is used for sectioning the dynamic CT-based 3D volume of the lung at 11 time points over the breathing cycle of a research animal. Based on the mechanical coupling between the vascular tree and the remaining lung tissues, the volume change of each individual lung segment over the breathing cycle was used to estimate the non-uniform ventilation of its associated terminal branch. The 3D lung sectioning technique was validated on an airway cast model of the same animal pruned to represent the truncated dynamic CT based airway geometry. The results showed that the 3D lung sectioning technique was able to estimate the volume of the missing peripheral airways within a tolerance of 2%. In addition, the time-varying non-uniform ventilation distribution predicted by the proposed sectioning technique was validated against CT measurements of lobar ventilation and showed good agreement. This significant modelling advance can be used to estimate subject-specific non-uniform boundary conditions to obtain subject-specific numerical models of the central airway flow."]},
{"title": "Machine learning classification of medication adherence in patients with movement disorders using non-wearable sensors", "highlights": ["Data driven methodology to detect adherence using non-invasive sensors in PD patient.", "Effectiveness of approach illustrated with case study.", "Solution for non-hospital remote monitoring of patients."], "abstract": ["Medication non-adherence is a major concern in the healthcare industry and has led to increases in health risks and medical costs. For many neurological diseases, adherence to medication regimens can be assessed by observing movement patterns. However, physician observations are typically assessed based on visual inspection of movement and are limited to clinical testing procedures. Consequently, medication adherence is difficult to measure when patients are away from the clinical setting. The authors propose a data mining driven methodology that uses low cost, non-wearable multimodal sensors to model and predict patients' adherence to medication protocols, based on variations in their gait. The authors conduct a study involving Parkinson's disease patients that are \u201con\u201d and \u201coff\u201d their medication in order to determine the statistical validity of the methodology. The data acquired can then be used to quantify patients' adherence while away from the clinic. Accordingly, this data-driven system may allow for early warnings regarding patient safety. Using whole-body movement data readings from the patients, the authors were able to discriminate between PD patients on and off medication, with accuracies greater than 97% for some patients using an individually customized model and accuracies of 78% for a generalized model containing multiple patient gait data. The proposed methodology and study demonstrate the potential and effectiveness of using low cost, non-wearable hardware and data mining models to monitor medication adherence outside of the traditional healthcare facility. These innovations may allow for cost effective, remote monitoring of treatment of neurological diseases."]},
{"title": "Tissue prolapse and stresses in stented coronary arteries: A computer model for multi-layer atherosclerotic plaque", "highlights": ["Tissue prolapse is studied computationally in stented coronary artery.", "Effects of plaque layers are revealed in the resulting tissue prolapse values.", "This can help in stent design and optimal choice of stent to prevent restenosis."], "abstract": ["Among the many factors influencing the effectiveness of cardiovascular stents, tissue prolapse indicates the potential of a stent to cause restenosis. The deflection of the arterial wall between the struts of the stent and the tissue is known as a prolapse or draping. The prolapse is associated with injury and damage to the vessel wall due to the high stresses generated around the stent when it expands. The current study investigates the impact of stenosis severity and plaque morphology on prolapse in stented coronary arteries. A finite element method is applied for the stent, plaque, and artery set to quantify the tissue prolapse and the corresponding stresses in stenosed coronary arteries. The variable size of atherosclerotic plaques is considered. A plaque is modelled as a multi-layered medium with different thicknesses attached to the single layer of an arterial wall. The results reveal that the tissue prolapse is influenced by the degree of stenosis severity and the thickness of the plaque layers. Stresses are observed to be significantly different between the plaque layers and the arterial wall tissue. Higher stresses are concentrated in fibrosis layer of the plaque (the harder core), while lower stresses are observed in necrotic core (the softer core) and the arterial wall layer. Moreover, the morphology of the plaque regulates the magnitude and distribution of the stress. The fibrous cap between the necrotic core and the endothelium constitutes the most influential layer to alter the stresses. In addition, the thickness of the necrotic core and the stenosis severity affect the stresses. This study reveals that the morphology of atherosclerotic plaques needs to be considered a key parameter in designing coronary stents."]},
{"title": "Real-time prediction of acute cardiovascular events using hardware-implemented Bayesian networks", "highlights": ["A system for detecting conditions dangerous to patient\u05f3s health or life is presented.", "The system is a microprocessor-based device attached to a medical monitor.", "The system works in real time providing guidance for medical personnel.", "The core of the reasoning algorithms is based on Bayesian networks."], "abstract": ["This paper presents a decision support system that aims to estimate a patient\u05f3s general condition and detect situations which pose an immediate danger to the patient\u05f3s health or life. The use of this system might be especially important in places such as accident and emergency departments or admission wards, where a small medical team has to take care of many patients in various general conditions. Particular stress is laid on cardiovascular and pulmonary conditions, including those leading to sudden cardiac arrest. The proposed system is a stand-alone microprocessor-based device that works in conjunction with a standard vital signs monitor, which provides input signals such as temperature, blood pressure, pulseoxymetry, ECG, and ICG. The signals are preprocessed and analysed by a set of artificial intelligence algorithms, the core of which is based on Bayesian networks. The paper focuses on the construction and evaluation of the Bayesian network, both its structure and numerical specification."]},
{"title": "Expert system supporting an early prediction of the bronchopulmonary dysplasia", "highlights": ["We construct a database of ", " results of both logit regression and SVM models.", "Our expert system finds the best method and model to use in given circumstances.", "Accuracy, sensitivity and specificity are estimated for user selected model.", "Bronchopulmonary dysplasia diagnosis is predicted with the accuracy up to 83.25%."], "abstract": ["This work presents a decision support system which uses machine learning to support early prediction of bronchopulmonary dysplasia (BPD) for extremely premature infants after their first week of life. For that purpose a knowledge database was created based on the historical data gathered including data on 109 patients with birth weight less than or equal to 1500", "\u00a0", "g. The core of the database consists of support vector machine and logit regression classification results calculated specifically for that system, and obtained by considering ", " different combinations of 14 risk factors. Based on the results obtained and user demands, the system recommends the best methods and the most suitable parameter subset among those currently available to the user. The program is also able to estimate the accuracy, sensitivity and specificity together with their standard deviations. The user is also given information on which additional parameter it is worth adding to his measurement system most and what an increase in prediction efficiency it is expected to trigger. The BPD can be predicted by the system with the accuracy reaching up to 83.25% in the best-case scenario, i.e. higher than for most of the models presented in the literature. This work presents a set of examples illustrating the difficulties in obtaining one single model that can be widely used, and thus explaining why an expert system approach is much more useful in day-to-day clinical practice. In addition, the work discusses the significance of the parameters used and the impact of a chosen method on the sensitivity and specificity."]},
{"title": "A similarity-based data warehousing environment for medical images", "highlights": ["We focus on the manipulation of medical images in a data warehousing.", "We show how to store intrinsic features from medical images in a data warehouse.", "We show how to process OLAP similarity queries in an image data warehousing.", "We propose a new Bitmap-based index to speed up OLAP similarity query processing.", "Tests demonstrated the feasibility of our proposal to manage real medical images."], "abstract": ["A core issue of the decision-making process in the medical field is to support the execution of analytical (OLAP) similarity queries over images in data warehousing environments. In this paper, we focus on this issue. We propose ", ", a non-conventional data warehousing environment that enables the storage of intrinsic features taken from medical images in a data warehouse and supports OLAP similarity queries over them. To comply with this goal, we introduce the concept of perceptual layer, which is an abstraction used to represent an image dataset according to a given feature descriptor in order to enable similarity search. Based on this concept, we propose the ", ", an extended data warehouse with dimension tables specifically designed to support one or more perceptual layers. We also detail how to build an ", " and how to load image data into it. Furthermore, we show how to process OLAP similarity queries composed of a conventional predicate and a similarity search predicate that encompasses the specification of one or more perceptual layers. Moreover, we introduce an index technique to improve the OLAP query processing over images. We carried out performance tests over a data warehouse environment that consolidated medical images from exams of several modalities. The results demonstrated the feasibility and efficiency of our proposed ", " to manage images and to process OLAP similarity queries. The results also demonstrated that the use of the proposed index technique guaranteed a great improvement in query processing."]},
{"title": "Thalamic segmentation based on improved fuzzy connectedness in structural MRI", "highlights": ["An improved fuzzy connectedness based thalamic segmentation method was proposed.", "Image local gradient was incorporated in calculation of fuzzy affinity.", "The weight of intensity and local gradient can be automatic adjusted.", "Confidence connectedness was used in automatic ROI update.", "Only one seed point was needed."], "abstract": ["Thalamic segmentation serves an important function in localizing targets for deep brain stimulation (DBS). However, thalamic nuclei are still difficult to identify clearly from structural MRI. In this study, an improved algorithm based on the fuzzy connectedness framework was developed. Three-dimensional T1-weighted images in axial orientation were acquired through a 3D SPGR sequence by using a 1.5", "\u00a0", "T GE magnetic resonance scanner. Twenty-five normal images were analyzed using the proposed method, which involved adaptive fuzzy connectedness combined with confidence connectedness (AFCCC). After non-brain tissue removal and contrast enhancement, the seed point was selected manually, and confidence connectedness was used to perform an ROI update automatically. Both image intensity and local gradient were taken as image features in calculating the fuzzy affinity. Moreover, the weight of the features could be automatically adjusted. Thalamus, ventrointermedius (Vim), and subthalamic nucleus were successfully segmented. The results were evaluated with rules, such as similarity degree (SD), union overlap, and false positive. SD of thalamus segmentation reached values higher than 85%. The segmentation results were also compared with those achieved by the region growing and level set methods, respectively. Higher SD of the proposed method, especially in Vim, was achieved. The time cost using AFCCC was low, although it could achieve high accuracy. The proposed method is superior to the traditional fuzzy connectedness framework and involves reduced manual intervention in time saving."]},
{"title": "ADRC or adaptive controller \u2013 A simulation study on artificial blood pump", "highlights": ["An Active Disturbance Rejection Controller was developed for a rotary blood pump.", "The performance of ADRC was studied in activity and pathology variation.", "ADRC was easy to design and tune.", "ADRC demonstrated robust performance with excellent disturbance rejection."], "abstract": ["Active disturbance rejection control (ADRC) has gained popularity because it requires little knowledge about the system to be controlled, has the inherent disturbance rejection ability, and is easy to tune and implement in practical systems. In this paper, the authors compared the performance of an ADRC and an adaptive controller for an artificial blood pump for end-stage congestive heart failure patients using only the feedback signal of pump differential pressure. The purpose of the control system was to provide sufficient perfusion when the patients\u2019 circulation system goes through different pathological and activity variations. Because the mean arterial pressure is equal to the total peripheral flow times the total peripheral resistance, this goal was converted to an expression of making the mean aortic pressure track a reference signal. The simulation results demonstrated that the performance of the ADRC is comparable to that of the adaptive controller with the saving of modeling and computational effort and fewer design parameters: total peripheral flow and mean aortic pressure with ADRC fall within the normal physiological ranges in activity variation (rest to exercise) and in pathological variation (left ventricular strength variation), similar to those values of adaptive controller."]},
{"title": "Mathematical study of probe arrangement and nanoparticle injection effects on heat transfer during cryosurgery", "highlights": ["The effect of probe and multiprobe on heat transfer is investigated numerically.", "Nanoparticles have varied influence on temperature distribution during cryosurgery.", "Different nanoparticles combined with multicryoprobe in numerical model are studied.", "Novel triangle arrangement was introduced and improved the temperature distribution."], "abstract": ["Blood vessels, especially large vessels have a greater thermal effect on freezing tissue during cryosurgery. Vascular networks act as heat sources in tissue, and cause failure in cryosurgery and reappearance of cancer. The aim of this study is to numerically simulate the effect of probe location and multiprobe on heat transfer distribution. Furthermore, the effect of nanoparticles injection is studied. It is shown that the small probes location near large blood vessels could help to reduce the necessary time for tissue freezing. Nanoparticles injection shows that the thermal effect of blood vessel in tissue is improved. Using Au, Ag and diamond nanoparticles have the most growth of ice ball during cryosurgery. However, polytetrafluoroethylene (PTFE) nanoparticle can be used to protect normal tissue around tumor cell due to its influence on reducing heat transfer in tissue. Introduction of Au, Ag and diamond nanoparticles combined with multicryoprobe in this model causes reduction of tissue average temperature about 50% compared to the one probe."]},
{"title": "Segmentation of retinal vessels by means of directional response vector similarity and region growing", "highlights": ["A novel retinal vessel segmentation method is proposed.", "A symmetry constrained multi-scale matched filtering technique is presented.", "A unique directional response statistics based vessel score formula is given.", "Proposal of a response vector similarity based region growing segmentation.", "Performance of the method is comparable to that of a human observer."], "abstract": ["This paper presents a novel retinal vessel segmentation method. Opposed to the general approach in similar directional methods, where only the maximal or summed responses of a pixel are used, here, the directional responses of a pixel are considered as a vector. The segmentation method is a unique region growing procedure which combines a hysteresis thresholding scheme with the response vector similarity of adjacent pixels. A vessel score map is constructed as the combination of the statistical measures of the response vectors and its local maxima to provide the seeds for the region growing procedure. A nearest neighbor classifier based on a rotation invariant response vector similarity measure is used to filter the seed points. Many techniques in the literature that capture the Gaussian-like cross-section of vessels suffer from the drawback of giving false high responses to the steep intensity transitions at the boundary of the optic disc and bright lesions. To overcome this issue, we also propose a symmetry constrained multiscale matched filtering technique. The proposed vessel segmentation method has been tested on three publicly available image sets, where its performance proved to be competitive with the state-of-the-art and comparable to the accuracy of a human observer, as well."]},
{"title": "Computer-aided diagnosis of plus disease via measurement of vessel thickness in retinal fundus images of preterm infants", "highlights": ["We present techniques for measurement of the width of the major temporal arcade.", "Gabor filters and morphological image processing methods are applied.", "The methods can detect plus disease and retinopathy of prematurity.", "Area under the receiver operating characteristic curve of 0.76 obtained."], "abstract": ["Changes in the characteristics of retinal vessels such as width and tortuosity can be signs of the presence of several diseases such retinopathy of prematurity (ROP) and diabetic retinopathy. Plus disease is an indicator of ROP which requires treatment and is signified by an increase in posterior venular width. In this work, we present image processing techniques for the detection, segmentation, tracking, and measurement of the width of the major temporal arcade (MTA), which is the thickest venular branch in the retina. Several image processing techniques have been employed, including the use of Gabor filters to detect the MTA, morphological image processing to obtain its skeleton, Canny\u05f3s method to detect and select MTA vessel-edge candidates, least-squares fitting to interpolate the MTA edges, and geometrical procedures to measure the width of the MTA. The results, obtained using 110 retinal fundus images of preterm infants, indicate a statistically highly significant difference in MTA width of normal cases as compared to cases with plus disease ", ". The results provide good accuracy in computer-aided diagnosis (CAD) of plus disease with an area under the receiver operating characteristic curve of 0.76. The proposed methods may be used in CAD of plus disease and timely treatment of ROP in a clinical or teleophthalmological setting."]},
{"title": "Application of different imaging modalities for diagnosis of Diabetic Macular Edema: A review", "highlights": ["Imaging modalities used for diagnosis of Diabetic Macular Edema (DME) are described.", "Computer-based detection of salient features of DME are presented.", "Automated grading of DME using fundus images is discussed in detail."], "abstract": ["Diabetic Macular Edema (DME) is caused by accumulation of extracellular fluid from hyperpermeable capillaries within the macula. DME is one of the leading causes of blindness among Diabetes Mellitus (DM) patients. Early detection followed by laser photocoagulation can save the visual loss. This review discusses various imaging modalities viz. biomicroscopy, Fluorescein Angiography (FA), Optical Coherence Tomography (OCT) and colour fundus photographs used for diagnosis of DME. Various automated DME grading systems using retinal fundus images, associated retinal image processing techniques for fovea, exudate detection and segmentation are presented. We have also compared various imaging modalities and automated screening methods used for DME grading. The reviewed literature indicates that FA and OCT identify DME related changes accurately. FA is an invasive method, which uses fluorescein dye, and OCT is an expensive imaging method compared to fundus photographs. Moreover, using fundus images DME can be identified and automated. DME grading algorithms can be implemented for telescreening. Hence, fundus imaging based DME grading is more suitable and affordable method compared to biomicroscopy, FA, and OCT modalities."]},
{"title": "Quantitative assessment of corneal vibrations during intraocular pressure measurement with the air-puff method in patients with keratoconus", "highlights": ["We showed how corneal vibrations could be automatically measured.", "We measured vibration parameters for large inter-individual variability in patients.", "We confirms the usefulness of the proposed method in this type of classification."], "abstract": ["One of the current methods for measuring intraocular pressure is the air-puff method. A tonometer which uses this method is the Corvis device. With the ultra-high-speed (UHS) Scheimpflug camera, it is also possible to observe corneal deformation during measurement. The use of modern image analysis and processing methods allows for analysis of higher harmonics of corneal deflection above 100", "\u00a0", "Hz.", "493 eyes of healthy subjects and 279 eyes of patients with keratoconus were used in the measurements. For each eye, 140 corneal deformation images were recorded during intraocular pressure measurement. Each image was recorded every 230", "\u00a0", "\u00b5s and had a resolution of 200\u00d7576 pixels. A new, original algorithm for image analysis and processing has been proposed. It enables to separate the eyeball reaction as well as low-frequency and high-frequency corneal deformations from the eye response to an air puff. Furthermore, a method for classification of healthy subjects and patients with keratoconus based on decision trees has been proposed.", "The obtained results confirm the possibility to distinguish between patients with keratoconus and healthy subjects. The features used in this classification are directly related to corneal vibrations. They are only available in the proposed software and provide specificity of 98%, sensitivity-85%, and accuracy-92%. This confirms the usefulness of the proposed method in this type of classification that uses corneal vibrations during intraocular pressure measurement with the Corvis tonometer.", "With the new proposed algorithm for image analysis and processing allowing for the separation of individual features from a corneal deformation image, it is possible to: automatically measure corneal vibrations in a few characteristic points of the cornea, obtain fully repeatable measurement of vibrations for the same registered sequence of images and measure vibration parameters for large inter-individual variability in patients."]},
{"title": "An ALE-based finite element model of flagellar motion driven by beating waves: A parametric study", "highlights": ["We present a two-dimensional model of flagellar motility using FEM-ALE formulation.", "Swimming velocity shows linear dependence on large amplitudes.", "Wavelength studies in channels reveal dissimilar trends for swimming velocity.", "Beating-wave propulsion in shear-thinning fluids is generally more efficient."], "abstract": ["A computational model of flagellar motility is presented using the finite element method. Two-dimensional traveling waves of finite amplitude are propagated down the flagellum and the swimmer is propelled through a viscous fluid according to Newton\u05f3s second law of motion. Incompressible Navier-Stokes equations are solved on a triangular moving mesh and arbitrary Lagrangian\u2013Eulerian formulation is employed to accommodate the deforming boundaries. The results from the present study are validated against the data available in the literature and close agreement with previous works is found. The effects of wave parameters as well as head morphology on the swimming characteristics are studied for different swimming conditions. We have found that the swimming velocities are linear functions of finite amplitudes and that the rate of work is independent of the channel height for large amplitudes. Furthermore, we have also demonstrated that for the range of wave parameters that are often encountered in human sperm motility studies, the propulsive velocity versus the wavelength exhibits dissimilar trends for different channel heights. Various head configurations were analyzed and it is also observed that wall proximity amplifies the effects induced by different head shapes. By taking non-Newtonian fluids into account, we present new efficiency analyzes through which we have found that the model microorganism swims much more efficiently in shear-thinning fluids."]},
{"title": "Prediction of recombinant protein overexpression in ", "highlights": ["Studying the new area of recombinant protein overexpression ", " prediction;", "Using the novel database of EcoliOverExpressionDB;", "Utilizing a new set of features as the model input;", "Investigating a broad range of machine learning techniques for statistical analyses."], "abstract": ["Recombinant protein overexpression, an important biotechnological process, is ruled by complex biological rules which are mostly unknown, is in need of an intelligent algorithm so as to avoid resource-intensive lab-based trial and error experiments in order to determine the expression level of the recombinant protein. The purpose of this study is to propose a predictive model to estimate the level of recombinant protein overexpression for the first time in the literature using a machine learning approach based on the sequence, expression vector, and expression host. The expression host was confined to ", " which is the most popular bacterial host to overexpress recombinant proteins. To provide a handle to the problem, the overexpression level was categorized as low, medium and high. A set of features which were likely to affect the overexpression level was generated based on the known facts (e.g. gene length) and knowledge gathered from related literature. Then, a representative sub-set of features generated in the previous objective was determined using feature selection techniques. Finally a predictive model was developed using random forest classifier which was able to adequately classify the multi-class imbalanced small dataset constructed. The result showed that the predictive model provided a promising accuracy of 80% on average, in estimating the overexpression level of a recombinant protein."]},
{"title": "Recursive identification of an arterial baroreflex model for the evaluation of cardiovascular autonomic modulation", "highlights": ["Recursive identication algorithm coupled to a model of the baroreflex.", "Time-varying estimation of autonomic activity during pharmacological manoeuver.", "Estimation of individualized sympathetic and parasympathetic modulation in a pediatric context.", "Validation of the model-based method using traditional time-frequency analysis."], "abstract": ["The evaluation of the time-varying vagal and sympathetic contributions to heart rate remains a challenging task because the observability of the baroreflex is generally limited and the time-varying properties are difficult to take into account, especially in non-stationnary conditions. The objective is to propose a model-based approach to estimate the autonomic modulation during a pharmacological challenge.", "A recursive parameter identification method is proposed and applied to a mathematical model of the baroreflex, in order to estimate the time-varying vagal and sympathetic contributions to heart rate modulation during autonomic maneuvers. The model-based method was evaluated with data from five newborn lambs, which were acquired during injection of vasodilator and vasoconstrictor drugs, on normal conditions and under beta-blockers, so as to quantify the effect of the pharmacological sympathetic blockade on the estimated parameters.", "After parameter identification, results show a close match between experimental and simulated signals for the five lambs, as the mean relative root mean squared error is equal to 0.0026 (\u00b10.003). The error, between simulated and experimental signals, is significantly reduced compared to a batch identification of parameters. The model-based estimation of vagal and sympathetic contributions were consistent with physiological knowledge and, as expected, it was possible to observe an alteration of the sympathetic response under beta-blockers. The simulated vagal modulation illustrates a response similar to traditional heart rate variability markers during the pharmacological maneuver. The model-based method, proposed in the paper, highlights the advantages of using a recursive identification method for the estimation of vagal and sympathetic modulation."]},
{"title": "Fluid structure interaction analysis reveals facial nerve palsy caused by vertebral-posterior inferior cerebellar artery aneurysm", "highlights": ["A rare case of aneurysm causing facial nerve palsy without hemifacial spasm.", "FSI analysis is useful for evaluating the motion of aneurysmal walls.", "The point of maximal mesh displacement contributes to the majority of nerve symptoms."], "abstract": ["Cranial nerve palsy caused by aneurysmal compression has not been fully evaluated. The main causes of symptoms are considered to be direct mechanical compression and aneurysm pulsations. Recent studies indicate that nerve dysfunction is mainly induced by pulsation rather than by direct compression, and successful cases of endovascular surgery have been reported. We describe a patient with an unruptured vertebral artery-posterior inferior cerebellar artery (VA-PICA) aneurysm compressing the facial nerve at the root exit zone (REZ). The patient presented with peripheral facial nerve palsy but not hemifacial spasm and was successfully treated by coil embolization. To investigate the mechanisms underlying peripheral facial nerve palsy, fluid structure interaction (FSI) analysis can approximate displacement and the magnitude of aneurysmal wall motion due to hemodynamic forces. In our case, maximum mesh displacement was observed at the aneurysmal wall attached to the facial nerve inside the pons rather than the REZ, which explains the clinical manifestation of facial nerve palsy in the absence of hemifacial spasm. This preliminary report demonstrates the utility of FSI analysis for investigating cranial nerve neuropathy."]},
{"title": "Reliable resource-constrained telecardiology via compressive detection of anomalous ECG signals", "highlights": ["Reliable resource-constrained telecardiology is proposed in a two-tier framework.", "Our technique suits rural communities with limited power and bandwidth.", "Compressive sampling and high-sensitivity detection remain at the core.", "Classifiers are designed by exploiting self-similarity and periodicity of ECG.", "High reliability is maintained even at substantial power and bandwidth savings."], "abstract": ["Telecardiology is envisaged as a supplement to inadequate local cardiac care, especially, in infrastructure deficient communities. Yet the associated infrastructure constraints are often ignored while designing a traditional telecardiology system that simply records and transmits user electrocardiogram (ECG) signals to a professional diagnostic facility. Against this backdrop, we propose a two-tier telecardiology framework, where constraints on resources, such as power and bandwidth, are met by compressively sampling ECG signals, identifying anomalous signals, and transmitting only the anomalous signals. Specifically, we design practical compressive classifiers based on inherent properties of ECG signals, such as self-similarity and periodicity, and illustrate their efficacy by plotting receiver operating characteristics (ROC). Using such classifiers, we realize a resource-constrained telecardiology system, which, for the PhysioNet databases, allows no more than 0.5% undetected patients even at an average downsampling factor of five, reducing the power requirement by 80% and bandwidth requirement by 83.4% compared to traditional telecardiology."]},
{"title": "Prediction of facial deformation after complete denture prosthesis using BP neural network", "highlights": ["BP neural network is used to predict feature positions on facial model.", "The deformation of facial model is simulated using Laplacian deformation technique.", "This method can predict facial deformation quickly and accurately.", "CT scan is not necessary."], "abstract": ["With the accelerated aging of world population, complete denture prosthesis plays an increasingly important role in mouth rehabilitation. In addition to recovering stomatognathic system function, restoring the appearance of a third of the area under the face has become a great challenge in complete denture prosthesis. This study analyzes the interactive relationship between the appearance of a third of the area under the face and complete denture, and proposes a new method to predict facial deformation after complete denture prosthesis. Firstly, to improve computational efficiency, the feature template is constructed to replace the deformed facial region. Secondly, a forecast model of elastic deformation is constructed using BP neural network and predicts elastic deformation amount because of the inhomogeneous, anisotropic and nonlinear material properties of soft tissue. Finally, a new feature template is calculated using deformation amount, and the deformation of preoperative model is simulated using Laplacian deformation technique. The average error rates of different hidden layer nodes in the neural network are analysed. Deformation and postoperative models are superimposed for match analysis. Experimental results show that this method can predict facial soft tissue deformation quickly and accurately."]},
{"title": "Dynamics of the HPA axis and inflammatory cytokines: Insights from mathematical modeling", "highlights": ["A 5-D model of the interaction between the HPA axis and immune system is introduced.", "This work models the dynamics of TNF-a, IL-6, ACTH and cortisol in LPS injection.", "The model captures the main qualitative features of cytokine and hormone dynamics.", "The role of HPA axis in transition from acute to prolonged inflammation is analyzed"], "abstract": ["In the work presented here, a novel mathematical model was developed to explore the bi-directional communication between the hypothalamic-pituitary-adrenal (HPA) axis and inflammatory cytokines in acute inflammation. The dynamic model consists of five delay differential equations 5D for two main pro-inflammatory cytokines (TNF-", " and IL-6) and two hormones of the HPA axis (ACTH and cortisol) and LPS endotoxin. The model is an attempt to increase the understanding of the role of primary hormones and cytokines in this complex relationship by demonstrating the influence of different organs and hormones in the regulation of the inflammatory response. The model captures the main qualitative features of cytokine and hormone dynamics when a toxic challenge is introduced. Moreover, in this work a new simple delayed model of the HPA axis is introduced which supports the understanding of the ultradian rhythm of HPA hormones both in normal and infection conditions. Through simulations using the model, the role of key inflammatory cytokines and cortisol in transition from acute to persistent inflammation through stability analysis is investigated. Also, by employing a Markov chain Monte Carlo (MCMC) method, parameter uncertainty and the effects of parameter variations on each other are analyzed. This model confirms the important role of the HPA axis in acute and prolonged inflammation and can be a useful tool in further investigation of the role of stress on the immune response to infectious diseases."]},
{"title": "A semi-automated method for measuring the evolution of both lumen area and blood flow in carotid from Phase Contrast MRI", "highlights": ["Narrow band region-based active contours are appropriate to delineate carotid lumen.", "Region-based active contours are better than edge-based ones for this application.", "User interactions control, if required, initialization and carotid lumen extent.", "Evaluation on a phantom and a patient database shows the relevance of the approach.", "Experiments on the patient database show limitations of fully manual analysis."], "abstract": ["Phase-Contrast (PC) velocimetry Magnetic Resonance Imaging (MRI) is a useful modality to explore cardiovascular pathologies, but requires the automatic segmentation of vessels and the measurement of both lumen area and blood flow evolutions. In this paper, we propose a semi-automated method for extracting lumen boundaries of the carotid artery and compute both lumen area and blood flow evolutions over the cardiac cycle. This method uses narrow band region-based active contours in order to correctly capture the lumen boundary without being corrupted by surrounding structures. This approach is compared to traditional edge-based active contours, considered in related works, which significantly underestimate lumen area and blood flow. Experiments are performed using both a sequence of a homemade phantom and sequences of 20 real carotids, including a comparison with manual segmentation performed by a radiologist expert. Results obtained on ", " phantom sequence show that the edge-based approach leads to an underestimate of carotid lumen area and related flows of respectively 18.68% and 4.95%. This appears significantly larger than weak errors obtained using the region-based approach (respectively 2.73% and 1.23%). Benefits appear even better on the real sequences. The edge-based approach leads to underestimates of 40.88% for areas and 13.39% for blood flows, compared to limited errors of 7.41% and 4.6% with our method. Experiments also illustrate the high variability and therefore the lack of reliability of manual segmentation."]},
{"title": "Nonlinear analysis of EEGs of patients with major depression during different emotional states", "highlights": ["This is the first work defining emotional changes in complexity in MDDs\u2019 brains.", "Higher complexity values were generated by MDD patients relative to controls", "The patients had higher complexities during noise than did the controls\u2019 brains.", "Patients\u2019 negative emotional bias was showed by their higher brain complexities."], "abstract": ["Although patients with major depressive disorder (MDD) have dysfunctions in cognitive behaviors and the regulation of emotions, the underlying brain dynamics of the pathophysiology are unclear. Therefore, nonlinear techniques can be used to understand the dynamic behavior of the EEG signals of MDD patients.", "To investigate and clarify the dynamics of MDD patients\u05f3 brains during different emotional states, EEG recordings were analyzed using nonlinear techniques. The purpose of the present study was to assess whether there are different EEG complexities that discriminate between MDD patients and healthy controls during emotional processing. Therefore, nonlinear parameters, such as Katz fractal dimension (KFD), Higuchi fractal dimension (HFD), Shannon entropy (ShEn), Lempel-Ziv complexity (LZC) and Kolmogorov complexity (KC), were computed from the EEG signals of two groups under different experimental states: noise (negative emotional content) and music (positive emotional content) periods.", "First, higher complexity values were generated by MDD patients relative to controls. Significant differences were obtained in the frontal and parietal scalp locations using KFD (", "<0.001), HFD (", "<0.05), and LZC (", "=0.05). Second, lower complexities were observed only in the controls when they were subjected to music compared to the resting baseline state in the frontal (", "<0.05) and parietal (", "=0.005) regions. In contrast, the LZC and KFD values of patients increased in the music period compared to the resting state in the frontal region (", "<0.05). Third, the patients\u05f3 brains had higher complexities when they were exposed to noise stimulus than did the controls\u05f3 brains. Moreover, MDD patients\u05f3 negative emotional bias was demonstrated by their higher brain complexities during the noise period than the music stimulus. Additionally, we found that the KFD, HFD and LZC values were more sensitive in discriminating between patients and controls than the ShEn and KC measures, according to the results of ANOVA and ROC calculations.", "It can be concluded that the nonlinear analysis may be a useful and discriminative tool in investigating the neuro-dynamic properties of the brain in patients with MDD during emotional stimulation."]},
{"title": "Genome wide classification and characterisation of CpG sites in cancer and normal cells", "highlights": ["Identification of common methylation patterns across different cancer types.", "Evidence for certain CpG being preferentially aberrantly methylated.", "Characterisation of the CpGs that are and are not aberrantly methylated.", "Data showing most CpG sites remain unmethylated in normal and cancerous cells.", "Identification of the motifs and features that classify the CpG sites are identified.", "The surrounding sequence has an influence on aberrant methylation."], "abstract": ["This study identifies common methylation patterns across different cancer types in an effort to identify common molecular events in diverse types of cancer cells and provides evidence for the sequence surrounding a CpG to influence its susceptibility to aberrant methylation. CpG sites throughout the genome were divided into four classes: sites that either become hypo or hyper-methylated in a variety cancers using all the freely available microarray data (HypoCancer and HyperCancer classes) and those found in a constant hypo (Never methylated class) or hyper-methylated (Always methylated class) state in both normal and cancer cells. Our data shows that most CpG sites included in the HumanMethylation450K microarray remain unmethylated in normal and cancerous cells; however, certain sites in all the cancers investigated become specifically modified. More detailed analysis of the sites revealed that majority of those in the never methylated class were in CpG islands whereas those in the HyperCancer class were mostly associated with miRNA coding regions. The sites in the Hypermethylated class are associated with genes involved in initiating or maintaining the cancerous state, being enriched for processes involved in apoptosis, and with transcription factors predicted to bind to these genes linked to apoptosis and tumourgenesis (notably including E2F). Further we show that more LINE elements are associated with the HypoCancer class and more Alu repeats are associated with the HyperCancer class. Motifs that classify the classes were identified to distinguish them based on the surrounding DNA sequence alone, and for the identification of DNA sequences that could render sites more prone to aberrant methylation in cancer cells. This provides evidence that the sequence surrounding a CpG site has an influence on whether a site is hypo or hyper methylated."]},
{"title": "Manifold ranking based scoring system with its application to cardiac arrest prediction: A retrospective study in emergency department patients", "highlights": ["We develop a novel manifold ranking based risk scoring system.", "We apply the novel scoring system for the prediction of cardiac arrest in emergency department patients.", "The risk scoring system is designed to handle both balanced and imbalanced datasets."], "abstract": ["The recently developed geometric distance scoring system has shown the effectiveness of scoring systems in predicting cardiac arrest within 72", "\u00a0", "h and the potential to predict other clinical outcomes. However, the geometric distance scoring system predicts scores based on only local structure embedded by the data, thus leaving much room for improvement in terms of prediction accuracy.", "We developed a novel scoring system for predicting cardiac arrest within 72", "\u00a0", "h. The scoring system was developed based on a semi-supervised learning algorithm, manifold ranking, which explores both the local and global consistency of the data. System evaluation was conducted on emergency department patients\u05f3 data, including both vital signs and heart rate variability (HRV) parameters. Comparison of the proposed scoring system with previous work was given in terms of sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV).", "Out of 1025 patients, 52 (5.1%) met the primary outcome. Experimental results show that the proposed scoring system was able to achieve higher area under the curve (AUC) on both the balanced dataset (0.907 vs. 0.824) and the imbalanced dataset (0.774 vs. 0.734) compared to the geometric distance scoring system.", "The proposed scoring system improved the prediction accuracy by utilizing the global consistency of the training data. We foresee the potential of extending this scoring system, as well as manifold ranking algorithm, to other medical decision making problems. Furthermore, we will investigate the parameter selection process and other techniques to improve performance on the imbalanced dataset."]},
{"title": "Investigation of the relationship between anxiety and heart rate variability in fibromyalgia: A new quantitative approach to evaluate anxiety level in fibromyalgia syndrome", "highlights": ["HRV subband analysis is the significant part of the study.", "The study is realized for helping the diagnosis of FMS with HRV subband analysis.", "There is a new vision on the evaluation of anxiety that accompanies fibromyalgia.", "Novelty is supporting the subjective verbal tests by objective physiological tests.", "This method may be a beneficial tool for estimation of anxiety."], "abstract": ["Fibromyalgia syndrome (FMS) is identified by widespread musculoskeletal pain, sleep disturbance, nonrestorative sleep, fatigue, morning stiffness and anxiety. Anxiety is very common in Fibromyalgia and generally leads to a misdiagnosis. Self-rated Beck Anxiety Inventory (BAI) and doctor-rated Hamilton Anxiety Inventory (HAM-A) are frequently used by specialists to determine anxiety that accompanies fibromyalgia. However, these semi-quantitative anxiety tests are still subjective as the tests are scored using doctor-rated or self-rated scales.", "In this study, we investigated the relationship between heart rate variability (HRV) frequency subbands and anxiety tests. The study was conducted with 56 FMS patients and 34 healthy controls. BAI and HAM-A test scores were determined for each participant. ECG signals were then recruited and 71 HRV subbands were obtained from these ECG signals using Wavelet Packet Transform (WPT). The subbands and anxiety tests scores were analyzed and compared using multilayer perceptron neural networks (MLPNN).", "The results show that a HRV high frequency (HF) subband in the range of 0.15235", "\u00a0", "Hz to 0.40235", "\u00a0", "Hz, is correlated with BAI scores and another HRV HF subband, frequency range of 0.15235", "\u00a0", "Hz to 0.28907", "\u00a0", "Hz is correlated with HAM-A scores. The overall accuracy is 91.11% for HAM-A and 90% for BAI with MLPNN analysis.", "Doctor-rated or self-rated anxiety tests should be supported with quantitative and more objective methods. Our results show that the HRV parameters will be able to support the anxiety tests in the clinical evaluation of fibromyalgia. In other words, HRV parameters can potentially be used as an auxiliary diagnostic method in conjunction with anxiety tests."]},
{"title": "Image segmentation and registration algorithm to collect thoracic skeleton semilandmarks for characterization of age and sex-based thoracic morphology variation", "highlights": ["Thoracic skeleton semilandmarks from 343 males and females (ages 0\u2013100) were collected.", "A segmentation and registration algorithm was developed for collecting semilandmarks.", "Between 2700 and 11,000 semilandmarks were collected from each rib and sternum.", "Over 55 million rib and sternum semilandmarks were collected across all subjects.", "The semilandmarks will be used to characterize thoracic morphology by age and sex."], "abstract": ["Thoracic anthropometry variations with age and sex have been reported and likely relate to thoracic injury risk and outcome. The objective of this study was to collect a large volume of homologous semilandmark data from the thoracic skeleton for the purpose of quantifying thoracic morphology variations for males and females of ages 0\u2013100 years. A semi-automated image segmentation and registration algorithm was applied to collect homologous thoracic skeleton semilandmarks from 343 normal computed tomography (CT) scans. Rigid, affine, and symmetric diffeomorphic transformations were used to register semilandmarks from an atlas to homologous locations in the subject-specific coordinate system. Homologous semilandmarks were successfully collected from 92% (7077) of the ribs and 100% (187) of the sternums included in the study. Between 2700 and 11,000 semilandmarks were collected from each rib and sternum and over 55 million total semilandmarks were collected from all subjects. The extensive landmark data collected more fully characterizes thoracic skeleton morphology across ages and sexes. Characterization of thoracic morphology with age and sex may help explain variations in thoracic injury risk and has important implications for vulnerable populations such as pediatrics and the elderly."]},
{"title": "Regularized logistic regression with adjusted adaptive elastic net for gene selection in high dimensional cancer classification", "highlights": ["The AAElastic showed superior results in terms all evaluation criteria.", "The AAElastic selected more correlated genes than the other methods.", "The AAElastic performed remarkably well in classification stability test.", "In terms of gene selection consistency, AAElastic significantly performed well."], "abstract": ["Cancer classification and gene selection in high-dimensional data have been popular research topics in genetics and molecular biology. Recently, adaptive regularized logistic regression using the elastic net regularization, which is called the adaptive elastic net, has been successfully applied in high-dimensional cancer classification to tackle both estimating the gene coefficients and performing gene selection simultaneously. The adaptive elastic net originally used elastic net estimates as the initial weight, however, using this weight may not be preferable for certain reasons: First, the elastic net estimator is biased in selecting genes. Second, it does not perform well when the pairwise correlations between variables are not high. Adjusted adaptive regularized logistic regression (AAElastic) is proposed to address these issues and encourage grouping effects simultaneously. The real data results indicate that AAElastic is significantly consistent in selecting genes compared to the other three competitor regularization methods. Additionally, the classification performance of AAElastic is comparable to the adaptive elastic net and better than other regularization methods. Thus, we can conclude that AAElastic is a reliable adaptive regularized logistic regression method in the field of high-dimensional cancer classification."]},
{"title": "Development of an automatic identification algorithm for antibiogram analysis", "highlights": ["Development of an Automatic Identification Algorithms (AIA) is proposed.", "AIA allows automatic scanning of inhibition zones obtained by antibiograms.", "88.8% of the tests showed no difference between AIA and human readings.", "AIA resolved the overlapping of inhibition zones and other reading problems.", "Formation of a second inhibition halo were overcomed by AIA."], "abstract": ["Routinely, diagnostic and microbiology laboratories perform antibiogram analysis which can present some difficulties leading to misreadings and intra and inter-reader deviations. An Automatic Identification Algorithm (AIA) has been proposed as a solution to overcome some issues associated with the disc diffusion method, which is the main goal of this work. AIA allows automatic scanning of inhibition zones obtained by antibiograms. More than 60 environmental isolates were tested using susceptibility tests which were performed for 12 different antibiotics for a total of 756 readings. Plate images were acquired and classified as standard or oddity. The inhibition zones were measured using the AIA and results were compared with reference method (human reading), using weighted kappa index and statistical analysis to evaluate, respectively, inter-reader agreement and correlation between AIA-based and human-based reading. Agreements were observed in 88% cases and 89% of the tests showed no difference or a ", " difference between AIA and human analysis, exhibiting a correlation index of 0.85 for all images, 0.90 for standards and 0.80 for oddities with no significant difference between automatic and manual method. AIA resolved some reading problems such as overlapping inhibition zones, imperfect microorganism seeding, non-homogeneity of the circumference, partial action of the antimicrobial, and formation of a second halo of inhibition. Furthermore, AIA proved to overcome some of the limitations observed in other automatic methods. Therefore, AIA may be a practical tool for automated reading of antibiograms in diagnostic and microbiology laboratories."]},
{"title": "Development of a novel computerised version of the Month Backwards Test: A comparison of performance in hospitalised elderly patients and final year medical students", "highlights": ["We examined performance on verbal and computerised versions of the months backward test in medical students and elderly medical inpatients.", "Performance on the two versions of the months backward test was similar but with slower completion times and more errors on the computerised version.", "The computerised version of the months backward test had higher correlation with the MoCA, especially the visuospatial domain.", "The computerised version of months backward test can provide rapid and reliable testing of cognitive function in clinical populations."], "abstract": ["The Months Backwards Test (MBT) is a commonly used bedside test of cognitive function, but there is uncertainty as to optimal testing procedures. We examined performance among hospitalised elderly patients and cognitively intact young persons with verbal and computerised versions of the test.", "Fifty acute elderly medical inpatients and fifty final year medical students completed verbal (MBTv) and computerised (MBTc) versions of the MBT and the Montreal Cognitive Assessment (MoCA). Completion time and errors were compared.", "Thirty four participants scored <26 on the MoCA indicating significant cognitive impairment. The mean MoCA scores in the elderly medical group (23.6\u00b13.4; range 13\u201328) were significantly lower than for the medical students (29.2\u00b10.6; range 28\u201330: ", "<0.01). For the verbal months backwards test (MBTv), there were significantly more errors and longer completion times in the elderly medical patients (25.1\u00b120.9 vs. 10.5\u00b14.5; ", "<0.05). Completion times were 2\u20133 times longer for the MBTc compared to the MBTv (patients: 63.5\u00b143.9 vs. students 20.3\u00b14.4; ", "<0.05). There was high correlation between the two versions of the MBT (", "=0.84) and also between the MBTc and the MoCA (", "=0.85). The MBTc had higher correlation with visuospatial function (MBTc ", "=0.70, MBTv ", "=0.57). An MBTc cut-off time of 30", "\u00a0", "s for distinguishing performance (pass/fail) had excellent sensitivity (100%) with modest specificity (44%) for cognitive impairment in elderly medical patients.", "The computerised MBT allows accurate and efficient testing of attention and general cognition in clinical populations."]},
{"title": "Unsupervised learning assisted robust prediction of bioluminescent proteins", "highlights": ["Combination of unsupervised learning with SMOTE for imbalance learning problems.", "Effective handling of between class and within class imbalance.", "Diversification of the training set with optimal class distribution.", "Does not require evolutionary information for prediction."], "abstract": ["Bioluminescence plays an important role in nature, for example, it is used for intracellular chemical signalling in bacteria. It is also used as a useful reagent for various analytical research methods ranging from cellular imaging to gene expression analysis. However, identification and annotation of bioluminescent proteins is a difficult task as they share poor sequence similarities among them. In this paper, we present a novel approach for within-class and between-class balancing as well as diversifying of a training dataset by effectively combining unsupervised ", "-Means algorithm with Synthetic Minority Oversampling Technique (SMOTE) in order to achieve the true performance of the prediction model. Further, we experimented by varying different levels of balancing ratio of positive data to negative data in the training dataset in order to probe for an optimal class distribution which produces the best prediction accuracy. The appropriately balanced and diversified training set resulted in near complete learning with greater generalization on the blind test datasets. The obtained results strongly justify the fact that optimal class distribution with a high degree of diversity is an essential factor to achieve near perfect learning. Using random forest as the weak learners in boosting and training it on the optimally balanced and diversified training dataset, we achieved an overall accuracy of 95.3% on a tenfold cross validation test, and an accuracy of 91.7%, sensitivity of 89. 3% and specificity of 91.8% on a holdout test set. It is quite possible that the general framework discussed in the current work can be successfully applied to other biological datasets to deal with imbalance and incomplete learning problems effectively."]},
{"title": "Segmentation of liver and spleen based on computational anatomy models", "highlights": ["Propose a novel framework for multi-organs segmentation.", "Incorporate an atlas concept within an organ bounding box construction.", "Iterative probabilistic atlas overcome a bias toward the specific patient study.", "Adaptive select reference bone for accurate alignment of the given data.", "Achieve the promising performance (", ") compared with conventional method."], "abstract": ["Accurate segmentation of abdominal organs is a key step in developing a computer-aided diagnosis (CAD) system. Probabilistic atlas based on human anatomical structure, used as a priori information in a Bayes framework, has been widely used for organ segmentation. How to register the probabilistic atlas to the patient volume is the main challenge. Additionally, there is the disadvantage that the conventional probabilistic atlas may cause a bias toward the specific patient study because of the single reference. Taking these into consideration, a template matching framework based on an iterative probabilistic atlas for liver and spleen segmentation is presented in this paper. First, a bounding box based on human anatomical localization, which refers to the statistical geometric location of the organ, is detected for the candidate organ. Then, the probabilistic atlas is used as a template to find the organ in this bounding box by using template matching technology. We applied our method to 60 datasets including normal and pathological cases. For the liver, the Dice/Tanimoto volume overlaps were 0.930/0.870, the root-mean-squared error (RMSE) was 2.906", "\u00a0", "mm. For the spleen, quantification led to 0.922 Dice/0.857 Tanimoto overlaps, 1.992", "\u00a0", "mm RMSE. The algorithm is robust in segmenting normal and abnormal spleens and livers, such as the presence of tumors and large morphological changes. Comparing our method with conventional and recently developed atlas-based methods, our results show an improvement in the segmentation accuracy for multi-organs (", ")."]},
{"title": "A novel fuzzy logic-based image steganography method to ensure medical data security", "highlights": ["Secure multiple medical signals by combining them into one file format.", "Propose a new steganography method in literature.", "Secure patients\u05f3 personal data by encryption and compression algorithms.", "Increase data repository and transmission capacity of multiple medical signals."], "abstract": ["This study aims to secure medical data by combining them into one file format using steganographic methods. The electroencephalogram (EEG) is selected as hidden data, and magnetic resonance (MR) images are also used as the cover image. In addition to the EEG, the message is composed of the doctor\u05f3s comments and patient information in the file header of images. Two new image steganography methods that are based on fuzzy-logic and similarity are proposed to select the non-sequential least significant bits (LSB) of image pixels. The similarity values of the gray levels in the pixels are used to hide the message. The message is secured to prevent attacks by using lossless compression and symmetric encryption algorithms. The performance of stego image quality is measured by mean square of error (MSE), peak signal-to-noise ratio (PSNR), structural similarity measure (SSIM), universal quality index (UQI), and correlation coefficient (", "). According to the obtained result, the proposed method ensures the confidentiality of the patient information, and increases data repository and transmission capacity of both MR images and EEG signals."]},
{"title": "Computer modelling integrated with micro-CT and material testing provides additional insight to evaluate bone treatments: Application to a beta-glycan derived whey protein mice model", "highlights": ["Computer prediction of a whey protein bone treatment on bone strength.", "Comparison of 3-point bending, micro/nano-indentation and CT material estimates.", "Evaluation of bone strength from geometry independent of material properties."], "abstract": ["The primary aim of this study was to evaluate the influence of a whey protein diet on computationally predicted mechanical strength of murine bones in both trabecular and cortical regions of the femur. There was no significant influence on mechanical strength in cortical bone observed with increasing whey protein treatment, consistent with cortical tissue mineral density (TMD) and bone volume changes observed. Trabecular bone showed a significant decline in strength with increasing whey protein treatment when nanoindentation derived Young\u05f3s moduli were used in the model. When microindentation, micro-CT phantom density or normalised Young\u05f3s moduli were included in the model a non-significant decline in strength was exhibited. These results for trabecular bone were consistent with both trabecular bone mineral density (BMD) and micro-CT indices obtained independently. The secondary aim of this study was to characterise the influence of different sources of Young\u05f3s moduli on computational prediction. This study aimed to quantify the predicted mechanical strength in 3D from these sources and evaluate if trends and conclusions remained consistent. For cortical bone, predicted mechanical strength behaviour was consistent across all sources of Young\u05f3s moduli. There was no difference in treatment trend observed when Young\u05f3s moduli were normalised. In contrast, trabecular strength due to whey protein treatment significantly reduced when material properties from nanoindentation were introduced. Other material property sources were not significant but emphasised the strength trend over normalised material properties. This shows strength at the trabecular level was attributed to both changes in bone architecture and material properties."]},
{"title": "Analysis of the impact of digital watermarking on computer-aided diagnosis in medical imaging", "highlights": ["A deep and detailed analysis of watermarking implications on CADx was carried out.", "A procedure to evaluate the impact of watermarking on CADx was proposed.", "The diagnosis in breast ultrasound imaging is guaranteed by the HCDH algorithm."], "abstract": ["Medical images (MI) are relevant sources of information for detecting and diagnosing a large number of illnesses and abnormalities. Due to their importance, this study is focused on breast ultrasound (BUS), which is the main adjunct for mammography to detect common breast lesions among women worldwide. On the other hand, aiming to enhance data security, image fidelity, authenticity, and content verification in e-health environments, MI watermarking has been widely used, whose main goal is to embed patient meta-data into MI so that the resulting image keeps its original quality. In this sense, this paper deals with the comparison of two watermarking approaches, namely spread spectrum based on the discrete cosine transform (SS-DCT) and the high-capacity data-hiding (HCDH) algorithm, so that the watermarked BUS images are guaranteed to be adequate for a computer-aided diagnosis (CADx) system, whose two principal outcomes are lesion segmentation and classification. Experimental results show that HCDH algorithm is highly recommended for watermarking medical images, maintaining the image quality and without introducing distortion into the output of CADx."]},
{"title": "Visualization of boundaries in CT volumetric data sets using dynamic ", "highlights": ["We build a generalized boundary model contaminated by noise.", "We prove boundary middle value has a good statistical property in our boundary model.", "A dynamic ", " histogram is established with a novel strategy of boundary extraction to avoid misclassification.", "A complete application is designed to implement our boundary visualization method."], "abstract": ["Direct volume rendering is widely used for three-dimensional medical data visualization such as computed tomography and magnetic resonance imaging. Distinct visualization of boundaries is able to provide valuable and insightful information in many medical applications. However, it is conventionally challenging to detect boundaries reliably due to limitations of the transfer function design. Meanwhile, the interactive strategy is complicated for new users or even experts. In this paper, we build a generalized boundary model contaminated by noise and prove boundary middle value (", ") has a good statistical property. Based on the model we propose a user-friendly strategy for the boundary extraction and transfer function design, using ", ", boundary height ", ", and gradient magnitude ", ". In fact, it is a dynamic iterative process. First, potential boundaries are sorted orderly from high to low according to the value of their height. Then, users iteratively extract the boundary with the highest value of ", " in a newly defined domain, where different boundaries are transformed to disjoint vertical bars using ", " histogram. In this case, the chance of misclassification among different boundaries decreases."]},
{"title": "Robust tooth surface reconstruction by iterative deformation", "highlights": ["The salient feature point extraction method can preserve morphological features.", "Different parts are set different deformation weights which help to get nice results.", "Iterative constrained deformation can avoid self-intersection and distortion.", "Robust and satisfactory adaptability in clinical application."], "abstract": ["Digital design technologies have been applied extensively in dental medicine, especially in the field of dental restoration. The all-ceramic crown is an important restoration type of dental CAD systems. This paper presents a robust tooth surface reconstruction algorithm for all-ceramic crown design. The algorithm involves three necessary steps: standard tooth initial positioning and division; salient feature point extraction using Morse theory; and standard tooth deformation using iterative Laplacian Surface Editing and mesh stitching. This algorithm can retain the morphological features of the tooth surface well. It is robust and suitable for almost all types of teeth, including incisor, canine, premolar, and molar. Moreover, it allows dental technicians to use their own preferred library teeth for reconstruction. The algorithm has been successfully integrated in our Dental CAD system, more than 1000 clinical cases have been tested to demonstrate the robustness and effectiveness of the proposed algorithm."]},
{"title": "Ischemia detection using Isoelectric Energy Function", "highlights": ["Proposed a new method for detection of ischemia using mathematical isoelectric energy function.", "The proposed method does not involve any training and complicated calculation.", "Detection of ischemia class for detected ischemic episode.", "Achieved 98.12% average sensitivity and 98.16% average specificity.", "The results are significantly better than those of existing methods for recordings of European ST-T database.", "The advantage of the proposed method includes the automatic discarding of noisy beats."], "abstract": ["A novel method has been proposed for the detection of ischemia using an isoelectric energy function (IEEF) resulting from ST segment deviations in ECG signals. The method consists of five stages: pre-processing, delineation, measurement of isoelectric energy, a beat characterization algorithm and detection of ischemia. The isoelectric energy threshold is used to differentiate ischemic beats from normal beats for ischemic episode detection. Then, ischemic episodes are classified as transmural or subendocardial. The method is validated for recordings of the annotated European ST-T database (EDB). The results show 98.12% average sensitivity (", ") and 98.16% average specificity (", "). These results are significantly better than those of existing methods cited in the literature. The advantage of the proposed method includes simplicity, ruggedness and automatic discarding of noisy beats."]},
{"title": "A novel approach to modeling acute normovolemic hemodilution", "highlights": ["We developed a computer-based model of acute normovolemic hemodilution (ANH).", "A multi-compartment fluid model accounts for hemorrhage and fluid resuscitation.", "The computer model simulates the effect of fluids that are most commonly used for resuscitation to compensate for blood loss.", "Various clinical scenarios can be tested by inputting relevant ANH parameters.", "Input of critical ANH parameters and simulation of important variables in real time."], "abstract": ["Acute normovolemic hemodilution (ANH) was introduced as a blood conservation technique to reduce patient exposure to allogenic blood transfusion during surgery. Despite years of research and experience, the best practice procedure, efficacy and safety of ANH remain in question. In this work, a numerical model is developed for the ANH procedure based upon a multi-compartmental, fluid model of the body. The model also analyzes the most commonly used acellular fluids for ANH or for fluid therapy following hemorrhage. The model allows user input of critical ANH parameters, providing the ability to simulate the patient\u05f3s response in real time to many clinical scenarios, using various types of resuscitation fluids. First, the patient\u05f3s response to a representative, clinical ANH protocol and surgery was simulated. Then, the effect of several variables was investigated including: type/amount of resuscitation fluid, number of blood units collected during ANH, and amount of surgical blood loss. Our simulations highlighted the importance of osmotic molecules within the blood in preventing excessive fluid retention and initiating fluid clearance after surgery. The developed model can be utilized as a tool to simulate and optimize a variety of proposed protocol related to the ANH procedure and surgery. It can also be utilized as an educational or training tool to become familiar with the ANH procedure."]},
{"title": "In vitro evaluation of basal shapes and offset values of artificial teeth for CAD/CAM complete dentures", "highlights": ["We evaluated optimal shape of artificial teeth for CAD/CAM complete dentures.", "We examined the optimal offset values required for teeth positioning.", "The structures in the basal areas showed accurate positioning for molars/premolars.", "The structures appeared to restrain motion during bonding.", "The optimal offset value was .20", "\u00a0", "mm for molar and premolar teeth."], "abstract": ["Statement of Problem. Artificial teeth are bonded onto the recesses of a milled denture base in a complete denture prepared using computer-aided design/computer-aided manufacturing (CAD/CAM). Little is known, however, about the effects of basal shapes and offset values on the accuracy of positions of the bonded artificial teeth."]},
{"title": "ARWAR: A network approach for predicting Adverse Drug Reactions", "highlights": ["We apply an existing approach to build a network of highly related drugs.", "We consider the informative knowledge hidden in the relationships among drugs.", "We augment the original Human Drug Network by adding new nodes and edges.", "We apply random walk with restart to relate each drug to its side-effects.", "Our method outperforms the existing approach significantly."], "abstract": ["Predicting novel drug side-effects, or Adverse Drug Reactions (ADRs), plays an important role in the drug discovery process. Existing methods consider mainly the chemical and biological characteristics of each drug individually, thereby neglecting information hidden in the relationships among drugs. Complementary to the existing individual methods, in this paper, we propose a novel network approach for ADR prediction that is called Augmented Random-WAlk with Restarts (ARWAR). ARWAR, first, applies an existing method to build a network of highly related drugs. Then, it augments the original drug network by adding new nodes and new edges to the network and finally, it applies Random Walks with Restarts to predict novel ADRs. Empirical results show that the ARWAR method presented here outperforms the existing network approach by 20% with respect to average Fmeasure. Furthermore, ARWAR is capable of generating novel hypotheses about drugs with respect to novel and biologically meaningful ADR."]},
{"title": "Multilayer descriptors for medical image classification", "highlights": ["Method presented for building an ", "-layer image using preprocessing methods.", "Performance of 2D descriptors improved using ", "-layer images.", "Multilayers and texture descriptors can be combined to enhance performance."], "abstract": ["In this paper, we propose a new method for improving the performance of 2D descriptors by building an ", "-layer image using different preprocessing approaches from which multilayer descriptors are extracted and used as feature vectors for training a Support Vector Machine. The different preprocessing approaches are used to build different ", "-layer images (", "=3, ", "=5, etc.). We test both color and gray-level images, two well-known texture descriptors (Local Phase Quantization and Local Binary Pattern), and three of their variants suited for ", " images (Volume Local Phase Quantization, Local Phase Quantization Three-Orthogonal-Planes, and Volume Local Binary Patterns). Our results show that multilayers and texture descriptors can be combined to outperform the standard single-layer approaches. Experiments on 10 datasets demonstrate the generalizability of the proposed descriptors. Most of these datasets are medical, but in each case the images are very different. Two datasets are completely unrelated to medicine and are included to demonstrate the discriminative power of the proposed descriptors across very different image recognition tasks.", "A MATLAB version of the complete system developed in this paper will be made available at ", "."]},
{"title": "Feature description with SIFT, SURF, BRIEF, BRISK, or FREAK? A general question answered for bone age assessment", "highlights": ["General purpose algorithms can give good results for a specific clinical task.", "Two methods for keypoint selection were applied: sparse and dense feature points.", "SIFT, SURF, BRIEF, BRISK and FREAK were evaluated/compared for bone age assessment.", "BAA is performed by extracting features from radiographs with SVM classification."], "abstract": ["Solving problems in medical image processing is either generic (being applicable to many problems) or specific (optimized for a certain task). For example, bone age assessment (BAA) on hand radiographs is a frequent but cumbersome task for radiologists. For this problem, many specific solutions have been proposed. However, general-purpose feature descriptors are used in many computer vision applications. Hence, the aim of this study is (i) to compare the five leading keypoint descriptors on BAA, and, in doing so, (ii) presenting a generic approach for a specific task. Two methods for keypoint selection were applied: sparse and dense feature points. For each type, SIFT, SURF, BRIEF, BRISK, and FREAK feature descriptors were extracted within the epiphyseal regions of interest (eROI). Classification was performed using a support vector machine. Reference data (1101 radiographs) of the University of Southern California was used for 5-fold cross-validation. The data was grouped into 30 classes representing the bone age range of 0\u201318 years. With a mean error of 0.605 years, dense SIFT gave best results and outperforms all published methods. The accuracy was 98.36% within the range of 2 years. Dense SIFT represents a generic method for a specific question."]},
{"title": "Multimodal hybrid reasoning methodology for personalized wellbeing services", "highlights": ["Studying personalized wellbeing services and enabling development of technology supported strategies.", "Integrating reasoning methodologies to support development of multimodal hybrid reasoning models.", "Utilizing explicit and implicit knowledge from guidelines and expert\u05f3s past experiences to enhance personalization.", "Integrating rule-based, case-based and preference-based reasoning linearly for precise and personalized recommendations."], "abstract": ["A wellness system provides wellbeing recommendations to support experts in promoting a healthier lifestyle and inducing individuals to adopt healthy habits. Adopting physical activity effectively promotes a healthier lifestyle. A physical activity recommendation system assists users to adopt daily routines to form a best practice of life by involving themselves in healthy physical activities. Traditional physical activity recommendation systems focus on general recommendations applicable to a community of users rather than specific individuals. These recommendations are general in nature and are fit for the community at a certain level, but they are not relevant to every individual based on specific requirements and personal interests. To cover this aspect, we propose a multimodal hybrid reasoning methodology (", ") that generates personalized physical activity recommendations according to the user\u05f3s specific needs and personal interests. The methodology integrates the rule-based reasoning (", "), case-based reasoning (", "), and preference-based reasoning (", ") approaches in a linear combination that enables personalization of recommendations. ", " uses explicit knowledge rules from physical activity guidelines, ", " uses implicit knowledge from experts\u05f3 past experiences, and ", " uses users\u05f3 personal interests and preferences. To validate the methodology, a weight management scenario is considered and experimented with. The ", " part of the methodology generates goal, weight status, and plan recommendations, the ", " part suggests the top three relevant physical activities for executing the recommended plan, and the ", " part filters out irrelevant recommendations from the suggested ones using the user\u05f3s personal preferences and interests. To evaluate the methodology, a ", " system is developed, which is improved first using ranged rules and ultimately using a ", ". A comparison of the results of these systems shows that ", " outperforms the ", " and ", " systems. ", " yields a 0.94% recall, a 0.97% precision, a 0.95% ", "-score, and low ", " and ", " errors."]},
{"title": "SME2EM: Smart mobile end-to-end monitoring architecture for life-long diseases", "highlights": ["We developed a novel mobile-based architecture for monitoring life-long diseases.", "It provides smart features to tackle several challenges (e.g., data explosion).", "Our mobile monitoring architecture is formally modeled and verified at design time.", "Its components are implemented as Web services and supported by Cloud services.", "Its applicability is experimentally evaluated by monitoring epileptic diseases."], "abstract": ["Monitoring life-long diseases requires continuous measurements and recording of physical vital signs. Most of these diseases are manifested through unexpected and non-uniform occurrences and behaviors. It is impractical to keep patients in hospitals, health-care institutions, or even at home for long periods of time. Monitoring solutions based on smartphones combined with mobile sensors and wireless communication technologies are a potential candidate to support complete mobility-freedom, not only for patients, but also for physicians. However, existing monitoring architectures based on smartphones and modern communication technologies are not suitable to address some challenging issues, such as intensive and big data, resource constraints, data integration, and context awareness in an integrated framework. This manuscript provides a novel mobile-based end-to-end architecture for live monitoring and visualization of life-long diseases. The proposed architecture provides smartness features to cope with continuous monitoring, data explosion, dynamic adaptation, unlimited mobility, and constrained devices resources. The integration of the architecture\u05f3s components provides information about diseases\u05f3 recurrences as soon as they occur to expedite taking necessary actions, and thus prevent severe consequences. Our architecture system is formally model-checked to automatically verify its correctness against designers\u05f3 desirable properties at design time. Its components are fully implemented as Web services with respect to the SOA architecture to be easy to deploy and integrate, and supported by Cloud infrastructure and services to allow high scalability, availability of processes and data being stored and exchanged. The architecture\u05f3s applicability is evaluated through concrete experimental scenarios on monitoring and visualizing states of epileptic diseases. The obtained theoretical and experimental results are very promising and efficiently satisfy the proposed architecture\u05f3s objectives, including resource awareness, smart data integration and visualization, cost reduction, and performance guarantee."]},
{"title": "Semi-automated quantification of retinal IS/OS damage in en-face OCT image", "highlights": ["Semi-automated method to quantify retinal IS/OS damage using level-sets.", "Mutual information as energy function to leverage statistics of the lesion.", "Thorough statistical analysis of algorithmic accuracy vis-\u00e0-vis manual method.", "Algorithmic damage quantification reported vis-\u00e0-vis observer repeatability.", "Quotients are defined to fairly compare among methods tested on disparate datasets."], "abstract": ["A variety of vision ailments are indicated by structural changes in the retinal substructures of the posterior segment of the eye. In particular, integrity of the inner-segment/outer-segment (IS/OS) junction directly relates to the visual acuity. In the en-face optical coherence tomography (OCT) image, IS/OS damage manifests as a dark spot in the foveal region, and its quantification, usually performed by experts, assumes diagnostic significance. In this context, in view of the general scarcity of experts, it becomes imperative to develop algorithmic methods to reduce expert time and effort. Accordingly, we propose a semi-automated method based on level sets. As the energy function, we adopt mutual information which exploits the difference in statistical properties of the lesion and its surroundings. On a dataset of 27 en-face OCT images, segmentation obtained by the proposed algorithm exhibits close visual agreement with that obtained manually. Importantly, our results also match manual results in various statistical criteria. In particular, we achieve a mean Dice coefficient of 85.69%, desirably close to the corresponding observer repeatability index of 89.45%. Finally, we quantify algorithmic accuracy in terms of two quotient measures, defined relative to observer repeatability, which could be used as bases for comparison with future algorithms, even if the latter are tested on disparate datasets."]},
{"title": "Modeling of transcatheter aortic valve replacement: Patient specific vs general approaches based on finite element analysis", "highlights": ["The modeling of calcium deposits and leaflets will improve the FEA of the TAVR.", "Neither patient specific nor generalized approach has clear advantage over the other.", "Generalized approach is more cost effective way of FEA-based TAVR modeling."], "abstract": ["There are two main methods used for transcatheter aortic valve replacement (TAVR) FEA modeling for medical devices development: patient specific and general approaches. Advantages and disadvantages of both approaches have never been compared in a single study.", "Here we propose a bioinformatic algorithm to evaluate the accuracy of patient specific and generalized FEA approaches with regards to proximity configuration of the implanted stent reconstructed by computed tomography. In addition, in this study we also assessed the impact of the level of detail on FEA accuracy in the patient specific approach.", "Our results demonstrate that in certain cases, the generalized approach can ensure the same accuracy as the patient specific approach. Therefore, considering high cost effectiveness of the generalized approach, we identify it as more feasible in the context of TAVR. Furthermore, we suggest that high level of detail can improve the reproducibility of modeling results in the patient specific approach.", "Our findings may help medical engineers to better understand the peculiarities of both approaches and therefore make the right decision when choosing a particular approach for computer modeling. Future studies are required to validate our observations."]},
{"title": "Dynamic time warping in phoneme modeling for fast pronunciation error detection", "highlights": ["A low-complexity phoneme modeling method based on Dynamic Time Warping is proposed.", "Four variants of fast mispronunciation detection method based on DTW are presented.", "The proposed method works on a small speech corpus with no additional linguistic data.", "The method proves to be faster and more efficient than methods of higher complexity."], "abstract": ["The presented paper describes a novel approach to the detection of pronunciation errors. It makes use of the modeling of well-pronounced and mispronounced phonemes by means of the Dynamic Time Warping (DTW) algorithm. Four approaches that make use of the DTW phoneme modeling were developed to detect pronunciation errors: Variations of the Word Structure (VoWS), Normalized Phoneme Distances Thresholding (NPDT), Furthest Segment Search (FSS) and Normalized Furthest Segment Search (NFSS). The performance evaluation of each module was carried out using a speech database of correctly and incorrectly pronounced words in the Polish language, with up to 10 patterns of every trained word from a set of 12 words having different phonetic structures. The performance of DTW modeling was compared to Hidden Markov Models (HMM) that were used for the same four approaches (VoWS, NPDT, FSS, NFSS). The average error rate (AER) was the lowest for DTW with NPDT (AER=0.287) and scored better than HMM with FSS (AER=0.473), which was the best result for HMM. The DTW modeling was faster than HMM for all four approaches. This technique can be used for computer-assisted pronunciation training systems that can work with a relatively small training speech corpus (less than 20 patterns per word) to support speech therapy at home."]},
{"title": "Large-scale CFD simulations of the transitional and turbulent regime for the large human airways during rapid inhalation", "highlights": ["Unsteady flow in the human large airways during a rapid inhalation is proposed.", "The finest mesh contained 350 million elements.", "Thanks to the high fidelity simulations, turbulence and transitional regime are observed."], "abstract": ["The dynamics of unsteady flow in the human large airways during a rapid inhalation were investigated using highly detailed large-scale computational fluid dynamics on a subject-specific geometry. The simulations were performed to resolve all the spatial and temporal scales of the flow, thanks to the use of massive computational resources. A highly parallel finite element code was used, running on two supercomputers, solving the transient incompressible Navier\u2013Stokes equations on unstructured meshes. Given that the finest mesh contained 350 million elements, the study sets a precedent for large-scale simulations of the respiratory system, proposing an analysis strategy for mean flow, fluctuations and wall shear stresses on a rapid and short inhalation (a so-called sniff). The geometry used encompasses the exterior face and the airways from the nasal cavity, through the trachea and up to the third lung bifurcation; it was derived from a contrast-enhanced computed tomography (CT) scan of a 48-year-old male. The transient inflow produces complex flows over a wide range of Reynolds numbers (Re). Thanks to the high fidelity simulations, many features involving the flow transition were observed, with the level of turbulence clearly higher in the throat than in the nose. Spectral analysis revealed turbulent characteristics persisting downstream of the glottis, and were captured even with a medium mesh resolution. However a fine mesh resolution was found necessary in the nasal cavity to observe transitional features. This work indicates the potential of large-scale simulations to further understanding of airway physiological mechanics, which is essential to guide clinical diagnosis; better understanding of the flow also has implications for the design of interventions such as aerosol drug delivery."]},
{"title": "Application of wavelet techniques for cancer diagnosis using ultrasound images: A Review", "highlights": ["Application of DWT for cancer detection using ultrasound images is reviewed.", "Breast, thyroid, ovary and prostate cancers are considered.", "Preprocessing, segmentation and feature extraction using DWT are studied."], "abstract": ["Ultrasound is an important and low cost imaging modality used to study the internal organs of human body and blood flow through blood vessels. It uses high frequency sound waves to acquire images of internal organs. It is used to screen normal, benign and malignant tissues of various organs. Healthy and malignant tissues generate different echoes for ultrasound. Hence, it provides useful information about the potential tumor tissues that can be analyzed for diagnostic purposes before therapeutic procedures. Ultrasound images are affected with speckle noise due to an air gap between the transducer probe and the body. The challenge is to design and develop robust image preprocessing, segmentation and feature extraction algorithms to locate the tumor region and to extract subtle information from isolated tumor region for diagnosis. This information can be revealed using a scale space technique such as the Discrete Wavelet Transform (DWT). It decomposes an image into images at different scales using low pass and high pass filters. These filters help to identify the detail or sudden changes in intensity in the image. These changes are reflected in the wavelet coefficients. Various texture, statistical and image based features can be extracted from these coefficients. The extracted features are subjected to statistical analysis to identify the significant features to discriminate normal and malignant ultrasound images using supervised classifiers. This paper presents a review of wavelet techniques used for preprocessing, segmentation and feature extraction of breast, thyroid, ovarian and prostate cancer using ultrasound images."]},
{"title": "A denoising algorithm for projection measurements in cone-beam computed tomography", "highlights": ["We develop a new denoising algorithm for CT projections.", "The proposed algorithms includes regularizations in terms of gradient and Hessian.", "We suggest solving the problem using a split Bregman iteration.", "We apply the proposed algorithm on simulated and real noisy CT projections.", "Our algorithm effectively suppresses noise and improves spatial resolution."], "abstract": ["The ability to reduce the radiation dose in computed tomography (CT) is limited by the excessive quantum noise present in the projection measurements. Sinogram denoising is, therefore, an essential step towards reconstructing high-quality images, especially in low-dose CT. Effective denoising requires accurate modeling of the photon statistics and of the prior knowledge about the characteristics of the projection measurements. This paper proposes an algorithm for denoising low-dose sinograms in cone-beam CT. The proposed algorithm is based on minimizing a cost function that includes a measurement consistency term and two regularizations in terms of the gradient and the Hessian of the sinogram. This choice of the regularization is motivated by the nature of CT projections. We use a split Bregman algorithm to minimize the proposed cost function. We apply the algorithm on simulated and real cone-beam projections and compare the results with another algorithm based on bilateral filtering. Our experiments with simulated and real data demonstrate the effectiveness of the proposed algorithm. Denoising of the projections with the proposed algorithm leads to a significant reduction of the noise in the reconstructed images without oversmoothing the edges or introducing artifacts."]},
{"title": "An experimental evaluation of electrical skin conductivity changes in postmortem interval and its assessment for time of death estimation", "highlights": ["Estimation of ToD is one of the most critical concepts in forensic medicine.", "Electrical conductivity changes were investigated in human cases.", "Electrical conductivity measurements give promising clues about postmortem interval.", "There is a positive relationship between conductivity and postmortem interval."], "abstract": ["In forensic medicine, estimation of the time of death (ToD) is one of the most important and challenging medico-legal problems. Despite the partial accomplishments in ToD estimations to date, the error margin of ToD estimation is still too large. In this study, electrical conductivity changes were experimentally investigated in the postmortem interval in human cases. Electrical conductivity measurements give some promising clues about the postmortem interval. A living human has a natural electrical conductivity; in the postmortem interval, intracellular fluids gradually leak out of cells. These leaked fluids combine with extra-cellular fluids in tissues and since both fluids are electrolytic, intracellular fluids help increase conductivity. Thus, the level of electrical conductivity is expected to increase with increased time after death. In this study, electrical conductivity tests were applied for six hours. The electrical conductivity of the cases exponentially increased during the tested time period, indicating a positive relationship between electrical conductivity and the postmortem interval."]},
{"title": "The Latin American laws of correct nutrition: Review, unified interpretation, model and tools", "highlights": ["We propose a model for the \u201cLaws of the Correct Nutrition\u201d in use in Latin America.", "The proposed model describes and specifies the Laws in a formal language.", "We validate and verify the model using a state of the art tool.", "We designed a nourishing menu enforcing the Laws through model.", "We developed a software tool as support for the creation of model-based valid menus."], "abstract": ["Background: The \u201cLaws of Correct Nutrition\u201d: the Law of Quantity, the Law of Quality, the Law of Harmony and the Law of Adequacy, provide the basis of a proper diet, i.e. one that provides the body with the energy required and nutrients it needs for daily activities and maintenance of vital functions. For several decades, these Laws have been the basis of nourishing menus designed in Latin America; however, they are stated in a colloquial language, which leads to differences in interpretation and ambiguities for non-experts and even experts in the field.", "Methods: We present a review of the different interpretations of the Laws and describe a consensus. We represent concepts related to nourishing menu design employing the Unified Modeling Language (UML). We formalize the Laws using the Object Constraint Language (OCL). We design a nourishing menu for a particular user through enforcement of the Laws.", "Results: We designed a domain model with the essential elements to plan a nourishing menu and we expressed the necessary constraints to make the model\u05f3s behavior conform to the four Laws. We made a formal verification and validation of the model via USE (UML-based Specification Environment) and we developed a software prototype for menu design under the Laws.", "Conclusion: Diet planning is considered as an art but consideration should be given to the need for a set of strict rules to design adequate menus. Thus, we model the \u201cLaws of Nutrition\u201d as a formal basis and standard framework for this task."]},
{"title": "Estimating the parameters of multi-state models with time-dependent covariates through likelihood decomposition", "highlights": ["Currently, various successive treatments are offered to people with ESRD.", "The demand increase requires modelling these treatments for provisional purposes.", "Modeling should allow for several treatments as for time-dependent covariates.", "We present and test an approach that allows for an unlimited number of treatments.", "This approach may be extended to allow for various other management constraints."], "abstract": ["Multi-state models become complex when the number of states is large, when back and forth transitions between states are allowed, and when time-dependent covariates are inevitable. However, these conditions are sometimes necessary in the context of medical issues. For instance, they were needed for modelling the future treatments of patients with end-stage renal disease according to age and to various treatments.", "The available modelling tools do not allow an easy handling of all issues; we designed thus a specific multi-state model that takes into account the complexity of the research question. Parameter estimation relied on decomposition of the likelihood and separate maximisations of the resulting likelihoods. This was possible because there were no interactions between patient treatment courses and because all exact times of transition from any state to another were known. Poisson likelihoods were calculated using the time spent at risk in each state and the observed transitions between each state and all others. The likelihoods were calculated on short time intervals during which age was considered as constant.", "The method was not limited by the number of parameters to estimate; it could be applied to a multi-state model with 10 renal replacement therapies. Supposing the parameters of the model constant over each of seven time intervals, this method was able to estimate one hundred age-dependent transitions.", "The method is easy to adapt to any disease with numerous states or grades as long as the disease does not imply interactions between patient courses."]},
{"title": "Application of robust Generalised Cross-Validation to the inverse problem of electrocardiology", "highlights": ["Robust Generalised Cross-Validation (RGCV) is introduced to ECG Imaging.", "RGCV is compared to more common methods for two heart beat scenarios.", "Zero order Tikhonov regularisation is the basis for the comparison.", "Various levels of white noise and geometric noise are considered.", "RGCV produces accurate and consistent epicardial potential distributions."], "abstract": ["Robust Generalised Cross-Validation was proposed recently as a method for determining near optimal regularisation parameters in inverse problems. It was introduced to overcome a problem with the regular Generalised Cross-Validation method in which the function that is minimised to obtain the regularisation parameter often has a broad, flat minimum, resulting in a poor estimate for the parameter. The robust method defines a new function to be minimised which has a narrower minimum, but at the expense of introducing a new parameter called the robustness parameter.", "In this study, the Robust Generalised Cross-Validation method is applied to the inverse problem of electrocardiology. It is demonstrated that, for realistic situations, the robustness parameter can be set to zero. With this choice of robustness parameter, it is shown that the robust method is able to obtain estimates of the regularisation parameter in the inverse problem of electrocardiology that are comparable to, or better than, many of the standard methods that are applied to this inverse problem."]},
{"title": "Geometry-based vs. intensity-based medical image registration: A comparative study on 3D CT data", "highlights": ["A review study comparing conventional intensity-based and geometry-based registration methodologies.", "Three major registration frameworks were examined: (a) intensity-based, exhaustive registration framework using three distinct cost functions (b) geometry-based registration framework, featuring three geometrical descriptors (c) the original implementation of the Iterative Closest Point algorithm.", "Use of geometrical feature descriptors for aligning 3D medical data.", "All compared techniques were applied to CT data pairs with known and unknown initial spatial differences.", "Geometry-based and intensity-based techniques perform similarly, as far as accuracy is concerned, but geometry-based methods significantly reduce processing time."], "abstract": ["Spatial alignment of Computed Tomography (CT) data sets is often required in numerous medical applications and it is usually achieved by applying conventional exhaustive registration techniques, which are mainly based on the intensity of the subject data sets. Those techniques consider the full range of data points composing the data, thus negatively affecting the required processing time. Alternatively, alignment can be performed using the correspondence of extracted data points from both sets. Moreover, various geometrical characteristics of those data points can be used, instead of their chromatic properties, for uniquely characterizing each point, by forming a specific geometrical descriptor. This paper presents a comparative study reviewing variations of geometry-based, descriptor-oriented registration techniques, as well as conventional, exhaustive, intensity-based methods for aligning three-dimensional (3D) CT data pairs. In this context, three general image registration frameworks were examined: a geometry-based methodology featuring three distinct geometrical descriptors, an intensity-based methodology using three different similarity metrics, as well as the commonly used Iterative Closest Point algorithm. All techniques were applied on a total of thirty 3D CT data pairs with both known and unknown initial spatial differences. After an extensive qualitative and quantitative assessment, it was concluded that the proposed geometry-based registration framework performed similarly to the examined exhaustive registration techniques. In addition, geometry-based methods dramatically improved processing time over conventional exhaustive registration."]},
{"title": "Blood vessel extraction and optic disc removal using curvelet transform and kernel fuzzy c-means", "highlights": ["An integrated framework for retinal blood vessel extraction.", "Matched filtering, Laplacian of Gaussian, KFCM and morphological operation.", "Accurate vessel extraction from DRIVE, STARE and DIARETDB1 databases.", "Vessel extraction accuracy-96.16% for DRIVE and 97.35% for STARE databases."], "abstract": ["This paper proposes an automatic blood vessel extraction method on retinal images using matched filtering in an integrated system design platform that involves curvelet transform and kernel based fuzzy c-means. Since curvelet transform represents the lines, the edges and the curvatures very well and in compact form (by less number of coefficients) compared to other multi-resolution techniques, this paper uses curvelet transform for enhancement of the retinal vasculature. Matched filtering is then used to intensify the blood vessels\u2019 response which is further employed by kernel based fuzzy c-means algorithm that extracts the vessel silhouette from the background through non-linear mapping. For pathological images, in addition to matched filtering, Laplacian of Gaussian filter is also employed to distinguish the step and the ramp like signal from that of vessel structure. To test the efficacy of the proposed method, the algorithm has also been applied to images in presence of additive white Gaussian noise where the curvelet transform has been used for image denoising. Performance is evaluated on publicly available DRIVE, STARE and DIARETDB1 databases and is compared with the large number of existing blood vessel extraction methodologies. Simulation results demonstrate that the proposed method is very much efficient in detecting the long and the thick as well as the short and the thin vessels with an average accuracy of 96.16% for the DRIVE and 97.35% for the STARE database wherein the existing methods fail to extract the tiny and the thin vessels."]},
{"title": "On fuzzy semantic similarity measure for DNA coding", "highlights": [" coding scheme centers codons\u05f3 clustering and genetic code context.", " exploits natural characteristics of nucleotides in codons.", " reveals a strong correlation between nucleotides in codons.", " attains a significant enhancement in coding regions identification."], "abstract": ["A coding measure scheme numerically translates the DNA sequence to a time domain signal for protein coding regions identification. A number of coding measure schemes based on numerology, geometry, fixed mapping, statistical characteristics and chemical attributes of nucleotides have been proposed in recent decades. Such coding measure schemes lack the biologically meaningful aspects of nucleotide data and hence do not significantly discriminate coding regions from non-coding regions.", "This paper presents a novel ", " (", ") coding scheme centering on ", " codons\u05f3 clustering and genetic code context of nucleotides. Certain natural characteristics of nucleotides i.e. appearance as a unique combination of triplets, preserving special structure and occurrence, and ability to own and share density distributions in codons have been exploited in ", ". The nucleotides\u05f3 fuzzy behaviors, semantic similarities and defuzzification based on the center of gravity of nucleotides revealed a strong correlation between nucleotides in codons. The proposed ", " coding scheme attains a significant enhancement in coding regions identification i.e. 36\u2013133% as compared to other existing coding measure schemes tested over more than 250 benchmarked and randomly taken DNA datasets of different organisms."]},
{"title": "Pharmacophore-based screening targeted at upregulated ", "highlights": ["We built Mendel, a workbench to analyze dysregulated genes in nasopharyngeal carcinoma.", "We analyzed druggable genes with high fold changes, cliques, and pathway constraints.", "Using pharmacophores, we virtually screened 22,723,923 compounds to identify 3 hits."], "abstract": ["Nasopharyngeal carcinoma (NpC) is rare in the west but common in Southeast Asia and only a few other locations. With the limited geographic incidence, it is relatively under-studied. It also has as co-determinant the Epstein-Barr virus (EBV), which may adapt to NpC therapies, so not only must a therapeutic compound be found, the discovery process must be rapid, to cope with the changing basis of the EBV. An R-based computer workbench, Mendel, was developed so biologists could quickly upload genomic data, pre-process them, and identify upregulated and downregulated genes. Mendel was used on 10 control and 31 diseased cell lines to discover 3 differentially expressed genes (DEGs) that meet thresholds on fold-changes, 3-clique membership, pathway constraints, and druggability. From the DEGs, we conducted a pharmacophore-based screening of 22,723,923 compounds using protein-protein interaction anchor-residue clusters as binding sites. Of the 4 hits, 3 passed all the ADME-Tox tests. These 3 hit compounds, 6-(4-iminiocyclohexa-2,5-dien-1-ylidene)-4-(thiazol-2-ylcarbamoyl)-1H-pyrimidine-2-thiolate, 1-[4-[2-[(3R)-3-hydroxy-2-oxo-indolin-3-yl]acetyl]phenyl]-3-phenyl-urea, and (2R)-N4-[4-(1-piperidyl)cyclohexyl]morpholine-2,4-dicarboxamide have predicted pIC50 values superior to the current drugs fluorouracil (5-FU) and taxotere, which have side effects and face EBV drug resistance."]},
{"title": "Mining frequent biological sequences based on bitmap without candidate sequence generation", "highlights": ["Frequent sequence mining is used to discover genetic laws from biological sequence.", "Less memory is required because bitmap is used for sequence record and joining.", "Quicksort list makes the tested sequence real ones instead of excessive candidates.", "Joining operation is simplified by subtraction and efficient for runtime saving.", "All frequent sequences can be mined without any loss, so the error rate is 0."], "abstract": ["Biological sequences carry a lot of important genetic information of organisms. Furthermore, there is an inheritance law related to protein function and structure which is useful for applications such as disease prediction. Frequent sequence mining is a core technique for association rule discovery, but existing algorithms suffer from low efficiency or poor error rate because biological sequences differ from general sequences with more characteristics. In this paper, an algorithm for mining ", "requent ", "iological ", "equence based on ", "itmap, ", ", is proposed. FBSB uses bitmaps as the simple data structure and transforms each row into a quicksort list QS-list for sequence growth. For the continuity and accuracy requirement of biological sequence mining, tested sequences used during the mining process of FBSB are real ones instead of generated candidates, and all the frequent sequences can be mined without any errors. Comparing with other algorithms, the experimental results show that FBSB can achieve a better performance on both run time and scalability."]},
{"title": "Shape changes of bioprinted tissue constructs simulated by the Lattice Boltzmann method", "highlights": ["A Lattice Boltzmann model describes shape changes of tissue constructs in hydrogel.", "We simulate the evolution of structures built by printing multicellular cylinders.", "A defect in a bioprinted tissue construct increases as cells rearrange.", "A stack of multicellular cylinders may give rise to a perfusable tissue construct."], "abstract": ["Tissue engineers seek to build living tissue constructs for replacing or repairing damaged tissues. Computational methods foster tissue engineering by pointing out dominant mechanisms involved in shaping multicellular systems. Here we apply the Lattice Boltzmann (LB) method to study the fusion of multicellular constructs. This process is of interest in bioprinting, in which multicellular spheroids or cylinders are embedded in a supportive hydrogel by a computer-controlled device. We simulated post-printing rearrangements of cells, aiming to predict the shape and stability of certain printed structures. To this end, we developed a two-dimensional LB model of a multicellular system in a hydrogel. Our parallel computing code was implemented using the Portable Extensible Toolkit for Scientific Computation (PETSc). To validate the LB model, we simulated the fusion of multicellular cylinders in a contiguous, hexagonal arrangement. Our two-dimensional LB simulation describes the evolution of the transversal cross section of the construct built from three-dimensional multicellular cylinders whose length is much larger than their diameter. Fusion eventually gave rise to a tubular construct, in qualitative agreement with bioprinting experiments. Then we simulated the time course of a defect in a bioprinted tube. To address practical problems encountered in tissue engineering, we also simulated the evolution of a planar construct, as well as of a bulky, perfusable construct made of multicellular cylinders. The agreement with experiments indicates that our LB model captures certain essential features of morphogenesis, and, therefore, it may be used to test new working hypotheses faster and cheaper than in the laboratory."]},
{"title": "A quantitative cell modeling and wound-healing analysis based on the Electric Cell-substrate Impedance Sensing (ECIS) method", "highlights": ["Cell behavior is continuously measured by the change in resistance on an ECIS system.", "ECIS allows an easier way in cell wound healing assay than does the traditional way.", "The wavelet transform allows an unprecedented multi-scale analysis in cell dynamics.", "The regression line of cell fluctuation versus resistance gives cell electroactivity.", "The cell power and electroactivity are used for quantitative wound healing analysis."], "abstract": ["In this paper, a quantitative modeling and wound-healing analysis of fibroblast and human keratinocyte cells is presented. Our study was conducted using a continuous cellular impedance monitoring technique, dubbed Electric Cell-substrate Impedance Sensing (ECIS). In fact, we have constructed a mathematical model for quantitatively analyzing the cultured cell growth using the time series data directly derived by ECIS in a previous work. In this study, the applicability of our model into the keratinocyte cell growth modeling analysis was assessed first. In addition, an electrical \u201cwound-healing\u201d assay was used as a means to evaluate the healing process of keratinocyte cells at a variety of pressures. Two innovative and new-defined indicators, dubbed cell power and cell electroactivity, respectively, were developed for quantitatively characterizing the biophysical behavior of cells. We then employed the wavelet transform method to perform a multi-scale analysis so the cell power and cell electroactivity across multiple observational time scales may be captured. Numerical results indicated that our model can well fit the data measured from the keratinocyte cell culture for cell growth modeling analysis. Also, the results produced by our quantitative analysis showed that the wound healing process was the fastest at the negative pressure of 125", "\u00a0", "mmHg, which consistently agreed with the qualitative analysis results reported in previous works."]},
{"title": "Age and gender effects on 15 platelet phenotypes in a Spanish population", "highlights": ["A comprehensive analysis of the age and gender effects on the platelet phenotypes.", "A more reasonable age stratification was proposed: childhood and the mature group.", "The platelet counts, MPV and IPF levels vary over time.", "The PFA phenotypes were mantained invariably over time."], "abstract": ["Several studies have analysed the platelet parameters in human blood, nevertheless there are no extensive analyses on the less common platelet phenotypes. The main objective of our study is to evaluate the age and gender effects on 15 platelet phenotypes.", "We studied 804 individuals, ranging in age from 2 to 93 years, included in the Genetic Analysis of Idiopathic Thrombophilia 2 (GAIT 2) Project. The 15 platelet phenotypes analysed were the platelets counts, platelet volumes, plateletcrits, immature platelet fraction (IPF) and platelet function assay (PFA). A regression-based method was used to evaluate the age and gender effects on these phenotypes.", "Our results were consistent with the previously reported results regarding platelet counts and plateletcrit (PCT). They showed a decrease with increasing age. The mean platelet volume (MPV), platelet distribution width (PDW) and platelet-large cell ratio (P-LCR) increased with age, but did not present any gender effect. All the IPF phenotypes increased with age, whereas the PFA phenotypes did not show any relation to age or gender.", "To sum up, our study provides a comprehensive analysis of the age and gender effects on the platelet phenotypes in a family-base sample. Our results suggest more reasonable age stratification into two distinct groups: childhood, ranging from 2 to 12 years, and the mature group, from 13 to 93 years. Moreover, the PFA phenotypes were maintained constant while the platelet counts, the MPV and IPF levels vary with age."]},
{"title": "Segmentation and optical flow estimation in cardiac CT sequences based on a spatiotemporal PDM with a correction scheme and the Hermite transform", "highlights": ["A new framework for segmentation and optical flow estimation applied to cardiac CT sequences is proposed.", "The segmentation is based on two processes: (1) A spatiotemporal point distribution model, and (2) A correction scheme that performs the segmentation of small details.", "The optical flow estimation uses a bio-inspired model based on the Hermite transform to code local image features.", "Both algorithms are combined to provide a powerful framework to assist the evaluation of heart mechanical problems."], "abstract": [" The left ventricle and the myocardium are two of the most important parts of the heart used for cardiac evaluation. In this work a novel framework that combines two methods to isolate and display functional characteristics of the heart using sequences of cardiac computed tomography (CT) is proposed. A shape extraction method, which includes a new segmentation correction scheme, is performed jointly with a motion estimation approach.", " For the segmentation task we built a Spatiotemporal Point Distribution Model (STPDM) that encodes spatial and temporal variability of the heart structures. Intensity and gradient information guide the STPDM. We present a novel method to correct segmentation errors obtained with the STPDM. It consists of a deformable scheme that combines three types of image features: local histograms, gradients and binary patterns. A bio-inspired image representation model based on the Hermite transform is used for motion estimation. The segmentation allows isolating the structure of interest while the motion estimation can be used to characterize the movement of the complete heart muscle.", " The work is evaluated with several sequences of cardiac CT. The left ventricle was used for evaluation. Several metrics were used to validate the proposed framework. The efficiency of our method is also demonstrated by comparing with other techniques.", " The implemented tool can enable physicians to better identify mechanical problems. The new correction scheme substantially improves the segmentation performance. Reported results demonstrate that this work is a promising technique for heart mechanical assessment."]},
{"title": "Optimum force system for intrusion and extrusion of maxillary central incisor in labial and lingual orthodontics", "highlights": ["Force system for intrusion and extrusion of incisor in LaO and LiO is proposed.", "Mathematical model of maxillary central incisor with normal inclination is developed.", "Different cases of heights of bracket slot from incisal edge are considered.", "The devised force system is validated with finite element analysis.", "Mathematical model can form the basis for future technological innovation."], "abstract": ["The objective of the present study was to specify an optimum force system for intrusion and extrusion of maxillary central incisor and to compare the effects of bracket positioning at different heights from the incisal edge in Labial Orthodontics (LaO) and Lingual Orthodontics (LiO).", "A mathematical model of maxillary central incisor with normal inclination was developed. Four cases of heights of bracket slot from incisal edge were considered both in LaO and LiO viz. 3", "\u00a0", "mm, 4", "\u00a0", "mm, 5", "\u00a0", "mm and 6", "\u00a0", "mm. Based on a mathematical model, an optimum force system consisting of an intrusive or extrusive force (", ") and a moment (", ") was devised and moment (", ") to force (", ") ratio (", ":", " ratio) was estimated in each case. Then, three-dimensional Computer Aided Design (CAD) models of incisor and surrounding structures were prepared. To validate an optimum force system, finite element analysis was carried out and force system with derived ", ":", " ratio was applied in each case.", "In finite element analysis, results were shown in the form of vector graph of nodal displacements along with undeformed and deformed models. The desired intrusion or extrusion of incisor was observed. Thus, force system devised from a mathematical model was validated with finite element analysis in each case.", "To achieve intrusion or extrusion, M:F ratios required in LaO were same i.e. 8:1 for aforementioned heights of bracket slot from incisal edge but different in LiO i.e. 0:1, 1:1, 2:1 and 3:1 for the heights of 3", "\u00a0", "mm, 4", "\u00a0", "mm, 5", "\u00a0", "mm and 6", "\u00a0", "mm respectively."]},
{"title": "Differential diagnosis of squamous cell carcinoma in situ using skin histopathological images", "highlights": ["A method for computer assisted diagnosis of squamous cell carcinoma in situ is presented.", "The method provides the pathologists a second consultative opinion in evaluating histopathological samples.", "In this algorithm, active contours, connected component analysis and sample-frequency distribution are used for recognition.", "The results of the study are in agreement with the gold standards provided by expert pathologists."], "abstract": ["Differential diagnosis of squamous cell carcinoma in situ is of great importance for prognosis and decision making in the disease treatment procedure. Currently, differential diagnosis is done by pathologists based on examination of the histopathological slides under the microscope, which is time consuming and prone to inter and intra observer variability. In this paper, we have proposed an automated method for differential diagnosis of SCC in situ from actinic keratosis, which is known to be a precursor of squamous cell carcinoma. The process begins with epidermis segmentation and cornified layer removal. Then, epidermis axis is specified using the paths in its skeleton and the granular layer is removed via connected components analysis. Finally, diagnosis is done based on the classification result of intensity profiles extracted from lines perpendicular to the epidermis axis. The results of the study are in agreement with the gold standards provided by expert pathologists."]},
{"title": "Automatic image classification for the urinoculture screening", "highlights": ["Realization of the AID software for automatic detection of urinary tract infections.", "AID is able to acquire and store the image of the Petri plate.", "AID accurately classifies the infection type and evaluates the bacterial load.", "AID allows result storing, repeatability and standardization.", "AID speeds up the analysis procedure and reduces its costs."], "abstract": ["Urinary tract infections (UTIs) are considered to be the most common bacterial infection and, actually, it is estimated that about 150 million UTIs occur world wide yearly, giving rise to roughly $6 billion in healthcare expenditures and resulting in 100,000 hospitalizations. Nevertheless, it is difficult to carefully assess the incidence of UTIs, since an accurate diagnosis depends both on the presence of symptoms and on a positive urinoculture, whereas in most outpatient settings this diagnosis is made without an ad hoc analysis protocol. On the other hand, in the traditional urinoculture test, a sample of midstream urine is put onto a Petri dish, where a growth medium favors the proliferation of germ colonies. Then, the infection severity is evaluated by a visual inspection of a human expert, an error prone and lengthy process. In this paper, we propose a fully automated system for the urinoculture screening that can provide quick and easily traceable results for UTIs. Based on advanced image processing and machine learning tools, the infection type recognition, together with the estimation of the bacterial load, can be automatically carried out, yielding accurate diagnoses. The proposed AID (Automatic Infection Detector) system provides support during the whole analysis process: first, digital color images of Petri dishes are automatically captured, then specific preprocessing and spatial clustering algorithms are applied to isolate the colonies from the culture ground and, finally, an accurate classification of the infections and their severity evaluation are performed. The AID system speeds up the analysis, contributes to the standardization of the process, allows result repeatability, and reduces the costs. Moreover, the continuous transition between sterile and external environments (typical of the standard analysis procedure) is completely avoided."]},
{"title": "A robust aCGH data recovery framework based on half quadratic minimization", "highlights": ["A general framework for recovering multisample aCGH data.", "The solution of proposed formulation is given by Half Quadratic (HQ) methods.", "The proposed framework accommodates different robust M-estimators.", "Using Accelerated Proximal Gradient (APG) for speeding up the convergence rate.", "Numerous experimental results on simulated datasets with non-Gaussian noise to evaluate the robustness of the proposed framework."], "abstract": ["This paper presents a general half quadratic framework for simultaneous analysis of the whole array comparative genomic hybridization (aCGH) profiles in a data set. The proposed framework accommodates different M-estimation loss functions and two underlying assumptions for aCGH profiles of a data set: sparsity and low rank. Using M-estimation loss functions, this framework is more robust to various types of noise and outliers. The solution of the proposed framework is given by half quadratic (HQ) minimization. To hasten this procedure, accelerated proximal gradient (APG) is utilized. Experimental results support the robustness of the proposed framework in comparison to the state-of-the-art algorithms."]},
{"title": "Wheezing recognition algorithm using recordings of respiratory sounds at the mouth in a pediatric population", "highlights": ["We recorded by Smartphone respiratory sounds at the mouth in pediatric population.", "Two clinical operators validated the presence or absence of wheezing in 97 toddlers.", "We used Short-Time Fourier Transform and SVM classifier for wheeze recognition.", "71.4% Sensitivity and 88.9% Specificity were observed for wheeze detection.", "An independent test found a fair agreement with a clinical operator."], "abstract": ["Respiratory diseases in children are a common reason for physician visits. A diagnostic difficulty arises when parents hear wheezing that is no longer present during the medical consultation. Thus, an outpatient objective tool for recognition of wheezing is of clinical value.", "We developed a wheezing recognition algorithm from recorded respiratory sounds with a Smartphone placed near the mouth. A total of 186 recordings were obtained in a pediatric emergency department, mostly in toddlers (mean age 20 months). After exclusion of recordings with artefacts and those with a single clinical operator auscultation, 95 recordings with the agreement of two operators on auscultation diagnosis (27 with wheezing and 68 without) were subjected to a two phase algorithm (signal analysis and pattern classifier using machine learning algorithms) to classify records.", "The best performance (71.4% sensitivity and 88.9% specificity) was observed with a Support Vector Machine-based algorithm. We further tested the algorithm over a set of 39 recordings having a single operator and found a fair agreement (kappa=0.28, CI", " [0.12, 0.45]) between the algorithm and the operator.", "The main advantage of such an algorithm is its use in contact-free sound recording, thus valuable in the pediatric population."]},
{"title": "Prediction of time dependent survival in HF patients after VAD implantation using pre- and post-operative data", "highlights": ["A methodology for the prediction of time dependent survival in patients after LVAD implantation.", "Data mining techniques applied.", "Pre-implantation data are exploited in order to estimate their predictive power.", "Prediction accuracy enhancement with the integration of post-implantation data.", "An optimal subset of features highly correlated with patient survival is identified."], "abstract": ["Heart failure is one of the most common diseases worldwide. In recent years, Ventricular Assist Devices (VADs) have become a valuable option for patients with advanced HF. Although it has been shown that VADs improve patient survival rates, several complications persist during left VAD (LVAD) support. The stratification scores currently employed are mostly generic, i.e. not specifically built for LVAD patients, and are based on pre-implantation patient data. In this work we apply data mining approaches for the prediction of time dependent survival in patients after LVAD implantation. Moreover, the predictions acquired with the use of pre-implantation data are enriched by employing post-implantation data, i.e. follow-up data. Different clinical scenarios have been depicted and the subsequent conditions are tested in order to identify the optimal set of pre- and post-implant features, as well as the most suitable algorithms for feature selection and prediction. The proposed approach is applied to a real dataset of 71 patients, reporting an accuracy of 84.5%, sensitivity of 87% and specificity of 82%. Based on the reported results, expert cardio-surgeons can be supported in planning the treatment of VAD patients."]},
{"title": "A practical efficient human computer interface based on saccadic eye movements for people with disabilities", "highlights": ["A wearable EOG-based system was developed to enable people with disabilities to type.", "Saccadic eye movements could be detected efficiently by an adaptive algorithm.", "Average accuracy of the system was 84% with a typing speed of 4.5 character per min.", "Participants with disabilities could learn to perform necessary eye movements."], "abstract": ["Human computer interfaces (HCI) provide new channels of communication for people with severe motor disabilities to state their needs, and control their environment. Some HCI systems are based on eye movements detected from the electrooculogram. In this study, a wearable HCI, which implements a novel adaptive algorithm for detection of saccadic eye movements in eight directions, was developed, considering the limitations that people with disabilities have. The adaptive algorithm eliminated the need for calibration of the system for different users and in different environments. A two-stage typing environment and a simple game for training people with disabilities to work with the system were also developed. Performance of the system was evaluated in experiments with the typing environment performed by six participants without disabilities. The average accuracy of the system in detecting eye movements and blinking was 82.9% at first tries with an average typing rate of 4.5", "\u00a0", "cpm. However an experienced user could achieve 96% accuracy and 7.2", "\u00a0", "cpm typing rate. Moreover, the functionality of the system for people with movement disabilities was evaluated by performing experiments with the game environment. Six people with tetraplegia and significant levels of speech impairment played with the computer game several times. The average success rate in performing the necessary eye movements was 61.5%, which increased significantly with practice up to 83% for one participant. The developed system is 2.6", "\u00a0", "\u00d74.5", "\u00a0", "cm in size and weighs only 15", "\u00a0", "g, assuring high level of comfort for the users."]},
{"title": "Automated measurement of parameters related to the deformities of lower limbs based on x-rays images", "highlights": ["A CAD system was developed for assessment of the deformities of lower extremities.", "Measurements of the quantities related to the deformities are fully automatic.", "Measurements are based on detection of characteristic points, axes and tangents.", "There is a very good correlation between manual and automated measurements."], "abstract": ["Measurement of the deformation of the lower limbs in the current standard full-limb X-rays images presents significant challenges to radiologists and orthopedists. The precision of these measurements is deteriorated because of inexact positioning of the leg during image acquisition, problems with selecting reliable anatomical landmarks in projective X-ray images, and inevitable errors of manual measurements. The influence of the random errors resulting from the last two factors on the precision of the measurement can be reduced if an automated measurement method is used instead of a manual one. In the paper a framework for an automated measurement of various metric and angular quantities used in the description of the lower extremity deformation in full-limb frontal X-ray images is described. The results of automated measurements are compared with manual measurements. These results demonstrate that an automated method can be a valuable alternative to the manual measurements."]},
{"title": "Parallel scheme for real-time detection of photosensitive seizures", "highlights": ["Healthy viewing.", "Real-time detection of hazardous video content for photosensitive seizures.", "Multicore parallel computation.", "Seizure pattern inducer testing benchmark."], "abstract": ["The production and distribution of videos and animations on gaming and self-authoring websites are booming. However, given this rise in self-authoring, there is increased concern for the health and safety of people who suffer from a neurological disorder called photosensitivity or photosensitive epilepsy. These people can suffer seizures from viewing video with hazardous content. This paper presents a spatiotemporal pattern detection algorithm that can detect hazardous content in streaming video in real time. A tool is developed for producing test videos with hazardous content, and then those test videos are used to evaluate the proposed algorithm, as well as an existing post-processing tool that is currently being used for detecting such patterns. To perform the detection in real time, the proposed algorithm was implemented on a dual core processor, using a pipelined/parallel software architecture. Results indicate that the proposed method provides better detection performance, allowing for the masking of seizure inducing patterns in real time."]},
{"title": "Unsupervised entity and relation extraction from clinical records in Italian", "highlights": ["Extraction of domain relevant entities and relations from clinical records in Italian.", "Domain knowledge for the identification and classification of medical and pharmaceutical entities.", "Clustering to discover possible relations.", "Experimental testing on a fairly large dataset of clinical records.", "Solution evaluated from both a quantivative and a qualitative point of view."], "abstract": ["This paper proposes and discusses the use of text mining techniques for the extraction of information from clinical records written in Italian. However, as it is very difficult and expensive to obtain annotated material for languages different from English, we only consider unsupervised approaches, where no annotated training set is necessary. We therefore propose a complete system that is structured in two steps. In the first one domain entities are extracted from the clinical records by means of a metathesaurus and standard natural language processing tools. The second step attempts to discover relations between the entity pairs extracted from the whole set of clinical records. For this last step we investigate the performance of unsupervised methods such as clustering in the space of entity pairs, represented by an ", " feature vector. The resulting clusters are then automatically labelled by using the most significant features. The system has been tested on a fairly large data set of clinical records in Italian, investigating the variation in the performance adopting different similarity measures in the feature space. The results of our experiments show that the unsupervised approach proposed is promising and well suited for a semi-automatic labelling of the extracted relations."]},
{"title": "Investigation of flow characteristics in regions of nasal polypoid change", "highlights": ["Airflow characteristics in the region of polypoid changes were investigated.", "Positive wall-normal pressure gradients were persistent during the respiration period.", "The regions having polypoid change were not correlated with stagnation areas.", "Local extreme wall shear stress did not characterize the regions with polypoid change."], "abstract": ["We used computational fluid dynamics to study the airflow characteristics in the ostiomeatal complex/middle turbinate of the human upper airway, where clinically relevant nasal polypoid changes occur (designated Regions A1\u2013A4). We assessed six different flow rates representing one full period of respiration, based on realistic human respiration data, in an anatomically correct numerical model of a patient with a history of polypectomy.", "The simulation results showed that Regions A1\u2013A4 were not correlated with the local stagnation points where a locally high level of wall pressure was achieved. They, however, exhibited a very distinctive feature in that the positive wall-normal pressure gradients evaluated at the epithelial surface were persistent at six different flow rates spanning the whole respiration period in these areas. Therefore, the regions where polypoid changes developed were thought to be subject to mechanical irritation of the epithelium constantly via locally accelerating airflow moving towards the surface from the airway. On the contrary, relatively large or small values for local wall shear stress were not correlated with Regions A1\u2013A4."]},
{"title": "Locally adaptive 2D\u20133D registration using vascular structure model for liver catheterization", "highlights": ["A registration algorithm between 2D DSA image and 3D CTA scans is proposed.", "A vascular structure model is constructed using the augmented fast marching method.", "The tree model is divided into subtrees based on the connectivity and the length.", "Our method performs the registration focusing on the selected subtree.", "Our method performs the registration very accurately."], "abstract": ["Two-dimensional\u2013three-dimensional (2D\u20133D) registration between intra-operative 2D digital subtraction angiography (DSA) and pre-operative 3D computed tomography angiography (CTA) can be used for roadmapping purposes. However, through the projection of 3D vessels, incorrect intersections and overlaps between vessels are produced because of the complex vascular structure, which makes it difficult to obtain the correct solution of 2D\u20133D registration. To overcome these problems, we propose a registration method that selects a suitable part of a 3D vascular structure for a given DSA image and finds the optimized solution to the partial 3D structure. The proposed algorithm can reduce the registration errors because it restricts the range of the 3D vascular structure for the registration by using only the relevant 3D vessels with the given DSA. To search for the appropriate 3D partial structure, we first construct a tree model of the 3D vascular structure and divide it into several subtrees in accordance with the connectivity. Then, the best matched subtree with the given DSA image is selected using the results from the coarse registration between each subtree and the vessels in the DSA image. Finally, a fine registration is conducted to minimize the difference between the selected subtree and the vessels of the DSA image. In experimental results obtained using 10 clinical datasets, the average distance errors in the case of the proposed method were 2.34\u00b11.94", "\u00a0", "mm. The proposed algorithm converges faster and produces more correct results than the conventional method in evaluations on patient datasets."]},
{"title": "Symbolic features and classification via support vector machine for predicting death in patients with Chagas disease", "highlights": ["We predicted the risk of death with a good performance of classifier.", "We extracted features from symbolic series and time\u2013frequency indices of HRV.", "Symbolic, time\u2013frequency and clinical indices prove to be a good predictor of death.", "SVM was useful for accurately classifying two classes, survival and nonsurvival.", "Conventional autonomic indices have prognostic importance in Chagas disease."], "abstract": ["This paper introduces a technique for predicting death in patients with Chagas disease using features extracted from symbolic series and time\u2013frequency indices of heart rate variability (HRV). The study included 150 patients: 15 patients who died and 135 who did not. The HRV series were obtained from 24-h Holter monitoring. Sequences of symbols from 5-min epochs from series of RR intervals were generated using symbolic dynamics and ordinal pattern statistics. Fourteen features were extracted from symbolic series and four derived from clinical aspects of patients. For classification, the 18 features from each epoch were used as inputs in a support vector machine (SVM) with a radial basis function (RBF) kernel. The results showed that it is possible to distinguish between the two classes, patients with Chagas disease who did or did not die, with a 95% accuracy rate. Therefore, we suggest that the use of new features based on symbolic series, coupled with classic time\u2013frequency and clinical indices, proves to be a good predictor of death in patients with Chagas disease."]},
{"title": "A new computational method for automatic dental measurement: The case of maxillary central incisor", "highlights": ["A new automatic approach (", ") to measure accurately human teeth is proposed.", " evaluates dimensional features in algorithmic way from a 3D discrete model.", " shown to be more repeatable and reproducible compared to traditional approaches.", "Measurements can be carried out quickly.", " does not require an operator with special skills (dentist or orthodontist)."], "abstract": ["This paper proposes a new automatic approach to determine the accurate measure of human teeth. The aim of the proposed computer based method is to reduce inaccuracy of measurement with respect to traditional approaches. Starting from a 3D model of the teeth which is obtained from 3D scanning, the method algorithmically evaluates the most important dimensional features detectable in central incisors. For this purpose, specific rules are put forward and implemented in original software with a view to identifying repere points, from which to detect dimensional features both unambiguously and accurately. The automatic method which is proposed here is verified by means of the analysis of real teeth and is then compared with the current state-of-the-art methods for teeth measurement."]},
{"title": "Morphometric analysis of calcification and fibrous layer thickness in carotid endarterectomy tissues", "highlights": ["Calcification is abundant in carotid endarterectomy (CEA) tissues.", "Calcification is located predominantly in the bulb/bifurcation segments of CEAs.", "Histologically determined calcified patch sizes span across 4-orders of magnitude.", "Morphology of calcified patches changes significantly with patch size."], "abstract": ["Advanced atherosclerotic lesions are commonly characterized by the presence of calcification. Several studies indicate that extensive calcification is associated with plaque stability, yet recent studies suggest that calcification morphology and location may adversely affect the mechanical stability of atherosclerotic plaques. The underlying cause of atherosclerotic calcification and the importance of intra-plaque calcium distribution remains poorly understood.", "The goal of this study was the characterization of calcification morphology based on histological features in 20 human carotid endarterectomy (CEA) specimens. Representative frozen sections (10", "\u00a0", "\u03bcm thick) were cut from the common, bulb, internal and external segments of CEA tissues and stained with von Kossa\u05f3s reagent for calcium phosphate. The morphology of calcification (calcified patches) and fibrous layer thickness were quantified in 135 histological sections.", "Intra-plaque calcification was distributed heterogeneously (calcification %-area: bulb segment: 14.2\u00b12.1%; internal segment: 12.9\u00b12.8%; common segment: 4.6\u00b11.1%; ", "=0.001). Calcified patches were found in 20 CEAs (patch size: <0.1mm", " to >1.0mm", "). Calcified patches were most abundant in the bulb and least in the common segment (bulb ", "=7.30\u00b11.08; internal ", "=4.81\u00b11.17; common ", "=2.56\u00b10.56; ", "=0.0007). Calcified ", "atch circularity decreased with increasing size (<0.1", "\u00a0", "mm", ": 0.77\u00b10.01, 0.1\u20131", "\u00a0", "mm", ": 0.62\u00b10.01, >1.0", "\u00a0", "mm", ": 0.51\u00b10.02; ", "=0.0001). A reduced fibrous layer thickness was associated with increased calcium patch size (", "<0.0001).", "In advanced carotid atherosclerosis, calcification appears to be a heterogeneous and dynamic atherosclerotic plaque component, as indicated by the simultaneous presence of few large stabilizing calcified patches and numerous small calcific patches. Future studies are needed to elucidate the associations of intra-plaque calcification size and distribution with atherothrombotic events."]},
{"title": "Numerical modeling of compensation mechanisms for peripheral arterial stenoses", "highlights": ["Dimensionally reduced models for arterial blood flow are presented.", "The impact of peripheral arterial stenoses on blood flow is studied.", "It is investigated to what extent metabolic regulation and arteriogenesis are able to recover reduced blood flow caused by a peripheral stenosis."], "abstract": ["The goal of this paper is to develop a numerical model for physiological mechanisms that help to compensate reduced blood flow caused by a peripheral arterial stenosis. Thereby we restrict ourselves to the following compensation mechanisms: Metabolic regulation and arteriogenesis, i.e., growth of pre-existing collateral arteries. Our model is based on dimensionally reduced differential equations to simulate large time periods with low computational cost. As a test scenario, we consider a stenosis located in the right posterior tibial artery of a human. We study its impact on blood supply for different narrowing degrees by the help of numerical simulations. Moreover, the efficiency of the above compensation mechanisms is examined. Our results reveal that even a complete occlusion of this artery exhibiting a cross-section area of 0.442", "\u00a0", "cm", " can be compensated at rest, if metabolic regulation is combined with collateral arteries whose total cross-section area accounts for 8.14% of the occluded artery."]},
{"title": "Detection of small bowel tumor based on multi-scale curvelet analysis and fractal technology in capsule endoscopy", "highlights": ["89 valid capsule videos were collected from 105 clinical trials with new developed WCE system.", "A novel automatic computer-aid system is proposed for small bowel tumor detection.", "The textural feature set is extracted by using discrete curvelet transform and fractal technology.", "The GA\u2013SVM method is proposed for feature selection and WCE image classification.", "Extensive comparison experiments demonstrate that the proposed system outperforms recent approaches."], "abstract": ["Wireless capsule endoscopy (WCE) has been a revolutionary technique to noninvasively inspect gastrointestinal (GI) tract diseases, especially small bowel tumor. However, it is a tedious task for physicians to examine captured images. To develop a computer-aid diagnosis tool for relieving the huge burden of physicians, the intestinal video data from 89 clinical patients with the indications of potential tumors was analyzed. Out of the 89 patients, 15(16.8%) were diagnosed with small bowel tumor. A novel set of textural features that integrate multi-scale curvelet and fractal technology were proposed to distinguish normal images from tumor images. The second order textural descriptors as well as higher order moments between different color channels were computed from images synthesized by the inverse curvelet transform of the selected scales. Then, a classification approach based on support vector machine (SVM) and genetic algorithm (GA) was further employed to select the optimal feature set and classify the real small bowel images. Extensive comparison experiments validate that the proposed automatic diagnosis scheme achieves a promising tumor classification performance of 97.8% sensitivity and 96.7% specificity in the selected images from our clinical data."]},
{"title": "Assessment of fetal maturation age by heart rate variability measures using random forest methodology", "highlights": ["Random Forest identifies maturation related hrv parameters without pre-selection.", "Complexity indices in short time scales and skewness mainly predicted fetal maturation.", "Traditional hrv parameters did not contribute considerably to the prediction.", "Prediction with Random Forest is comparable to multivariate linear regression."], "abstract": ["Fetal maturation age assessment based on heart rate variability (HRV) is a predestinated tool in prenatal diagnosis. To date, almost linear maturation characteristic curves are used in univariate and multivariate models. Models using complex multivariate maturation characteristic curves are pending.", "To address this problem, we use Random Forest (RF) to assess fetal maturation age and compare RF with linear, multivariate age regression. We include previously developed HRV indices such as traditional time and frequency domain indices and complexity indices of multiple scales.", "We found that fetal maturation was best assessed by complexity indices of short scales and skewness in state-dependent datasets (quiet sleep, active sleep) as well as in state-independent recordings. Additionally, increasing fluctuation amplitude contributed to the model in the active sleep state. None of the traditional linear HRV parameters contributed to the RF models. Compared to linear, multivariate regression, the mean prediction of gestational age (GA) is more accurate with RF than in linear, multivariate regression (quiet state: ", " vs. ", ", active state: ", " vs. ", ", state independent: ", " vs. ", ").", "We conclude that classification and regression tree models such as RF methodology are appropriate for the evaluation of fetal maturation age. The decisive role of adjustments between different time scales of complexity may essentially extend previous analysis concepts mainly based on rhythms and univariate complexity indices. Those system characteristics may have implication for better understanding and accessibility of the maturating complex autonomic control and its disturbance."]},
{"title": "Computer simulations of pressure and velocity fields in a human upper airway during sneezing", "highlights": ["A computational study of velocity and pressure fields during a sneeze was performed.", "Both normal and suppressed sneezes were investigated.", "Air pressure along the airway during a sneeze is much higher than normal breathing.", "Suppressing the sneeze increases the pressure in the respiratory tract markedly.", "The increased pressure can explain the reasons of reported injuries during a sneeze."], "abstract": ["In this paper, the airflow field including the velocity, pressure and turbulence intensity distributions during sneezing of a female subject was simulated using a computational fluid dynamics model of realistic upper airways including both oral and nasal cavities. The effects of variation of reaction of the subject during sneezing were also investigated. That is, the impacts of holding the nose or closing the mouth during sneezing on the pressure and velocity distributions were studied. Few works have studied the sneeze and therefore different aspects of this phenomenon have remained unknown. To cover more possibilities about the inlet condition of trachea in different sneeze scenarios, it was assumed that the suppressed sneeze happens with either the same inlet pressure or the same flow rate as the normal sneeze. The simulation results showed that during a normal sneeze, the pressure in the trachea reaches about 7000", "\u00a0", "Pa, which is much higher than the pressure level of about 200", "\u00a0", "Pa during the high activity exhalation. In addition, the results showed that, suppressing the sneeze by holding the nose or mouth leads to a noticeable increase in pressure difference in the tract. This increase was about 5 to 24 times of that during a normal sneeze. This significant rise in the pressure can justify some reported damage due to suppressing a sneeze."]},
{"title": "Computational comparison of three posterior lumbar interbody fusion techniques by using porous titanium interbody cages with 50% porosity", "highlights": ["A numerical model containing porous cages and a lumbar segment was developed.", "The bone tissue ingrowth into the pores of the cage is considered.", "Bone fusion enhances the stability of the lumbar segment with a porous cage.", "Bone fusion also reduces the peak von Mises stress in the cortical bone and porous cage."], "abstract": ["This study investigated the biomechanical response of porous cages and lumbar spine segments immediately after surgery and after bone fusion, in addition to the long-term effects of various posterior lumbar interbody fusion (PLIF) techniques, by using the finite element method. Lumbar L3-L4 models based on three PLIF techniques (a single cage at the center of the intervertebral space, a single cage half-anterior to the intervertebral space, and two cages bilateral to the intervertebral space) with and without bone ingrowth were used to determine the biomechanical response of porous cages and lumbar segments instrumented with porous titanium cages (cage porosity=50%, pore diameter=1", "\u00a0", "mm). The results indicated that bone fusion enhanced the stability of the lumbar segments with porous cages without any posterior instrumentation and reduced the peak von Mises stress in the cortical bones and porous cages. Two cages placed bilateral to the intervertebral space achieved the highest structural stability in the lumbar segment and lowest von Mises stress in the cages under both bone fusion conditions. Under identical loading (2-N", "\u00a0", "m), the range of motion in the single cage at the center of the intervertebral space with bone fusion decreased by 11% (from 1.18\u00b0 to 1.05\u00b0) during flexion and by 66.5% (from 4.46\u00b0 to 1.5\u00b0) during extension in the single cage half-anterior to the intervertebral space with bone fusion compared with no-fusion models. Thus, two porous titanium cages with 50% porosity can achieve high stability of a lumbar segment with PLIF. If only one cage is available, placing the cage half-anterior to the intervertebral space is recommended for managing degenerated lumbar segments."]},
{"title": "Automatic cytoplasm and nuclei segmentation for color cervical smear image using an efficient gap-search MRF", "highlights": ["We proposed a novel gap-search Markov random field (MRF) for accurate cervical smear image segmentation.", "This method could acquire three regions (nuclei, cytoplasm, and background) automatically by a label-map mechanism.", "The gap-search algorithm is faster than other three algorithms in the experiments.", "A copy of source codes will be released as an open source project for continuing studies."], "abstract": ["Accurate and effective cervical smear image segmentation is required for automated cervical cell analysis systems. Thus, we proposed a novel superpixel-based Markov random field (MRF) segmentation framework to acquire the nucleus, cytoplasm and image background of cell images. We seek to classify color non-overlapping superpixel-patches on one image for image segmentation. This model describes the whole image as an undirected probabilistic graphical model and was developed using an automatic label-map mechanism for determining nuclear, cytoplasmic and background regions. A gap-search algorithm was designed to enhance the model efficiency. Data show that the algorithms of our framework provide better accuracy for both real-world and the public Herlev datasets. Furthermore, the proposed gap-search algorithm of this model is much more faster than pixel-based and superpixel-based algorithms."]},
{"title": "Detection of bladder metabolic artifacts in ", "highlights": ["A novel method for detecting bladder metabolic artifacts in 18F-FDG PET images.", "A ", "-means clustering method combining both multimodal PET-CT data and spatial information.", "The method was tested on 52 PET/CT images of cervical cancer patients.", "The method suppresses most of the bladder artifacts while preserving tumor uptake."], "abstract": ["Positron emission tomography using ", "F-fluorodeoxyglucose (", "F-FDG-PET) is a widely used imaging modality in oncology. It enables significant functional information to be included in analyses of anatomical data provided by other image modalities. Although PET offers high sensitivity in detecting suspected malignant metabolism, ", "F-FDG uptake is not tumor-specific and can also be fixed in surrounding healthy tissue, which may consequently be mistaken as cancerous. PET analyses may be particularly hampered in pelvic-located cancers by the bladder\u05f3s physiological uptake potentially obliterating the tumor uptake. In this paper, we propose a novel method for detecting ", "F-FDG bladder artifacts based on a multi-feature double-step classification approach. Using two manually defined seeds (tumor and bladder), the method consists of a semi-automated double-step clustering strategy that simultaneously takes into consideration standard uptake values (SUV) on PET, Hounsfield values on computed tomography (CT), and the distance to the seeds. This method was performed on 52 PET/CT images from patients treated for locally advanced cervical cancer. Manual delineations of the bladder on CT images were used in order to evaluate bladder uptake detection capability. Tumor preservation was evaluated using a manual segmentation of the tumor, with a threshold of 42% of the maximal uptake within the tumor. Robustness was assessed by randomly selecting different initial seeds. The classification averages were ", " for sensitivity, ", " specificity, and ", " accuracy. These results suggest that this method is able to detect most ", "F-FDG bladder metabolism artifacts while preserving tumor uptake, and could thus be used as a pre-processing step for further non-parasitized PET analyses."]},
{"title": "Automated retinal image quality assessment on the UK Biobank dataset for epidemiological studies", "highlights": ["Changes in retinal vasculature prospectively associated with disease outcomes.", "Large population based studies help to resolve uncertainties in these associations.", "QUARTZ software extracts morphometric data from large numbers of retinal images.", "Automated image quality assessment is required to achieve full automation.", "This addition into QUARTZ makes processing the entire UK Biobank dataset feasible."], "abstract": ["Morphological changes in the retinal vascular network are associated with future risk of many systemic and vascular diseases. However, uncertainty over the presence and nature of some of these associations exists. Analysis of data from large population based studies will help to resolve these uncertainties. The QUARTZ (QUantitative Analysis of Retinal vessel Topology and siZe) retinal image analysis system allows automated processing of large numbers of retinal images. However, an image quality assessment module is needed to achieve full automation. In this paper, we propose such an algorithm, which uses the segmented vessel map to determine the suitability of retinal images for use in the creation of vessel morphometric data suitable for epidemiological studies. This includes an effective 3-dimensional feature set and support vector machine classification. A random subset of 800 retinal images from UK Biobank (a large prospective study of 500,000 middle aged adults; where 68,151 underwent retinal imaging) was used to examine the performance of the image quality algorithm. The algorithm achieved a sensitivity of 95.33% and a specificity of 91.13% for the detection of inadequate images. The strong performance of this image quality algorithm will make rapid automated analysis of vascular morphometry feasible on the entire UK Biobank dataset (and other large retinal datasets), with minimal operator involvement, and at low cost."]},
{"title": "Improvement of automated image stitching system for DR X-ray images", "highlights": ["A new stitching method was proposed for X-ray images of full-spine and lower limb.", "The multi-time phase correlation solves the problem of inaccuracy of overlapping region.", "The overlapping region is less than 30% that can also be stitching successfully.", "It can tackle problem as image translation and rotation.", "The proposed method has a higher accuracy and a shorter average stitching time compared with previous methods."], "abstract": ["The full bone structure of X-ray images cannot be captured in a single scan with Digital radiography (DR) system. The stitching method of X-ray images is very important for scoliosis or lower limb malformation diagnosing and pre-surgical planning. Based on the image registration technology, this paper proposes a new automated image stitching method for full-spine and lower limb X-ray images. The stitching method utilized down-sampling to decrease the size of image and reduce the amount of computation; improved phase correlation algorithm was adopted to find the overlapping region; correlation coefficient was used to evaluate the similarity of overlapping region; weighted blending is brought in to produce a panorama image. The performance of the proposed method was evaluated by 40 pairs of images from patients with scoliosis or lower limb malformation. The stitching method was fully automated without any user input required. The experimental results were compared with previous methods by analyzing the same database. It is demonstrated that the improved phase correlation has higher accuracy and shorter average stitching time than previous methods. It could tackle problems including image translation, rotation and small overlapping in image stitching."]},
{"title": "A neural algorithm for the non-uniform and adaptive sampling of biomedical data", "highlights": ["Algorithm to under-sample data with a frequency lower than Nyquist limit.", "An adaptive neural predictor selects the subsequent samples to be measured.", "Example applications: EMG, ECG, EEG and acceleration data.", "The method outperforms uniform sampling and compressive sensing.", "Many potential applications to save energy and to reduce the memory storage."], "abstract": ["Body sensors are finding increasing applications in the self-monitoring for health-care and in the remote surveillance of sensitive people. The physiological data to be sampled can be non-stationary, with bursts of high amplitude and frequency content providing most information. Such data could be sampled efficiently with a non-uniform schedule that increases the sampling rate only during activity bursts.", "A real time and adaptive algorithm is proposed to select the sampling rate, in order to reduce the number of measured samples, but still recording the main information. The algorithm is based on a neural network which predicts the subsequent samples and their uncertainties, requiring a measurement only when the risk of the prediction is larger than a selectable threshold.", "Four examples of application to biomedical data are discussed: electromyogram, electrocardiogram, electroencephalogram, and body acceleration. Sampling rates are reduced under the Nyquist limit, still preserving an accurate representation of the data and of their power spectral densities (PSD). For example, sampling at 60% of the Nyquist frequency, the percentage average rectified errors in estimating the signals are on the order of 10% and the PSD is fairly represented, until the highest frequencies. The method outperforms both uniform sampling and compressive sensing applied to the same data.", "The discussed method allows to go beyond Nyquist limit, still preserving the information content of non-stationary biomedical signals. It could find applications in body sensor networks to lower the number of wireless communications (saving sensor power) and to reduce the occupation of memory."]},
{"title": "Spectral Heart Rate Variability analysis using the heart timing signal for the screening of the Sleep Apnea\u2013Hypopnea Syndrome", "highlights": ["Screening of SAHS using spectral Heart Rate Variability analysis is assessed.", "Comparison between Heart Timing, Heart Rate and Heart Period signals.", "Analysis of LF/HF and VLF features derived from clinical HRV spectral bands.", "Statistical differences found between patients and controls.", "Different spectral estimation methods analyzed: no statistical differences found."], "abstract": ["Some approaches have been published in the past using Heart Rate Variability (HRV) spectral features for the screening of Sleep Apnea\u2013Hypopnea Syndrome (SAHS) patients. However there is a big variability among these methods regarding the selection of the source signal and the specific spectral components relevant to the analysis. In this study we investigate the use of the Heart Timing (HT) as the source signal in comparison to the classical approaches of Heart Rate (HR) and Heart Period (HP). This signal has the theoretical advantage of being optimal under the Integral Pulse Frequency Modulation (IPFM) model assumption. Only spectral bands defined as standard for the study of HRV are considered, and for each method the so-called LF/HF and VLF", " features are derived. A comparative statistical analysis between the different resulting methods is performed, and subject classification is investigated by means of ROC analysis and a Na\u00efve-Bayes classifier. The standard Apnea-ECG database is used for validation purposes. Our results show statistical differences between SAHS patients and controls for all the derived features. In the subject classification task the best performance in the testing set was obtained using the LF/HF ratio derived from the HR signal (Area under ROC curve=0.88). Only ", "light differences are obtained due to the effect of changing the source signal. The impact of using the HT signal in this domain is therefore limited, and has not shown relevant differences with respect to the use of the classical approaches of HR or HP."]},
{"title": "Enhancement and bias removal of optical coherence tomography images: An iterative approach with adaptive bilateral filtering", "highlights": ["A new speckle filters based on Gamma statistics is proposed for the enhancement of OCT images.", "With AUB filter, bias is estimated with a local ML estimator and removed from the adaptive bilateral filter output.", "With IAUB filter, an iterative approach further refines the AUB filter output.", "Experiments show that the proposed frameworks provide better performance compared to the state-of-the-art methods."], "abstract": ["Optical coherence tomography (OCT) has continually evolved and expanded as one of the most valuable routine tests in ophthalmology. However, noise (speckle) in the acquired images causes quality degradation of OCT images and makes it difficult to analyze the acquired images. In this paper, an iterative approach based on bilateral filtering is proposed for speckle reduction in multiframe OCT data. Gamma noise model is assumed for the observed OCT image. First, the adaptive version of the conventional bilateral filter is applied to enhance the multiframe OCT data and then the bias due to noise is reduced from each of the filtered frames. These unbiased filtered frames are then refined using an iterative approach. Finally, these refined frames are averaged to produce the denoised OCT image. Experimental results on phantom images and real OCT retinal images demonstrate the effectiveness of the proposed filter."]},
{"title": "Advising patients on selecting trustful apps for diabetes self-care", "highlights": ["A proposed method to help patients identify trustful apps for diabetes self-care.", "A pictorial identification schema is used to review diabetes self-care apps.", "This method does not require specific skills.", "Two different profiles apply the tool on the selected apps and discuss the results."], "abstract": ["There has been a dramatic increase in mobile apps for diabetes self-care. However, their quality is not guaranteed and patients do not have the appropriate tools for careful evaluation.", "This work aims to propose a tool to help patients with diabetes select an appropriate app for self-care.", "After identifying the conceptual framework of diabetes self-care, we searched Apple US app store and reviewed diabetes self-care apps, considering both generic and diabetes-specific features. Based on an existing tool for representing the benefits and weaknesses of medical apps, we created the pictorial identification schema/Diabetes Self-care tool, which specifically identified medical apps in the diabetes domain.", "Of the 952 apps retrieved, 67 were for diabetes self-care, while 26 were excluded because they were not updated in the last 12 months. Of the remaining 41, none cost more than 15 USD, and 36 implemented manual data entry. Basic features (data logging, data representation, and data delivery) were implemented in almost all apps, whereas advanced features (e.g., insulin calculator) were implemented in a small percentage of apps. The pictorial identification schema for diabetes was completed by one patient and one software developer for 13 apps. Both users highlighted weaknesses related to the functionalities offered and to their interface, but the patient focused on usability, whereas the software developer focused on technical implementation.", "The Pictorial Identification Schema/Diabetes Self-care is a promising graphical tool for perceiving the weaknesses and benefits of a diabetes self-care app that includes multiple user profile perspectives."]},
{"title": "Feature-based MRI data fusion for cardiac arrhythmia studies", "highlights": ["MRI techniques can be used to capture and characterize cardiac states.", "Feature-based data fusion combines and enhances MRI data.", "Canonical Correlation Analysis is an effective tool to fuse and quantify MRI data."], "abstract": ["Current practices in studying cardiac arrhythmias primarily use electrical or optical surface recordings of a heart, spatially limited transmural recordings, and mathematical models. However, given that such arrhythmias occur on a 3D myocardial tissue, information obtained from such practices lack in dimension, completeness, and are sometimes prone to oversimplification. The combination of complementary Magnetic-Resonance Imaging (MRI)-based techniques such as Current Density Imaging (CDI) and Diffusion Tensor Imaging (DTI) could provide more depth to current practices in assessing the cardiac arrhythmia dynamics in entire cross sections of myocardium. In this work, we present an approach utilizing feature-based data fusion methods to demonstrate that complimentary information obtained from electrical current distribution and structural properties within a heart could be quantified and enhanced. Twelve (12) pairs of CDI and DTI image data sets were gathered from porcine hearts perfused through a Langendorff setup. Images were fused together using feature-based data fusion techniques such as Joint Independent Component Analysis (jICA), Canonical Correlation Analysis (CCA), and their combination (CCA+jICA). The results suggest that the complimentary information of cardiac states from CDI and DTI are enhanced and are better classified with the use of data fusion methods. For each data set, an increase in mean correlations of fused images were observed with 38% increase from CCA+jICA compared to the original images while mean mutual information of the fused images from jICA and CCA+jICA increased by approximately three-fold. We conclude that MRI-based techniques present potential viable tools in furthering studies for cardiac arrhythmias especially Ventricular Fibrillation."]},
{"title": "Functional grouping of similar genes using eigenanalysis on minimum spanning tree based neighborhood graph", "highlights": ["Eigenanalysis on spanning tree neighborhood graph(E-MST) for clustering is introduced.", "Diameter based criteria to determine the sufficiency of neighborhood is proposed.", "The objective criterion reveals spectral properties of neighborhood graph.", "Experimental analysis shows the impressive results of E-MST."], "abstract": ["Gene expression data clustering is an important biological process in DNA microarray analysis. Although there have been many clustering algorithms for gene expression analysis, finding a suitable and effective clustering algorithm is always a challenging problem due to the heterogeneous nature of gene profiles. Minimum Spanning Tree (MST) based clustering algorithms have been successfully employed to detect clusters of varying shapes and sizes. This paper proposes a novel clustering algorithm using Eigenanalysis on Minimum Spanning Tree based neighborhood graph (E-MST). As MST of a set of points reflects the similarity of the points with their neighborhood, the proposed algorithm employs a similarity graph obtained from ", " rounds of MST (", "-MST neighborhood graph). By studying the spectral properties of the similarity matrix obtained from ", "-MST graph, the proposed algorithm achieves improved clustering results. We demonstrate the efficacy of the proposed algorithm on 12 gene expression datasets. Experimental results show that the proposed algorithm performs better than the standard clustering algorithms."]},
{"title": "Digital modeling technology for full dental crown tooth preparation", "highlights": ["A new automatic digital modeling technology is proposed to design full dental crown tooth preparation.", "Margin line of the tooth preparation could be designed in advance.", "The implicit surface of a radial basis function was used to quickly and accurately construct the tooth preparation\u05f3s occlusal surface.", "Our method can guarantee the design accuracy of the full crown tooth preparation (e.g. the shoulder width and the convergence)."], "abstract": ["A dental defect is one of the most common oral diseases, and it often requires a full crown restoration. In this clinical operation, the dentist must manually prepare the affected tooth for the full crown so that it has a convergence angle between 4\u00b0 and 10\u00b0, no undercuts, and uniform and even shoulder widths and depths using a high speed diamond bur in the patient\u05f3s mouth within one hour, which is a difficult task that requires visual\u2013manual operation. The quality of the tooth preparation has an important effect on the success rate of the subsequent prosthodontic treatment. This study involved research into digital modeling technology for full dental crown tooth preparation. First, the margin line of the tooth preparation was designed using a semi-automatic interactive process. Second, the inserting direction was automatically computed. Then, the characteristic parameters and the constraints on the tooth preparation were defined for the model. Next, the shoulder and axial surface of the tooth preparation were formed using parametric modeling. Finally, the implicit surface of a radial basis function was used to construct the tooth preparation\u05f3s occlusal surface. The experimental results verified that the method of digital modeling for full crown preparation proposed in this study can quickly and accurately implement personalized designs of various parameters, such as the shoulder width and the convergence angle; it provides a digital design tool for full crown preparation."]},
{"title": "Formation of reentrant circuits in the mid-myocardial infarct border zone", "highlights": ["Ventricular tachycardia is commonly caused by a reentrant circuit.", "Reentrant circuits are not always mappable on endocardial or epicardial surfaces.", "In the study, a model is presented of mid-myocardial reentrant tachycardia.", "The model is based on convex wavefront curvature, which causes functional block.", "Both slabs and conduits of surviving viable mid-myocardial tissue can support reentry."], "abstract": ["In this study, the mechanisms for onset and maintenance of mid-myocardial (intramural) reentrant circuits are considered, based upon anatomical structure.", "A model of electrical activation wavefront curvature in the mid-myocardial postinfarction border zone is developed. Two arrhythmogenic structures are considered: 1. a constrained slab of viable tissue, and 2. a strand of surviving myocardial fibers with distal expansion. Equations are formulated to estimate activation coupling intervals, and ranges in taper and circuit dimensions, that will support functional conduction block during premature stimulation and reentrant ventricular tachycardia.", "For onset and maintenance of reentry, the arrhythmogenic regions forming both slab and strand circuits are in the range of 50\u2013600", "\u00a0", "\u00b5m at their thinnest dimension. For constrained slabs, unidirectional block leading to reentry forms in the thin-to-thick direction during premature stimulation, and functional block at lateral boundaries enable formation of a double-loop circuit. The activation wavefront proceeds around the impediment and then curves in the opposite direction through the slab, reentering the previously excited tissue. For strands, unidirectional block forms at a distal expansion in response to premature stimulation. The strand reentrant circuit is bounded by infarcted tissue causing anatomical block, and can be single-loop or coaxial. For all architectures, circuit dimensions ranging from 1.6\u00d71.6", "\u00a0", "mm to 3.5\u00d73.5", "\u00a0", "mm support functional block when premature stimulus coupling intervals are 117\u2013150", "\u00a0", "ms and ventricular tachycardia cycle lengths are 160\u2013350", "\u00a0", "ms.", "For slab and strand mid-myocardial arrhythmogenic structures, taper and circuit dimensions govern ranges in premature excitation coupling intervals and tachycardia cycle lengths necessary to support functional block."]},
{"title": "Optimum wavelet based masking for the contrast enhancement of medical images using enhanced cuckoo search algorithm", "highlights": ["An Optimum Wavelet Based Masking (OWBM) algorithm for medical image enhancement.", "Hybrid algorithm with cuckoo search and adaptive genetic operators is proposed.", "Dynamic nest rebuilding of the cuckoo search is performed.", "The proposed algorithm performed better results compared to the reported medical enhancement techniques."], "abstract": ["Unsharp masking techniques are a prominent approach in contrast enhancement. Generalized masking formulation has static scale value selection, which limits the gain of contrast. In this paper, we propose an Optimum Wavelet Based Masking (OWBM) using Enhanced Cuckoo Search Algorithm (ECSA) for the contrast improvement of medical images. The ECSA can automatically adjust the ratio of nest rebuilding, using genetic operators such as adaptive crossover and mutation. First, the proposed contrast enhancement approach is validated quantitatively using Brain Web and MIAS database images. Later, the conventional nest rebuilding of cuckoo search optimization is modified using Adaptive Rebuilding of Worst Nests (ARWN). Experimental results are analyzed using various performance matrices, and our OWBM shows improved results as compared with other reported literature."]},
{"title": "Predicting bacteriophage proteins located in host cell with feature selection technique", "highlights": ["Novel analytical method is developed to predict the phage proteins located in host cell.", "A significant feature selection technique is proposed and used to optimize features of proteins", "A powerful predictor is constructed to identify phage proteins distribution in host cell."], "abstract": ["A bacteriophage is a virus that can infect a bacterium. The fate of an infected bacterium is determined by the bacteriophage proteins located in the host cell. Thus, reliably identifying bacteriophage proteins located in the host cell is extremely important to understand their functions and discover potential anti-bacterial drugs. Thus, in this paper, a computational method was developed to recognize bacteriophage proteins located in host cells based only on their amino acid sequences. The analysis of variance (ANOVA) combined with incremental feature selection (IFS) was proposed to optimize the feature set. Using a jackknife cross-validation, our method can discriminate between bacteriophage proteins located in a host cell and the bacteriophage proteins not located in a host cell with a maximum overall accuracy of 84.2%, and can further classify bacteriophage proteins located in host cell cytoplasm and in host cell membranes with a maximum overall accuracy of 92.4%. To enhance the value of the practical applications of the method, we built a web server called PHPred (\u3008", "\u3009). We believe that the PHPred will become a powerful tool to study bacteriophage proteins located in host cells and to guide related drug discovery."]},
{"title": "In silico design of high-affinity ligands for the immobilization of inulinase", "highlights": ["For the immobilization matrices we revealed the binding site with the inulinase.", "Ion exchange resins and fibers were experimentally tested.", "The maximal affinity to the inulinase was possessed by the matrix of KU-2.", "Affinities were in good agreement with catalytic activity of immobilized inulinase.", "Correlation factors are \u22120.84 (for yeast inulinase) and \u22120.81 (for plant inulinase)."], "abstract": ["Using computer modeling, virtual screening of high-affinity ligands for immobilization of inulinase \u2013 an enzyme that cleaves inulin and fructose-containing polymers to fructose \u2013 has been performed. The inulinase molecule from ", " (pdb: 3SC7) taken from the database of protein structures was used as a protein model and the target for flexible docking.", "The set of ligands studied included simple sugars (activators, inhibitors, products of enzymatic catalysis), as well as high-molecular weight compounds (polycation and polyanion exchange resins, glycoproteins, phenylalanine-proline peptide, polylactate, and caffeine). Based on the comparative analysis of the values of the total energy and the localization of ligand binding sites, we made several assumptions concerning the mechanisms of interaction of the suggested matrices for the immobilization of enzyme molecules and the structural features of such complexes. It was also assumed that the candidates for immobilization agents meeting the industrial requirements may be glycoproteins, for which we propose an additional incorporation of cysteine residues into their structure, aimed to create disulfide \u00abanchors\u00bb to the surface."]},
{"title": "Simulation study of electric-guided delivery of 0.4", "highlights": ["The performance of an intranasal delivery system targeting the ostiomeatal complex (OMC) was numerically evaluated in an image-based nose-sinus model for monodisperse and polydisperse aerosols.", "With the synergy of optimized point-release and electric control, an OMC delivery efficiency of 34.4% was predicted for the polydisperse charged aerosols.", "Sensitivity analysis showed that OMC deposition was highly sensitive to particle charge and size and less sensitive to the inhalation flow rate.", "Simulations showed significantly enhanced OMC dosage using the proposed delivery system than standard nasal devices."], "abstract": ["Despite the high prevalence of rhinosinusitis, current inhalation therapy shows limited efficacy due to extremely low drug delivery efficiency to the paranasal sinuses. Novel intranasal delivery systems are needed to enhance targeted delivery to the sinus with therapeutic dosages. An optimization framework for intranasal drug delivery was developed to target polydisperse charged aerosols to the ostiomeatal complex (OMC) with electric guidance. The delivery efficiency of a group of charged aerosols recently reported in the literature was numerically assessed and optimized in an anatomically accurate nose-sinus model. Key design variables included particle charge number, particle size and distribution, electrode strength, and inhalation velocity. Both monodisperse and polydisperse aerosol profiles were considered. Results showed that the OMC delivery efficiency was highly sensitive to the applied electric field and electrostatic charges carried by the particles. Through the synthesis of electric-guidance and point drug release, focused deposition with significantly enhanced dosage in the OMC can be achieved. For 0.4", "\u00a0", "\u00b5m charged aerosols, an OMC delivery efficiency of 51.6% was predicted for monodisperse aerosols and 34.4% for polydisperse aerosols. This difference suggested that the aerosol profile exerted a notable effect on intranasal deliveries. Sensitivity analysis indicated that the OMC deposition fraction was highly sensitive to the charge and size of particles and was less sensitive to the inhalation velocity considered in this study. Experimental studies are needed to validate the numerically optimized designs. Further studies are warranted to investigate the targeted OMC delivery with both electric and acoustics controls, the latter of which has the potential to further deliver the drug particles into the sinus cavity."]},
{"title": "Patient non-specific algorithm for seizures detection in scalp EEG", "highlights": ["It reduces the time spent by physicians in the visual inspection of long term EEG records.", "It is patient non-specific, so it can be applied to new records that have not been presented to the algorithm.", "Its performance is comparable with other more complicated algorithms."], "abstract": ["Epilepsy is a brain disorder that affects about 1% of the population in the world. Seizure detection is an important component in both the diagnosis of epilepsy and seizure control. In this work a patient non-specific strategy for seizure detection based on Stationary Wavelet Transform of EEG signals is developed. A new set of features is proposed based on an average process. The seizure detection consisted in finding the EEG segments with seizures and their onset and offset points. The proposed offline method was tested in scalp EEG records of 24\u201348", "\u00a0", "h of duration of 18 epileptic patients. The method reached mean values of specificity of 99.9%, sensitivity of 87.5% and a false positive rate per hour of 0.9."]},
{"title": "Multi-scale texture-based level-set segmentation of breast B-mode images", "highlights": ["Multi-scale texture-based level-set segmentation of breast B-mode images.", "Computation of a fine and delicate as well as a coarse lesion contour.", "Evaluation by manually delineated and a corrected lesion contour.", "Improvements in contour-based and region-based error metrics w.r.t state-of-the-art.", "Potential contribution to lesion classification via fine and coarse area difference."], "abstract": ["Automatic segmentation of ultrasonographic breast lesions is very challenging, due to the lesions\u2032 spiculated nature and the variance in shape and texture of the B-mode ultrasound images. Many studies have tried to answer this challenge by applying a variety of computational methods including: Markov random field, artificial neural networks, and active contours and level-set techniques. These studies focused on creating an automatic contour, with maximal resemblance to a manual contour, delineated by a trained radiologist. In this study, we have developed an algorithm, designed to capture the spiculated boundary of the lesion by using the properties from the corresponding ultrasonic image. This is primarily achieved through a unique multi-scale texture identifier (inspired by visual system models) integrated in a level-set framework. The algorithm\u05f3s performance has been evaluated quantitatively via contour-based and region-based error metrics. We compared the algorithm\u2013generated contour to a manual contour delineated by an expert radiologist. In addition, we suggest here a new method for performance evaluation where corrections made by the radiologist replace the algorithm-generated (original) result in the correction zones. The resulting corrected contour is then compared to the original version. The evaluation showed: (1) Mean absolute error of 0.5 pixels between the original and the corrected contour; (2) Overlapping area of 99.2% between the lesion regions, obtained by the algorithm and the corrected contour. These results are significantly better than those previously reported. In addition, we have examined the potential of our segmentation results to contribute to the discrimination between malignant and benign lesions."]},
{"title": "Post-processing techniques for making reliable measurements from curve-skeletons", "highlights": ["Post-processing techniques for making reliable measurements from curve-skeletons.", "Based on a sampling of an object on a scale related to its local thickness.", "User independent.", "Applied to computed tomography scans of trabecular bone and tumour vasculature.", "Measured lengths and cross-sectional thicknesses much less sensitive to noise."], "abstract": ["Interconnected 3-D networks occur widely in biology and the geometry of such branched networks can be described by curve-skeletons, allowing parameters such as path lengths, path tortuosities and cross-sectional thicknesses to be quantified. However, curve-skeletons are typically sensitive to small scale surface features which may arise from noise in the imaging data. In this paper, new post-processing techniques for curve-skeletons are presented which ensure that measurements of lengths and thicknesses are less sensitive to these small scale surface features. The techniques achieve sub-voxel accuracy and are based on a minimal sphere-network representation in which the object is modelled as a string of minimally overlapping spheres, and as such samples the object on a scale related to the local thickness. A new measure of cross-sectional dimension termed the modal radius is defined and shown to be more robust in comparison with the standard measure (the internal radius), while retaining the desirable feature of capturing the size of structures in terms of a single measure. The techniques are demonstrated by application to trabecular bone and tumour vascular network case studies where the volumetric data was obtained by high resolution computed tomography."]},
{"title": "Robust semi-automated quantification of cardiac MR perfusion using level set: Application to hypertrophic cardiomyopathy patient data", "highlights": ["Semi-automated quantification of multi-slice cardiac MR perfusion is proposed.", "LV center and RV insertion points are manually chosen, and remaining steps are automated.", "Myocardium is automatically segmented using distance regularized level set evolution.", "Accuracy of myocardial upslope quantification is evaluated on clinical HCM data."], "abstract": ["Recently there have been several clinical MR perfusion studies in patients with hypertrophic cardiomyopathy (HCM) who may suffer from myocardial ischemia due to coronary microvascular dysfunction. In these studies, data analysis relied on a manual procedure of tracing epicardial and endocardial borders. The goal of this work is to develop and validate a robust semi-automated analysis method for myocardial perfusion quantification in clinical HCM data.", "Dynamic multi-slice stress perfusion MRI data were acquired from 18 HCM patients. The proposed semi-automated method required user input of two landmark selections: LV center point and RV insertion point. Automated segmentations of the endocardial and epicardial borders were performed in three short-axis slices using distance regularized level set evolution on RV, LV, and myocardial enhancement frames.", "The proposed automated epicardial border detection method resulted in average radial distance errors of 7.5%, 9.5%, and 11.6% in basal, mid, and apical slices, respectively, when compared to manual tracing of the borders as a reference. In linear regression analysis, the highest correlation of myocardial upslope measurements was observed between the manual method and the proposed method in the anterolateral section (", "=0.964), and the lowest correlation was observed in the inferoseptal section (", "=0.866).", "The proposed semi-automated method for myocardial MR perfusion quantification is feasible in HCM patients who typically show (1) irregular myocardial shape and (2) low image contrast between the myocardium and its surrounding regions due to coronary microvascular disease."]},
{"title": "The dicrotic notch analyzed by a numerical model", "highlights": ["Aortic valve closure is the most widespread explanation for the dicrotic notch.", "Reflected pressure waves may participate in forming the dicrotic notch.", "A 1D numerical model based on Navier-Stokes equations can test this hypotheses.", "Changes in terminal resistance modify reflected waves and thus the dicrotic notch.", "Vasoconstrictors modify the morphology of the dicrotic notch in human subjects."], "abstract": ["Divergent concepts on the origin of the dicrotic notch are widespread in medical literature and education. Since most medical textbooks explain the origin of the dicrotic notch as caused by the aortic valve closure itself, this is commonly transmitted in medical physiology courses. We present clinical data and numerical simulations to demonstrate that reflected pressure waves could participate as one of the causes of the dicrotic notch. Our experimental data from continuous arterial pressure measurements from adult patients undergoing vascular surgery suggest that isolated changes in peripheral vascular resistance using an intravenous bolus of phenylephrine (a selective alpha 1-receptor agonist and thus a potent vasoconstrictor) modify the dicrotic notch. We then explore the mechanisms behind this phenomenon by using a numerical model based on integrated axisymmetric Navier-Stokes equations to compute the hemodynamic flow. Our model illustrates clearly how modifications in peripheral artery resistance may result in changes in the amplitude of the dicrotic notch by modifying reflected pressure waves. We believe that this could be a useful tool in teaching medical physiology courses."]},
{"title": "Identification of mutated driver pathways in cancer using a multi-objective optimization model", "highlights": ["An effective algorithm for identifying driver pathways in cancer is proposed.", "The algorithm relies on the high heterogeneity and coverage of driver pathways.", "The algorithm show credible performance without relying on prior knowledge.", "An integrated model is designed to improve the performance in a biological context."], "abstract": ["New-generation high-throughput technologies, including next-generation sequencing technology, have been extensively applied to solve biological problems. As a result, large cancer genomics projects such as the Cancer Genome Atlas (TCGA) and the International Cancer Genome Consortium are producing large amount of rich and diverse data in multiple cancer types. The identification of mutated driver genes and driver pathways from these data is a significant challenge. Genome aberrations in cancer cells can be divided into two types: random \u2018passenger mutation\u2019 and functional \u2018driver mutation\u2019. In this paper, we introduced a Multi-objective Optimization model based on a Genetic Algorithm (MOGA) to solve the maximum weight submatrix problem, which can be employed to identify driver genes and driver pathways promoting cancer proliferation. The maximum weight submatrix problem defined to find mutated driver pathways is based on two specific properties, i.e., high coverage and high exclusivity. The multi-objective optimization model can adjust the trade-off between high coverage and high exclusivity. We proposed an integrative model by combining gene expression data and mutation data to improve the performance of the MOGA algorithm in a biological context."]},
{"title": "Summarising the retinal vascular calibres in healthy, diabetic and diabetic retinopathy eyes", "highlights": ["A novel and improved method for quantifying the retinal vessel calibres was presented.", "Junction exponents were experimentally derived for different groups, based on the fractal dimension and the branch exponent.", "New formulas were introduced for summarising the vessel calibres in healthy, diabetic and diabetic retinopathy eyes.", "The improvement in terms of the mean absolute percentage error ranged from 0.24% to 0.49%."], "abstract": ["Retinal vessel calibre has been found to be an important biomarker of several retinal diseases, including diabetic retinopathy (DR). Quantifying the retinal vessel calibres is an important step for estimating the central retinal artery and vein equivalents. In this study, an alternative method to the already established branching coefficient (BC) is proposed for summarising the vessel calibres in retinal junctions. This new method combines the mean diameter ratio with an alternative to Murray\u05f3s cube law exponent, derived by the fractal dimension,experimentally, and the branch exponent of cerebral vessels, as has been suggested in previous studies with blood flow modelling. For the above calculations, retinal images from healthy, diabetic and DR subjects were used. In addition, the above method was compared with the BC and was also applied to the evaluation of arteriovenous ratio as a biomarker of progression from diabetes to DR in four consecutive years, i.e. three/two/one years before the onset of DR and the first year of DR. Moreover, the retinal arteries and veins around the optic nerve head were also evaluated. The new approach quantifies the vessels more accurately. The decrease in terms of the mean absolute percentage error was between 0.24% and 0.49%, extending at the same time the quantification beyond healthy subjects."]},
{"title": "Breast mass classification on mammograms using radial local ternary patterns", "highlights": ["RLTP takes into account the pattern orientation with respect to the lesion center.", "RLTP is useful for differentiating the circumscribed and spiculated margins.", "RLTP is superior to the RI-LBP for breast lesion classification.", "The proposed feature is segmentation free and robust to image rotation."], "abstract": ["Textural features can be useful in differentiating between benign and malignant breast lesions on mammograms. Unlike previous computerized schemes, which relied largely on shape and margin features based on manual contours of masses, textural features can be determined from regions of interest (ROIs) without precise lesion segmentation. In this study, therefore, we investigated an ROI-based feature, namely, radial local ternary patterns (RLTP), which takes into account the direction of edge patterns with respect to the center of masses for classification of ROIs for benign and malignant masses. Using an artificial neural network (ANN), support vector machine (SVM) and random forest (RF) classifiers, the classification abilities of RLTP were compared with those of the regular local ternary patterns (LTP), rotation invariant uniform (RIU2) LTP, texture features based on the gray level co-occurrence matrix (GLCM), and wavelet features. The performance was evaluated with 376 ROIs including 181 malignant and 195 benign masses. The highest areas under the receiver operating characteristic curves among three classifiers were 0.90, 0.77, 0.78, 0.86, and 0.83 for RLTP, LTP, RIU2-LTP, GLCM, and wavelet features, respectively. The results indicate the usefulness of the proposed texture features for distinguishing between benign and malignant lesions and the superiority of the radial patterns compared with the conventional rotation invariant patterns."]},
{"title": "Automatic segmentation of cartilage in high-field magnetic resonance images of the knee joint with an improved voxel-classification-driven region-growing algorithm using vicinity-correlated subsampling", "highlights": ["Voxel-classification-based segmentation suffers from high dimensionality of MR images.", "Subsampling of voxels and feature selection can reduce complexity of classification.", "Random voxel subsampling in training data causes poor cartilage segmentation results.", "Vicinity-correlated subsampling of voxels improves cartilage segmentation accuracies.", "A small subset of significant features is adequate to segment cartilage compartments."], "abstract": ["Anatomical structures that can deteriorate over time, such as cartilage, can be successfully delineated with voxel-classification approaches in magnetic resonance (MR) images. However, segmentation via voxel-classification is a computationally demanding process for high-field MR images with high spatial resolutions. In this study, the whole femoral, tibial, and patellar cartilage compartments in the knee joint were automatically segmented in high-field MR images obtained from Osteoarthritis Initiative using a voxel-classification-driven region-growing algorithm with sample-expand method. Computational complexity of the classification was alleviated via subsampling of the background voxels in the training MR images and selecting a small subset of significant features by taking into consideration systems with limited memory and processing power. Although subsampling of the voxels may lead to a loss of generality of the training models and a decrease in segmentation accuracies, effective subsampling strategies can overcome these problems. Therefore, different subsampling techniques, which involve uniform, Gaussian, vicinity-correlated (VC) sparse, and VC dense subsampling, were used to generate four training models. The segmentation system was experimented using 10 training and 23 testing MR images, and the effects of different training models on segmentation accuracies were investigated. Experimental results showed that the highest mean Dice similarity coefficient (DSC) values for all compartments were obtained when the training models of VC sparse subsampling technique were used. Mean DSC values optimized with this technique were 82.6%, 83.1%, and 72.6% for femoral, tibial, and patellar cartilage compartments, respectively, when mean sensitivities were 79.9%, 84.0%, and 71.5%, and mean specificities were 99.8%, 99.9%, and 99.9%."]},
{"title": "Quantification of liver fat: A comprehensive review", "highlights": ["An overview on recent advances in liver fat quantification from histology, MR, US and CT is provided.", "The role of dedicated imaging modalities for quantification of liver fat is highlighted.", "The potential role of automated image processing methodologies to aid in image analysis is assessed."], "abstract": ["Fat accumulation in the liver causes metabolic diseases such as obesity, hypertension, diabetes or dyslipidemia by affecting insulin resistance, and increasing the risk of cardiac complications and cardiovascular disease mortality. Fatty liver diseases are often reversible in their early stage; therefore, there is a recognized need to detect their presence and to assess its severity to recognize fat-related functional abnormalities in the liver. This is crucial in evaluating living liver donors prior to transplantation because fat content in the liver can change liver regeneration in the recipient and donor. There are several methods to diagnose fatty liver, measure the amount of fat, and to classify and stage liver diseases (e.g. hepatic steatosis, steatohepatitis, fibrosis and cirrhosis): biopsy (the gold-standard procedure), clinical (medical physics based) and image analysis (semi or fully automated approaches). Liver biopsy has many drawbacks: it is invasive, inappropriate for monitoring (i.e., repeated evaluation), and assessment of steatosis is somewhat subjective. Qualitative biomarkers are mostly insufficient for accurate detection since fat has to be quantified by a varying threshold to measure disease severity. Therefore, a quantitative biomarker is required for detection of steatosis, accurate measurement of severity of diseases, clinical decision-making, prognosis and longitudinal monitoring of therapy. This study presents a comprehensive review of both clinical and automated image analysis based approaches to quantify liver fat and evaluate fatty liver diseases from different medical imaging modalities."]},
{"title": "Automatic segmentation of maxillofacial cysts in cone beam CT images", "highlights": ["Accurate segmentation of cysts and tumors is important step in computer assisted surgery.", "Manual segmentation performed by radiologists is tedious and time consuming.", "Normal head and face structure is roughly symmetric with respect to midsagittal plane.", "An automatic method based on asymmetry analysis is proposed which is general enough to segment various types of jaw cysts."], "abstract": ["Accurate segmentation of cysts and tumors is an essential step for diagnosis, monitoring and planning therapeutic intervention. This task is usually done manually, however manual identification and segmentation is tedious. In this paper, an automatic method based on asymmetry analysis is proposed which is general enough to segment various types of jaw cysts.", "The key observation underlying this approach is that normal head and face structure is roughly symmetric with respect to midsagittal plane: the left part and the right part can be divided equally by an axis of symmetry. Cysts and tumors typically disturb this symmetry. The proposed approach consists of three main steps as follows: At first, diffusion filtering is used for preprocessing and symmetric axis is detected. Then, each image is divided into two parts. In the second stage, free form deformation (FFD) is used to correct slight displacement of corresponding pixels of the left part and a reflected copy of the right part. In the final stage, intensity differences are analyzed and a number of constraints are enforced to remove false positive regions.", "The proposed method has been validated on 97 Cone Beam Computed Tomography (CBCT) sets containing various jaw cysts which were collected from various image acquisition centers. Validation is performed using three similarity indicators (Jaccard index, Dice's coefficient and Hausdorff distance). The mean Dice's coefficient of 0.83, 0.87 and 0.80 is achieved for Radicular, Dentigerous and KCOT classes, respectively. For most of the experiments done, we achieved high true positive (TP). This means that a large number of cyst pixels are correctly classified. Quantitative results of automatic segmentation show that the proposed method is more effective than one of the recent methods in the literature."]},
{"title": "Automatic segmentation of vertebral contours from CT images using fuzzy corners", "highlights": ["CT images of whole body spine is segmented using active contour method.", "Fully automated initialization method for active contour method is proposed.", "A novel fuzzy corner metric for identifying region of interest is employed.", "Automatic contour initialization using alpha hull constructed over detected fuzzy corners.", "Validation was done using the Dice coefficient and Hausdorff distance as compared to ground truth segmentation"], "abstract": ["Automatic segmentation of bone in computed tomography (CT) images is critical for the implementation of computer-assisted diagnosis which has increasing potential in the evaluation of various spine disorders. Of the many techniques available for delineating the region of interest (ROI), active contour methods (ACM) are well-established techniques that are used to segment medical images. The initialization for these methods is either through manual intervention or by applying a global threshold, thus making them semi-automatic in nature. The paper presents a methodology for automatic contour initialization in ACM and demonstrates the applicability of the method for medical image segmentation from spinal CT images. Initially, a set of feature markers from the image is extracted to construct an initial contour for the ACM. A fuzzified corner metric, based on image intensity, is proposed to identify the feature markers to be enclosed by the contour. A concave hull based on ", " shape, is constructed using these fuzzy corners to give the initial contour. The proposed method was evaluated against conventional feature detectors and other initialization methods. The results show the method\u05f3s robust performance in the presence of simulated Gaussian noise levels. The method enables the ACM to efficiently converge to the ground truth segmentation. The reference standard for comparison was the annotated images from a radiologist, and the Dice coefficient and Hausdorff distance measures were used to evaluate the segmentation."]},
{"title": "Edge density based automatic detection of inflammation in colonoscopy videos", "highlights": ["A model based method for automatic inflammation detection in colonoscopy videos is introduced.", "The method relies on a high quality display provided by Olympus colonoscopy probe.", "The proposed method is suitable for parallel implementation and real-time processing of high-resolution colonoscopy videos.", "Real-time inflammation detection can provide the gastroenterologist with a useful tool to enable faster and more accurate diagnosis."], "abstract": ["Colon cancer is one of the deadliest diseases where early detection can prolong life and can increase the survival rates. The early stage disease is typically associated with polyps and mucosa inflammation. The often used diagnostic tools rely on high quality videos obtained from colonoscopy or capsule endoscope. The state-of-the-art image processing techniques of video analysis for automatic detection of anomalies use statistical and neural network methods. In this paper, we investigated a simple alternative model-based approach using texture analysis. The method can easily be implemented in parallel processing mode for real-time applications. A characteristic texture of inflamed tissue is used to distinguish between inflammatory and healthy tissues, where an appropriate filter kernel was proposed and implemented to efficiently detect this specific texture. The basic method is further improved to eliminate the effect of blood vessels present in the lower part of the descending colon. Both approaches of the proposed method were described in detail and tested in two different computer experiments. Our results show that the inflammatory region can be detected in real-time with an accuracy of over 84%. Furthermore, the experimental study showed that it is possible to detect certain segments of video frames containing inflammations with the detection accuracy above 90%."]},
{"title": "Segmentation of the spinous process and its acoustic shadow in vertebral ultrasound images", "highlights": ["Texture descriptors and state-of-the art features allowed accurate segmentation.", "The features were optimized for vertebral region discrimination in ultrasound.", "Regularization accounts for geometrical properties of vertebral ultrasound images."], "abstract": ["Spinal ultrasound imaging is emerging as a low-cost, radiation-free alternative to conventional X-ray imaging for the clinical follow-up of patients with scoliosis. Currently, deformity measurement relies almost entirely on manual identification of key vertebral landmarks. However, the interpretation of vertebral ultrasound images is challenging, primarily because acoustic waves are entirely reflected by bone. To alleviate this problem, we propose an algorithm to segment these images into three regions: the spinous process, its acoustic shadow and other tissues. This method consists, first, in the extraction of several image features and the selection of the most relevant ones for the discrimination of the three regions. Then, using this set of features and linear discriminant analysis, each pixel of the image is classified as belonging to one of the three regions. Finally, the image is segmented by regularizing the pixel-wise classification results to account for some geometrical properties of vertebrae. The feature set was first validated by analyzing the classification results across a learning database. The database contained 107 vertebral ultrasound images acquired with convex and linear probes. Classification rates of 84%, 92% and 91% were achieved for the spinous process, the acoustic shadow and other tissues, respectively. Dice similarity coefficients of 0.72 and 0.88 were obtained respectively for the spinous process and acoustic shadow, confirming that the proposed method accurately segments the spinous process and its acoustic shadow in vertebral ultrasound images. Furthermore, the centroid of the automatically segmented spinous process was located at an average distance of ", " from that of the manually labeled spinous process, which is on the order of image resolution. This suggests that the proposed method is a promising tool for the measurement of the Spinous Process Angle and, more generally, for assisting ultrasound-based assessment of scoliosis progression."]},
{"title": "PSGMiner: A modular software for polysomnographic analysis", "highlights": ["PSGMiner is a fast and extensible visualization and analysis platform.", "PSGMiner is a tool for clinicians and methodologists.", "Different classifications can be made with various attributes having the desired epoch lengths.", "The software proves quite easy to make any change in any part of the software or to add a new module into it.", "The data obtained can be exported, which allows them to be used in different classification software."], "abstract": ["Sleep disorders affect a great percentage of the population. The diagnosis of these disorders is usually made by polysomnography. This paper details the development of new software to carry out feature extraction in order to perform robust analysis and classification of sleep events using polysomnographic data. The software, called PSGMiner, is a tool, which visualizes, processes and classifies bioelectrical data. The purpose of this program is to provide researchers with a platform with which to test new hypotheses by creating tests to check for correlations that are not available in commercially available software. The software is freely available under the GPL3 License.", "PSGMiner is composed of a number of diverse modules such as feature extraction, annotation, and machine learning modules, all of which are accessible from the main module. Using the software, it is possible to extract features of polysomnography using digital signal processing and statistical methods and to perform different analyses. The features can be classified through the use of five classification algorithms. PSGMiner offers an architecture designed for integrating new methods.", "Automatic scoring, which is available in almost all commercial PSG software, is not inherently available in this program, though it can be implemented by two different methodologies (machine learning and algorithms). While similar software focuses on a certain signal or event composed of a small number of modules with no expansion possibility, the software introduced here can handle all polysomnographic signals and events.", "The software simplifies the processing of polysomnographic signals for researchers and physicians that are not experts in computer programming. It can find correlations between different events which could help predict an oncoming event such as sleep apnea. The software could also be used for educational purposes."]},
{"title": "Chaos based crossover and mutation for securing DICOM image", "highlights": ["Medical image encryption scheme based on combining multiple chaotic maps is proposed.", "Combined logistic-tent and logistic\u2013sine system are developed.", "Crossover and mutation are adapted for confusing and diffusing the image pixels.", "Various attack analyses are performed and the results are discussed."], "abstract": ["This paper proposes a novel encryption scheme based on combining multiple chaotic maps to ensure the safe transmission of medical images. The proposed scheme uses three chaotic maps namely logistic, tent and sine maps. To achieve an efficient encryption, the proposed chao-cryptic system employs a bio-inspired crossover and mutation units to confuse and diffuse the Digital Imaging and Communications in Medicine (DICOM) image pixels. The crossover unit extensively permutes the image pixels row-wise and column-wise based on the chaotic key streams generated from the Combined Logistic\u2013Tent (CLT) system. Prior to mutation, the pixels of the crossed over image are decomposed into two images with reduced bit depth. The decomposed images are then mutated by XOR operation with quantized chaotic sequences from Combined Logistic\u2013Sine (CLS) system. In order to validate the sternness of the proposed algorithm, the developed chao-cryptic scheme is subjected to various security analyses such as statistical, differential, key space, key sensitivity, intentional cropping attack and chosen plaintext attack analyses. The experimental results prove the proposed DICOM cryptosystem has achieved a desirable amount of protection for real time medical image security applications."]},
{"title": "Rhythm-based heartbeat duration normalization for atrial fibrillation detection", "highlights": ["We investigate rhythm-based heartbeat normalization for improved atrial fibrillation detection.", "Heartbeat durations are normalized based on estimated rhythm of heartbeats in a window.", "It allows to measure irregularity of heartbeats for both AF and non-AF rhythms in the same scale.", "The normalization is found helpful for improving performance and robustness of AF detection.", "The normalization could be crucial for developing a clinically acceptable screening device."], "abstract": ["Screening of atrial fibrillation (AF) for high-risk patients including all patients aged 65 years and older is important for prevention of risk of stroke. Different technologies such as modified blood pressure monitor, single lead ECG-based finger-probe, and smart phone using plethysmogram signal have been emerging for this purpose. All these technologies use irregularity of heartbeat duration as a feature for AF detection. We have investigated a normalization method of heartbeat duration for improved AF detection.", "AF is an arrhythmia in which heartbeat duration generally becomes irregularly irregular. From a window of heartbeat duration, we estimate the possible rhythm of the majority of heartbeats and normalize duration of all heartbeats in the window based on the rhythm so that we can measure the irregularity of heartbeats for both AF and non-AF rhythms in the same scale. Irregularity is measured by the entropy of distribution of the normalized duration. Then we classify a window of heartbeats as AF or non-AF by thresholding the measured irregularity. The effect of this normalization is evaluated by comparing AF detection performances using duration with the normalization, without normalization, and with other existing normalizations.", "Sensitivity and specificity of AF detection using normalized heartbeat duration were tested on two landmark databases available online and compared with results of other methods (with/without normalization) by receiver operating characteristic (ROC) curves. ROC analysis showed that the normalization was able to improve the performance of AF detection and it was consistent for a wide range of sensitivity and specificity for use of different thresholds. Detection accuracy was also computed for equal rates of sensitivity and specificity for different methods. Using normalized heartbeat duration, we obtained 96.38% accuracy which is more than 4% improvement compared to AF detection without normalization.", "The proposed normalization method was found useful for improving performance and robustness of AF detection. Incorporation of this method in a screening device could be crucial to reduce the risk of AF-related stroke. In general, the incorporation of the rhythm-based normalization in an AF detection method seems important for developing a robust AF screening device."]},
{"title": "Multi-channel ECG data compression using compressed sensing in eigenspace", "highlights": ["A CS framework of data reduction is proposed for multichannel ECG (MECG) signals in eigenspace.", "PCA is used to exploit the spatial correlation across the channels resulting into sparse eigenspace signals.", "Using the compressed sensing (CS) approach, the significant eigenspace signals are gone through further dimensionality reduction.", "OMP is used for the CS recovery by exploiting the eigenspace/other domain sparsity of the PCA transformed MECG signals.", "The approach leads to higher compression efficiency, which makes it useful for resource-constrained MECG telemonitoring applications."], "abstract": ["In recent years, compressed sensing (CS) has emerged as a potential alternative to traditional data compression techniques for resource-constrained telemonitoring applications. In the present work, a CS framework of data reduction is proposed for multi-channel electrocardiogram (MECG) signals in eigenspace. The sparsity of dimension-reduced eigenspace MECG signals is exploited to apply CS. First, principal component analysis (PCA) is applied over the MECG data to retain diagnostically important ECG features in a few principal eigenspace signals based on maximum variance. Then, the significant eigenspace signals are randomly projected over a sparse binary sensing matrix to obtain the reduced dimension compressive measurement vectors. The compressed measurements are quantized using a uniform quantizer and encoded by a lossless Huffman encoder. The signal recovery is carried out by an orthogonal matching pursuit (OMP) algorithm. The proposed method is evaluated on the MECG signals from PTB and CSE multilead measurement library databases. The average value of percentage root mean square difference (PRD) across the PTB database is found to be 5.24% at a compression ratio ", " in Lead ", " of PTB database. The visual signal quality of the reconstructed MECG signals is validated through mean opinion score (MOS), found to be 6.66%, which implies ", " quality signal reconstruction."]},
{"title": "TDSDMI: Inference of time-delayed gene regulatory network using S-system model with delayed mutual information", "highlights": ["Delayed mutual information is proposed to delete redundant regulator factors.", "Restricted GEP is proposed as a new representation of time-delayed S-system model.", "A hybrid particle swarm optimization is used to optimize the parameters of TDSS .", "Two step method TDSDMI is proposed to infer time-delayed gene regulatory network."], "abstract": ["Regulatory interactions among target genes and regulatory factors occur instantaneously or with time-delay. In this paper, we propose a novel approach namely TDSDMI based on time-delayed S-system model (TDSS) model and delayed mutual information (DMI) to infer time-delay gene regulatory network (TDGRN). Firstly DMI is proposed to delete redundant regulator factors for each target gene. Secondly restricted gene expression programming (RGEP) is proposed as a new representation of the TDSS model to identify instantaneous and time-delayed interactions. To verify the effectiveness of the proposed method, TDSDMI is applied to both simulated and real biological datasets. Experimental results reveal that TDSDMI performs better than the recent reconstruction methods."]},
{"title": "Monitoring of autonomic response to sociocognitive tasks during treatment in children with Autism Spectrum Disorders by wearable technologies: A feasibility study", "highlights": ["Wearable sensors can be used to measure autonomic response in ASD during treatment.", "Correlation between autonomic response and engagement of the children.", "Longitudinal changes in the children's autonomic response during the engagement.", "Physiological data can help clinicians for objectivize and personalize the therapy."], "abstract": ["Autism Spectrum Disorders (ASD) represent a heterogeneous set of neurodevelopmental disorders characterized by impairments in social domain, where the autonomic nervous system (ANS) plays an important role. Several researchers have studied the ANS in ASD, during specific cognitive or sensory stimuli while few studies have examined response during social interactions. Wearable technologies can be very helpful in monitoring autonomic response in children with ASD in semi-naturalistic setting. The novelty of this study is to use such technologies to acquire physiological signals during therapeutic sessions supported by interactive \u201cserious games\u201d and to correlate the ANS response to the engagement of the child during sociocognitive tasks for an evaluation of the treatment effect and for the personalization of the therapy.", "A wearable chest belt for electrocardiographic (ECG) signal recording was used and specific algorithms for the extraction of clinically relevant features (Heart Rate \u2013 HR, Root Mean Square of the Successive Differences \u2013 RMSSD and Respiratory Sinus Arrhythmia \u2013 RSA) were developed. Sociocognitive tasks were mediated by \u201cserious games\u201d implemented on two tablets, which allowed a precise coding of the behaviors of the children. A longitudinal assessment of the physiological response of the children during six months of treatment was performed.", "A link between physiological response, i.e. decrease in RMSSD and RSA, and engagement of the children during sociocognitive tasks was found. Longitudinal changes in the children's autonomic response, including a decrease of RSA during the engagement throughout the therapeutic sessions, were found.", "These results foster the feasibility of this methodology to be applied in a clinical setting for the monitoring of the ANS response of children with ASD during treatment. A larger sample of patients is needed to confirm these preliminary findings."]},
{"title": "Gray level co-occurrence and random forest algorithm-based gender determination with maxillary tooth plaster images", "highlights": ["In this study, GLCM and RF based gender determination has been performed.", "Maxillary tooth plaster model images have been used to determine gender.", "Automatic segmentation was carried out by using image processing methods.", "The features were extracted automatically without requiring any manual measurement.", "This study is a multi-disciplinary study."], "abstract": ["Gender is one of the intrinsic properties of identity, with performance enhancement reducing the cluster when a search is performed. Teeth have durable and resistant structure, and as such are important sources of identification in disasters (accident, fire, etc.). In this study, gender determination is accomplished by maxillary tooth plaster models of 40 people (20 males and 20 females). The images of tooth plaster models are taken with a lighting mechanism set-up. A gray level co-occurrence matrix of the image with segmentation is formed and classified via a Random Forest (RF) algorithm by extracting pertinent features of the matrix. Automatic gender determination has a 90% success rate, with an applicable system to determine gender from maxillary tooth plaster images."]},
{"title": "Pseudo progression identification of glioblastoma with dictionary learning", "highlights": ["Discriminating between the pseudo progression and the true progression of Glioblastoma with AUC of 0.87.", "Dictionary learning developed to process the DTI/FA volumetric images.", "A feature selection approach for the large number of sparse representation calculated by dictionary learning.", "ROI segmentation is not needed.", "Location restriction regarding the tumor position has been solved."], "abstract": [" Although the use of temozolomide in chemoradiotherapy is effective, the challenging clinical problem of pseudo progression has been raised in brain tumor treatment. This study aims to distinguish pseudo progression from true progression.", " Between 2000 and 2012, a total of 161 patients with glioblastoma multiforme (GBM) were treated with chemoradiotherapy at our hospital. Among the patients, 79 had their diffusion tensor imaging (DTI) data acquired at the earliest diagnosed date of pseudo progression or true progression, and 23 had both DTI data and genomic data. Clinical records of all patients were kept in good condition. Volumetric fractional anisotropy (FA) images obtained from the DTI data were decomposed into a sequence of sparse representations. Then, a feature selection algorithm was applied to extract the critical features from the feature matrix to reduce the size of the feature matrix and to improve the classification accuracy.", " The proposed approach was validated using the 79 samples with clinical DTI data. Satisfactory results were obtained under different experimental conditions. The area under the receiver operating characteristic (ROC) curve (AUC) was 0.87 for a given dictionary with 1024 atoms. For the subgroup of 23 samples, genomics data analysis was also performed. Results implied further perspective on pseudo progression classification.", " The proposed method can determine pseudo progression and true progression with improved accuracy. Laboring segmentation is no longer necessary because this skillfully designed method is not sensitive to tumor location."]},
{"title": "Automatic iterative segmentation of multiple sclerosis lesions using Student's ", "highlights": ["Automatic MS lesion segmentation in FLAIR images using Student's ", " mixture models.", "Segmentation performed in an iterative manner allowing successive lesion refinement.", "Very good spatial and volumetric agreement with raters.", "Results comparable and, in some cases, better than the current state-of-the-art."], "abstract": ["Multiple sclerosis (MS) is a demyelinating autoimmune disease that attacks the central nervous system (CNS) and affects more than 2 million people worldwide. The segmentation of MS lesions in magnetic resonance imaging (MRI) is a very important task to assess how a patient is responding to treatment and how the disease is progressing. Computational approaches have been proposed over the years to segment MS lesions and reduce the amount of time spent on manual delineation and inter- and intra-rater variability and bias. However, fully-automatic segmentation of MS lesions still remains an open problem. In this work, we propose an iterative approach using Student's ", " mixture models and probabilistic anatomical atlases to automatically segment MS lesions in Fluid Attenuated Inversion Recovery (FLAIR) images. Our technique resembles a refinement approach by iteratively segmenting brain tissues into smaller classes until MS lesions are grouped as the most hyperintense one. To validate our technique we used 21 clinical images from the 2015 Longitudinal Multiple Sclerosis Lesion Segmentation Challenge dataset. Evaluation using Dice Similarity Coefficient (DSC), True Positive Ratio (TPR), False Positive Ratio (FPR), Volume Difference (VD) and Pearson's ", " coefficient shows that our technique has a good spatial and volumetric agreement with raters' manual delineations. Also, a comparison between our proposal and the state-of-the-art shows that our technique is comparable and, in some cases, better than some approaches, thus being a viable alternative for automatic MS lesion segmentation in MRI."]},
{"title": "Shape, texture and statistical features for classification of benign and malignant vertebral compression fractures in magnetic resonance images", "highlights": ["Vertebral compression fractures (VCFs) involve partial collapse of vertebral bodies.", "Statistics of gray levels, texture features, and shape features were computed.", "Classification of normal vertebral bodies versus VCFs was performed.", "Area under the receiver operating characteristic curve ", "=0.97 was obtained.", "=0.92 was obtained in classification of benign versus malignant VCFs."], "abstract": ["Vertebral compression fractures (VCFs) result in partial collapse of vertebral bodies. They usually are nontraumatic or occur with low-energy trauma in the elderly secondary to different etiologies, such as insufficiency fractures of bone fragility in osteoporosis (benign fractures) or vertebral metastasis (malignant fractures). Our study aims to classify VCFs in T1-weighted magnetic resonance images (MRI).", "We used the median sagittal planes of lumbar spine MRIs from 63 patients (38 women and 25 men) previously diagnosed with VCFs. The lumbar vertebral bodies were manually segmented and statistical features of gray levels were computed from the histogram. We also extracted texture and shape features to analyze the contours of the vertebral bodies. In total, 102 lumbar VCFs (53 benign and 49 malignant) and 89 normal lumbar vertebral bodies were analyzed. The ", "-nearest-neighbor method, a neural network with radial basis functions, and a na\u00efve Bayes classifier were used with feature selection. We compared the classification obtained by these classifiers with the final diagnosis of each case, including biopsy for the malignant fractures and clinical and laboratory follow up for the benign fractures.", "The results obtained show an area under the receiver operating characteristic curve of 0.97 in distinguishing between normal and fractured vertebral bodies, and 0.92 in discriminating between benign and malignant fractures.", "The proposed classification methods based on shape, texture, and statistical features have provided high accuracy and may assist in the diagnosis of VCFs."]},
{"title": "Assessment of advanced glycated end product accumulation in skin using auto fluorescence multispectral imaging", "highlights": ["Advanced glycated end products (AGE) are imaged with a two camera setup.", "A UV LED light source at 365", "\u00a0", "nm is used for inducing AGE auto fluorescence (AF).", "Skin tissue AGE is assessed as the relative amount of AF in the 475", "\u00a0", "nm region.", "Modulated light is used for ambient light cancelation.", "A significant correlation to current single-point systems shows proof-of-principle."], "abstract": ["Several studies have shown that advanced glycation end products (AGE) play a role in both the microvascular and macrovascular complications of diabetes and are closely linked to inflammation and atherosclerosis. AGEs accumulate in skin and can be detected using their auto fluorescence (AF).", "A significant correlation exists between AGE AF and the levels of AGEs as obtained from skin biopsies. A commercial device, the AGE Reader, has become available to assess skin AF for clinical purposes but, while displaying promising results, it is limited to single-point measurements performed in contact to skin tissue. Furthermore, in vivo imaging of AGE accumulation is virtually unexplored.", "We proposed a non-invasive, contact-less novel technique for quantifying fluorescent AGE deposits in skin tissue using a multispectral imaging camera setup (MSI) during ultraviolet (UV) exposure. Imaging involved applying a region-of-interest mask, avoiding specular reflections and a simple calibration. Results of a study conducted on 16 subjects with skin types ranging from fair to deeply pigmented skin, showed that AGE measured with MSI in forearm skin was significantly correlated with the AGE reference method (AGE Reader on forearm skin, ", "=0.68, ", "=0.005). AGE measured in facial skin was borderline significantly related to AGE Reader on forearm skin (", "=0.47, ", "=0.078). These results support the use of the technique in devices for non-touch measurement of AGE content in either facial or forearm skin tissue over time."]},
{"title": "A new method for QRS complex detection in multichannel ECG: Application to self-monitoring of fetal health", "highlights": ["Novel unsupervised approach for enhanced QRS signal extraction from multichannel ECG.", "Application to fetal QRS detection from abdominal recordings.", "The method exploits the pseudo-periodicity of maternal and fetal QRS.", "Overcomes the limitation of methods based on the independence of signal sources.", "The approach foresees the application to self-monitoring of fetal in pregnant women."], "abstract": ["This paper proposes a new approach for QRS complex detection in multichannel ECG and presents its application to fetal QRS (fQRS) detection in signals acquired from maternal abdominal leads. The method exploits the characteristics of pseudo-periodicity and time shape of QRS, it consists of devising a quality index (QI) which synthesizes these characteristics and of finding the linear combination of the acquired ECGs, which maximizes this QI. In the application for fQRS detection two QIs are devised, one QI (", ") for maternal ECG (mECG) and one QI (", ") for fetal ECG (fECG). The method is completely unsupervised and based on the following steps: signal pre-processing; maternal QRS-enhanced signal extraction by finding the linear combination that maximize the ", "; detection of maternal QRSs; mECG component approximation and canceling by weighted Singular Value Decomposition (SVD); fQRS-enhanced signal extraction by finding the linear combination that maximize the ", " and fQRS detection. The proposed method was compared with our previously developed Independent Component Analysis (ICA) based method as well as with simple mECG canceling and simple ICA methods. The comparison was carried out by evaluating the performances of the procedures in fQRS detection. The new method outperformed the results of the other approaches on the annotated open set of the Computing in Cardiology Challenge 2013 database. The proposed method seems to be promising for its implementation on portable device and for use in self-monitoring of fetal health in pregnant women."]},
{"title": "Zonated quantification of steatosis in an entire mouse liver", "highlights": ["A new spatial quantification shows heterogeneity of liver biomarkers on two scales.", "Steatosis was assessed in histological serial sections covering an entire mouse liver.", "The steatosis is distributed periportally with a notable variation across the organ.", "Steatosis distributions influence results of a whole-body pharmacokinetics simulation.", "This quantification can complement visual evaluation of histological images."], "abstract": ["Many physiological processes and pathological conditions in livers are spatially heterogeneous, forming patterns at the lobular length scale or varying across the organ. Steatosis, a common liver disease characterized by lipids accumulating in hepatocytes, exhibits heterogeneity at both these spatial scales. The main goal of the present study was to provide a method for zonated quantification of the steatosis patterns found in an entire mouse liver. As an example application, the results were employed in a pharmacokinetics simulation.", "For the analysis, an automatic detection of the lipid vacuoles was used in multiple slides of histological serial sections covering an entire mouse liver. Lobuli were determined semi-automatically and zones were defined within the lobuli. Subsequently, the lipid content of each zone was computed. The steatosis patterns were found to be predominantly periportal, with a notable organ-scale heterogeneity.", "The analysis provides a quantitative description of the extent of steatosis in unprecedented detail. The resulting steatosis patterns were successfully used as a perturbation to the liver as part of an exemplary whole-body pharmacokinetics simulation for the antitussive drug dextromethorphan. The zonated quantification is also applicable to other pathological conditions that can be detected in histological images. Besides being a descriptive research tool, this quantification could perspectively complement diagnosis based on visual assessment of histological images."]},
{"title": "Novel risk index for the identification of age-related macular degeneration using radon transform and DWT features", "highlights": ["Automated detection of normal and AMD classes using fundus images.", "Radon transform and discrete wavelet transform are used for feature extraction.", "Proposed method is evaluated using private and public (ARIA and STARE) datasets.", "Obtained highest classification accuracy of 100% for STARE dataset.", "AMD risk index is proposed to discriminate the two classes using a single number."], "abstract": ["Age-related Macular Degeneration (AMD) affects the central vision of aged people. It can be diagnosed due to the presence of drusen, Geographic Atrophy (GA) and Choroidal Neovascularization (CNV) in the fundus images. It is labor intensive and time-consuming for the ophthalmologists to screen these images. An automated digital fundus photography based screening system can overcome these drawbacks. Such a safe, non-contact and cost-effective platform can be used as a screening system for dry AMD. In this paper, we are proposing a novel algorithm using Radon Transform (RT), Discrete Wavelet Transform (DWT) coupled with Locality Sensitive Discriminant Analysis (LSDA) for automated diagnosis of AMD. First the image is subjected to RT followed by DWT. The extracted features are subjected to dimension reduction using LSDA and ranked using t-test. The performance of various supervised classifiers namely Decision Tree (DT), Support Vector Machine (SVM), Probabilistic Neural Network (PNN) and k-Nearest Neighbor (k-NN) are compared to automatically discriminate to normal and AMD classes using ranked LSDA components. The proposed approach is evaluated using private and public datasets such as ARIA and STARE. The highest classification accuracy of 99.49%, 96.89% and 100% are reported for private, ARIA and STARE datasets. Also, AMD index is devised using two LSDA components to distinguish two classes accurately. Hence, this proposed system can be extended for mass AMD screening."]},
{"title": "Transition modeling of neuropsychiatric impairment in HIV", "highlights": ["A continuous time Markov model adequately described efavirenz treatment-associated neuropsychiatric impairment.", "Efavirenz dose reduction from 600 to 400", "\u00a0", "mg would reduce the duration of neuropsychiatric impairment in HIV patients who get impaired without affecting efficacy.", "Delayed neuropsychiatric symptoms are less likely related to efavirenz exposure."], "abstract": ["Few studies have reported analyses of neuropsychiatric impairment (NPI) data from HIV patients, in a real world clinical setting with the aim of establishing association between anti-retroviral drug concentrations and NPI development and resolution. No study has modeled the effect of efavirenz exposure beyond the pre-steady state period on the frequency and duration of NPI. The data used consists of 196 HIV patients whose efavirenz pharmacokinetic parameters were previously determined. Neuropsychiatric evaluation was done at baseline, week 2 and week 12. Patients were classified into NORMAL and NPI states. The duration of NPI was further classified as transient (NPI at week 2 but not at week 12), persistent (NPI at week 2 and 12) and delayed (NPI at week 12 but not at week 2). The proportion of patients in each duration category out of the total NPI patients was calculated. A continuous time Markov model was developed in NONMEM 7.3 and used to describe the relationship between efavirenz exposure and the duration of NPI. Monte Carlo simulations with the model were used to describe the effect of efavirenz dose reduction from 600", "\u00a0", "mg to 400", "\u00a0", "mg on the duration of NPI. The model adequately described the data. The influence of efavirenz exposure on the rate of development of NPI decayed with a half-life of 8.4 days. Efavirenz dose reduction to 400", "\u00a0", "mg significantly reduces the duration of NPI, but has no impact on delayed NPI symptoms or efficacy."]},
{"title": "Distributed synthesis is simply undecidable", "highlights": ["Distributed synthesis in the classical setting (synchronous, Pnueli\u2013Rosner) is undecidable for very weak languages.", "The safety fragment of the intersection of LTL and ACTL is enough.", "The reachability fragment of the intersection of LTL and ACTL is enough.", "The set of architectures, for which synthesis is decidable, is not effected by these language restrictions."], "abstract": ["The distributed synthesis problem of safety and reachability languages is known to be undecidable. In this article, we establish that this is the case for very simple languages, namely for safety and reachability specifications in the intersection of LTL and ACTL."]},
{"title": "A novel approach for quantification of time\u2013intensity curves in a DCE-MRI image series with an application to prostate cancer", "highlights": ["The method for quantification of DCE time\u2013intensity curve shape patterns is proposed.", "To the best of author's knowledge this is the first method to the considered problem which uses the supervised classification.", "The proposed method can be used to improve specificity of MRI in characterizing cancerous lesions."], "abstract": ["This paper considers the problem of an automatic quantification of DCE-MRI curve shape patterns. In particular, the semi-quantitative approach which classifies DCE time\u2013intensity curves into clusters representing the tree main shape patterns is proposed. The approach combines heuristic rules with the naive Bayes classifier. In particular, the descriptive parameters are firstly derived from pixel-by-pixel analysis of the DCE time intensity curves and then used to recognise the curves which without a doubt represent the three main shape patterns. These curves are next used to train the naive Bayes classifier intended to classify the remaining curves within the dataset. Results of applying the proposed approach to the DCE-MRI scans of patients with prostate cancer are presented and discussed. Additionally, the overall performance of the approach is estimated through the comparison with the ground truth results provided by the expert."]},
{"title": "Multidimensional zero-correlation attacks on lightweight block cipher HIGHT: Improved cryptanalysis of an ISO standard", "highlights": ["Multidimensional zero-correlation attacks on round-reduced ISO-standard HIGHT.", "Key recovery for 27 (out of 32) rounds of HIGHT in classical single-key setting.", "Our 27-round attack significantly improves upon state-of-the-art in time and memory complexity.", "First key recovery for 26-round original HIGHT in classical single-key setting."], "abstract": ["HIGHT is a block cipher designed in Korea with the involvement of Korea Information Security Agency. It was proposed at CHES 2006 for usage in lightweight applications such as sensor networks and RFID tags. Lately, it has been adopted as ISO standard. Though there is a great deal of cryptanalytic results on HIGHT, its security evaluation against the recent zero-correlation linear attacks is still lacking. At the same time, the Feistel-type structure of HIGHT suggests that it might be susceptible to this type of cryptanalysis. In this paper, we aim to bridge this gap.", "We identify zero-correlation linear approximations over 16 rounds of HIGHT. Based upon those, we attack 27-round HIGHT (round 4 to round 30) with improved time complexity and practical memory requirements. This attack of ours is the best result on HIGHT to date in the classical single-key setting. We also provide the first attack on 26-round HIGHT (round 4 to round 29) with the full whitening key."]},
{"title": "A simple algorithm for computing positively weighted straight skeletons of monotone polygons", "highlights": ["We study the characteristics of the straight skeleton (SK) of monotone polygons.", "We devise an algorithm to compute the SK of monotone polygons in ", " time.", "This algorithm can also compute the positively weighted SK in ", " time."], "abstract": ["We study the characteristics of straight skeletons of monotone polygonal chains and use them to devise an algorithm for computing positively weighted straight skeletons of monotone polygons. Our algorithm runs in ", " time and ", " space, where ", " denotes the number of vertices of the polygon."]},
{"title": "Longest common substrings with ", "highlights": ["Two new algorithms for the longest common substring with ", " mismatches problem.", "A practical solution for arbitrary ", " which uses constant space.", "A theoretical solution for one mismatch which runs in quasilinear time."], "abstract": ["The longest common substring with ", "-mismatches problem is to find, given two strings ", " and ", ", a longest substring ", " of ", " and ", " of ", " such that the Hamming distance between ", " and ", " is \u2264", ". We introduce a practical ", " time and ", " space solution for this problem, where ", " and ", " are the lengths of ", " and ", ", respectively. This algorithm can also be used to compute the matching statistics with ", "-mismatches of ", " and ", " in ", " time and ", " space. Moreover, we also present a theoretical solution for the ", " case which runs in ", " time, assuming ", ", and uses ", " space, improving over the existing ", " time and ", " space bound of Babenko and Starikovskaya ", "."]},
{"title": "A filtration method for order-preserving matching", "highlights": ["We present a sublinear solution based on filtration for order-preserving matching.", "Any algorithm for exact string matching can be used as a filtering method.", "If the filtration algorithm is sublinear, the total method is sublinear on average.", "We show by practical experiments that the new solution is more efficient than earlier algorithms."], "abstract": ["The problem of order-preserving matching has gained attention lately. The text and the pattern consist of numbers. The task is to find all the substrings in the text which have the same length and relative order as the pattern. The problem has applications in analysis of time series. We present a new sublinear solution based on filtration. Any algorithm for exact string matching can be used as a filtering method. If the filtration algorithm is sublinear, the total method is sublinear on average. We show by practical experiments that the new solution is more efficient than earlier algorithms."]},
{"title": "Optimized explicit finite-difference schemes for spatial derivatives using maximum norm", "highlights": ["We propose an optimized scheme using the maximum norm and the simulated annealing.", "The maximum norm offers the largest set of possible solutions for solvers to search.", "The explicit finite-difference operator is greatly improved by our optimized scheme.", "We use a tight error limitation to make accuracy improvement to be high and solid.", "Our optimized scheme allows greater saving of computational cost and memory demand."], "abstract": ["Conventional explicit finite-difference methods have difficulties in handling high-frequency components due to strong numerical dispersions. One can reduce the numerical dispersions by optimizing the constant coefficients of the finite-difference operator. Different from traditional optimized schemes that use the 2-norm and the least squares, we propose to construct the objective functions using the maximum norm and solve the objective functions using the simulated annealing algorithm. Both theoretical analyses and numerical experiments show that our optimized scheme is superior to traditional optimized schemes with regard to the following three aspects. First, it provides us with much more flexibility when designing the objective functions; thus we can use various possible forms and contents to make the objective functions more reasonable. Second, it allows for tighter error limitation, which is shown to be necessary to avoid rapid error accumulations for simulations on large-scale models with long travel times. Finally, it is powerful to obtain the optimized coefficients that are much closer to the theoretical limits, which means greater savings in computational efforts and memory demand."]},
{"title": "On the family of critical section problems", "highlights": ["A unified framework is proposed for mutual exclusion, mutual inclusion and such what we call critical section problem.", "Critical section problem is characterized by a pair of integers.", "The family of critical section problems is closed under complement operation."], "abstract": ["Mutual exclusion is a fundamental process synchronization problem in concurrent systems. In this paper, we propose a unified framework for mutual exclusion, ", "-mutual exclusion, mutual inclusion, ", "-mutual inclusion and such, what we call critical section problem. Then, we show that critical section problem is characterized by a pair of integers."]},
{"title": "Direct simulation of drying colloidal suspension on substrate using immersed free surface model", "highlights": ["We model mesoscale gas\u2013liquid\u2013solid three-phase flow.", "We estimate efficiently capillary force exerted on mobile particles at free surface.", "We estimate accurately curvature of free surface.", "We simulate structure formation of colloidal suspension on substrate during drying.", "We clarify effect of wettability of particles on structure formation."], "abstract": ["This paper presents a new direct simulation method for a drying colloidal suspension on a substrate. A key issue of the present method is the immersed free surface model proposed by the authors, which enables us to estimate accurately and efficiently capillary forces exerted on particles on a free surface. Using the immersed free surface model along with immersed boundary method and level set method, the present method leads to a three-way coupling of the fluid flow, the free surface motion and the particle motion. In addition, the present method includes a way of curvature estimation using virtual grid differencing to calculate accurately a surface tension. The way of curvature estimation is quantitatively validated through the simulation of a still droplet. The immersed free surface model is quantitatively validated through the simulation of a sphere moving across a free surface and the simulation of two spheres moving along a free surface. Finally, simulations of drying colloidal suspension containing 130 particles are performed to demonstrate the applicability of the present method to actual systems."]},
{"title": "Exploiting temporal stability and low-rank structure for motion capture data refinement", "highlights": ["We propose a temporally stable and noise robust matrix completion model.", "A trust data detection method will boost the automation and performance of our method.", "The optimization method for solving the proposed model is efficient and robust.", "The parameters of our model are insensitive for a large range of values.", "The outputs of our method are much more stable than the other methods."], "abstract": ["Inspired by the development of the matrix completion theories and algorithms, a low-rank based motion capture (mocap) data refinement method has been developed, which has achieved encouraging results. However, it does not guarantee a stable outcome if we only consider the low-rank property of the motion data. To solve this problem, we propose to exploit the temporal stability of human motion and convert the mocap data refinement problem into a robust matrix completion problem, where both the low-rank structure and temporal stability properties of the mocap data as well as the noise effect are considered. An efficient optimization method derived from the augmented Lagrange multiplier algorithm is presented to solve the proposed model. Besides, a trust data detection method is also introduced to improve the degree of automation for processing the entire set of the data and boost the performance. Extensive experiments and comparisons with other methods demonstrate the effectiveness of our approaches on both predicting missing data and de-noising."]},
{"title": "A hybrid particle-mesh method for incompressible active polar viscous gels", "highlights": ["We present a hybrid particle-mesh method for incompressible active polar viscous gels.", "Analytical Lagrange multiplier imposes constant magnitude of the polarization field.", "Performing advection using particles is advantageous for flow dominated problems.", "Remeshing is performed using moment-conserving interpolation kernels.", "Incompressibility is imposed exactly using staggered finite difference meshes."], "abstract": ["We present a hybrid particle-mesh method for numerically solving the hydrodynamic equations of incompressible active polar viscous gels. These equations model the dynamics of polar active agents, embedded in a viscous medium, in which stresses are induced through constant consumption of energy. The numerical method is based on Lagrangian particles and staggered Cartesian finite-difference meshes. We show that the method is second-order and first-order accurate with respect to grid and time-step sizes, respectively. Using the present method, we simulate the hydrodynamics in rectangular geometries, of a passive liquid crystal, of an active polar film and of active gels with topological defects in polarization. We show the emergence of spontaneous flow due to Fr\u00e9edericksz transition, and transformation in the nature of topological defects by tuning the activity of the system."]},
{"title": "Hybrid approaches for multiple-species stochastic reaction\u2013diffusion models", "highlights": ["A novel hybrid stochastic/deterministic reaction\u2013diffusion simulation method is given.", "Can massively speed up stochastic simulations while preserving stochastic effects.", "Can handle multiple reacting species.", "Can handle moving boundaries."], "abstract": ["Reaction\u2013diffusion models are used to describe systems in fields as diverse as physics, chemistry, ecology and biology. The fundamental quantities in such models are individual entities such as atoms and molecules, bacteria, cells or animals, which move and/or react in a stochastic manner. If the number of entities is large, accounting for each individual is inefficient, and often partial differential equation (PDE) models are used in which the stochastic behaviour of individuals is replaced by a description of the averaged, or mean behaviour of the system. In some situations the number of individuals is large in certain regions and small in others. In such cases, a stochastic model may be inefficient in one region, and a PDE model inaccurate in another. To overcome this problem, we develop a scheme which couples a stochastic reaction\u2013diffusion system in one part of the domain with its mean field analogue, i.e. a discretised PDE model, in the other part of the domain. The interface in between the two domains occupies exactly one lattice site and is chosen such that the mean field description is still accurate there. In this way errors due to the flux between the domains are small. Our scheme can account for multiple dynamic interfaces separating multiple stochastic and deterministic domains, and the coupling between the domains conserves the total number of particles. The method preserves stochastic features such as extinction not observable in the mean field description, and is significantly faster to simulate on a computer than the pure stochastic model."]},
{"title": "Assessment of pulse arrival time for arterial stiffness monitoring on body composition scales", "highlights": ["A concept of scales application for arterial health monitoring is presented.", "Pulse arrival time is estimated from impedance plethysmogram and electrocardiogram.", "The proposed method agrees well with arterial tonometry.", "The system is able to track small changes in pulse arrival time.", "The system has a potential to be used in clinical and ambulatory settings."], "abstract": ["This study presents a system that aims to estimate changes in arterial health status in an unobtrusive way. It might be especially useful in long-term self-monitoring of cardiovascular performance for successful treatment and empowerment of patients. This system applies the electrocardiographic and impedance plethysmographic signals acquired using modified body composition scales for the calculation of pulse arrival time, which is directly related to arterial stiffness. The proposed device was tested in a cohort of 14 subjects. The modified scales were compared to the commercial PulsePen tonometer and the results showed significant relationship between these different devices (", ", ", "). The system also showed the ability to track small pulse arrival time variations induced by paced respiration. These findings suggest that scales evaluating parameters of cardiovascular function have potential to become a convenient device for self-monitoring of arterial stiffness."]},
{"title": "Mesh adaptation on the sphere using optimal transport and the numerical solution of a Monge\u2013Amp\u00e8re type equation", "highlights": ["Numerical solution of the Monge\u2013Amp\u00e8re equation on the surface of a sphere.", "Optimal transport for mesh adaptation.", "New linearisation of the Monge\u2013Amp\u00e8re equation and exponential maps.", "Proof of existence of solutions.", "Comparisons with centroidal Voronoi tessellations on the sphere."], "abstract": ["An equation of Monge\u2013Amp\u00e8re type has, for the first time, been solved numerically on the surface of the sphere in order to generate optimally transported (OT) meshes, equidistributed with respect to a monitor function. Optimal transport generates meshes that keep the same connectivity as the original mesh, making them suitable for r-adaptive simulations, in which the equations of motion can be solved in a moving frame of reference in order to avoid mapping the solution between old and new meshes and to avoid load balancing problems on parallel computers.", "The semi-implicit solution of the Monge\u2013Amp\u00e8re type equation involves a new linearisation of the Hessian term, and exponential maps are used to map from old to new meshes on the sphere. The determinant of the Hessian is evaluated as the change in volume between old and new mesh cells, rather than using numerical approximations to the gradients.", "OT meshes are generated to compare with centroidal Voronoi tessellations on the sphere and are found to have advantages and disadvantages; OT equidistribution is more accurate, the number of iterations to convergence is independent of the mesh size, face skewness is reduced and the connectivity does not change. However anisotropy is higher and the OT meshes are non-orthogonal.", "It is shown that optimal transport on the sphere leads to meshes that do not tangle. However, tangling can be introduced by numerical errors in calculating the gradient of the mesh potential. Methods for alleviating this problem are explored.", "Finally, OT meshes are generated using observed precipitation as a monitor function, in order to demonstrate the potential power of the technique."]},
{"title": "High order ADER schemes for a unified first order hyperbolic formulation of continuum mechanics: Viscous heat-conducting fluids and elastic solids", "highlights": ["High order schemes for a unified first order hyperbolic formulation of continuum mechanics.", "The mathematical model applies simultaneously to fluid mechanics and solid mechanics.", "Viscous fluids are treated in the frame of hyper-elasticity as generalized visco-plastic solids.", "Formal asymptotic analysis reveals the connection with the Navier\u2013Stokes equations.", "The distortion tensor ", " in the model appears to be well-suited for flow visualization."], "abstract": ["This paper is concerned with the numerical solution of the ", " first order hyperbolic formulation of continuum mechanics recently proposed by Peshkov and Romenski ", ", further denoted as ", ". In that framework, the viscous stresses are computed from the so-called ", ", which is one of the primary state variables in the proposed first order system. A very important key feature of the HPR model is its ability to describe ", " the behavior of inviscid and viscous compressible Newtonian and non-Newtonian ", " with heat conduction, as well as the behavior of elastic and visco-plastic ", ". Actually, the model treats viscous and inviscid fluids as generalized visco-plastic solids. This is achieved via a stiff source term that accounts for strain relaxation in the evolution equations of ", ". Also heat conduction is included via a first order hyperbolic system for the thermal impulse, from which the heat flux is computed. The governing PDE system is hyperbolic and fully consistent with the first and the second principle of thermodynamics. It is also fundamentally ", " from first order Maxwell\u2013Cattaneo-type relaxation models based on extended irreversible thermodynamics. The HPR model represents therefore a ", " and ", " description of continuum mechanics, which applies at the same time to ", " and ", ". In this paper, the direct connection between the HPR model and the classical hyperbolic\u2013parabolic Navier\u2013Stokes\u2013Fourier theory is established for the first time via a formal asymptotic analysis in the stiff relaxation limit.", "From a numerical point of view, the governing partial differential equations are very challenging, since they form a large nonlinear hyperbolic PDE system that includes stiff source terms and non-conservative products. We apply the successful family of one-step ADER\u2013WENO finite volume (FV) and ADER discontinuous Galerkin (DG) finite element schemes to the HPR model in the stiff relaxation limit, and compare the numerical results with exact or numerical reference solutions obtained for the Euler and Navier\u2013Stokes equations. Numerical convergence results are also provided. To show the universality of the HPR model, the paper is rounded-off with an application to wave propagation in elastic solids, for which one only needs to switch off the strain relaxation source term in the governing PDE system.", "We provide various examples showing that for the purpose of ", ", the distortion tensor ", " seems to be particularly useful."]},
{"title": "A simple robust and accurate ", "highlights": ["New sub-cell finite volume limiter for DG schemes on unstructured meshes in 2D and 3D.", "Simple ", " troubled zones indicator.", "Fully automatic sub-grid generation via nodes known from high order iso-parametric FEM.", "The new limiter maintains the DG sub-grid resolution property even at discontinuities.", "Element-wise checkpointing and restarting allows to cure floating point errors (NaN) at runtime."], "abstract": ["In this paper we propose a simple, robust and accurate nonlinear ", " stabilization of the Discontinuous Galerkin (DG) finite element method for the solution of nonlinear hyperbolic PDE systems on ", " triangular and tetrahedral meshes in two and three space dimensions. This novel ", " limiter, which has been recently proposed for the simple Cartesian grid case in ", ", is able to resolve discontinuities at a sub-grid scale and is substantially extended here to general unstructured simplex meshes in 2D and 3D. It can be summarized as follows:", "At the beginning of each time step, an approximation of the local minimum and maximum of the discrete solution is computed for each cell, taking into account also the vertex neighbors of an element. Then, an ", " discontinuous Galerkin scheme of approximation degree ", " is run for one time step to produce a so-called ", ". Subsequently, an ", " detection step checks the unlimited candidate solution at time ", " for positivity, absence of floating point errors and whether the discrete solution has remained within or at least very close to the bounds given by the local minimum and maximum computed in the first step. Elements that do not satisfy all the previously mentioned detection criteria are flagged as troubled cells. For these troubled cells, the candidate solution is ", " as inappropriate and consequently needs to be ", ". Within these troubled cells the old discrete solution at the previous time ", " is scattered onto small sub-cells (", " sub-cells per element edge), in order to obtain a set of sub-cell averages at time ", ". Then, a more robust second order TVD finite volume scheme is applied to update the sub-cell averages within the troubled DG cells from time ", " to time ", ". The new sub-grid data at time ", " are finally gathered back into a valid cell-centered DG polynomial of degree ", " by using a classical conservative and higher order accurate finite volume reconstruction technique.", "Consequently, if the number ", " is sufficiently large (", "), the subscale resolution capability of the DG scheme is fully maintained, while preserving at the same time an essentially non-oscillatory behavior of the solution at discontinuities. Many standard DG limiters only ", " the discrete solution in troubled cells, based on the limiting of higher order moments or by applying a nonlinear WENO/HWENO ", " on the data at the new time ", ". Instead, our new DG limiter entirely ", " the troubled cells by solving the governing PDE system again starting from valid data at the old time level ", ", but using this time a more robust scheme on the sub-grid level. In other words, the piecewise polynomials produced by the new limiter are the result of a more robust solution of the PDE system itself, while most standard DG limiters are simply based on a mere nonlinear data post-processing of the discrete solution. Technically speaking, the new method corresponds to an element-wise ", " and ", " of the solver, using a lower order scheme on the sub-grid. As a result, the present DG limiter is even able to ", " floating point errors like NaN values that have occurred after divisions by zero or after the computation of roots from negative numbers. This is a unique feature of our new algorithm among existing DG limiters.", "The new ", " sub-cell stabilization approach is developed within a high order accurate one-step ADER-DG framework on multidimensional unstructured meshes for hyperbolic systems of conservation laws as well as for hyperbolic PDE with non-conservative products. The method is applied to the Euler equations of compressible gas dynamics, to the ideal magneto-hydrodynamics equations (MHD) as well as to the seven-equation Baer\u2013Nunziato model of compressible multi-phase flows. A large set of standard test problems is solved in order to assess the accuracy and robustness of the new limiter."]},
{"title": "Rigorously modeling self-stabilizing fault-tolerant circuits: An ultra-robust clocking scheme for systems-on-chip", "highlights": ["We introduce a novel modeling framework for fault-tolerant VLSI circuits.", "We cast a self-stabilizing clocking scheme from a companion article in this model.", "We discuss the implications of theory and model for the resulting implementation.", "We present the measures taken to avoid metastable upsets despite faults.", "We provide experimental data from a prototype FPGA implementation of the algorithm."], "abstract": ["We present the first implementation of a distributed clock generation scheme for Systems-on-Chip that recovers from an unbounded number of arbitrary transient faults despite a large number of arbitrary permanent faults. We devise self-stabilizing hardware building blocks and a hybrid synchronous/asynchronous state machine enabling metastability-free transitions of the algorithm's states. We provide a comprehensive modeling approach that permits to prove, given correctness of the constructed low-level building blocks, the high-level properties of the synchronization algorithm (which have been established in a more abstract model). We believe this approach to be of interest in its own right, since this is the first technique permitting to mathematically verify, at manageable complexity, high-level properties of a fault-prone system in terms of its very basic components. We evaluate a prototype implementation, which has been designed in VHDL, using the Petrify tool in conjunction with some extensions, and synthesized for an Altera Cyclone FPGA."]},
{"title": "Degradation of thermal interface materials for high-temperature power electronics applications", "highlights": ["Ag grease, solder joint and Ag, Sn foils as TIMs all degrade remarkably at 170", "\u00a0", "\u00b0C.", "The degradation can be attributed to evolution of microstructure at interfaces.", "The degradation challenges finding of TIMs for high temperature power electronics."], "abstract": ["The specific thermal resistance values of several thermal interface materials (TIMs) intended to thermally enhance Cu contact pairs and their degradation under isothermal ageing at 170", "\u00a0", "\u00b0C have been investigated using Cu stack samples consisting of 10 Cu discs and 9 layers of the TIMs. The results obtained indicate that the specific thermal resistance values of the as-prepared Cu stack samples, one with conductive Ag thermal grease, one with Sn\u20133.5Ag solder joints and one with 25", "\u00a0", "\u03bcm thick Sn foil as TIMs are significantly lower than those of the Cu stack sample without any TIM. However, after the isothermal ageing at 170", "\u00a0", "\u00b0C for 90", "\u00a0", "days, the specific thermal resistance values of the samples with these TIMs are not substantially different from those of the sample without any TIM. Also reported in this paper is an estimation of testing errors for the specific thermal resistance values, microstructure characterization of the aged samples and effect of the degradation of these TIMs on the thermal performance of a high-temperature half bridge power switch module."]},
{"title": "Third-order symplectic integration method with inverse time dispersion transform for long-term simulation", "highlights": ["We use the inverse time dispersion transform to the 3rd-order symplectic integration.", "It can greatly reduce the time-dispersion error, especially for long travel times.", "It is superior to the conventional 2nd- and high-order finite-difference methods.", "It allows the maximum temporal interval under stability conditions thus is efficient.", "We can obtain more accurate simulation results but with a lower computational cost."], "abstract": ["The symplectic integration method is popular in high-accuracy numerical simulations when discretizing temporal derivatives; however, it still suffers from time-dispersion error when the temporal interval is coarse, especially for long-term simulations and large-scale models. We employ the inverse time dispersion transform (ITDT) to the third-order symplectic integration method to reduce the time-dispersion error. First, we adopt the pseudospectral algorithm for the spatial discretization and the third-order symplectic integration method for the temporal discretization. Then, we apply the ITDT to eliminate time-dispersion error from the synthetic data. As a post-processing method, the ITDT can be easily cascaded in traditional numerical simulations. We implement the ITDT in one typical exiting third-order symplectic scheme and compare its performances with the performances of the conventional second-order scheme and the rapid expansion method. Theoretical analyses and numerical experiments show that the ITDT can significantly reduce the time-dispersion error, especially for long travel times. The implementation of the ITDT requires some additional computations on correcting the time-dispersion error, but it allows us to use the maximum temporal interval under stability conditions; thus, its final computational efficiency would be higher than that of the traditional symplectic integration method for long-term simulations. With the aid of the ITDT, we can obtain much more accurate simulation results but with a lower computational cost."]},
{"title": "Second-order two-scale finite element algorithm for dynamic thermo\u2013mechanical coupling problem in symmetric structure", "highlights": ["Second-order two-scale (SOTS) expansions for dynamic thermo\u2013mechanical coupling problems in symmetric structure are obtained.", "SOTS finite element algorithm is proposed and unconditional stable implicit scheme is established.", "Anisotropic material is obtained by homogenization with enhanced strength and minor thermal expansion in circumferential direction.", "The mutual interaction and simultaneous vibration of the temperature and displacement field are simulated.", "A new computational tool is presented for analyzing thermo\u2013mechanical response of composites under strong aerothermodynamic environment."], "abstract": ["The new second-order two-scale (SOTS) finite element algorithm is developed for the dynamic thermo\u2013mechanical coupling problems in axisymmetric and spherical symmetric structures made of composite materials. The axisymmetric structure considered is periodic in both radial and axial directions and homogeneous in circumferential direction. The spherical symmetric structure is only periodic in radial direction. The dynamic thermo\u2013mechanical coupling model is presented and the equivalent compact form is derived. Then, the cell problems, effective material coefficients and the homogenized thermo\u2013mechanical coupling problem are obtained successively by the second-order asymptotic expansion of the temperature increment and displacement. The homogenized material obtained is manifested with the anisotropic property in the circumferential direction. The explicit expressions of the homogenized coefficients in the plane axisymmetric and spherical symmetric cases are given and both the derivation of the analytical solutions of the cell functions and the quasi-static thermoelasticity problems are discussed. Based on the SOTS method, the corresponding finite-element procedure is presented and the unconditionally stable implicit algorithm is established. Some numerical examples are solved and the mutual interaction between the temperature and displacement field is studied under the condition of structural vibration. The computational results demonstrate that the second-order asymptotic analysis finite-element algorithm is feasible and effective in simulating and predicting the dynamic thermo\u2013mechanical behaviors of the composite materials with small periodic configurations in axisymmetric and spherical symmetric structures. This may provide a vital computational tool for analyzing composite material internal temperature distribution and structural deformation induced by the dynamic thermo\u2013mechanical coupling response under strong aerothermodynamic environment."]},
{"title": "Liquid crystal electrography: Electric field mapping and detection of peak electric field strength in AlGaN/GaN high electron mobility transistors", "highlights": ["Liquid crystal was applied on top of the SiN passivation surface of AlGaN/GaN HEMT.", "Transmitted light in source\u2013drain region was recorded through crossed polarizers.", "Decrease in transmitted light intensity with increased source\u2013drain bias was recorded.", "Liquid crystal molecules shown to orientate with the horizontal electric field.", "Experimental results confirm simulated field strengths."], "abstract": ["The liquid crystal mixture E7, based on cyanobiphenyl, has been successfully employed to map electric field strength and distribution in AlGaN/GaN high electron mobility transistors. Using a transmitted light image through crossed polarizers the optical response of the liquid crystal deposited onto the surface of the devices was recorded as a function of source\u2013drain bias, ", ". At a critical voltage of 4", "\u00a0", "V the preferred direction of orientation of the long axes of the liquid crystal molecules in the drain access region aligned with one of the polarizers resulting in reduced transmitted light intensity. This indicates that at this electric field strength molecule orientation in most of the liquid crystal film is dominated by the electric field effect rather than the influence of surface anchoring. The experimental results were compared to device simulations. Electric field strength above the surface at ", "\u00a0", "=", "\u00a0", "4", "\u00a0", "V was simulated to reach or exceed 0.006", "\u00a0", "MV/cm. This electric field is consistent with the field expected for E7 to overcome internal elastic energy. This result illustrates the usefulness of liquid crystals to directly determine and map electric fields in electronic devices, including small electric field strengths."]},
{"title": "Effects of the intermetallic compound microstructure on the tensile behavior of Sn3.0Ag0.5Cu/Cu solder joint under various strain rates", "highlights": ["The IMC roughness and tensile strength of solder joint has a positive correlation.", "The tensile strength has a positive correlation with the strain rate.", "Ductile fracture in the bulk solder plays dominant role at the low strain rate.", "Brittle facture in the IMC layer plays dominant role at the high strain rate."], "abstract": ["The effects of the intermetallic compound (IMC) microstructure and the strain rate on the tensile strength and failure mode of Pb-free solder joints are investigated. The samples of Sn3.0Ag0.5Cu/Cu solder joints are aged isothermally at 150", "\u00a0", "\u00b0C for 0, 72, 288 and 500", "\u00a0", "h, and the thickness of the IMC layer and the roughness of the solder/IMC interface are measured and used to characterize the microstructure evolution of the IMC layer. The tensile tests of the aged solder joints are conducted under the strain rates of 2", "\u00a0", "\u00d7", "\u00a0", "10", ", 2", "\u00a0", "\u00d7", "\u00a0", "10", " and 2", "\u00a0", "s", ". The results indicate that both the thickness and roughness of the IMC layer have influence on the strength and failure mode of the solder joint. With the increase of the aging time, the thickness of the IMC layer increases and the roughness of the solder/IMC interface decreases, as a result, the tensile strength of the solder joint decreases and the dominant failure mode migrates from the ductile fracture in the bulk solder to the brittle fracture in the IMC layer. There is a positive correlation between the tensile strength of the solder joint and the stain rate applied during the test. With the increase of the strain rate, the failure mode migrates from the ductile fracture in the bulk solder to the brittle fracture in the IMC layer."]},
{"title": "High-speed video analysis for kink formation in a bond wire looping", "highlights": ["An experimental study on bond wire looping by using high-speed video analysis was presented.", "The video data shows that capillary velocity, trajectory position and the position of kinks.", "The tool\u2019s transverse move in its zigzag motion phase is a necessary step to create kinks.", "Capillary nozzle applies small and localized moment to the segment of fresh wire to create a new kink.", "The residual curvature of kinks would be major target for wire looping design."], "abstract": []},
{"title": "Study of hot electrons in AlGaN/GaN HEMTs under RF Class B and Class J operation using electroluminescence", "highlights": ["Electroluminescence is a good indicator for average hot electron contributions during operation", "Under RF (Class B and Class J), on average hot electrons have a lower impact compared to DC", "Peak values might be important in the degradation mechanism", "Field-driven degradation could be important but not detectable with Electroluminescence, possibly higher under Class J"], "abstract": ["Electroluminescence microscopy and spectroscopy have been used to investigate hot electron concentration and electron temperature during RF operation. Two modes of operation were chosen, Class B and Class J, and compared with DC conditions. Hot electron concentration and temperature were on average lower for both RF modes than under comparative DC conditions. While these average values suggest that degradation due exclusively to hot electrons may be lower for RF than for DC conditions, the peak values in EL intensity and electric field along dynamic load lines have also to be taken into account and these are higher under Class J than Class B."]},
{"title": "HEX: Scaling honeycombs is easier than scaling clock trees", "highlights": ["The HEX grid is a Byzantine fault-tolerant and self-stabilizing clock distribution.", "We argue that HEX is a robust alternative to buffered clock trees.", "HEX guarantees a small clock skew between neighbors with moderately balanced delays.", "HEX seamlessly integrates with multiple synchronized clock sources.", "We provide both theoretical analysis and simulations of the worst-case/average skew."], "abstract": ["We argue that a hexagonal grid with simple intermediate nodes is a robust alternative to buffered clock trees typically used for clock distribution in VLSI circuits, multi-core processors, and other applications that require accurate synchronization: Our HEX grid is Byzantine fault-tolerant, self-stabilizing, and seamlessly integrates with multiple synchronized clock sources, as used in multi-synchronous Globally Synchronous Locally Asynchronous (GALS) architectures. Moreover, HEX guarantees a small clock skew between neighbors even for wire delays that are only moderately balanced. We provide both a theoretical analysis of the worst-case skew and simulation results that demonstrate a very small average skew."]},
{"title": "The effect of electroplating parameters and substrate material on tin whisker formation", "highlights": ["Effect of electrodeposition parameters on tin whisker growth has been evaluated.", "Reduced whisker growth from deposits prepared at higher deposition current density.", "Low whisker growth observed for bright tin deposits on copper.", "No increase in whisker growth for tin deposits on copper stored at 55", "\u00a0", "\u00b0C/85% humidity."], "abstract": ["Electroplated tin finishes are widely used in the electronics industry due to their excellent solderability, electrical conductivity and corrosion resistance. However, the spontaneous growth of tin whiskers during service can result in localised electrical shorting or other harmful effects. Until recently, the growth of tin whiskers was successfully mitigated by alloying the tin with lead. However, restriction in the use of lead in electronics as a result of EU legislation (RoHS) has led to renewed interest in finding a successful alternative mitigation strategy.", "Whisker formation has been investigated for a bright tin electrodeposit to determine whether whisker growth can, at least partially, be mitigated by control of electroplating parameters such as deposition current density and deposit thickness. The influence of substrate material and storage at 55", "\u00a0", "\u00b0C/85% humidity on whisker growth have also been investigated.", "Whisker growth studies indicate that deposition parameters have a significant effect on both whisker density and whisker morphology. As deposition current density is increased there is a reduction in whisker density and a transition towards the formation of large eruptions rather than potentially more harmful filament whiskers. Increasing the tin coating thickness also results in a reduction in whisker density. Results demonstrate that whisker growth is most prolific from tin deposits on brass, whilst that from tin deposits on rolled silver is greater than that observed for tin deposits on copper."]},
{"title": "Quantification of cracked area in thermal path of high-power multi-chip modules using transient thermal impedance measurement", "highlights": ["Demonstrate the validity of the structure function method for detecting defects in high power multi-chip packages.", "Demonstrate the direct correlation between the cracked area of the solder layer and the resulting change in the thermal resistance R", "Compare Scanning Acoustic Microscopy images with the structure function throughout a power cycling test which help to build correlation between the two methods.", "Provide experimental evidence of the theoretical relationship of the so called (K-value) given by the differential structure function", "Provide insight into a powerful experimental technique for reliability and structural integrity testing of high-power multi-chip power modules."], "abstract": ["Transient thermal impedance measurement is commonly used to characterize the dynamic behaviour of the heat flow path in power semiconductor packages. This can be used to derive a \u201cstructure function\u201d which is a graphical representation of the internal structure of the thermal stack. Changes in the structure function can thus be used as a non-destructive testing tool for detecting and locating defects in the thermal path. This paper evaluates the use of the structure function for testing the integrity of the thermal path in high power multi-chip modules. A 1.2", "\u00a0", "kV/200", "\u00a0", "A IGBT module is subjected to power cycling with a constant current. The structure function is used to estimate the level of disruption at the interface between the substrate and the baseplate/case. Comparison with estimations of cracked area obtained by scanning acoustic microscopy (SAM) imaging shows excellent agreement, demonstrating that the structure function can be used as a quantitative tool for estimating the level of degradation. Metallurgical cross-sectioning confirms that the degradation is due to fatigue cracking of the substrate mount-down solder."]},
{"title": "Study of reliability-efficiency tradeoff of active thermal control for power electronic systems", "highlights": ["A review on active thermal control for power electronic modules is given.", "Potential estimation for active thermal control is shown based on a mission profile.", "Active thermal control is implemented on a laboratory setup.", "The lifetime extension or de-rating of active thermal control is compared with the reduced efficiency."], "abstract": ["Active thermal control for power modules can potentially extend the lifetime of the converter. This paper investigates the trade-off between the lifetime extension or de-rating and its cost due to the efficiency reduction. A short review on the existing approaches using control to reduce thermal stress of power modules is presented. Based on a given junction temperature profile, a method to evaluate the active thermal control's trade-off is presented. The concept is validated on a laboratory setup, where active thermal control is implemented by adapting the switching frequency. A discussion of the possible lifetime extension at the efficiency's expense is finally given."]},
{"title": "A thermal cycling reliability study of ultrasonically bonded copper wires", "highlights": ["A thermal cycling reliability study of ultrasonically bonded copper wires on Cu substrates is reported.", "Results showed no discernible degradation or wear out from initial condition to 2900 passive thermal cycles from \u2212", "\u00a0", "55 to 125", "\u00a0", "\u00b0C.", "Results obtained from nanoindentation showed cyclic hardening that might explain the increase in shear force after cycling."], "abstract": ["In this work we report on a reliability investigation regarding heavy copper wires ultrasonically bonded onto active braze copper substrates. The results obtained from both a non-destructive approach using 3D X-ray tomography and shear tests showed no discernible degradation or wear out from initial conditions to 2900 passive thermal cycles from \u2212", "\u00a0", "55 to 125", "\u00a0", "\u00b0C. Instead, an apparent increase in shear strength is observed as the number of thermal cycles increases. Nanoindentation hardness investigations suggest the occurrence of cyclic hardening. Microstructural investigations of the interfacial morphologies before and after cycling and after shear testing are also presented and discussed."]},
{"title": "Face hallucination based on sparse local-pixel structure", "highlights": ["Our framework aims to shape the prior model using sparse representation.", "Global structure and local-pixel structure are incorporated to produce plausible facial details.", "A method to learn local-pixel structures based on sparse representation is proposed.", "The proposed method is competitive with other, state-of-the-art face-hallucination methods."], "abstract": ["In this paper, we propose a face-hallucination method, namely face hallucination based on sparse local-pixel structure. In our framework, a high resolution (HR) face is estimated from a single frame low resolution (LR) face with the help of the facial dataset. Unlike many existing face-hallucination methods such as the from local-pixel structure to global image super-resolution method (LPS-GIS) and the super-resolution through neighbor embedding, where the prior models are learned by employing the least-square methods, our framework aims to shape the prior model using sparse representation. Then this learned prior model is employed to guide the reconstruction process. Experiments show that our framework is very flexible, and achieves a competitive or even superior performance in terms of both reconstruction error and visual quality. Our method still exhibits an impressive ability to generate plausible HR facial images based on their sparse local structures."]},
{"title": "Human behaviour recognition in data-scarce domains", "highlights": ["We challenge the notion that the exact temporal structure of activities needs to be modelled.", "We compare performance against a Hidden Markov Model baseline.", "The weak temporal structure of our approach makes it less sensitive to observation order.", "Hidden Markov Models cannot be used to classify some activity sequences.", "Our approach outperforms the baseline by 17%."], "abstract": ["This paper presents the novel theory for performing multi-agent activity recognition without requiring large training corpora. The reduced need for data means that robust probabilistic recognition can be performed within domains where annotated datasets are traditionally unavailable. Complex human activities are composed from sequences of underlying primitive activities. We do not assume that the exact temporal ordering of primitives is necessary, so can represent complex activity using an unordered bag. Our three-tier architecture comprises low-level video tracking, event analysis and high-level inference. High-level inference is performed using a new, cascading extension of the Rao\u2013Blackwellised Particle Filter. Simulated annealing is used to identify pairs of agents involved in multi-agent activity. We validate our framework using the benchmarked PETS 2006 video surveillance dataset and our own sequences, and achieve a mean recognition ", "-Score of 0.82. Our approach achieves a mean improvement of 17% over a Hidden Markov Model baseline."]},
{"title": "Incremental partial least squares analysis of big streaming data", "highlights": ["We propose a two-stage Incremental PLS (IPLS) dimension reduction method.", "IPLS has low time complexity, linear with both the numbers of samples and features.", "Empirical results show IPLS performs better than some state-of-the-arts methods."], "abstract": ["Incremental feature extraction is effective for facilitating the analysis of large-scale streaming data. However, most current incremental feature extraction methods are not suitable for processing streaming data with high feature dimensions because only a few methods have low time complexity, which is linear with both the number of samples and features. In addition, feature extraction methods need to improve the performance of further classification. Therefore, incremental feature extraction methods need to be more efficient and effective. Partial least squares (PLS) is known to be an effective dimension reduction technique for classification. However, the application of PLS to streaming data is still an open problem. In this study, we propose a highly efficient and powerful dimension reduction algorithm called incremental PLS (IPLS), which comprises a two-stage extraction process. In the first stage, the PLS target function is adapted so it is incremental by updating the historical mean to extract the leading projection direction. In the second stage, the other projection directions are calculated based on the equivalence between the PLS vectors and the Krylov sequence. We compared the performance of IPLS with other state-of-the-art incremental feature extraction methods such as incremental principal components analysis, incremental maximum margin criterion, and incremental inter-class scatter using real streaming datasets. Our empirical results showed that IPLS performed better than other methods in terms of its efficiency and further classification accuracy."]},
{"title": "Euclidean-distance-based canonical forms for non-rigid 3D shape retrieval", "highlights": ["Our method computes canonical forms of 3D meshes, without using geodesic distances.", "Our method maximises the Euclidean distance between a small set of feature points.", "Our method has a lower time complexity than other approaches yet provides comparable retrieval results."], "abstract": ["Retrieval of 3D shapes is a challenging problem, especially for non-rigid shapes. One approach giving favourable results uses multidimensional scaling (MDS) to compute a canonical form for each mesh, after which rigid shape matching can be applied. However, a drawback of this method is that it requires geodesic distances to be computed between all pairs of mesh vertices. Due to the super-quadratic computational complexity, canonical forms can only be computed for low-resolution meshes. We suggest a linear time complexity method for computing a canonical form, using Euclidean distances between pairs of a small subset of vertices. This approach has comparable retrieval accuracy but lower time complexity than using global geodesic distances, allowing it to be used on higher resolution meshes, or for more meshes to be considered within a time budget."]},
{"title": "A scale- and orientation-adaptive extension of Local Binary Patterns for texture classification", "highlights": ["A scale- and rotation-invariant feature representation based on LBP is presented.", "LBPs are computed adaptively based on the estimated scale of an image.", "An estimate of global orientation is used to align LBP at a high angular resolution.", "Features independent of the intrinsic texture scale increase the adaptability.", "Significant improvements are achieved in scenarios with scaling and rotation."], "abstract": ["Local Binary Patterns (LBPs) have been used in a wide range of texture classification scenarios and have proven to provide a highly discriminative feature representation. A major limitation of LBP is its sensitivity to affine transformations. In this work, we present a scale- and rotation-invariant computation of LBP. Rotation-invariance is achieved by explicit alignment of features at the extraction level, using a robust estimate of global orientation. Scale-adapted features are computed in reference to the estimated scale of an image, based on the distribution of scale normalized Laplacian responses in a scale-space representation. Intrinsic-scale-adaption is performed to compute features, independent of the intrinsic texture scale, leading to a significantly increased discriminative power for a large amount of texture classes. In a final step, the rotation- and scale-invariant features are combined in a multi-resolution representation, which improves the classification accuracy in texture classification scenarios with scaling and rotation significantly."]},
{"title": "A generalisable framework for saliency-based line segment detection", "highlights": ["A novel information-theoretic salient line segment detection framework is proposed.", "A saliency-based filtering formulation enables efficient salient line detection.", "The approach detects the most discriminative lines avoiding repetitive scene parts.", "The framework is generalisable as demonstrated by an extension to depth imagery.", "Superior repeatability is demonstrated compared to existing line detectors."], "abstract": ["Here we present a novel, information-theoretic salient line segment detector. Existing line detectors typically only use the image gradient to search for potential lines. Consequently, many lines are found, particularly in repetitive scenes. In contrast, our approach detects lines that define regions of significant divergence between pixel intensity or colour statistics. This results in a novel detector that naturally avoids the repetitive parts of a scene while detecting the strong, discriminative lines present. We furthermore use our approach as a ", " on existing line detectors to more efficiently detect salient line segments. The approach is highly generalisable, depending only on image statistics rather than image gradient; and this is demonstrated by an extension to depth imagery. Our work is evaluated against a number of other line detectors and a quantitative evaluation demonstrates a significant improvement over existing line detectors for a range of image transformations."]},
{"title": "An automated pattern recognition system for classifying indirect immunofluorescence images of HEp-2 cells and specimens", "highlights": ["We propose systems for classifying immunofluorescence images of HEp-2 cells.", "Images are classified at both the cell level and the specimen level.", "Ensemble SVM classification based on sparse coding of texture features was effective.", "Cell pyramids and artificial dataset augmentation increased mean class accuracy.", "The proposed systems came first in the I3A contest associated with ICPR 2014."], "abstract": ["Immunofluorescence antinuclear antibody tests are important for diagnosis and management of autoimmune conditions; a key step that would benefit from reliable automation is the recognition of subcellular patterns suggestive of different diseases. We present a system to recognize such patterns, at cellular and specimen levels, in images of HEp-2 cells. Ensembles of SVMs were trained to classify cells into six classes based on sparse encoding of texture features with cell pyramids, capturing spatial, multi-scale structure. A similar approach was used to classify specimens into seven classes. Software implementations were submitted to an international contest hosted by ICPR 2014 (Performance Evaluation of Indirect Immunofluorescence Image Analysis Systems). Mean class accuracies obtained on heldout test data sets were 87.1% and 88.5% for cell and specimen classification respectively. These were the highest achieved in the competition, suggesting that our methods are state-of-the-art. We provide detailed descriptions and extensive experiments with various features and encoding methods."]},
{"title": "Robust multimodal face and fingerprint fusion in the presence of spoofing attacks", "highlights": ["We examine spoofing robustness in multibiometrics attacking ", " out of ", " instances.", "1-Median filtering detecting score anomalies is proposed for spoof-resistant fusion.", "In contrast to sum-rule, operation with stable ", "-spoof performance can be achieved.", "Best performance (84%) for bagging of classifiers approach on LivDet2013 CrossMatch."], "abstract": ["Anti-spoofing is attracting growing interest in biometrics, considering the variety of fake materials and new means to attack biometric recognition systems. New unseen materials continuously challenge state-of-the-art spoofing detectors, suggesting for additional systematic approaches to target anti-spoofing. By incorporating liveness scores into the biometric fusion process, recognition accuracy can be enhanced, but traditional sum-rule based fusion algorithms are known to be highly sensitive to single spoofed instances. This paper investigates 1-median filtering as a spoofing-resistant generalised alternative to the sum-rule targeting the problem of partial multibiometric spoofing where ", " out of ", " biometric sources to be combined are attacked. Augmenting previous work, this paper investigates the dynamic detection and rejection of liveness-recognition pair outliers for spoofed samples in true multi-modal configuration with its inherent challenge of normalisation. As a further contribution, bootstrap aggregating (bagging) classifiers for fingerprint spoof-detection algorithm is presented. Experiments on the latest face video databases (Idiap Replay-Attack Database and CASIA Face Anti-Spoofing Database) and fingerprint spoofing database (Fingerprint Liveness Detection Competition 2013) illustrate the efficiency of proposed techniques."]},
{"title": "Generalized mean for robust principal component analysis", "highlights": ["We propose a robust principal component analysis.", "The generalized mean is used in the proposed method instead of the arithmetic mean.", "A novel method is also presented to solve our optimization problem."], "abstract": ["In this paper, we propose a robust principal component analysis (PCA) to overcome the problem that PCA is prone to outliers included in the training set. Different from the other alternatives which commonly replace ", "-norm by other distance measures, the proposed method alleviates the negative effect of outliers using the characteristic of the generalized mean keeping the use of the Euclidean distance. The optimization problem based on the generalized mean is solved by a novel method. We also present a generalized sample mean, which is a generalization of the sample mean, to estimate a robust mean in the presence of outliers. The proposed method shows better or equivalent performance than the conventional PCAs in various problems such as face reconstruction, clustering, and object categorization."]},
{"title": "Incompressible smoothed particle hydrodynamics (SPH) with reduced temporal noise and generalised Fickian smoothing applied to body\u2013water slam and efficient wave\u2013body interaction", "highlights": ["We conduct truly incompressible SPH simulations for free-surface flows.", "We further develop a stabilising particle shifting algorithm.", "An improved free-surface boundary treatment is presented.", "Slamming cases, and a novel, efficient wave\u2013structure interaction technique are presented.", "Accuracy is good, pressures are virtually noise free."], "abstract": ["Incompressible smoothed particle hydrodynamics generally requires particle distribution smoothing to give stable and accurate simulations with noise-free pressures. The diffusion-based smoothing algorithm of Lind et al. (J. Comp. Phys. 231 (2012) 1499\u20131523) has proved effective for a range of impulsive flows and propagating waves. Here we apply this to body\u2013water slam and wave\u2013body impact problems and discover that temporal pressure noise can occur for these applications (while spatial noise is effectively eliminated). This is due to the free-surface treatment as a discontinuous boundary. Treating this as a continuous very thin boundary within the pressure solver is shown to effectively cure this problem. The particle smoothing algorithm is further generalised so that a non-dimensional diffusion coefficient is applied which suits a given time step and particle spacing.", "We model the particular problems of cylinder and wedge slam into still water. We also model wave-body impact by setting up undisturbed wave propagation within a periodic domain several wavelengths long and inserting the body. In this case, the loads become cyclic after one wave period and are in good agreement with experiment. This approach is more efficient than the conventional wave flume approach with a wavemaker which requires many wavelengths and a beach absorber.", "Results are accurate and virtually noise-free, spatially and temporally. Convergence is demonstrated. Although these test cases are two-dimensional with simple geometries, the approach is quite general and may be readily extended to three dimensions."]},
{"title": "Distance fields on unstructured grids: Stable interpolation, assumed gradients, collision detection and gap function", "highlights": ["Presents asynchronous variational integrators in the context of finite elements with continuous strain fields.", "Illustrates an enhanced interpretation of the current space\u2013time front.", "Provides a strategy for estimating the critical time step size using CAG elements, nodal integration or SFEM."], "abstract": ["This article presents a novel approach to collision detection based on distance fields. A novel interpolation ensures stability of the distances in the vicinity of complex geometries. An assumed gradient formulation is introduced leading to a ", "-continuous distance function. The gap function is re-expressed allowing penalty and Lagrange multiplier formulations. The article introduces a node-to-element integration for first order elements, but also discusses signed distances, partial updates, intermediate surfaces, mortar methods and higher order elements. The algorithm is fast, simple and robust for complex geometries and self contact. The computed tractions conserve linear and angular momentum even in infeasible contact. Numerical examples illustrate the new algorithm in three dimensions."]},
{"title": "A Laplacian-based algorithm for non-isothermal atomistic-continuum hybrid simulation of micro and nano-flows", "highlights": ["An atomistic-continuum hybrid model for incompressible fluids is proposed.", "The model handles both momentum and heat transfer.", "Comparison with full-MD benchmarks is very good.", "For large geometries, the computational savings are large.", "Three methods for drastically reducing numerical noise are proposed."], "abstract": ["We propose a new hybrid algorithm for incompressible micro and nanoflows that applies to non-isothermal steady-state flows and does not require the calculation of the Irving\u2013Kirkwood stress tensor or heat flux vector. The method is validated by simulating the flow in a channel under the effect of a gravity-like force with bounding walls at two different temperatures and velocities. The model shows very accurate results compared to benchmark full MD simulations. In the temperature results, in particular, the contribution of viscous dissipation is correctly evaluated."]},
{"title": "A displacement-based finite element formulation for incompressible and nearly-incompressible cardiac mechanics", "highlights": ["A finite element displacement formulation stemming from the Perturbed Lagrangian.", "The efficiency is enhanced with a Shamanskii\u2013Newton\u2013Raphson scheme.", "Comparison with Perturbed Lagrangian, Lagrange Multiplier and penalty methods.", "Presence of locking in the penalty method, for whole-cycle cardiac mechanics.", "The presented form combines good convergence behavior with a simplified structure."], "abstract": ["The Lagrange Multiplier (LM) and penalty methods are commonly used to enforce incompressibility and compressibility in models of cardiac mechanics. In this paper we show how both formulations may be equivalently thought of as a weakly penalized system derived from the statically condensed Perturbed Lagrangian formulation, which may be directly discretized maintaining the simplicity of penalty formulations with the convergence characteristics of LM techniques. A modified Shamanskii\u2013Newton\u2013Raphson scheme is introduced to enhance the nonlinear convergence of the weakly penalized system and, exploiting its equivalence, modifications are developed for the penalty form. Focusing on accuracy, we proceed to study the convergence behavior of these approaches using different interpolation schemes for both a simple test problem and more complex models of cardiac mechanics. Our results illustrate the well-known influence of locking phenomena on the penalty approach (particularly for lower order schemes) and its effect on accuracy for whole-cycle mechanics. Additionally, we verify that direct discretization of the weakly penalized form produces similar convergence behavior to mixed formulations while avoiding the use of an additional variable. Combining a simple structure which allows the solution of computationally challenging problems with good convergence characteristics, the weakly penalized form provides an accurate and efficient alternative to incompressibility and compressibility in cardiac mechanics."]},
{"title": "Computational homogenization of liquid-phase sintering with seamless transition from macroscopic compressibility to incompressibility", "highlights": ["A novel method for computational homogenization of liquid-phase sintering.", "Seamless transition from macroscopically compressible to incompressible behavior.", "FE", " example showing heterogeneous macroscopic porosity."], "abstract": ["Liquid phase sintering of particle agglomerates is modeled on the mesoscale as the viscous deformation of particle\u2013particle contact, whereby the single driving force is the surface tension on the particle/pore interface. On the macroscale, a quasistatic equilibrium problem allows for the prediction of the shrinkage of the sintering body. The present paper presents a novel FE", " formulation of the two-scale sintering problem allowing for the transition to zero porosity, implying macroscale incompressibility. The seamless transition from compressibility to incompressibility on the macroscale is accomplished by introducing a mixed variational format. This has consequences also for the formulation of the mesoscale problem, that is complemented with an extra constraint equation regarding the prolongation of the volumetric part of the macroscopic rate-of-deformation. The numerical examples shows the sintering of a single representative volume element (RVE) which is sheared beyond the point where the porosity vanishes while subjected to zero macroscopic pressure."]},
{"title": "An impulse-based energy tracking method for collision resolution", "highlights": ["DEM can be based on penalties or impulses to resolve collisions.", "The impulse-based energy tracking method (ETM) resolves multiple collisions consistently.", "ETM models multiple collisions iteratively yet simultaneously, while tracking the system energy.", "ETM does not rely on penalties and does not require computation of penetration of bodies.", "The method is validated in the context of energy conservation and meso-scale angles of repose."], "abstract": ["Discrete element methods can be based on either penalties or impulses to resolve collisions. A generic impulse based method, the energy tracking method (ETM), is described to resolve collisions between multiple non-convex bodies in three dimensions. As opposed to the standard sequential impulse method (SQM) and simultaneous impulse method (SMM), which also apply impulses to avoid penetration, the energy tracking method changes the relative velocity between two colliding bodies iteratively yet simultaneously. Its main novelty is that impulses are applied gradually at multi-point contacts, and energy changes at the contact points are tracked to ensure conservation. Three main steps are involved in the propagation of the impulses during the single- and multi-contact resolution: compression, restitution-related energy loss, and separation. Numerical tests show that the energy tracking method captures the energy conservation property of perfectly elastic single- and multi-point collisions. ETM exhibits improved angular velocity estimation, as compared to SMM and SQM, as demonstrated by two numerical examples that model multi-point contact between box-shaped objects. Angles of repose estimated for multi-object pack repositioning of spheres, cubes, and crosses are in good agreement with the reported experimental values."]},
{"title": "Structural topology and shape optimization using a level set method with distance-suppression scheme", "highlights": ["A level set method with distance-suppression scheme is developed.", "An energy functional is developed and built into the level set equation.", "The need for re-initialization can be eliminated.", "The initialization of the level set function can be simplified."], "abstract": ["In level set methods for structural topology and shape optimization, the level set function gradients at the design interface need to be controlled in order to ensure stability of the optimization process. One popular way to do this is to enforce the level set function to be a signed distance function by periodically using initialization schemes, which is commonly known as re-initialization. However, such re-initialization schemes are time-consuming, as additional partial differential equations need to be solved in every iteration step. Furthermore, the use of re-initialization brings some undesirable problems; for example, it may move the zero level set away from the expected position. This paper presents a level set method with distance-suppression scheme for structural topology and shape optimization. An energy functional is introduced into the level set equation to maintain the level set function to close to a signed distance function near the structural boundaries, meanwhile forcing the level set function to be a constant at locations far away from the structural boundaries. As a result, the present method not only can avoid the need for re-initialization but also can simplify the setting of the initial level set function. The validity of the proposed method is tested on the mean compliance minimization problem and the compliant mechanisms synthesis problem. Different aspects of the proposed method are demonstrated on a number of benchmarks from the literature of structural optimization."]},
{"title": "An unfitted Nitsche method for incompressible fluid\u2013structure interaction using overlapping meshes", "highlights": ["Unfitted finite element method for a fluid\u2013structure interaction.", "Proof of stability and accuracy.", "Different coupling scheme\u2019s for time advancement: fully coupled or loosely coupled."], "abstract": ["We consider the extension of the Nitsche method to the case of fluid\u2013structure interaction problems on unfitted meshes. We give a stability analysis for the space semi-discretized problem and show how this estimate may be used to derive optimal error estimates for smooth solutions, irrespectively of the mesh/interface intersection. We also discuss different strategies for the time discretization, using either fully implicit or explicit coupling (", ") schemes. Some numerical examples illustrate the theoretical discussion."]},
{"title": "A semi-automatic method for robust and efficient identification of neighboring muscle cells", "highlights": ["We evolve the threshold selection method.", "We decompose the histogram distribution into different layers by Fourier transform.", "We select the optimal threshold based on the practical needs.", "We evolve the ultimate erosion theorem to achieve high accuracy."], "abstract": ["Segmentation and identification of muscle cells robustly and efficiently is of considerable importance in determining the muscle\u2019s physiological conditions. It is challenging due to frequently occurring artifacts, indistinct boundary between adjacent cells, the arbitrary shape and large number of cells. Currently, the widely used segmentation and quantification tools are usually manual or semi-automatic, which is time-consuming and labor intensive. In this paper, a semi-automatic method is proposed to segment the muscle cells robustly and efficiently. The proposed approach utilizes and evolves three fundamental image processing techniques, threshold selection, morphological ultimate erosion and morphological dilation. Experimental results verified the effectiveness of the proposed method."]},
{"title": "An isoparametric approach to high-order curvilinear boundary-layer meshing", "highlights": ["We present a method to generate high-order 3D curvilinear boundary layer meshes.", "The method refines a valid coarse mesh using an isoparametric mapping.", "Resulting elements are of arbitrary order and can be sized to any desired thickness.", "We demonstrate the resulting meshes are valid given a valid coarse mesh.", "The method is modular and allows fast generation of meshes for convergence studies."], "abstract": ["The generation of high-order curvilinear meshes for complex three-dimensional geometries is presently a challenging topic, particularly for meshes used in simulations at high Reynolds numbers where a thin boundary layer exists near walls and elements are highly stretched in the direction normal to flow. In this paper, we present a conceptually simple but very effective and modular method to address this issue. We propose an isoparametric approach, whereby a mesh containing a valid coarse discretization comprising of high-order triangular prisms near walls is refined to obtain a finer prismatic or tetrahedral boundary-layer mesh. The validity of the prismatic mesh provides a suitable mapping that allows one to obtain very fine mesh resolutions across the thickness of the boundary layer. We describe the method in detail for a high-order approximation using modal basis functions, discuss the requirements for the splitting method to produce valid prismatic and tetrahedral meshes and provide a sufficient criterion of validity in both cases. By considering two complex aeronautical configurations, we demonstrate how highly stretched meshes with sufficient resolution within the laminar sublayer can be generated to enable the simulation of flows with Reynolds numbers of ", " and above."]},
{"title": "A sequential-adaptive strategy in space\u2013time with application to consolidation of porous media", "highlights": ["A novel approach for space\u2013time adaptive finite element analysis is presented.", "Global error control is obtained by the technique of solving a dual problem.", "Space and time errors are estimated using a hierarchical decomposition of the dual.", "Recursive adaptations of the whole space\u2013time mesh is avoided.", "The coupled consolidation problem in geomechanics is considered as an application."], "abstract": ["Issues related to space\u2013time adaptivity for a class of nonlinear and time-dependent problems are discussed. The dG(k)-methods are adopted for the time integration, and the a posteriori error control is based on the appropriate dual problem in space\u2013time. One key ingredient is to decouple the error generation in space and time with a hierarchical decomposition of the discrete space of dual solutions. The main idea put forward in the paper is to increase the computational efficiency of the adaptive scheme by avoiding recursive adaptations of the whole time-mesh; rather, the space-mesh and the time-step defining each finite space\u2013time slab are defined in a truly sequential fashion.", "The proposed adaptive strategy is applied to the coupled consolidation problem in geomechanics involving large deformations. Its performance is investigated with the aid of a numerical example in 2D."]},
{"title": "Nonlinear structural design using multiscale topology optimization. Part II: Transient formulation", "highlights": [], "abstract": ["We extend the hierarchical multiscale design framework of Nakshatrala et\u00a0al. (2013) to nonlinear elastodynamics wherein we use topology optimization to design material micro-structures to achieve desired energy propagation in nonlinear elastic material systems subjected to impact loading. As in Part I, a well-posed topology optimization problem is obtained via (a) relaxation to design the macroscale which requires homogenization theory to relate the macroscopic homogenized response to its micro-structure and (b) via restriction to design the microscale to obtain a well-defined micro-structural length scale. It is assumed that the primary wavelengths of interest are much longer than the micro-structural length scale and hence the effective properties are computed using the static homogenization theory. An adjoint sensitivity analysis is performed to compute the derivatives of the objective function with respect to the micro-structural design parameters and a gradient-based optimization algorithm is used to update the design. The numerical implementation of the computationally challenging terminal-value adjoint problems is discussed and a structural design example for tailored energy propagation is provided."]},
{"title": "Non-intrusive reduced order modelling of fluid\u2013structure interactions", "highlights": [], "abstract": ["A novel non-intrusive reduced order model (NIROM) for fluid\u2013structure interaction (FSI) has been developed. The model is based on proper orthogonal decomposition (POD) and radial basis function (RBF) interpolation method. The method is independent of the governing equations, therefore, it does not require modifications to the source code. This is the first time that a NIROM was constructed for FSI phenomena using POD and RBF interpolation method. Another novelty of this work is the first implementation of the FSI NIROM under the framework of an unstructured mesh finite element multi-phase model (Fluidity) and a combined finite-discrete element method based solid model (Y2D).", "The capability of this new NIROM for FSI is numerically illustrated in three coupling simulations: a one-way coupling case (flow past a cylinder), a two-way coupling case (a free-falling cylinder in water) and a vortex-induced vibration of an elastic beam test case. It is shown that the FSI NIROM results in a large CPU time reduction by several orders of magnitude while the dominant details of the high fidelity model are captured."]},
{"title": "Topology optimization for turbulent flow with Spalart\u2013Allmaras model", "highlights": [], "abstract": ["In this research, a new finite element (FE) based topology optimization (TO) for turbulent flow was developed using the Spalart\u2013Allmaras (S\u2013A) equation, which is one of the Reynolds-Averaged Navier\u2013Stokes (RANS) equations. Despite many innovative works on the subject of fluidic TO, it is rare to consider the influence of turbulent flow in TO. To consider the effect of complex turbulent fluid motion, this research considered the S\u2013A model developed mainly for aerodynamic flows. For a successful TO, one primary issue is modification of the S\u2013A turbulent model to account for topological evolution during the optimization process. To address this issue, we proposed the addition of penalization terms to the original governing equations. To show the validity of the present approach and the effect of turbulent flow on optimal layouts, some two dimensional benchmark designs studied for laminar flow were reconsidered. By considering the effect of turbulent flow, the eddy viscosity values were increased at some local regions due to the Boussinesq hypothesis, and naturally optimal layouts affected by the spatially varying viscosity were obtained in turbulent flow. It was also found that the balance between the inertia force and the viscosity force plays an important role in topological designs."]},
{"title": "Isogeometric analysis and hierarchical refinement for higher-order phase-field models", "highlights": [], "abstract": ["While the interest in higher-order models in physics and mechanics grows, their numerical simulation still poses a challenge, especially for arbitrary shaped three-dimensional domains. This contribution presents the mathematical framework as well as the application to different problems in the field of material science, fracture mechanics and diffusion problems. All models under consideration require at least ", " continuity, which prevents the application of standard finite element analysis and local mesh refinements.", "Introducing isogeometric analysis (IGA) for the discretization in a finite element framework enables us to deal with these requirements. Moreover, a general hierarchical refinement scheme based on a subdivision projection is presented here for one, two and three dimensional simulations. This technique allows to enhance the approximation space using finer splines on each level but preserves the partition of unity as well as the continuity properties of the original discretization.", "Using this mathematical framework, the improved convergence of a Kuramoto\u2013Sivashinsky model, a mesh-adapted thermal diffusion simulation and computations of a priori unknown crack propagation in different fracture modes underline the versatility of the presented hierarchical refinement scheme."]},
{"title": "Simulation of angiogenesis in a multiphase tumor growth model", "highlights": [], "abstract": ["The avascular multiphase model for tumor growth, developed by the authors in previous works, is enhanced to include angiogenesis. The original model comprises the extracellular matrix (ECM) as porous solid phase and three fluid phases: living and necrotic tumor cells (TCs), host cells (HCs), and the interstitial fluid. In this paper we add transport of tumor angiogenic factor (TAF) and of endothelial cells. The density of the endothelial cells represents the newly created vessels in a smeared manner. Co-opted blood vessels can be added as line element with flow or can be taken into account as boundary condition. The model is hence of the continuum\u2013discrete type. Two examples show the potential of the newly enhanced model. The first deals with growth of a 2D tumor spheroid in a square tissue domain. From a blood vessel, posed on one side of the domain, angiogenesis takes place through the migration of endothelial cells from the vessel to the tumor. The second one is the simulation of cutaneous melanoma growth with the diffusion of TAF from the living tumor cells and the consequent development of a new vessel network, represented by the endothelial cells density. The introduction of angiogenesis will allow for simulating the delivery of chemotherapeutic and nanoparticle-mediated agents to the vascular tumor, and for evaluation of the therapeutic effect."]},
{"title": "A discontinuous Galerkin method for nonlinear shear-flexible shells", "highlights": [], "abstract": ["In this paper, a discontinuous Galerkin method for a nonlinear shear-flexible shell theory is proposed that is suitable for both thick and thin shell analysis. The proposed method extends recent work on Reissner\u2013Mindlin plates to avoid locking without the use of projection operators, such as mixed methods or reduced integration techniques. Instead, the flexibility inherent to discontinuous Galerkin methods in the choice of approximation spaces is exploited to satisfy the thin plate compatibility conditions a priori. A benefit of this approach is that only generalized displacements appear as unknowns. We take advantage of this to craft the method in terms of a discrete energy minimization principle, thereby restoring the Rayleigh\u2013Ritz approach. In addition to providing a straightforward and elegant derivation of the discrete equilibrium equations, the variational character of the method could afford numerous advantages in terms of mesh adaptation and available solution techniques. The proposed method is exercised on a set of benchmarks and example problems to assess its performance numerically, and to test for shear and membrane locking."]},
{"title": "An efficient and robust rotational formulation for isogeometric Reissner\u2013Mindlin shell elements", "highlights": [], "abstract": ["This work is concerned with the development of an efficient and robust isogeometric Reissner\u2013Mindlin shell formulation for the mechanical simulation of thin-walled structures. Such structures are usually defined by non-uniform rational B-splines (NURBS) surfaces in industrial design software. The usage of isogeometric shell elements can avoid costly conversions from NURBS surfaces to other surface or volume geometry descriptions. The shell formulation presented in this contribution uses a continuous orthogonal rotation described by Rodrigues\u2019 tensor in every integration point to compute the current director vector. The rotational state is updated in a multiplicative manner. Large deformations and finite rotations can be described accurately. The proposed formulation is robust in terms of stable convergence behavior in the nonlinear equilibrium iteration for large load steps and geometries with large and arbitrary curvature, and in terms of insensitivity to shell intersections with kinks under small angles. Three different integration schemes and their influence on accuracy and computational costs are assessed. The efficiency and robustness of the proposed isogeometric shell formulation is shown with the help of several examples. Accuracy and efficiency is compared to an isogeometric shell formulation with the more common discrete rotational concept and to Lagrange-based finite element shell formulations. The competitiveness of the proposed isogeometric shell formulation in terms of computational costs to attain a pre-defined error level is shown."]},
{"title": "Bayesian inference and model comparison for metallic fatigue data", "highlights": [], "abstract": ["In this work, we present a statistical treatment of stress-life (S-N) data drawn from a collection of records of fatigue experiments that were performed on 75S-T6 aluminum alloys. Our main objective is to predict the fatigue life of materials by providing a systematic approach to model calibration, model selection and model ranking with reference to S-N data. To this purpose, we consider fatigue-limit models and random fatigue-limit models that are specially designed to allow the treatment of the run-outs (right-censored data). We first fit the models to the data by maximum likelihood methods and estimate the quantiles of the life distribution of the alloy specimen. To assess the robustness of the estimation of the quantile functions, we obtain bootstrap confidence bands by stratified resampling with respect to the cycle ratio. We then compare and rank the models by classical measures of fit based on information criteria. We also consider a Bayesian approach that provides, under the prior distribution of the model parameters selected by the user, their simulation-based posterior distributions. We implement and apply Bayesian model comparison methods, such as Bayes factor ranking and predictive information criteria based on cross-validation techniques under various a priori scenarios."]},
{"title": "A direct hybrid finite element\u2013wave based modelling technique for efficient analysis of poroelastic materials in steady-state acoustic problems", "highlights": [], "abstract": ["This work proposes a hybrid modelling technique for efficient analysis of poroelastic materials, which are widely used for noise reduction in acoustic problems. By combining the finite element method and the wave based method in a direct manner, the proposed hybrid technique maximises the advantages and compensates the drawbacks of both numerical methods. The considered poroelastic domain described by Biot\u2019s theory is divided into two groups of domains according to their geometrical characteristics and boundary conditions. The group with complex geometries and/or boundary conditions leading to singularities is discretised into a large number of small finite elements. The other group consisting of large, geometrically moderate poroelastic domains is partitioned into wave based subdomains where the field variables are expanded with analytical poroelastic wave functions. Both groups modelled by the finite element method and the wave based method, respectively, are combined in a hybrid framework in this work to ensure their interacting dynamic behaviours. The properties of the hybrid model are investigated and are compared to existing modelling methods for some numerical examples. The proposed direct hybrid modelling technique provides stable predictions and exhibits fast convergence performances for the analysis of poroelastic materials, especially when singularities arise in the poroelastic domain."]},
{"title": "S\u2013R decomposition based numerical manifold method", "highlights": [], "abstract": ["The numerical manifold method (NMM) surmounting the mesh dependence has successfully solved very complicated problems involving small deformation and large movement, but had few applications to large deformation and large rotation problems because the false volume expansion and other issues exist. In this study it is shown that the false volume expansion in NMM can be excellently resolved by using the S\u2013R (strain\u2013rotation) decomposition theorem which can precisely reflect complex physical behaviors occurring in the process of large rotation and large deformation. The numerical methods based on the S\u2013R decomposition theorem have been limited to the static analysis of large deformations. To remove this limitation, a new formulation taking into account dynamical features is proposed based on the weak form of momentum conservation law. Under the framework of NMM, the generalized-", " method is employed to discretize the temporal variables. The updates of variables are described using the updated co-moving coordinate system. Thus, a new method named S\u2013R-D-based NMM is established. The new formulation can be implemented in any other partition of unity based methods as well, so as to improve the performances of such methods in the analysis of dynamic large deformations."]},
{"title": "Dual form of discontinuous deformation analysis", "highlights": [], "abstract": ["Discontinuous deformation analysis (DDA) is a numerical method for analyzing dynamic behaviors of an assemblage of distinct blocks, with the block displacements as the basic variables. The contact conditions are approximately satisfied by the open\u2013close iteration, which needs to fix or remove repeatedly the virtual springs between blocks in contact. The results from DDA are strongly dependent upon stiffness of these virtual springs. Excessively hard or soft springs all incur numerical problems. This is believed to be the biggest obstacle to more extensive application of DDA. To avoid the introduction of virtual springs, huge efforts have been made with little progress related to low efficiency in solution. In this study, the contact forces, instead of the block displacements, are taken as the basic variables. Stemming from the equations of momentum conservation of each block, the block displacements can be expressed in terms of the contact forces acting on the block. From the contact conditions a finite-dimensional quasi-variational inequality is derived with the contact forces as the independent variables. On the basis of the projection\u2013contraction algorithm for the standard finite-dimensional variational inequalities, an iteration algorithm, called the compatibility iteration, is designed for the quasi-variational inequality. The main processes can be highly parallelized with no need to assemble the global stiffness matrix. A number of numerical tests, including those very challenging, suggest that the proposed procedure has reached practical level in accuracy, robustness and efficiency, and the goal to abandon completely virtual springs has been reached."]},
{"title": "Robust shape optimization of continuous structures via the level set method", "highlights": [], "abstract": ["This work proposes a stochastic shape optimization method for continuous structures using the level-set method. Such a method aims to minimize the expected compliance and its variance as measures of the structural robustness. The behavior of continuous structures is modeled by linear elasticity equations with uncertain loading and material. This uncertainty can be modeled using random variables with different probability distributions as well as random fields. The proper problem formulation is ensured by the proof of the existence colorrev of solution under certain geometrical constraints on the set of admissible shapes. The proposed method addresses the stochastic linear elasticity problem in its weak form obtaining the explicit expressions for the continuous shape derivatives. Some numerical examples are presented to show the effectiveness of the proposed approach."]},
{"title": "B\u00e9zier extraction and adaptive refinement of truncated hierarchical NURBS", "highlights": [], "abstract": ["This contribution presents B\u00e9zier extraction of truncated hierarchical ", "-splines and the application of the approach to adaptive isogeometric analysis. The developed procedures allow for the implementation of hierarchical ", "-splines and NURBS without the need for an explicit truncation of the basis. Moreover, standard procedures of adaptive finite element analysis for error estimation and marking of elements are directly applicable due to the strict use of an element viewpoint.", "Starting from a multi-level nested mesh that results from uniform ", "-refinement, standard B\u00e9zier extraction is applied to active elements that contribute to the hierarchical approximation. This results in a multi-level system of equations without communication between individual hierarchy levels. A hierarchical subdivision operator is developed to recover this communication by transforming the multi-level system of equations into a hierarchical system of equations. It is demonstrated that this approach implicitly defines the truncated hierarchical basis in terms of a simple matrix multiplication. In this way, the implementation effort is reduced to a minimum as shape function routines and B\u00e9zier extraction procedures remain unchanged compared to standard isogeometric analysis.", "The convergence and the computational efficiency of the approach are examined in three different demonstration problems of heat conduction, linear elasticity, and the phase-field modelling of brittle fracture."]},
{"title": "Topology optimization with mixed finite elements on regular grids", "highlights": [], "abstract": ["Recently, new families of mixed finite elements have been proposed to address the analysis of linear elastic bodies on regular grids adopting a limited number of degrees of freedom per element. A two-dimensional mixed discretization is implemented to formulate an alternative topology optimization problem where stresses play the role of main variables and both compressible and incompressible materials can be dealt with. The structural compliance is computed through the evaluation of the complementary energy, whereas the enforcement of stress constraints is straightforward. Numerical simulations investigate the features of the proposed approach: comparisons with a conventional displacement-based scheme are provided for compressible materials; stress-constrained solutions for structures made of incompressible media are introduced."]},
{"title": "A performance study of NURBS-based isogeometric analysis for interior two-dimensional time-harmonic acoustics", "highlights": [], "abstract": ["This work evaluates the performance of a NURBS-based isogeometric finite element formulation for solving stationary acoustic problems in two dimensions. An initial assessment is made by studying eigenvalue problems for a square and a circular domain. The spectral approximation properties of NURBS functions of varying order are compared to those of conventional polynomials and are found to be superior, yielding more accurate representations of eigenvalues as well as eigenmodes. The higher smoothness of NURBS shape functions yields better approximations over an extended frequency range when compared to conventional polynomials. Two numerical case studies, including a geometrically complex domain, are used to benchmark the method versus the traditional finite element method. A convergence analysis confirms the higher efficiency of the isogeometric method on a per-degree-of-freedom basis. Simulations over a wider frequency range also illustrate that the method suffers less from the dispersion effects that deteriorate the acoustic response towards higher frequencies. The tensor product structure of NURBS, however, also imposes practical considerations when modelling a complex geometry consisting of multiple patches."]},
{"title": "A finite element framework for modeling internal frictional contact in three-dimensional fractured media using unstructured tetrahedral meshes", "highlights": ["Frictional contact of multiple interacting and intersecting 3D fractures is modeled.", "A square-root singular variation of the penalty parameter reduces traction error.", "Stress intensity factors for contacting cracks are validated against analytical solutions."], "abstract": ["This paper introduces a three-dimensional finite element (FE) formulation to accurately model the linear elastic deformation of fractured media under compressive loading. The presented method applies the classic Augmented Lagrangian(AL)-Uzawa method, to evaluate the growth of multiple interacting and intersecting discrete fractures. The volume and surfaces are discretized by unstructured quadratic triangle-tetrahedral meshes; quarter-point triangles and tetrahedra are placed around fracture tips. Frictional contact between crack faces for high contact precisions is modeled using isoparametric integration point-to-integration point contact discretization, and a gap-based augmentation procedure. Contact forces are updated by interpolating tractions over elements that are adjacent to fracture tips, and have boundaries that are excluded from the contact region. Stress intensity factors are computed numerically using the methods of displacement correlation and disk-shaped domain integral. A novel square-root singular variation of the penalty parameter near the crack front is proposed to accurately model the contact tractions near the crack front. Tractions and compressive stress intensity factors are validated against analytical solutions. Numerical examples of cubes containing one, two, twenty four and seventy interacting and intersecting fractures are presented."]},
{"title": "Buckling and vibration analysis of a pressurized CNT reinforced functionally graded truncated conical shell under an axial compression using HDQ method", "highlights": [], "abstract": ["The present research deals with bifurcation and vibration responses of a composite truncated conical shell with embedded single-walled carbon nanotubes (SWCNTs) subjected to an external pressure and axial compression simultaneously. The distribution of reinforcements through the thickness of the shell is assumed to be either uniform or functionally graded. The equations of motion are established using Green\u2013Lagrange type nonlinear kinematics within the framework of Novozhilov nonlinear shell theory. Linear membrane prebuckling analysis is conducted to extract the prebuckling deformations. The stability equations are derived by applying the adjacent equilibrium criterion to the prebuckling state of the conical shell. A semi-analytical solution on the basis of the trigonometric expansion through the circumferential direction along with the harmonic differential quadrature (HDQ) discretization in the meridional direction is developed. A series of comparison studies are carried out to assure the accuracy and the convergence of the HDQ method. The research indicates that the superb accuracy and efficiency of solutions with few grid points are attributed to the higher-order harmonic approximation function in the HDQ method. Parametric studies are also presented to investigate the influence of boundary conditions, semi-vertex angle of the cone, volume fraction and distribution of CNTs on stability and vibration characteristics of the truncated conical shell. The results show that both volume fraction and distribution of CNTs play a pivotal role in the natural frequencies, buckling mode and buckling loads of the FG-CNTRC truncated conical shell."]},
{"title": "Subdivision based mixed methods for isogeometric analysis of linear and nonlinear nearly incompressible materials", "highlights": [], "abstract": ["This paper addresses the use of isogeometric analysis to solve solid mechanics problems involving nearly incompressible materials. The present work is focused on extension of two-field mixed variational formulations in both small and large strains to isogeometric analysis. ", " stable displacement\u2013pressure combinations for mixed formulations are developed based on the subdivision property of NURBS. Stability and convergence properties of the proposed displacement\u2013pressure combinations are illustrated by computing numerical ", " constants and error norms. The performance of the proposed formulations is assessed by studying several benchmark examples involving nearly incompressible and incompressible elastic and elasto-plastic materials in both small and large strain regime."]},
{"title": "Fast image processing for optical metrology utilizing heterogeneous computer architectures", "highlights": ["Image processing applications can benefit from heterogeneous computing architectures.", "Using FPGAs, GPUs and CPUs together enables a fast image processing pipeline.", "FPGA architectures within smart cameras can increase throughput and decrease latency.", "The image processing pipeline was demonstrated at the example of optical metrology."], "abstract": ["Industrial image processing tasks, especially in the domain of optical metrology, are becoming more and more complex. While in recent years standard PC components were sufficient to fulfill the requirements, special architectures have to be used to build high-speed image processing systems today. For example, for adaptive optical systems in large scale telescopes, the latency between capturing an image and steering the mirrors is critical for the quality of the resulting images. Commonly, the applied image processing algorithms consist of several tasks with different granularities and complexities. Therefore, we combined the advantages of multicore CPUs, GPUs, and FPGAs to build a heterogeneous image processing pipeline for adaptive optical systems by presenting new architectures and algorithms. Each architecture is well-suited to solve a particular task efficiently, which is proven by a detailed evaluation. With the developed pipeline it is possible to achieve a high throughput and to reduce the latency of the whole steering system significantly."]},
{"title": "Unstructured Moving Particle Pressure Mesh (UMPPM) method for incompressible isothermal and non-isothermal flow computation", "highlights": [], "abstract": ["In this work, we intend to address the limitation of our earlier particle method, namely the Moving Particle Pressure Mesh (MPPM) method in handling arbitrary-shaped flow boundaries. The application of the Cartesian pressure mesh system adopted in our original MPPM method, which serves as the main key in recovering the divergence-free velocity condition for incompressible flow in the framework of particle method, is rather limited to rectangular flow domain. Here, the hybrid unstructured pressure mesh is adopted to remove the geometrical constraint of our earlier MPPM method. Coupled with the moving particle strategy in the Moving Particle Semi-implicit (MPS) method, the new method is named as the Unstructured Moving Particle Pressure Mesh (UMPPM) method in the current work. A consistent Laplacian model, namely the Consistent Particle Method (CPM) recently reported in the open literature is incorporated as well in the framework of UMPPM for discretizing the viscous term on the scattered particle cloud, while its implicit form is solved in the current work for overall robustness. Finally, we shall verify our UMPPM method with a series of benchmark solutions (for isothermal and non-isothermal flows) available from the literatures, including those obtained from the commercial code. It is appealing to find that the numerical solutions of UMPPM compare well with the benchmark solutions. In some cases, the accuracy of our UMPPM is better than that of the existing particle method such as Smoothed Particle Hydrodynamics (SPH)."]},
{"title": "Powell\u2013Sabin B-splines for smeared and discrete approaches to fracture in quasi-brittle materials", "highlights": ["A Powell\u2013Sabin B-splines formulation for higher-order gradient damage models.", "A discrete crack model exploiting Powell\u2013Sabin B-splines based on the cohesive-zone concept.", "A remeshing strategy for Powell\u2013Sabin B-splines with discrete cracks."], "abstract": ["Non-Uniform Rational B-splines (NURBS) and T-splines can have some drawbacks when modelling damage and fracture. The use of Powell\u2013Sabin B-splines, which are based on triangles, can by-pass these drawbacks. Herein, smeared as well as discrete approaches to fracture in quasi-brittle materials using Powell\u2013Sabin B-splines are considered.", "For the smeared formulation, an implicit fourth-order gradient damage model is adopted. Since quadratic Powell\u2013Sabin B-splines employ ", "-continuous basis functions throughout the domain, they are well-suited for solving the fourth order partial differential equation that emerges in this higher order damage model. Moreover, they can be generated from an arbitrary triangulation without user intervention. Since Powell\u2013Sabin B-splines are generated from a classical triangulation, they are not necessarily boundary-fitting and in that case they are not isogeometric in the strict sense.", "For discrete fracture approaches, the degree of continuity of T-splines is reduced to ", " at the crack tip. Hence, stresses need to be evaluated and weighted at the integration points in the vicinity of the crack tip in order to decide when the critical stress is reached. In practice, stress fields are highly irregular around crack tips. Furthermore, aligning a T-spline mesh with the new crack segment can be difficult. Powell\u2013Sabin B-splines also remedy these drawbacks as they are ", "-continuous at the crack tip and stresses can be directly computed, which vastly increases the accuracy and simplifies the implementation. Moreover, re-meshing is more straightforward using Powell\u2013Sabin B-splines. A current limitation is that, in three dimensions, there is no procedure (yet) for constructing Powell\u2013Sabin B-splines on arbitrary tetrahedral meshes."]},
{"title": "Detection of Lagrangian Coherent Structures in the SPH framework", "highlights": [], "abstract": ["The present work is dedicated to the detection of Lagrangian Coherent Structures (LCSs) in viscous flows through the Finite-Time Lyapunov Exponents (FTLEs) which have been addressed by several works in the recent literature. Here, a novel numerical technique is presented in the context of the Smoothed Particle Hydrodynamics (SPH) models. Thanks to the Lagrangian character of SPH, the trajectory of each fluid particle is explicitly tracked over the whole simulation. This allows for a direct evaluation of the FTLE field supplying a new way for the data analysis of complex flows. The evaluation of FTLE can be either implemented as a post-processing or nested into the SPH scheme conveniently. In the numerical results, three test-cases are presented giving a proof of concept for different conditions. The last test-case regards a naval engineering problem for which the present algorithm is successfully used to capture the submerged vortical tunnels caused by the splashing bow wave."]},
{"title": "Multi-fidelity non-intrusive polynomial chaos based on regression", "highlights": [], "abstract": ["In this paper we present a multi-fidelity (MF) extension of non-intrusive polynomial chaos based on regression (point collocation) for uncertainty quantification purposes. The proposed method uses the principle of a global correction function from a previous similar method that uses spectral projection to estimate the coefficients. Due to its usage of regression to estimate the coefficients, the present method offers high flexibility in the sampling and generation of the polynomial basis. The method takes advantage of a nested sampling plan to create the samples for the low-fidelity (LF) and correction expansions where the high-fidelity (HF) samples are a subset of the LF ones. To build the polynomial basis, a total order or hyperbolic truncation strategy is used with a highly flexible combination of the LF and correction polynomial expansions. The method is demonstrated on some artificial test problems and aerodynamic problems of the Euler flow around an airfoil and common three-dimensional research models. In order to derive the strategies for successful MF approximation, the effect of the correlation and the errors between the LF and HF functions is also studied. The results show that high correlation and moderately low errors are important to improve the MF approximation\u2019s accuracy. On a common research model problem, the MF approach with partially-converged simulations as the LF samples can successfully reduce the computational cost to about 40% for similar accuracy compared to an approach using a single HF expansion."]},
{"title": "Analytical evaluation of a time- and energy-efficient security protocol for IP-enabled sensors", "highlights": ["There is a lack of end-to-end security mechanisms tailored to IP-enabled sensors.", "Ladon is proposed as end-to-end authentication and authorization protocol.", "We evaluate the time and energy cost of executing Ladon on the protected sensors.", "Ladon is proved to be a suitable security protocol for resource-deprived devices."], "abstract": ["With the development of the 6LoWPAN standard, sensors can be natively integrated into the IP world, becoming tiny information providers that are directly addressable by any Internet-connected party. To protect the information gathered by sensors from any potential attacker on the Internet, it is essential to have trustworthy real-time information about the legitimacy of every attempt to interact with a sensor. Our approach to address this issue is Ladon, a new security protocol specifically tailored to the characteristics of low capacity devices. In this paper, we study the performance of Ladon, showing that it successfully meets the requirements of the targeted environments. To that end, we evaluate the delay and energy consumption of the execution of Ladon. The obtained results show that the cost of Ladon is bounded, even in situations of high packet loss rates (20\u201380%) and comparable to that of other protocols that implement fewer security features."]},
{"title": "A comparative molecular dynamics-phase-field modeling approach to brittle fracture", "highlights": [], "abstract": []},
{"title": " methods for time-harmonic acoustics", "highlights": [], "abstract": ["This paper explores the application of ", " principle. They are "]},
{"title": "Optimized energy limited cooperative spectrum sensing in cognitive radio networks", "highlights": ["A bi-threshold cooperative spectrum sensing method for energy efficiency and throughput enhancement is proposed.", "We present a convex optimization analysis.", "The method shows a good tradeoff between energy consumption and throughput.", "Both energy efficiency and throughput can be improved, for a wide range of SNRs."], "abstract": ["In cognitive radio networks, it is well known that the cooperative spectrum sensing can overcome damaging effects of fading and shadowing. However, it also increases the amount of energy consumption which is a critical factor in low powered wireless communications. In this paper based on bi-threshold energy detection, we maximize the network throughput such that the energy consumption is below a predefined value and also sufficient protection of primary users against interference is guaranteed. Convex optimization analysis is presented to jointly obtain the optimal values of sensing time and detection thresholds. Simulation results show that the proposed method is very flexible such that a good tradeoff between achievable throughput and energy efficiency can be established, while it often outperforms the conventional sensing method significantly."]},
{"title": "Algebraic approximation of sub-grid scales for the variational multiscale modeling of transport problems", "highlights": [], "abstract": []},
{"title": "Efficient implicit integration for finite-strain viscoplasticity with a nested multiplicative split", "highlights": [], "abstract": ["An efficient and reliable stress computation algorithm is presented, which is based on implicit integration of the local evolution equations of multiplicative finite-strain plasticity/viscoplasticity. The algorithm is illustrated by an example involving a combined nonlinear isotropic/kinematic hardening; numerous backstress tensors are employed for a better description of the ", ".", "The considered material model exhibits the so-called weak ", "Concerning the accuracy of the stress computation, the new algorithm is comparable to the "]},
{"title": "Key propagation in wireless sensor networks", "highlights": ["In a sensor network, communication channels connecting sequences of adjacent nodes are created by key propagation.", "All the nodes in a communication channel share the same key.", "The key connecting the start node and the first subsequent node is propagated to all the other nodes in the channel.", "Communication channels are bidirectional, they can be used to transmit a message from/to any node in the channel."], "abstract": ["With reference to a network consisting of sensor nodes connected by wireless links, we approach the problem of the distribution of the cryptographic keys. We present a solution based on communication channels connecting sequences of adjacent nodes. All the nodes in a channel share the same key. This result is obtained by propagating the key connecting the first two nodes to all the other nodes in the channel. The key propagation mechanism is also used for key replacement, as is required, for instance, in group communication to support forms of forward and backward secrecy, when a node leaves a group or a new node is added to an existing group."]},
{"title": "A lightweight security scheme for query processing in clustered wireless sensor networks", "highlights": ["Low-overhead secured query-processing mechanism in clustered WSN environment.", "Preserves security features- confidentiality, integrity and defence against replay attack.", "Query is sent from base station to cluster heads in encrypted form.", "The cluster heads register their member nodes by exchanging messages.", "The cluster heads send aggregated responses to the base station."], "abstract": ["When wireless sensor networks (WSNs) are deployed in areas inaccessible by human beings, security becomes extremely important, as they are prone to different types of malicious attacks. We propose a scheme to build a security mechanism in a query-processing paradigm within WSNs with clustered architecture. This work aims to preserve the basic security features such as confidentiality and integrity as well as to protect from replay attack in presence of mote class attacker. Considering the limitations of such an attacker, the probability of attacking cluster head and member nodes is higher than attacking the base station. Paying attention to this fact, in all communication between cluster head and member nodes, the key is neither transmitted nor pre-deployed. Performance of the scheme is evaluated and compared through qualitative and quantitative analyses; results show the present scheme\u2019s dominance over the competing schemes."]},
{"title": "Structural topology optimization with minimum distance control of multiphase embedded components by level set method", "highlights": [], "abstract": ["This paper presents a novel ", " method for designing structures with multiphase embedded components under minimum distance constraints in the level set framework. By using the level set representation for both the component layout and the host structure topology, the shapes of the components can be easily preserved, and optimal structural topologies with smooth boundary/material interface can be obtained. With the purpose of preventing the components moving too close to each other, a minimum distance constraint based on virtual boundary offset is proposed. Different from existing distance detection methods relying on explicit topology representation, the proposed constraint is imposed as a unified integral form, for which the design sensitivity can be readily obtained. Moreover, this constraint is effective for detecting the distance between any complex-shaped components. Several numerical examples are presented to demonstrate the validity and effectiveness of the proposed method."]},
{"title": "Cloud model-based method for range-constrained thresholding", "highlights": ["A cloud model-based framework for range-constrained thresholding with uncertainty.", "Improving four traditional methods under the new framework.", "Representing the image using cloud model.", "Implementing image transformation to focus on mid-region of the image.", "Cloud model-based framework is efficient and effective."], "abstract": ["Thresholding is a popular image segmentation method that converts a grayscale image into a binary image. In this paper, we propose a cloud model-based framework for range-constrained thresholding with uncertainty, and improve four traditional methods. The method involves four major steps, including representing the image using cloud model, estimating the automatic threshold for gray level ranges of object and background, implementing image transformation to focus on mid-region of the image, and determining the binary threshold within the constrained gray level range. Cloud model can effectively represent various visual properties of the image, such as intensity-based class uncertainty, intra-class homogeneity, and between-class contrast. The approach is validated both quantitatively and qualitatively. Compared with the traditional state-of-art algorithms on a variety of synthetic and real images, with or without noisy, as well as laser cladding images, the experimental results suggest that the presented method is efficient and effective."]},
{"title": "Primary user behavior aware spectrum allocation scheme for cognitive radio networks", "highlights": ["A proportional-fair oriented sharing-scheme is proposed to allocate channels.", "A PU activity model is developed to provide most stable channels to users.", "A time varying framing process is used to make variable sized frames at MAC layer.", "The proposed scheme reduces the delay, collisions and increases the throughput."], "abstract": ["Cognitive radio (CR) technology can solve the problems of spectrum scarcity and low spectrum utilization. However, random behavior of the primary user (PU) appears to be an enormous challenge. In this paper, we propose a PU behavior aware joint channel selection and allocation scheme. In the first step, the channels are ranked based on statistics of the PU usage whereas in the second phase, a proportional fair oriented channel allocation scheme is employed to allocate channels among CRs. We also introduce the concept of a time-varying framing process (TVFP) that minimizes the overall data transmission time. Simulation results show that the proposed scheme outperforms existing schemes in terms of the transmission-time and the number of collisions with the PUs. In addition, it helps to save a significant amount of transmission power. Moreover, it provides a significantly higher system throughput as compared to the existing schemes."]},
{"title": "Effects of node mobility on energy balancing in wireless networks", "highlights": ["Utilizing mobility for energy balancing is the key insight of our work.", "We construct a Linear Programming (LP) framework that jointly captures data routing, mobility, and energy dissipation aspects.", "Mobility can improve the energy balancing up to a certain level.", "We delineate future research opportunities in the area."], "abstract": ["Having a direct effect on network lifetime, balanced energy consumption is one of the key challenges in wireless networks. In this paper, we investigate the effects of node mobility on energy balancing in wireless networks. We construct a Linear Programming (LP) framework that jointly captures data routing, mobility, and energy dissipation aspects. We explore the design space by performing numerical analysis using the developed LP framework. Our results show that mobility has significant effects on the energy dissipation trends of wireless nodes. Mobility can improve the energy balancing up to a certain level, however extreme mobility may lead to a degradation in energy balancing of wireless networks."]},
{"title": "Fast and efficient lossless adaptive compression scheme for wireless sensor networks", "highlights": ["A fast and low memory data compression scheme is proposed for WSNs.", "The scheme performs compression losslessly using 8 variable-length code options.", "The scheme which uses a very simple data model is fast and computationally simple.", "The scheme is efficient and requires no coding dictionary.", "The scheme achieve better compression performance than previously proposed schemes."], "abstract": ["The number of wireless sensor network deployments for real-life applications has rapidly increased in recent years. However, power consumption is a critical problem affecting the lifetime of wireless sensor networks (WSNs). A number of techniques have been proposed to solve this power problem. Among the proposed techniques, data compression scheme is one that can be used to reduce the volume of data to be transmitted. This paper therefore proposes a fast and efficient lossless adaptive compression scheme (FELACS) for WSNs. FELACS was proposed to enable a fast and low memory compression algorithm for WSNs. FELACS generates its coding tables on the fly and compresses data very fast. FELACS is lightweight, robust to packet losses and has very low complexity. FELACS achieved compression rates of 4.11 bits per sample. In addition, it achieved power savings up to 70.61% using the real-world test datasets."]},
{"title": "A fractional phase-field model for two-phase flows with tunable sharpness: Algorithms and simulations", "highlights": [], "abstract": ["We develop a fractional extension of a mass-conserving Allen\u2013Cahn phase field model that describes the mixture of two incompressible fluids. The fractional order controls the sharpness of the interface, which is typically diffusive in integer-order phase-field models. The model is derived based on an energy variational formulation. An additional constraint is employed to make the Allen\u2013Cahn formulation mass-conserving and comparable to the Cahn\u2013Hilliard formulation but at reduced cost. The spatial discretization is based on a Petrov\u2013Galerkin spectral method whereas the temporal discretization is based on a stabilized ADI scheme both for the phase-field equation and for the Navier\u2013Stokes equation. We demonstrate the spectral accuracy of the method with fabricated smooth solutions and also the ability to control the interface thickness between two fluids with different viscosity and density in simulations of two-phase flow in a pipe and of a rising bubble. We also demonstrate that using a formulation with variable fractional order we can deal simultaneously with both erroneous boundary effects and sharpening of the interface at no extra resolution."]},
{"title": "Lifetime maximization through dynamic ring-based routing scheme for correlated data collecting in WSNs", "highlights": ["A solution to correlation data aggregation in WSNs is addressed in this paper.", "Data aggregation is processed along the ring to mitigate hotspots problem.", "The proposed scheme is proved to improve network lifetime by 200% or more.", "Our scheme is more practicality which allows low complexity implementation."], "abstract": ["This work provides a novel dynamic ring-based routing scheme for correlation data aggregation named Ring-Based Correlation Data Routing (RBCDR) scheme. In this scheme, first, nodal data is routed to rings which have abundant energy in minimum hops, and then all data aggregation is processed along the ring, after that, the aggregated data is routed to the sink with shortest route. Compared with current research, RBCDR scheme has higher network lifetime. RBCDR scheme processes data aggregation in non-hotspots regions which have abundant energy and then routes all aggregated data to the sink, achieving less data sent to the sink and thus decreasing the energy consumption in hotspots near the sink, therefore, it significantly improves the network lifetime. Through theoretical analysis and simulation results, our scheme is proved to improve network lifetime by 200\u2013340%, compared with sink-centered baseline version data aggregation scheme."]},
{"title": "An augmented virtual fixture to improve task performance in robot-assisted live-line maintenance", "highlights": ["We introduce the concept of augmented virtual fixtures to reduce tracking error.", "We experimentally validate effectiveness of the proposed scheme.", "The application, targeted in this paper, is live transmission line maintenance.", "The proposed scheme shows a marked improvement over the virtual fixture mode alone.", "Specifically, for presented experiments, our scheme reduces position error by 71%."], "abstract": ["Virtual fixtures can be used in haptic-enabled hydraulic telemanipulators to facilitate certain tasks. Using this concept, however, the operator may tend to move the master fast due to relying on the virtual fixture. As a result, the slave manipulator could start to lag due to latency in the hydraulic actuation control system. This paper describes how to mitigate the position errors between master and slave robots by overlaying an augmentation force on the master that is collinear but opposite of the master instantaneous velocity. The magnitude of this force is proportional to the position error at the slave end-effector. Experiments, conducted on a teleoperated hydraulic manipulator to perform several live-line maintenance tasks, show that the augmented scheme exhibits less position error at the slave side, better task quality, but longer task completion time as compared to the virtual fixture alone."]},
{"title": "Energy-aware routing algorithm for wireless sensor networks", "highlights": ["A new energy aware routing algorithm has been proposed for cluster based wireless sensor networks.", "It achieves ", "(1) message complexity per sensor node and ", "(", ") time complexity for a WSN having ", " sensor nodes.", "It efficiently forms the directed virtual backbone of cluster heads to facilitate data routing to the sink.", "It is successful in balancing the relaying load among the sensor nodes with respect to their residual energy."], "abstract": ["The main constraint of wireless sensor networks (WSNs) is the limited and generally irreplaceable power source of the sensor nodes. Therefore, designing energy saving routing algorithm is one of the most focused research issues. In this paper, we propose an energy aware routing algorithm for cluster based WSNs. The algorithm is based on a clever strategy of cluster head (CH) selection, residual energy of the CHs and the intra-cluster distance for cluster formation. To facilitate data routing, a directed virtual backbone of CHs is constructed which is rooted at the sink. The proposed algorithm is also shown to balance energy consumption of the CHs during data routing process. We prove that the algorithm achieves constant message and linear time complexity. We test the proposed algorithm extensively. The experimental results show that the algorithm outperforms other existing algorithms in terms of network lifetime, energy consumption and other parameters."]},
{"title": "Field programmable gate array implementation of spectrum allocation technique for cognitive radio networks", "highlights": ["To maximize the network utilization, spectrum allocation technique fairly allocates the channels to secondary users.", "SA problem is solved by Differential Evolution algorithm and compared the performance with PSO and FA.", "DE improved the quality of solution and time complexity by 29.9%, 242.32% and 19.04%, 46.3% compared to PSO and FA.", "We propose FPGA based coprocessor for DE-SA IP and interfaced to PowerPC.", "The coprocessor accelerates SA task by 76.79\u2013105x and 5.19\u20136.91x compared to float and fixed DE-SA software."], "abstract": ["Cognitive radio is an emerging technology in wireless communications for dynamically accessing under-utilized spectrum resources. In order to maximize the network utilization, vacant channels are assigned to cognitive users without interference to primary users. This is performed in the spectrum allocation (SA) module of the cognitive radio cycle. Spectrum allocation is a NP hard problem, thus the algorithmic time complexity increases with the cognitive radio network parameters. This paper addresses this by solving the SA problem using Differential Evolution (DE) algorithm and compared its quality of solution and time complexity with Particle Swarm Optimization (PSO) and Firefly algorithms. In addition to this, an Intellectual Property (IP) of DE based SA algorithm is developed and it is interfaced with PowerPC440 processor of Xilinx Virtex-5 FPGA via Auxiliary Processor Unit (APU) to accelerate the execution speed of spectrum allocation task. The acceleration of this coprocessor is compared with the equivalent floating and fixed point arithmetic implementation of the algorithm in the PowerPC440 processor. The simulation results show that the DE algorithm improves quality of solution and time complexity by 29.9% and 242.32%, 19.04% and 46.3% compared to PSO and Firefly algorithms. Furthermore, the implementation results show that the coprocessor accelerates the SA task by 76.79\u2013105\u00d7 and 5.19\u20136.91\u00d7 compared to floating and fixed point implementation of the algorithm in PowerPC processor. It is also observed that the power consumption of the coprocessor is 26.5", "\u00a0", "mW."]},
{"title": "Coding ECG beats using multiscale compressed sensing based processing", "highlights": ["Multiscale compressive sensing based processing is applied for Electrocardiograms.", "The wavelet coefficients at different subbands are sparse in nature.", "Compressed measurements are taken at wavelet scales and measurements are encoded for further compression.", "Method is evaluated using pathological ECG signals from CSE database, synthetic and normal ECGs.", "Distortion introduced is evaluated by quantitative and qualitative analysis like PRD, WEDD, RMSE and MOS."], "abstract": ["Compressed sensing recovers a sparse signal from a small set of linear, nonadaptive measurements. A sparse signal can be represented by compressed measurements with a reduced number of projections on a set of random vectors. In this paper, a multiscale compressed sensing based processing is investigated for an electrocardiogram signal which yields coded measurements. In case of an electrocardiogram (ECG) signal, the coded measurements are expected to retain the clinical information. To achieve this, compressed sensing based processing is applied at each wavelet scale and measurements are coded using Huffman coder. The measurements at each scale use random sensing matrix with independent identically distributed (i.i.d.) entries formed by sampling a Gaussian distribution. The proposed method is evaluated using pathological ECG signals from the CSE database, synthetic and normal ECGs. It helps preserve the pathological information and clinical components in compressed signal. The compressed signal quality is evaluated using standard distortion measures and mean opinion score (MOS). The MOS values for the signals range from ", " to ", " with a wavelet energy based diagnostic distortion (WEDD) value of ", " which falls under the excellent category."]},
{"title": "Distributed multi-agent scheme support for service continuity in IMS-4G-Cloud networks", "highlights": ["Quality of Service supports service continuity in heterogeneous networks is achieved by agent-based concepts.", "The shorter handoff delay and better QoS for real-time service applications is proposed.", "The QoS mechanism and the intelligent agent are required for cooperative QoS-awareness networking.", "The QoS management mechanism based on agents develops a cost-effective manner in IMS-4G-Cloud network."], "abstract": ["In this study, the Quality of Service (QoS) needed to support service continuity in heterogeneous networks is achieved by a Distributed Multi-Agent Scheme (DMAS) based on cooperation concepts and an awareness algorithm. A set of problem solving agents autonomously process local tasks and cooperatively interoperate via an in-cloud blackboard system to provide QoS and mobility information. A Q-Learning awareness algorithm calculates the exceptive rewards of a handoff to all access networks. These rewards are then used by problem solving agents to determine what actions must be performed. Agents located in the integrated IMS-4G-Cloud networks handle service continuity by using a handoff mechanism. Through operations and cooperation among active agents, these phases select a policy for predictive and anticipated IP Multimedia Subsystem (IMS) handoff management. Compared with conventional IMS handoff management, the proposed DMAS scheme achieves shorter handoff delay and better QoS for real-time service applications."]},
{"title": "Bandwidth efficient cluster-based data aggregation for Wireless Sensor Network", "highlights": ["Cluster head is elected according to the highest energy among the CM, number of neighbor nodes with one hop connectivity.", "Network uses random distribution of heterogeneous nodes with mobile sink.", "Packets are generated at variable rate by each node and aggregated at CH and then at sink.", "It uses the perfectly compressible aggregation function on data generated by nodes.", "The performance is measured in terms of PDR and throughput to show the effective utilization of bandwidth."], "abstract": ["A fundamental challenge in the design of Wireless Sensor Network (WSNs) is the proper utilization of resources that are scarce. The critical challenge is to maximize the bandwidth utilization in data gathering and forwarding from sensor nodes to the sink. The main design objective is to utilize the available bandwidth efficiently. The proposed Bandwidth Efficient Cluster-based Data Aggregation (BECDA) algorithm presents the solution for the effective data gathering with in-network aggregation. It considers the network with heterogeneous nodes in terms of energy and mobile sink to aggregate the data packets. The optimal approach is achieved by intra and inter-cluster aggregation on the randomly distributed nodes with the variable data generation rate. The proposed algorithm uses the correlation of data within the packet for applying the aggregation function on the data generated by nodes. BECDA shows significant improvement in PDR (67.44% and 26.79%) and throughput (41.25% and 26.16%) as compared to the state-of-the-art solutions."]},
{"title": "Exploring DWT\u2013SVD\u2013DCT feature parameters for robust multiple watermarking against JPEG and JPEG2000 compression", "highlights": ["A blind image watermarking scheme exploiting the DWT\u2013SVD\u2013DCT features is presented.", "The proposed PQIM reaches a trade-off between robustness and imperceptibility.", "Multiple watermarks can be embedded into a host image.", "The watermarks exhibit exceptional robustness against JPEG and JPEG2000 compression."], "abstract": ["This paper presents a novel scheme to implement blind image watermarking based on the feature parameters extracted from a composite domain including the discrete wavelet transform (DWT), singular value decomposition (SVD), and discrete cosine transform (DCT). Multiple bits can be embedded into a single image block by adjusting designated parameters via a progressive quantization index modulation technique. The quantization with respect to the feature parameters obtained in the DWT\u2013SVD\u2013DCT domain leads to efficient watermark extraction without referring to the original image. Experimental results show that the embedded watermarks exhibit exceptional robustness against image compression using JPEG and JPEG2000 coding standards."]},
{"title": "Flexible architecture for cluster evolution in cloud computing", "highlights": ["FACE supports system primitives that allow application developers to develop various applications in clouds.", "FACE allows application developers to customize data partitioning, localization, and processing procedures.", "FACE designs its system primitives in a language-independent and platform-independent way.", "FACE makes extensible the Master of a MapReduce system by application developers."], "abstract": ["MapReduce is considered the key behind the success of cloud computing because it not only makes a cluster highly scalable but also allows applications to use resources in a cluster. However, MapReduce achieves this simplicity at the expense of flexibility for data partitioning, localization, and processing procedures by handling all issues on behalf of application developers. Unfortunately, MapReduce currently has no solution capable of giving application developers flexibility in customizing data partitioning, localization, and processing procedures. To address the aforementioned flexibility constraints of MapReduce, we propose an architecture called Flexible Architecture for Cluster Evolution (FACE) which is both language-independent and platform-independent. FACE allows a MapReduce cluster to be designed to match various application requirements by customizing data partitioning, localization, and processing procedures. We compare the performance of FACE with that of a general MapReduce system and then demonstrate performance improvements with our implemented procedures."]},
{"title": "Experimental analysis of solar energy harvesting circuits efficiency for low power applications", "highlights": ["Two solar energy harvesting circuits have been tested for low power applications.", "The directly coupling technique has been compared with an MPPT integrated circuit.", "A mathematical energy consumption analysis shows both circuits\u2019 efficiency.", "For all tests performed the directly coupled circuit has been more efficient.", "Solar cell\u2019s input current simulations have confirmed the experimental results."], "abstract": ["This work presents an analysis on efficiency of solar energy harvesting circuits focused on low power, low voltage sensor platforms. Two different approaches were tested in order to operate a solar panel closely to its maximum power point. The first circuit precisely matches the solar cells with the batteries. The second one is based on a Maximum Power Point Tracker (MPPT) chip. The paper addresses the circuits\u2019 design and evaluation. Two tests were performed outdoors, under two different irradiance conditions. Although the MPPT chip may be efficient for a variety of low power devices, experiments have shown that it did not extract more energy from the environment than the directly coupled circuit. A mathematical energy consumption analysis shows that, in both cases, the directly coupled circuit is more efficient. Therefore, this work shows that there is still a lack of industry solutions for low power, low voltage, solar harvesting circuits."]},
{"title": "Dynamic security management for real-time embedded applications in industrial networks", "highlights": ["Introducing security-critical applications based on embedded control server systems for industrial networks.", "Establishing the security-aware task, security overhead and risk models for aperiodic real-time applications.", "Combining the soft real-time and security requirements into a unified framework.", "Deploying proportional controllers to achieve satisfied fine-grained control."], "abstract": ["Widely deployed real-time embedded systems can improve the performance of industrial applications, but these systems also face the critical challenge of providing high quality security in an unpredictable network environment. We measure the time and energy consumptions of commonly used cryptographic algorithms on a real embedded platform and introduce a method to quantify the security risk of real-time applications. We propose a Dynamic Security Risk Management (DSRM) mechanism to manage the aperiodic real-time tasks for networked industrial applications. Inspired by the feedback design philosophy, DSRM is designed as a two-level control mechanism. The upper-level component makes efforts to admit or reject the arrival tasks and assigns the reasonable security level for each admitted task. With three proportional feedback controllers at the lower level, the security level of each ready task can be adjusted adaptively according to the dynamic environments. Simulation results show the superiority of the proposed mechanism."]},
{"title": "An incremental privacy-preservation algorithm for the (", "highlights": ["An efficient algorithm is developed to prevent incremental privacy breach.", "Only the most recent previously-released data is required for privacy preservation.", "The solution can always be guaranteed the optimal result."], "abstract": ["An important issue to be addressed when data are to be published is data privacy. In this paper, the problem of data privacy based on a prominent privacy model, ", "-Anonymous, is addressed. Our scenario is that when a new dataset is to be released, there may be, at the same time, datasets that were released elsewhere. A problem arises because some attackers might obtain multiple versions of the same dataset and compare them with the newly released dataset. Although the privacy of all of the datasets has been well-preserved individually, such a comparison can lead to a privacy breach, which is a so-called \u201cincremental privacy breach\u201d. To address this problem effectively, we first study the characteristics of the effects of multiple dataset releases with a theoretical approach. It has been found that a privacy breach that is subjected to an increment occurs when there is overlap between any parts of the new dataset with any parts of an existing dataset. Based on our proposed studies, a polynomial-time algorithm is proposed. This algorithm needs to consider only one previous version of the dataset, and it can also skip computing the overlapping partitions. Thus, the computational complexity of the proposed algorithm is reduced from ", " to only ", " where ", " is the number of partitions, ", " is the number of tuples, and ", " is the number of released datasets. At the same time, the privacy of all of the released datasets as well as the optimal solution can be always guaranteed. In addition, experiment results that illustrate the efficiency of our algorithm on real-world datasets are presented."]},
{"title": "An experimental study of attacks on the availability of Glossy", "highlights": ["Presentation of novel attacks aiming to break Glossy's constructive interference.", "Results showing Glossy\u2019s robustness except for attacks on relay counter.", "Discussion of potential security solutions to protect Glossy-based networks."], "abstract": ["Glossy is a reliable and low latency flooding mechanism designed primarily for distributed communication in wireless sensor networks (WSN). Glossy achieves its superior performance over tree-based wireless sensor networks by exploiting identical concurrent transmissions. WSNs are subject to wireless attacks aimed to disrupt the legitimate network operations. Real-world deployments require security and the current Glossy implementation has no built-in security mechanisms. In this paper, we explore the effectiveness of several attacks that attempt to break constructive interference in Glossy. Our results show that Glossy is quite robust to approaches where attackers do not respect the timing constraints necessary to create constructive interference. Changing the packet content, however, has a severe effect on the packet reception rate that is even more detrimental than other physical layer denial-of-service attacks such as jamming. We also discuss potential countermeasures to address these security threats and vulnerabilities."]},
{"title": "Discreet verification of user identity in pervasive computing environments using a non-intrusive technique", "highlights": ["A user-centric, unobtrusive approach for verifying the user\u2019s identity is proposed.", "The approach uses knowledge about the user\u2019s behaviour to infer their identity.", "The approach uses a simple numerical algorithm to assert the user\u2019s identity."], "abstract": ["This paper presents a new approach for verifying user identity in pervasive environments using a non-intrusive behaviour tracking technique that offers minimum interruption to the user\u2019s activities. The technique, termed Non-intrusive Identity Assertion System (NIAS), uses knowledge of how the user uses the environment\u2019s services to infer their identity. The technique monitors user behaviour through identifying certain types of activity without the need for detailed tracking of user behaviour, thus minimising intrusion on the user\u2019s normal activities. The technique was evaluated using a simulated environment to assess its reliability. Simulation results show that the technique can be applied in various situations such as strict and relaxed security settings by applying different security policies. They also show that the technique is particularly effective where the environment has a mixture of high and low security resources in which case reliability could exceed 99%."]},
{"title": "A genetic-based approach to web service composition in geo-distributed cloud environment", "highlights": ["A service composition model in geo-distributed cloud environment is proposed.", "The composition approach considers both QoS of Web services and QoS of network.", "We propose a genetic algorithm to solve the composition problem.", "The notion of skyline is used to generate the initial population."], "abstract": ["Independent fine-grain web services can be integrated to a value-added coarse-grain service through service composition technologies in Service Oriented Architecture. With the advent of cloud computing, more and more web services in cloud may provide the same function but differ in performance. In addition, the development of cloud computing presents a geographically distributed manner, which elevates the impact of the network on the QoS of composited web services. Therefore, a significant research problem in service composition is how to select the best candidate service from a set of functionally equivalent services in terms of a service level agreement (SLA). In this paper, we propose a composition model that takes both QoS of services and cloud network environment into consideration. We also propose a web service composition approach based on genetic algorithm for geo-distributed cloud and service providers who want to minimize the SLA violations."]},
{"title": "A distributed storage framework of FlowTable in software defined network", "highlights": ["SDN-enabled network functions are limited by the switch TCAM capacity.", "We propose a distributed storage framework of FlowTable.", "Our framework can achieve high availability and failure resilience.", "There is a tradeoff between cost and high availability/failure resilience.", "We achieve the storage capacity without inviting large communication overhead."], "abstract": ["Openflow, a novel Software Defined Network (SDN) technology, is developing rapidly and has already been utilized in many fields. It facilitates decoupling between the control and forwarding plane, enabling users to code the network functions easily and replace the traditional high-cost network functions devices. The FlowTable of Openflow, its base of operating the network packets, consists of many flow entries and is stored in the Ternary Content Addressable Memories (TCAMs) of the Openflow switch. When the FlowTable occupies the entire storage space of the Openflow switch and more flow entries are added, the delete operation on the TCAMs increases, and the latency and loss of packets deteriorates \u2013 this is the primary issue. In order to solve this problem, this paper proposes a distributed storage framework which stores the FlowTable in multiple Openflow switches, equipped with small TCAMs. To conclude, this paper simulates the algorithms used in the framework and builds a testbed. The experimental results prove the framework\u2019s feasibility and successful performance."]},
{"title": "Interference-aware spectrum sensing mechanisms in cognitive radio networks", "highlights": ["Study spectrum sensing mechanisms by the optimization of network parameters and strategies.", "The proposed mechanisms have fewer sensing overhead.", "SUs can access into the spectrum holes efficiently, and transmit with a low power."], "abstract": ["This paper focuses on the spectrum sensing mechanisms, which could improve network throughput through the sensing strategy optimization and cooperative spectrum sensing methods. In order to guarantee an integrated and effective research, we take the whole channel scenarios into consideration, i.e., Single Secondary user with Single and Multiple Channels (SSSC and SSMC), Multiple Secondary users with Single and Multiple Channels (MSSC and MSMC). Moreover, according to the specific feature of each scenario, different sensing methods are adopted, i.e., optimal sensing period to maximize network throughput for SSSC, a novel sensing method to minimize searching time for SSMC, partial cooperative spectrum sensing mechanism for MSMC, and setting a spectrum pool in the fusion center to record the channel states for MSMC. Simulation results demonstrate that our methods can improve spectrum efficiency, network throughput and channel utilization, especially when the spectrum is utilized inadequately."]},
{"title": "BotGrab: A negative reputation system for botnet detection", "highlights": ["A novel negative reputation system is proposed to detect bot-infected hosts.", "It considers both malicious activities and history of coordinated group activities.", "A proposed online incremental clustering technique facilitates the online learning.", "The negative reputation threshold can adjust the sensitivity of the system.", "It can successfully detect various botnets with a high DR and a low FAR."], "abstract": ["Botnets continue to be used by attackers to perform various malicious activities on the Internet. Over the past years, many botnet detection techniques have been proposed; however, most of them cannot detect botnets in an early stage of their lifecycle, or they often depend on a specific command and control protocol. In this paper, we propose BotGrab, a general botnet detection system that considers both malicious activities and the history of coordinated group activities in the network to identify bot-infected hosts. BotGrab tracks suspected hosts participating in some coordinated group activities and calculates a negative reputation score for each of them based on the history of their participation in these activities. A suspected host will be identified as being bot-infected if it has a high negative reputation score or performs some malicious activities while having a low negative reputation score. We demonstrate the effectiveness of BotGrab to detect various botnets including HTTP-, IRC-, and P2P-based botnets using a testbed network consisting of some bot-infected hosts."]},
{"title": "Multi-modal decision fusion for continuous authentication", "highlights": ["Behavioral biometrics: keystroke dynamics, mouse movement, stylometry.", "A parallel binary decision fusion architecture with 11 sensors.", "A dataset collected from 67 users each working in an office environment for a week.", "Achieve below 1% error rates (FAR, FRR) after only 30", "\u00a0", "s of activity.", "Characterize robustness of system to adversarial attacks."], "abstract": ["Active authentication is the process of continuously verifying a user based on their on-going interaction with a computer. In this study, we consider a representative collection of behavioral biometrics: two low-level modalities of keystroke dynamics and mouse movement, and a high-level modality of stylometry. We develop a sensor for each modality and organize the sensors as a parallel binary decision fusion architecture. We consider several applications for this authentication system, with a particular focus on secure distributed communication. We test our approach on a dataset collected from 67 users, each working individually in an office environment for a period of approximately one week. We are able to characterize the performance of the system with respect to intruder detection time and robustness to adversarial attacks, and to quantify the contribution of each modality to the overall performance."]},
{"title": "An SDMA-based MAC protocol for wireless ad hoc networks with smart antennas", "highlights": ["An SDMA-based MAC protocol (S-MAC) for wireless ad hoc networks is proposed.", "Exploiting the creation of spatial channels to enhance network capacity.", "Providing collision-free access to the communication medium based on the location of a node.", "Improving the performance of throughput and spatial channel reuse."], "abstract": ["A wireless ad hoc network consists of a set of wireless devices. The wireless devices are capable of communicating with each other without the assistance of base stations. Space Division Multiple Access (SDMA) is a new technology designed to optimize the performance of current and future mobile communication systems. In this paper, an SDMA-based MAC protocol (S-MAC) for wireless ad hoc networks with smart antennas is proposed. The proposed protocol exploits the SDMA system to allow reception of more than one packet from spatially separated transmitters. Using SDMA technology provides collision-free access to the communication medium based on the location of a node. The proposed protocol solves the hidden terminal problem, the exposed terminal problem, and the deafness problem. Simulation results demonstrate the effectiveness of the proposed S-MAC in improving throughput and increasing spatial channel reuse."]},
{"title": "A design of low swing and multi threshold voltage based low power 12T SRAM cell", "highlights": ["A novel low power 12T MTCMOS based SRAM cell is proposed.", "Charge recycling technique used for reducing the current leakage during transition mode.", "Use voltage sources to reduce the dynamic power dissipation.", "Improving the stability of SRAM cell."], "abstract": ["This paper focuses on the design of a novel low power twelve transistor static random access memory (12T SRAM) cell. In the proposed structure two voltage sources are used, one connected with the bit line and the other one connected with the bitbar line in order to reduce the swing voltage at the output nodes of the bit and the bitbar lines, respectively. Reduction in swing voltage reduces the dynamic power dissipation when the SRAM cell is in working mode. Low threshold voltage (LVT) transmission gate (TG) and two high threshold voltage (HVT) sleep transistors are used for applying the charge recycling technique. The charge recycling technique reduces leakage current when the transistors change its state from sleep to active (OFF to ON condition) and active to sleep (ON to OFF condition) modes. Reduction in leakage current causes the reduction in static power dissipation. Stability of the proposed SRAM has also improved due to the reduction in swing voltage. Simulation results of power dissipation, access time, current leakage, stability and power delay product of the proposed SRAM cell have been determined and compared with those of some other existing models of SRAM cell. Simulation has been done in 45", "\u00a0", "nm CMOS environment. Microwind 3.1 is used for schematic design and layout design purpose."]},
{"title": "CSTORE: A desktop-oriented distributed public cloud storage system", "highlights": ["A desktop-oriented distributed public cloud storage system is proposed.", "Three-level mapping hash method is used to distribute and locate data.", "Using migration and rank extension to implement load balancing and fault recovery.", "Sequence numbers are used to guarantee consistency.", "Implement data deduplication and use Bloom filter to recycle rubbish."], "abstract": ["Previous distributed file systems aim at storing very large data sets. Their architectures are often designed to support large-scale data-intensive applications, which cannot cope with massive daily users who want to store their data on the Internet. In this paper, CSTORE is proposed to support mass data storage for a large number of users. The user-independent metadata management can ensure data security through assigning an independent namespace to every user. Operating logs are applied to synchronize simultaneous sessions of the same user and resolve conflicts. We also implement a block-level deduplication strategy based on our three-level mapping hash method for the large quantity of repeated data. The migration and rank extension on the hash rules are defined to achieve load balancing and capacity expansion. Performance measurements under a variety of workloads show that CSTORE offers the better scalability and performance than other public cloud storage systems."]},
{"title": "Smart wireless sensor networks for online faults diagnosis in induction machine", "highlights": ["A new method has been proposed for online faults diagnosis in induction motors based on smart WSN combined with motor current signature analysis using FFT.", "The proposed method is novel as it is important to install low cost sensors and detection mechanisms along with induction machines to achieve short detection time and an automated way of reporting the fault.", "The system can distinguish a faulty motor from a healthy motor with a probability of 99% with less than 5% of false alarm.", "Simulation results presented show the efficiency of the proposed method to detect faults in induction machine."], "abstract": ["Online induction machine faults diagnosis is a concern to guarantee the overall production process efficiency. Nowadays, the industry demands the integration of smart wireless sensors networks (WSN) to improve the fault detection in order to reduce cost, maintenance and power consumption. Induction motors can develop one or more faults at the same time that can produce sever damages. The origin of most recurrent faults in rotary machines is in the components: stator, rotor, bearing and others. This work presents a novel methodology for the online faults diagnosis in induction motors. This technique uses the smart WSN to obtain the machine condition based on the motor stator current analysis. The implementation of the proposed smart sensor methodology allows the system to perform online fault detection in a fully automated way. Simulation results presented show the efficiency of the proposed method to detect simple and multiple faults in induction machine. It provides detailed analysis to address challenges in designing and deploying WSNs in industrial environments, and its reliability."]},
{"title": "Using Software Defined Networking to manage and control IEC 61850-based systems", "highlights": ["We describe the IEC 61850 communication model and provide an SDN-based framework.", "This framework mainly uses the OpenFlow, sFlow, and OVSDB protocols.", "It controls an IEC 61850 network, including resource analysis and management.", "It integrates traffic engineering techniques, such as QoS or traffic filtering."], "abstract": ["Smart Grid makes use of Information and Communications Technology (ICT) infrastructures for the management of the generation, transmission and consumption of electrical energy to increase the efficiency of remote control and automation systems. One of the most widely accepted standards for power system communication is IEC 61850, which defines services and protocols with different requirements that need to be fulfilled with traffic engineering techniques. In this paper, we discuss the implementation of a novel management framework to meet these requirements through control and monitoring tools that provide a global view of the network. With this purpose, we provide an overview of relevant Software Defined Networking (SDN) related approaches, and we describe an architecture based on OpenFlow that establishes different types of flows according to their needs and the network status. We present the implementation of the architecture and evaluate its capabilities using the Mininet network emulator."]},
{"title": "A combinatorial optimization algorithm for multiple cloud service composition", "highlights": ["The COM2 algorithm efficiently considers multiple clouds while composing services.", "The proposed algorithm successfully competes with previous algorithms.", "Low examined service number is achieved without impacting the combined cloud number."], "abstract": ["Service composition is an evolving approach that increases the number of applications of cloud computing by reusing existing services. However, the available methods focus on generating composite services from a single cloud, which limits the benefits that are derived from other clouds. This paper proposes a novel COMbinatorial optimization algorithm for cloud service COMposition (COM2) that can efficiently utilize multiple clouds. The proposed algorithm ensures that the cloud with the maximum number of services will always be selected before other clouds, which increases the possibility of fulfilling service requests with minimal overhead. The experimental results demonstrate that the COM2 successfully competes with previous multiple cloud service composition algorithms by examining a small number of services\u2014which directly relates to execution time\u2014without compromising the number of combined clouds."]},
{"title": "Process fault detection based on dynamic kernel slow feature analysis", "highlights": ["A nonlinear dynamic process monitoring method is presented.", "The proposed method can extract the inherent slow features from the high-dimensional observed data.", "A statistic index is built based on slow features to carry out process monitoring.", "The method is more sensitive to process faults than the conventional KPCA-based method."], "abstract": ["A fault detection method based on dynamic kernel slow feature analysis (DKSFA) is presented in the paper. SFA is a new feature extraction technology which can find a group of slowly varying feature outputs from the high-dimensional inputs. In order to analyze the nonlinear dynamic characteristics of the process data, DKSFA is presented which applies the augmented matrix to consider the dynamic characteristic and uses kernel slow feature analysis (KSFA) to extract the nonlinear slow features hidden in the observed data. For the purpose of fault detection, the ", " monitoring statistic index is built based on DKSFA model and its confidence limit is computed by kernel density estimation. Simulations on a nonlinear system and Tennessee Eastman (TE) benchmark process show that the proposed method has a better fault detection performance compared with the conventional (kernel principal component analysis) KPCA-based method."]},
{"title": "A cloud based and Android supported scalable home automation system", "highlights": ["A cloud based and Android supported scalable home automation system is proposed.", "The mechanism relies on the distributed Google Cloud Platform.", "Google Cloud Messaging supports the communication infrastructure in the system.", "The prototype is evaluated based on a list of criteria for an adequate system."], "abstract": ["In this paper, an Android based home automation system that allows multiple users to control the appliances by an Android application or through a web site is presented. The system has three hardware components: a local device to transfer signals to home appliances, a web server to store customer records and support services to the other components, and a mobile smart device running Android application. Distributed cloud platforms and Google services are used to support messaging between the components. The prototype implementation of the proposed system is evaluated based on the criteria considered after the requirement analysis for an adequate home automation system. The paper presents the outcomes of a survey carried out regarding the properties of home automation systems, and also the evaluation results of the experimental tests conducted with volunteers on running prototype."]},
{"title": "Enhancing secure routing in Mobile Ad Hoc Networks using a Dynamic Bayesian Signalling Game model", "highlights": ["Collaboration between mobile nodes is significant in Mobile Ad Hoc Networks.", "Dynamic Bayesian Signalling Game model is proposed to enhance secure routing and motivate effective cooperation among nodes.", "A novel scheme of payoff formulation is developed to motivate the misbehaving nodes.", "We used a belief updating system to update a node\u2019s belief in terms of action chosen, message sent, and strategy chosen.", "The proposed scheme could significantly minimize the misbehaving activities of malicious nodes and thereby enhance secure routing."], "abstract": ["Collaboration between mobile nodes is significant in Mobile Ad Hoc Networks (MANETs). The great challenges of MANETs are their vulnerabilities to various security attacks. Because of the lack of centralized administration, secure routing is challenging in MANETs. Effective secure routing is quite essential to protect nodes from anonymous behaviours. Game theory is currently employed as a tool to analyse, formulate and solve selfishness issues in MANETs. This work uses a Dynamic Bayesian Signalling Game to analyse strategy profiles for regular and malicious nodes. We calculate the Payoff to nodes for motivating the particular nodes involved in misbehaviour. Regular nodes monitor continuously to evaluate their neighbours by using the belief evaluation and belief updating system of the Bayes rule. Simulation results show that the proposed scheme could significantly minimize the misbehaving activities of malicious nodes and thereby enhance secure routing."]},
{"title": "Application of van der Pol\u2013Duffing oscillator in weak signal detection", "highlights": ["Proposed van der Pol\u2013Duffing oscillator based weak signal detection method.", "The signal-to-noise ratio of the weak signal can be detected \u221245", "\u00a0", "dB.", "Compared to Duffing oscillator more robust against the different frequency signal."], "abstract": ["This study presents a new weak signal detection method based on the van der Pol\u2013Duffing oscillator. The principle of the proposed method is described. A weak signal is detected through the transition from the chaotic to the periodic state. Numerical simulation shows that the van der Pol\u2013Duffing oscillator is sensitive to a weak signal under strong noise conditions. Several aspects of the proposed method, including the noise influence, influence of different frequency signals, and influence of the phase shift, are studied in detail. Results indicate that the application of the van der Pol\u2013Duffing oscillator to weak signal detection is feasible."]},
{"title": "An efficient network on-chip architecture based on isolating local and non-local communications", "highlights": ["We propose two-layer network on chip to separate local and non-local traffics.", "For each locality rate, we obtain the best division ratio for the bitwidth of channels.", "We define locality based on the number of hops between source and destination nodes.", "For each traffic, locality is defined to include 50 percent of all communication."], "abstract": ["In this paper, a locality aware NoC communication architecture is proposed. The architecture may reduce the energy consumption and latency in MultiProcessor Systems on Chips (MPSoCs). It consists of two network layers which one layer is dedicated to the packets transmitted to near destinations and the other layer is used for the packets transmitted to far destinations. The actual physical channel width connecting the cores is divided between the two layers. The locality is defined based on the number of hops between the nodes. The relative significances of the two types of communications determine the optimum ratio for the channel width division. To assess the efficiency of the proposed method, we compare its communication latency with that of conventional one for different channel widths, communication traffic profiles, and mesh sizes."]},
{"title": "Finger knuckle biometrics \u2013 A review", "highlights": ["Describes the role of finger knuckle surface in biometric authentication.", "Enumerates various available open datasets for finger knuckle biometric trait.", "Presents different view point in the classification of various finger knuckle feature recognition methods.", "Elaborates on finger knuckle surface preprocessing, feature extraction, classification and fusion methodologies.", "Addresses significant issues that are mandatory for deploying finger knuckle biometric system."], "abstract": ["Biometric-based personal authentication is receiving a widespread interest in the area of research due to its high applicability in a wide range of security applications. Among these, hand-based biometric systems are considered to be more successful in terms of accuracy and computational complexity. In hand-based biometrics, finger knuckle surface is considered as one of the emerging potential biometric traits for personal authentication. This is due to its stable and unique inherent patterns present in the outer surface of the finger back knuckle region. Further, this finger knuckle has a high potentiality towards discriminating individuals with high accuracy. In this paper, we present a review of various system models that are implemented for personal authentication using finger knuckle biometrics. Furthermore, the challenges that could arise during the implementation of the large scale real time biometric system with finger knuckle print are explored."]},
{"title": "Considerations for packet delivery reliability over polling-based wireless networks in smart grids", "highlights": ["We measure and model the path loss for wireless links in low voltage substations.", "No effect of EMI on Zigbee links is normally observed in high voltage substations.", "The probability of unsuccessful polling transactions ", " is analytically studied.", "Sending multiple copies of data by WiFi-5", "\u00a0", "GHz transmitters achieves near zero ", "."], "abstract": ["Supervisory control and data acquisition (SCADA) systems currently use the polling technique for monitoring electric utility networks. Unfortunately, conventional SCADA systems do not suit the needs of smart grids in terms of the required data rate. Polling-based wireless networks can extend the capabilities of SCADA systems as they provide low cost transceivers and bounded packet delay. However, the harsh environment of power stations negatively impacts the performance of wireless links. This paper introduces a field measurement-based study that focuses on the effect of power system noise and transients on packet delivery reliability of Zigbee and WiFi polling-based wireless networks. Extensive experiments show that the electromagnetic interference emitted from high voltage substations, during normal operation conditions, do not significantly affect wireless communication in the gigahertz range. Moreover, we analytically and experimentally demonstrate that abnormal operation conditions of power systems may negatively impact the reliability of packet delivery in polling-based wireless networks. Furthermore, we show that this negative impact can be mitigated by following some proposed technical considerations regarding the wireless standard, the operating frequency, the location, and the number of wireless transceivers used."]},
{"title": "A new method for removal of powerline interference in ECG and EEG recordings", "highlights": ["A method based on radial basis function and Wiener filter system is proposed for filtering powerline in biomedical recordings.", "The proposed solution addresses both ECG and EEG recordings.", "Several simulations have demonstrated the enhancement of the proposed method in comparison to other techniques.", "The results suggest that clinical information can be maintained.", "This method provides the best approach for obtaining both more signal reduction and low distortion of the signal results."], "abstract": ["Advanced medical diagnosing and research requires precise information which can be obtained from measured electrophysiological data, e.g., electroencephalogram (EEG) and electrocardiograph (ECG). However, they are often contaminated with noise and a variety of bioelectric signals called artefacts, e.g., electromyography (EMG). These noise and artefacts which need to be reduced make it difficult to distinguish normal from abnormal physiological activity. Electromagnetic contamination of recorded signals represents a major source of noise in electrophysiology and impairs the use of recordings for research. In this paper we present an effective method for cancelling 50", "\u00a0", "Hz (or 60", "\u00a0", "Hz) interference using a radial basis function (RBF) Wiener hybrid filter. One of the main points of this paper is the hybridization of the RBF filter and a Wiener filter to make full use of both merits. The effectiveness and validity of those filters are verified by applying them to ECG and EEG recordings. The results show that the proposed method is able to reduce powerline interference (PLI) from the noisy ECG and EEG signals more accurately and consistently in comparison to some of the state of-the-art methods and this method can be efficiently used with very low signal-to-noise ratios, while preserving original signal waveform."]},
{"title": "Optimized FPGA based continuous wavelet transform", "highlights": ["We design highly optimized architecture for continuous wavelet transform in FPGA.", "Lookup tables are highly recommended in case of implementing complex algorithms.", "Zero-free wavelet function and scale reduction save extensive amount of block RAM.", "Spartan 3AN FPGA device is suitable for real time EEG analysis with the CWT."], "abstract": ["A memory efficient field programmable gate array (FPGA) method is described that facilitates the processing of the continuous wavelet transform (CWT) arithmetic operations. The CWT computations were performed in Fourier space and implemented on FPGA following several optimization schemes. First, the adapted wavelet function was stored in a lookup table instead of computing the equation each time. Second, the utilization of FPGA memory was highly optimized by only storing the nonzero values of the wavelet function. This reduces 89% of the memory storage and allows fitting the entire design into the FPGA. Third, the design decreases the number of multiplications and shortens the time to produce the CWT coefficients. The proposed design was tested using EEG data and demonstrated to be suitable for extracting features from the event related potentials. Fourth, wavelet function scales were eliminated which saves further resources. The achieved computation speed allows for real time CWT application."]},
{"title": "Design and optimization of backside illuminated image sensor for epiretinal implants", "highlights": ["Backside illuminated pixel structure is proposed and evaluated as epiretinal prosthesis implant.", "The maximum fill factor is observed due to separation of photosensitive area.", "The application of deep backside Deep Trench Isolation (DTI) results in a significant reduction of crosstalk."], "abstract": ["Backside illuminated pixel structure is proposed and evaluated as the building block for the image sensor being used as epiretinal prosthesis implant. The image sensor pixel is designed with the parameters of 90", "\u00a0", "nm technology node of standard CMOS (Complementary Metal Oxide Semiconductor) process. The image sensor is consisted of a p-sub/n-well structure as the photosensitive area with the pixel pitch of 20", "\u00a0", "\u03bcm. The maximum fill factor is observed due to separation of photosensitive area with the readout transistor surface in backside illumination technology. 90% quantum efficiency at 600", "\u00a0", "nm wavelength and the dark current of 74.6", "\u00a0", "nA/cm", " at room temperature is achieved for the optimized pixel. The application of deep backside Deep Trench Isolation (DTI), with high depth n-well doping profiles, results in a significant reduction of crosstalk (5.6% total crosstalk)."]},
{"title": "Semantics-based approach for detecting flaws, conflicts and redundancies in XACML policies", "highlights": ["We provide policy analysis scheme to detect access contradictions among web services.", "We propose semantic-based policy analysis through deductive logic and inference rules.", "We present flaw, conflict and redundancy detection algorithms for XACML policy analysis.", "We show through experiments that SBA-XACML provides efficient detection mechanisms."], "abstract": ["XACML (eXtensible Access Control Markup Language) policies, which are widely adopted for defining and controlling dynamic access among Web/cloud services, are becoming more complex in order to handle the significant growth in communication and cooperation between individuals and composed services. However, the large size and complexity of these policies raise many concerns related to their correctness in terms of flaws, conflicts and redundancies presence. This paper addresses this problem through introducing a novel set and semantics based scheme that provides accurate and efficient analysis of XACML policies. First, our approach resolves the complexity of policies by elaborating an intermediate set-based representation to which the elements of XACML are automatically converted. Second, it allows to detect flaws, conflicts and redundancies between rules by offering new mechanisms to analyze the meaning of policy rules through semantics verification by inference rule structure and deductive logic. All the approach components and algorithms realizing the proposed analysis semantics have been implemented in one development framework. Experiments carried out on synthetic and real-life XACML policies explore the relevance of our analysis algorithms with acceptable overhead. Please visit ", " to download the framework."]},
{"title": "Hybrid computing techniques for fault detection and isolation, a review", "highlights": ["Data driven soft computing tools are explored to detect unprecedented changes.", "Failed state components are also determined.", "Individual schemes of fuzzy logic, ANN and GA are first implemented on a fault diagnosis problem.", "Hybrid versions including ANFIS and GA are applied to enhance the quality of fault diagnosis.", "Results are encouraging, hybrid GA+ANFIS, outperformed significantly other techniques."], "abstract": ["The classical model-based methods had often proven to be unable to provide acceptable solutions to modern fault diagnosis systems. Therefore, model-free or soft computing techniques such as fuzzy logic, Artificial Neural Networks (ANNs) and Genetic Algorithms (GAs) had become more attractive in industrial applications of fault diagnosis. In this paper, three SC schemes are explored to solve the problem of detecting unprecedented changes and finding the failed state components. First, individual fuzzy systems, ANN and GA are implemented on a fault diagnosis scheme. Then, hybrids of these techniques are applied to enhance the fault diagnosis precision. This approach allows gaining critical information about fault presence or its absence in the shortest possible time. The proposed scheme was simulated and evaluated extensively on a benchmark laboratory scale coupled-two-tank system. The results are encouraging, showing especially, that hybrid GA", "\u00a0", "+", "\u00a0", "ANFIS (GANFIS), outperformed significantly the other techniques."]},
{"title": "A novel algorithm using affine-invariant features for pose-variant face recognition", "highlights": ["Multi-scale autoconvolution features maintain invariance to noise and pose changes.", "Histogram equalization reduces the effect on MSA features caused by illumination.", "The number of (", ",", "\u00a0", ") pairs affects the size of MSA features and recognition rates.", "MSA", "\u00a0", "+", "\u00a0", "PCA algorithm exhibits excellent abilities on completely different databases."], "abstract": ["Pose variation is the major factor significantly affecting recognition efficiency in the field of face recognition. In this paper, we propose a novel algorithm for pose-variant face recognition. We first use the affine-invariant multi-scale autoconvolution (MSA) transformation to extract pose-invariant features. Following this, we use principal component analysis (PCA) to decorrelate the feature sets and reduce the number of required MSA dimensions. The PCA components with sufficiently large corresponding eigenvalues are then passed to the ", " nearest classifier. Compared with other pose-variant face recognition techniques, our proposed algorithm exhibits a superior ability to pose changes, illumination changes, and different face databases, moreover records a higher recognition rate."]},
{"title": "Study of wrist pulse signals using time domain spatial features", "highlights": ["Wrist pulse signals are analyzed using spatial features obtained from a bi-modal Gaussian model.", "Statistically significant group differences are found for two cases: before and after lunch, before and after exercise.", "A recursive cluster elimination based support vector machine classifier is used for classification.", "High classification accuracy is obtained for both exercise case (99.71%) and lunch case (99.94%).", "There is tangible scope for using these results in various healthcare applications."], "abstract": ["Blood travels throughout the body and thus its flow is modulated by changes in body condition. As a consequence, the wrist pulse signal contains important information about the status of the human body. In this work we have employed signal processing techniques to extract important information from these signals. Radial artery pulse pressure signals are acquired at wrist position noninvasively for several subjects for two cases of interest, viz. before and after exercise, and before and after lunch. Further analysis is performed by fitting a bi-modal Gaussian model to the data and extracting spatial features from the fit. The spatial features show statistically significant (", "\u00a0", "<", "\u00a0", "0.001) changes between the groups for both the cases, which indicates that they are effective in distinguishing the changes taking place due to exercise or food intake. Recursive cluster elimination based support vector machine classifier is used to classify between the groups. A high classification accuracy of 99.71% is achieved for the exercise case and 99.94% is achieved for the lunch case. This paper demonstrates the utility of certain spatial features in studying wrist pulse signals obtained under various experimental conditions. The ability of the spatial features in distinguishing changing body conditions can be potentially used for various healthcare applications."]},
{"title": "The Hamiltonian-based odd\u2013even turn model for maximally adaptive routing in 2D mesh networks-on-chip", "highlights": ["A highly adaptive routing method is proposed for wormhole-switched 2D mesh networks.", "The degree of adaptiveness is maximized by minimizing the number of prohibited turns.", "The deadlock-freedom is guaranteed without adding virtual channels.", "The proposed method is minimal and can be used for unicast/multicast routing.", "The number of hotspots is diminished and the traffic distribution is efficient."], "abstract": ["Networks-on-Chip (NoCs) have emerged as a promising solution for the communication crisis in today\u2019s high-performance Multi-Processor System-on-Chip (MPSoC) architectures. Routing methods have a prominent role in taking advantage of the potential benefits offered by NoCs. As a result, designing high-performance and efficient routing algorithms is highly desirable. In this paper, the Hamiltonian-based Odd\u2013Even (HOE) turn model is proposed for both unicast and multicast routing in wormhole-switched 2D mesh networks. HOE is able to maximize the degree of adaptiveness by minimizing the number of prohibited turns, such that the algorithm remains deadlock-free without adding virtual channels. By increasing the number of alternative minimal paths, the hotspots are less likely to be created and the traffic is efficiently distributed throughout the network. The simulation results in terms of latency and power consumption indicate the better performance of the proposed method in comparison with the existing routing methods."]},
{"title": "Hindi phoneme classification using Wiener filtered wavelet packet decomposed periodic and aperiodic acoustic feature", "highlights": ["24 subband WP decomposition according to the auditory ERB scale.", "Proposed wavelet subband specific periodic and aperiodic decomposition.", "Wiener filter is used at frontend for noise minimization.", "Hindi phoneme classification task has been carried out.", "Proposed technique outperforms others classify voiced phonemes."], "abstract": ["Wavelet packet (WP) acoustic features are found to be very promising in unvoiced phoneme classification task but they are less effective to capture periodic information from voiced speech. This motivated us to develop a wavelet packet based feature extraction technique that signifies both the periodic and aperiodic information. This method is based on parallel distributed processing technique inspired by the human speech perception process. This front end feature processing technique employs Equivalent Rectangular Bandwidth (ERB) filter like wavelet speech feature extraction method called Wavelet ERB Sub-band based Periodicity and Aperiodicity Decomposition (WERB-SPADE). Winer filter is used at front end to minimize the noise for further processing. The speech signal is filtered by 24 band ERB like wavelet filter banks, and then the output of each sub-band is processed through comb filter. Each comb filter is designed individually for each sub-band to decompose the signal into periodic and aperiodic features. Thus it carries the periodic information without losing certain important information like formant transition incorporated in aperiodic features. Hindi phoneme classification experiments with a standard HMM recognizer under both clean-training and multi-training condition is conducted. This technique shows significant improvement in voiced phoneme class without affecting the performance of unvoiced phoneme class."]},
{"title": "Adaptive block truncation coding technique using edge-based quantization approach", "highlights": ["An Adaptive block truncation coding technique (ABTC-EQ) is proposed.", "Edge image is taken from the input image and divides it into blocks of pixels.", "Quantization is done based on the edge information of each of these blocks.", "Bi-clustering is done for the non-edge block and tri-clustering for the edge block.", "Experimental analysis shows that ABTC-EQ outperforms BTC and its variants."], "abstract": ["In this paper a new approach of edge-based quantization for the compression of gray scale images using an Adaptive Block Truncation Coding technique (ABTC-EQ) is proposed, to improve the compression ratio (CR) with high picture quality. Quantization is done based on the edge information contained in each block of pixels of the image. Conventional BTC method retains the visual quality of the reconstructed image but it shows some artifacts near the edges. In conventional BTC and variants, same quantization is done for all pixel values with different block sizes so that CR is static for images with a fixed block size. But in the case of proposed method since the quantization is done based on the edge information, CR become dynamic and consequently achieves better visual quality with better CR. The experimental analysis based on subjective and quantitative analysis proved that the proposed method outperforms other BTC variants."]},
{"title": "The pseudo-distance technique for parallel lossless compression of color-mapped images", "highlights": ["A pseudo-distance technique (PDT2) for lossless compression of images is given.", "Parallelization of the technique is presented.", "The performance of the PDT2 in Hyper-Threading is evaluated."], "abstract": ["Data compression is a challenging process with important practical applications. Specialized techniques for lossy and lossless data compression have been the subject of numerous investigations during last several decades. Previously, we studied the use of the pseudo-distance technique (PDT) in lossless compression of color-mapped images and its parallel implementation. In this paper we present a new technique (PDT2) to improve compression gain of PDT. We also present a parallelized implementation of the new technique, which results in substantial gains in compression time while providing the desired compression efficiency. We demonstrate that on non-dithered images PDT2 outperforms PDT by 22.4% and PNG by 29.3%. On dithered images, PDT2 achieves compression gains of 7.1% over PDT and 23.8% over PNG. We also show that the parallel implementation of PDT2, while compromising compression less than 0.3%, achieves near linear speedup and utilization of Intel Hyper-Threading technology on supported systems improves speedup on average 18%."]},
{"title": "A hybrid and dynamic reliable transport protocol for wireless sensor networks", "highlights": ["A hybrid and dynamic reliable transport protocol for wireless sensor networks is proposed.", "It provides hybrid Acknowledgement/Negative Acknowledgement scheme.", "It controls the booting sensor nodes problem and the all-packets-lost problem.", "The performance of proposed protocol is tested under TinyOS Simulator."], "abstract": ["Wireless sensor networks are formed by a large number of sensor nodes which are commonly known as motes. In the past few years, several reliable, congestion controlled and energy efficient transport layer protocols in wireless sensor networks have been developed and proposed in the literature. In this paper, we have presented a hybrid and dynamic reliable transport protocol which provides the mechanism to dynamically assign the timing parameters to the nodes as well as enhance the protocol performance by using a hybrid Acknowledgement/Negative Acknowledgement scheme. The performance of proposed protocol is tested under TinyOS Simulator varying different parameters and protocol settings and found that proposed protocol is able to program all the nodes when given proper pump/fetch ratios, is able to solve the booting sensor nodes problem by being able to wait till all the nodes finished booting and solves the all-packets-lost problem by acknowledging the receipt of its first packet delivered that is the inform message."]},
{"title": "Optimal and suboptimal adaptive algorithms for rate and power transmission in OFDM-based Cognitive Radio systems", "highlights": ["Investigate adaptive rate and power allocation in OFDM-based Cognitive Radio system.", "Investigate an optimal adaptive rate and power transmission algorithms.", "Proposed novel suboptimal algorithm for power allocation.", "Compare proposed suboptimal algorithm with optimal and conventional algorithms."], "abstract": ["This paper investigates an optimal adaptive rate and power transmission algorithms for Orthogonal Frequency Division Multiplexing (OFDM) \u2013 based Cognitive Radio (CR) systems. The aim was to study the problem of maximizing the overall rate achieved by the Secondary User (SU), while keeping the interference powers introduced by the SU on the spectrum band of Primary User\u2019s (PU) below the specified thresholds and considering the total transmit power budget constraints. In addition, the novel suboptimal power allocation algorithm was proposed and consequently the maximum modulation level according to allocated power based on maximizing the overall achievable rate was obtained. The performance of the proposed suboptimal algorithm is compared with the optimal and existing algorithms including uniform loading and water filling algorithms. Numerical results revealed that the proposed suboptimal algorithm had a better performance than the uniform loading and water filling algorithms."]},
{"title": "An analysis of computational models for accelerating the subtractive pixel adjacency model computation", "highlights": ["Two computational parallel models are developed using FPGA and GPU platforms.", "To the best of our knowledge, these are the first acceleration schemas for SPAM.", "An architectural and performance analysis of both computational models is presented.", "The FPGA architecture accelerates SPAM making an optimal use of hardware resources.", "The GPU model accelerates SPAM\u2019s 1st stage facing a bottleneck on its 2nd stage."], "abstract": ["Detecting covert information in images by means of steganalysis techniques has become increasingly necessary due to the amount of data being transmitted mainly through the Internet. However, these techniques are computationally expensive and not much attention has been paid to reduce their cost by means of available parallel computational platforms. This article presents two computational models for the Subtractive Pixel Adjacency Model (SPAM) which has shown the best detection rates among several assessed steganalysis techniques. A hardware architecture tailored for reconfigurable fabrics is presented achieving high performance and fulfilling hard real-time constraints. On the other hand, a parallel computational model for the CUDA architecture is also proposed. This model presents high performance during the first stage but it faces a bottleneck during the second stage of the SPAM process. Both computational models are analyzed in detail in terms of their algorithmic structure and performance results. To the best of Authors\u2019 knowledge these are the first design proposals to accelerate the SPAM model calculation."]},
{"title": "Peak-power reduction using improved partial transmit sequence in orthogonal frequency division multiplexing systems", "highlights": ["The major drawback of OFDM system is its high PAPR.", "The drawback is overcome by use of GPW and RPW with All Pass Filtering technique.", "The GPW and RPW generate more optimum phase factors when implemented together.", "All Pass Filtering technique maintains magnitude response and optimum phase shift."], "abstract": ["Orthogonal Frequency Division Multiplexing (OFDM) system has lead to significant advancement in wireless communication systems. In OFDM system multi-carriers are present. During modulation the sub-carriers are added together with same phase which increases the value of Peak-to-Average Power Ratio (PAPR). High PAPR leads to more interference and reduced resolution of analog to digital converter (A/D) and digital to analog converter (D/A). The Partial Transmit Sequence (PTS) is a popular technique used for PAPR reduction in OFDM systems. The modified PTS technique proposed in this paper overcomes the drawbacks of Original PTS (O-PTS) by making use of Group Phase Weighting Factor (GPW) and Recursive Phase Weighting Factor (RPW) along with All Pass Filtering. Simulations show that the proposed scheme performs very well in terms of PAPR and achieves almost the same Bit Error Rate (BER) performance under Rayleigh fading channel."]},
{"title": "Energy-efficient image transmission in wireless multimedia sensor networks using block-based Compressive Sensing", "highlights": ["Energy efficient CS methodology for image transmission in WMSNs is proposed.", "Unique encoding algorithm for CS measurements with the Bernoulli matrix is formulated.", "Experimental analysis in the Atmega 128 of Mica2 to compute the execution time.", "Optimal range of communication distance for the proposed methodology is evaluated."], "abstract": ["Wireless multimedia sensor networks (WMSNs) are capable of retrieving audio, image and video data in addition to scalar sensor data. The lifetime of these networks is mainly dependent on the communication and computational energy consumption of the node. In this paper, compressed sensing (CS)-based image transmission is proposed to reduce the energy consumption considerably with acceptable image quality. A unique encoding algorithm is formulated for the CS measurements attained with the Bernoulli measurement matrix. The proposed CS method produces better results at a lower sparsity range. Experimental analysis is performed using the Atmega 128 processor of Mica2 to compute the execution time and energy consumption in the hardware platform. The proposed CS method has a considerable reduction in energy consumption and better image quality than the conventional CS method. The simulation results show the efficiency of the proposed method."]},
{"title": "Investigation of the severity level of diabetic retinopathy using supervised classifier algorithms", "highlights": ["A method for investigating the severity of diabetic retinopathy is proposed.", "Earlier diagnosis of the disease prevents vision loss in diabetic patients.", "The exudates are the major symptoms and are detected via a segmentation algorithm.", "The degree of severity is assessed using the SVM and PNN classifier algorithms.", "The SVM classifier is found to provide better results than the PNN classifier."], "abstract": ["Diabetic retinopathy is a condition that occurs in individuals with several years of diabetes mellitus and causes a characteristic group of lesions in the retina and progressively damages it. Detecting retinal fundus diseases in advance helps ophthalmologists to apply proper treatments that may cure the disease or decrease its severity and thus protect patients from vision loss. Diabetic retinopathy is usually diagnosed by ophthalmologists using dilated images that are captured by pouring a chemical solution into the patient\u2019s eye, which causes inconvenience and irritation to the patient. In this paper, we propose a method to detect lesion exudates automatically with the aid of a non-dilated retinal fundus image to help ophthalmologists diagnose the disease. The exudates from the low contrast images are detected and localised using a neighbourhood based segmentation technique. A support vector machine (SVM) and probabilistic neural network (PNN) classifiers are proposed to assess the severity of the disease, and the results are compared with the same segmentation technique. The average classification accuracy for the SVM and PNN classifiers are determined to be 97.89% and 94.76%, respectively."]},
{"title": "The hardware accelerator debate: A financial risk case study using many-core computing", "highlights": ["Parallel algorithms for real-time aggregate risk analysis is developed.", "The algorithms are evaluated on hardware accelerators such as GPUs and Phis.", "Both hardware accelerators are useful in different contexts for risk analysis.", "The Phi generates best timing results when employed independently.", "The GPU is most effective when used in a hybrid platform."], "abstract": ["The risk of reinsurance portfolios covering globally occurring natural catastrophes, such as earthquakes and hurricanes, is quantified by employing simulations. These simulations are computationally intensive and require large amounts of data to be processed. The use of many-core hardware accelerators, such as the Intel Xeon Phi and the NVIDIA Graphics Processing Unit (GPU), are desirable for achieving high-performance risk analytics. In this paper, we set out to investigate how accelerators can be employed in risk analytics, focusing on developing parallel algorithms for Aggregate Risk Analysis, a simulation which computes the Probable Maximum Loss of a portfolio taking both primary and secondary uncertainties into account. The key result is that both hardware accelerators are useful in different contexts; without taking data transfer times into account the Phi had lowest execution times when used independently and the GPU along with a host in a hybrid platform yielded best performance."]},
{"title": "Automatic face recognition system based on the SIFT features", "highlights": ["We proposed and implemented a new face corpus creation algorithm.", "We created a new facial corpus from the data of the Czech News Agency.", "We evaluated a novel face recognition method, the SIFT based Kepenekci approach.", "We proposed and evaluated two novel confidence measure techniques.", "We proposed, implemented and evaluated the fully automatic face recognition system."], "abstract": ["The main goal of this paper is to propose and implement an experimental fully automatic face recognition system which will be used to annotate photographs during insertion into a database. Its main strength is to successfully process photos of a great number of different individuals taken in a totally uncontrolled environment. The system is available for research purposes for free. It uses our previously proposed SIFT based Kepenekci approach for the face recognition, because it outperforms a number of efficient face recognition approaches on three large standard corpora (namely FERET, AR and LFW). The next goal is proposing a new corpus creation algorithm that extracts the faces from the database and creates a facial corpus. We show that this algorithm is beneficial in a preprocessing step of our system in order to create good quality face models. We further compare the performance of our SIFT based Kepenekci approach with the original Kepenekci method on the created corpus. This comparison proves that our approach significantly outperforms the original one. The last goal is to propose two novel supervised confidence measure methods based on ", " class probability and a multi-layer perceptron to identify incorrectly recognized faces. These faces are then removed from the recognition results. We experimentally validated that the proposed confidence measures are very efficient and thus suitable for our task."]},
{"title": "Design, implementation and evaluation of a smartphone position discovery service for accurate context sensing", "highlights": ["We analyze the impact of smartphone carry positions on readings provided by sensors.", "We use accelerometer, gyroscope and light sensors to detect position information.", "Two-stage method based on position is shown to enhance fall classification accuracy."], "abstract": ["Detecting user context with high accuracy using smartphone sensors is a difficult task. A key challenge is dealing with the impact of different smartphone positions on sensor values. Users carry their smartphones in different positions such as holding in their hand or keeping inside their pants or jacket pocket, and each of these smartphone positions affects various sensor values in different ways. This paper addresses the issue of poor accuracy in detecting user context due to varying smartphone positions. It describes the design and prototype development of a smartphone position discovery service that accurately detects a smartphone position, and then demonstrates that the accuracy of an existing context aware application is significantly enhanced when run in conjunction with this proposed smartphone position discovery service."]},
{"title": "Edge detection with fuzzy cellular automata transition function optimized by PSO", "highlights": ["This study is the application of 2D linear cellular automata (CA) rules with the help of fuzzy membership function to the problems of edge detection.", "An efficient and simple thresholding technique of edge detection based on CA transition rules optimized by Particle Swarm Optimization method (PSO) is proposed.", "Results of the proposed to the selected 22 images from the Berkeley Segmentation Dataset (BSDS) are presented.", "Comparison with some classical Sobel and Canny results is included.", "Baddeley Delta Metric (BDM) is used for the performance index to compare the results."], "abstract": ["In this paper we discuss the application of two-dimensional linear cellular automata (CA) rules with the help of fuzzy heuristic membership function to the problems of edge detection in image processing applications. We proposed an efficient and simple thresholding technique of edge detection based on fuzzy cellular automata transition rules optimized by Particle Swarm Optimization method (PSO). Finally, we present some results of the proposed linear rules for edge detection to the selected 22 images from the Berkeley Segmentation Dataset (BSDS) and compare with some classical Sobel and Canny results. Also, Baddeley Delta Metric (BDM) is used for the performance index to compare the results."]},
{"title": "Raspberry Pi as a Sensor Web node for home automation", "highlights": ["Implementation of Sensor Web node as a part of Internet of Things using a Raspberry Pi.", "Ubiquitous solution of Home automation system: appears everywhere and anywhere.", "Do-It-Yourself approach: user build own solution that meets specific needs.", "Interdisciplinary approach: electronic, programming, services and soft computing.", "System for monitoring confidence of fire created and developed starting from a scratch."], "abstract": ["The world of home automation is an exciting field that has exploded with new technologies and today is known as an area where \u201cThe internet of things\u201d (IoT) vision becomes reality. The primary advantages that stem from this concept include how each device forms a small part of the Internet, by which the advanced system is able to interact and communicate, maximizes safety, security, comfort, convenience and energy-savings. This paper proposes an implementation of Sensor Web node as a part of IoT using a Raspberry Pi \u2013 inexpensive, fully customizable and programmable small computer with support for a large number of peripherals and network communication. Using this technology, in an example of monitoring and determining the confidence of fire in building, a full system, based on Sensor Web elements, is created and developed starting from a scratch. The given example confirms the advantage of Raspberry Pi \u2013 flexibility and extensive possibility of its usage."]},
{"title": "Performance analysis of turbo-coded MIMO\u2013OFDM system for underwater communication", "highlights": ["We investigate the performance of VBLAST assisted MIMO\u2013OFDM system in underwater communications.", "Performance is analyzed with non-linear MUD when communicating over multi-path environment.", "BER performance is compared for coded and uncoded ZF/OSIC and MMSE/OSIC.", "Coded multi-carrier MIMO system out performs uncoded system in terms of BER performance.", "Coded multi-carrier MIMO system with MMSE/OSIC provides better BER performance than system with coded ZF/OSIC."], "abstract": ["In this article, we investigate the performance of a coded multiple-input multiple-output (MIMO) multi-carrier (MC) system in underwater communication, where acoustic interference and ambient noise are the two major channel impairments. The channel model considered in this work is based on the shallow water model that has eleven paths. In our work, the acoustic signal is spread with the aid of a high rate spreading sequence to alleviate the effects of acoustic interference. At the receiver, non-linear detectors based on Zero-Forcing (ZF) and Minimum-Mean-Square-Error (MMSE) are employed for signal detection. The simulation results reveal that the system under consideration with the MMSE detector provides better performance in terms of the achievable bit error rate than the system with the ZF detector while achieving higher data rates."]},
{"title": "An approach for prevention of privacy breach and information leakage in sensitive data mining", "highlights": ["It prevents homogeneity, skewness, similarity and background knowledge attacks.", "The privacy is ensured while publishing sensitive data.", "Only fewer partitioning need to be done for a stronger privacy requirement.", "It gives better efficiency over the previous approaches."], "abstract": ["Government agencies and many non-governmental organizations often need to publish sensitive data that contain information about individuals. The sensitive data or private data is an important source of information for the agencies like government and non-governmental organization for research and allocation of public funds, medical research and trend analysis. The important problem here is publishing data without revealing the sensitive information of individuals. This sensitive or private information of any individual is essential to several data repositories like medical data, census data, voter registration data, social network data and customer data. In this paper a personalized anonymization approach is proposed which preserves the privacy while the sensitive data is published. The main contributions of this paper are three folds: (i) the definition of the data collection and publication process, (ii) the privacy framework model and (iii) personalized anonymization approach. The experimental analysis is presented at the end; it shows this approach performs better over the distinct ", "-diversity measure, probabilistic ", "-diversity measure and ", "-anonymity with ", "-closeness measure."]},
{"title": "Motion analysis of a mobile manipulator executing pick-up tasks", "highlights": ["Reaching Target-point by the end-effector of a mobile manipulator.", "The optimized trajectory, minimizing the execution time of the task is generated.", "Introduction of manipulability index for configuring the manipulator far from singularities.", "The feasibility of the computed trajectories is validated against experimentation on a real robot."], "abstract": ["This paper presents an analysis of a mobile manipulator movement executing a pick-up task. The robot has to reach a target point with its end-effector. The configuration of the manipulator and the pose of the mobile robot define the inputs of the problem.", "The random profile approach is applied to deal with the aforementioned issue. The trajectory which minimizes the execution time of the task is generated. Furthermore, the manipulability index is introduced in the optimization process in order to allow a comfortable configuration of the manipulator to reach the target-point. The obtained results in free and cluttered environments are presented and discussed. The feasibility of the computed trajectories is also validated against experimentation, in the case of the RobuTER mobile robot carrying the Ultra-Light Manipulator."]},
{"title": "Q-aware: Quality of service based cloud resource provisioning", "highlights": ["Cloud workloads have been analyzed and clustered through workload patterns.", "QoS metrics of each workload have been identified.", "We have analyzed the effect of number of workloads and resources on execution time and cost.", "Proposed technique demonstrates the minimization of cost and time simultaneously while adhering to workload deadline."], "abstract": ["Provisioning of appropriate resources to cloud workloads depends on the Quality of Service (QoS) requirements of cloud workloads. Based on application requirements of cloud users, discovery and allocation of best workload \u2013 resource pair is an optimization problem. Acceptable QoS cannot be provided to the cloud users until provisioning of resources is offered as a crucial ability. QoS parameters based resource provisioning technique is therefore required for efficient provisioning of resources. In this paper, QoS metric based resource provisioning technique has been proposed. The proposed technique caters to provisioned resource distribution and scheduling of resources. The main aim of this research work is to analyze the workloads, categorize them on the basis of common patterns and then provision the cloud workloads before actual scheduling. The experimental results demonstrate that QoS metric based resource provisioning technique is efficient in reducing execution time and execution cost of cloud workloads along with other QoS parameters."]}
]